%!TEX root = ./main.tex
\section{Identification-Robust Inference}
We now turn our attention to inference based on the identification results from above.
As we explain below, inference under binary mis-classification is complicated by problems of weak identification and parameters on the boundary.
For simplicity we fix the exogenous covariates at some specified level and suppress dependence on $\mathbf{x}$ in the notation.
This is appropriate if the covariates have a discrete support.
We discuss how to incorporate covariates more generally in Section \ref{sec:covariates}.

\subsection{The Non-standard Inference Problem}
\label{sec:problem}
Lemmas \ref{lem:wald}--\ref{lem:eta3} yield the following system of linear moment equalities in the reduced form parameters $\boldsymbol{\theta} = (\theta_1, \theta_2, \theta_3)$ from Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}:
\begin{align*}
  \mbox{Cov}(y,z) - \mbox{Cov}(T,z) \theta_1 &= 0\\
  \mbox{Cov}(y^2,z) - 2\mbox{Cov}(yT,z) \theta_1 + \mbox{Cov}(T,z)\theta_2 &= 0\\
  \mbox{Cov}(y^3,z) - 3 \mbox{Cov}(y^2T,z) \theta_1 + 3\mbox{Cov}(yT,z) \theta_2 - \mbox{Cov}(T,z) \theta_3 &= 0
\end{align*}
Non-linearity arises solely through the relationship between the reduced from parameters $\boldsymbol{\theta}$ and the structural parameters $(\alpha_0, \alpha_1, \beta)$.
To convert the preceding moment equations into unconditional moment equalities, we define the additional reduced form parameters $\boldsymbol{\kappa} = (\kappa_1, \kappa_2, \kappa_3)$ as follows:
\begin{align*}
\kappa_1 &= c - \alpha_0 \theta_1\\
  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]
\end{align*}
Building on this notation, let
\begin{equation}
  \boldsymbol{\psi}_1' = (-\theta_1, 1, 0, 0, 0, 0), \quad
  \boldsymbol{\psi}_2' = (\theta_2, 0, -2\theta_1, 1, 0, 0), \quad
  \boldsymbol{\psi}_3' = (-\theta_3, 0, 3\theta_2, 0, -3\theta_1, 1)
  \label{eq:psi_def}
\end{equation}
and collect these in the matrix
$\boldsymbol{\Psi} = \left[
  \begin{array}{ccc}
    \boldsymbol{\psi}_1 & \boldsymbol{\psi_2} & \boldsymbol{\psi_3}
\end{array}\right]$.
Defining the observed data vector $\mathbf{w}_i' = (T_i, y_i, y_iT_i, y_i^2, y_i^2 T_i, y_i^3)$ for observation $i$, can re-write the moment equations as:
\begin{equation}
\mathbb{E}\left[
  \big(\boldsymbol{\Psi}'(\boldsymbol{\theta})\mathbf{w}_i - \boldsymbol{\kappa}\big) \otimes 
\left(
\begin{array}{c}
  1 \\ z_i
\end{array}\right)
\right] = \mathbf{0}
\label{eq:MCs_endog}
\end{equation}

Equation \ref{eq:MCs_endog} is a just-identified, linear system of moment equalities in the reduced form parameters $(\boldsymbol{\theta},\boldsymbol{\kappa})$ and yields explicit GMM estimators $(\widehat{\boldsymbol{\kappa}},\widehat{\boldsymbol{\theta}})$.
From Theorem \ref{thm:main_ident}, knowledge of $\boldsymbol{\theta}$ suffices to identify $\beta$.
From the definitions of $\boldsymbol{\kappa}$ above and $\boldsymbol{\theta}$  in Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}, however, the moment equalities from Equation \ref{eq:MCs_endog} do not depend on $(\alpha_0, \alpha_1)$ if $\beta$ equals zero.
By continuity, they are \emph{nearly} uninformative about the mis-classification probabilities if $\beta$ is small.
But unless $\beta = 0$, knowledge of $(\alpha_0, \alpha_1)$ is necessary to recover $\beta$, via Lemma \ref{lem:wald}. 
Thus, we face a weak identification problem.\footnote{This is essentially equivalent to the problem of estimating mixture probabilities when the means of the component distributions are very similar to each other.}
Indeed, the GMM estimator of $\widehat{\beta}$ based on Equation \ref{eq:MCs_endog} may even fail to exist.
Using arguments from the proof of Theorem \ref{thm:main_ident}, this estimator is given by is
\[
  \widehat{\beta} = \mbox{sign}\big(\widehat{\theta}_1\big) \sqrt{3\left( \widehat{\theta}_2/\widehat{\theta}_1 \right)^2 - 2 \left(\widehat{\theta}_3/\widehat{\theta}_1 \right)}
\]
Under our assumptions, $3(\theta_2/\theta_1)^2 > 2 (\theta_3/\theta_1)$ provided that $\beta \neq 0$, but this may not be true of the sample analogue.
Indeed, because $\widehat{\theta}_1$ appears in the denominator, the terms within the square root will be highly variable if $\beta$ is small.
Even if the GMM estimator exists, it may violate the partial identification bounds for $(\alpha_0, \alpha_1)$ from Theorem \ref{thm:sharpII}, or imply that $(\alpha_0,\alpha_1)$ are not valid probabilities. 
Importantly, the partial identification bounds remain informative even if $\beta$ is small or zero: so long as Assumption \ref{assump:model} (ii) holds, the first-stage probabilities bound $\alpha_0$ and $\alpha_1$ from above.


Exactly the same inferential difficulties arise in the case where $T^*$ and $z$ are assumed to be jointly exogenous, as in \cite{KRS,BBS,FL,Mahajan,Lewbel}.\footnote{We provide details for \cite{FL} and \cite{Mahajan} in Appendix \ref{sec:FL_mahajan}.}
This issue, however, has received little attention in the literature. 
\cite{KRS} ensure that $(\alpha_0, \alpha_1)$ are valid probabilities by employing a logit specification.
Frazis and Loewenstein employ a pseudo-Bayesian approach to ensure that $\alpha_0$ and $\alpha_1$ are valid probabilities, and to impose partial identification bounds related to those from our Theorem \ref{thm:sharpI}, i.e.\ without using the non-differential measurement error restrictions.
Because they provide neither simulation evidence nor a theoretical justification for their procedure, however, it is unclear whether this method will yield valid Frequentist coverage.
We are unaware of any papers in the related literature that discuss the weak identification problem arising when $\beta$ is small.


\subsection{Overview of the Inference Procedure}
\label{sec:overview}
In the following sections we develop a procedure for uniformly valid inference in models with a mis-classified binary regressor.
%This procedure is computationally attractive, and can be applied to both the endogenous and exogenous regressor cases, under either point or partial identification.
Our purpose is to construct a confidence interval for $\beta$ that is robust to possible weak identification, respects the restricted parameter space for $(\alpha_0, \alpha_1)$, and incorporates both the information in the equality moment conditions from Equation \ref{eq:MCs_endog} along with the partial identification bounds from Theorem \ref{thm:sharpII}.\footnote{Note that $\beta=0$ if and only if $\theta_1 = 0$. Thus, if one is merely interested in testing $H_0\colon \beta=0$, one can ignore the mis-classification error problem and test $H_0\colon \theta_1 = 0$ using the standard IV estimator and standard error, provided that $z$ is a strong instrument.}
As argued in the preceding section, our partial identification bounds remain informative even when the equality moment conditions contain essentially no information about $\beta$.
Before proceeding, we introduce some notation.
Let
\begin{equation}
  \mathbb{E} \left[ m^I_j(\mathbf{w}_i,\boldsymbol{\vartheta}_0) \right]
  \geq \mathbf{0} \quad j = 1, \cdots, J_I; \quad \quad
 \mathbb{E} \left[ m^E_j(\mathbf{w}_i,\boldsymbol{\vartheta}_0) \right]
 = \mathbf{0} \quad  j = J_I + 1, \cdots,J  
   \label{eq:mdefs}
 \end{equation}
 where $\boldsymbol{\vartheta}_0$ is the true parameter vector, $m_j^I$ corresponds to the $j$th inequality moment condition, $m_j^E$ corresponds to the $j$th equality moment condition, $J$ denotes the total number of moment conditions, $J_I$ denotes the number of moment equalities, and $J_E = J - J_I$ denotes the number of equality moment conditions.
Further define 
\begin{equation}
  \bar{m}_n(\boldsymbol{\vartheta}) = \left[
  \begin{array}{c}
   \bar{m}_n^I(\boldsymbol{\vartheta}) \\
   \bar{m}_n^E(\boldsymbol{\vartheta}) \\
  \end{array}
\right], \quad
  \bar{m}_n^I(\boldsymbol{\vartheta}) = \left[
  \begin{array}{c}
    \bar{m}_{n,1}^I(\boldsymbol{\vartheta})\\
    \vdots \\
    \bar{m}_{n,J_I}^I(\boldsymbol{\vartheta})\\
  \end{array}
\right], \quad
  \bar{m}_n^E(\boldsymbol{\vartheta}) = \left[
  \begin{array}{c}
    \bar{m}_{n,J_I+1}^E(\boldsymbol{\vartheta})\\
    \vdots \\
    \bar{m}_{n,J}^E(\boldsymbol{\vartheta})\\
  \end{array}
\right]
\end{equation}
where 
$\bar{m}^I_{n,j}(\boldsymbol{\vartheta}) = n^{-1} \sum_{i=1}^{n} m_j^I(\mathbf{w}_i, \boldsymbol{\vartheta})$
and $\bar{m}^E_{n,j}$ is defined analogously.
Finally, let $\Sigma(\boldsymbol{\vartheta})$ denote the asymptotic variance matrix of $\sqrt{n}\; \bar{m}_n(\boldsymbol{\vartheta})$, with $j$th diagonal element $\sigma^2_j(\boldsymbol{\vartheta})$ along with the corresponding sample analogues $\widehat{\Sigma}_n(\boldsymbol{\vartheta})$ and $\widehat{\sigma}^2_j(\boldsymbol{\vartheta})$.

We proceed by inverting an Anderson-Rubin type test statistic, namely the modified method of moments (MMM) statistic 
\begin{equation}
  T_n(\boldsymbol{\vartheta}) = \sum_{j=1}^{J_1}\min\left\{ 0, \left( \frac{\sqrt{n}\; \bar{m}_n^I(\boldsymbol{\vartheta})}{\widehat{\sigma}_n(\boldsymbol{\vartheta})} \right)^2 \right\} + \sum_{j=J_1 + 1}^{J} \left( \frac{\sqrt{n}\; \bar{m}_n^E(\boldsymbol{\vartheta})}{\widehat{\sigma}_n(\boldsymbol{\vartheta})} \right)^2.
  \label{eq:MMM}
\end{equation}
To improve the power of the test while maintaining valid size, we employ the generalized moment selection (GMS) approach of \cite{AndrewsSoares}.
In particular, when approximating the asymptotic distribution of $T_n$ under the null $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$, we drop any inequality moment condition $m_j^I$ for which $\sqrt{n}\big[\bar{m}_n(\boldsymbol{\vartheta}_0)/ \widehat{\sigma}_n(\boldsymbol{\vartheta}_0)\big] > \sqrt{\log(n)}$.\footnote{Full details appear in Section \ref{sec:details} below.}
The GMS procedure yields a uniformly valid test of the \emph{joint} null hypothesis for the full parameter vector $\boldsymbol{\vartheta}$. 
In our model, this includes the parameter of interest $\beta$ along various nuisance parameters: the mis-classification probabilities $\alpha_0$ and $\alpha_1$, the reduced form parameters $\boldsymbol{\kappa}$, defined in Section \ref{sec:problem}, and a vector $\mathbf{q}$ of parameters that enter the moment inequalities.\footnote{These are defined below in Section \ref{sec:inequalities}.}
Under a given joint null hypothesis for $(\beta, \alpha_0, \alpha_1)$, however, $\boldsymbol{\kappa}$ and $\mathbf{q}$ are strongly identified and lie on the interior their respective parameter spaces. 
Accordingly, in Section \ref{sec:prelim} we explain how to concentrate these parameters out of the GMS procedure, by deriving an appropriate correction to the asymptotic variance matrix of the moment conditions.
Note that we cannot concentrate out $\alpha_0$ and $\alpha_1$ as we did with $\boldsymbol{\kappa}$ and $\mathbf{q}$, because the mis-classification probabilities may be weakly identified or lie on the boundary of their parameter space.

This leaves us with a uniformly valid test of any joint null hypothesis for $(\beta, \alpha_0, \alpha_1)$.
To construct a marginal confidence interval for $\beta$ we proceed as follows.
Suppose that $z$ is a strong instrument.
Then the usual IV estimator provides a valid confidence interval for the reduced from parameter $\theta_1$.
By Lemma \ref{lem:wald}, knowledge of $(1 - \alpha_0 - \alpha_1)$ suffices to determine $\beta$ from $\theta_1$.
Thus, a valid confidence interval for $(1 - \alpha_0 - \alpha_1)$ can be combined with the IV interval for $\theta_1$ to yield a corresponding interval for $\beta$, via a Bonferroni-type correction.
To construct the required interval for $(1 - \alpha_0 - \alpha_1)$, notice from Equations \ref{eq:theta1_def}--\ref{eq:theta3_def} that $\beta$ only enters $\bar{m}_n$ through $\theta_1$.
But, again, provided that $z$ is a strong instrument, inference for $\theta_1$ is standard.
We can thus pre-estimate it along with $\boldsymbol{\kappa}$ and $\mathbf{q}$, yielding a uniformly valid GMS test of any joint null hypothesis for $(\alpha_0, \alpha_1)$.
A valid confidence interval for $(1 - \alpha_0 - \alpha_1)$ is easily obtained by projecting the joint confidence set.
This is computationally trivial because the parameter space for $(\alpha_0, \alpha_1)$ is bounded and two-dimensional.\footnote{We considered two alternatives to the Bonferroni-based inference procedure described here.
  The first constructs a marginal confidence interval for $\beta$ by projecting a joint confidence set for $(\beta, \alpha_1, \alpha_0)$, i.e.\ \emph{without} preliminary estimation of $\theta_1$.
This method is more computationally demanding than our two-dimensional projection and involves a parameter space that is unbounded along the $\beta$-dimension.
From a practical perspective, the relevant question is whether the reduction in conservatism from projecting a lower dimensional set is outweighed by the additional conservatism induced by the Bonferroni correction. 
In our experiments, the full three-dimensional projection and Bonferroni procedure produced broadly similar results: neither reliably dominated in terms of confidence interval width.
Given its substantially lower computational burden, we prefer the Bonferroni procedure.
We also experimented with two recently proposed methods for sub-vector inference: \cite{kaido2016confidence} and \cite{BugniCanayShi}. In both cases we obtained significant size distortions, suggesting that our model may not satisfy the high-level regularity conditions required by these papers.}
If desired, one could also carry out a valid test of the null hypothesis that there is no mis-classification, $\alpha_0 = \alpha_1 = 0$, using the joint test for $(\alpha_0, \alpha_1)$. 
In the following sections we provide full details of our Bonferroni-based confidence interval procedure for $\beta$.
We begin by defining the unconditional moment inequalities $m^I$ in Section \ref{sec:inequalities}.



\subsection{Moment Inequalities from Partial Identification Results}
\label{sec:inequalities}
As noted above, the partial identification bounds from Theorems \ref{thm:sharpI} and \ref{thm:sharpII} remain informative about $(\alpha_0, \alpha_1)$ even when $\beta$ is small.
To incorporate them in our inference procedure, we first express them as unconditional moment inequalities.
The bounds from Theorem \ref{thm:sharpI} are given by
\begin{equation*}
  p_k - \alpha_0 \geq 0, \quad \quad 1 - p_k - \alpha_1 \geq 0, \quad \mbox{for all } k 
\end{equation*}
where the first-stage probabilities $p_k$ are defined in Equation \ref{eq:pk_def}.
Written as unconditional moment inequalities, these become
\begin{equation}
  \mathbb{E}\left[ m_1^I(\mathbf{w}_i,\boldsymbol{\vartheta} ) \right] \geq \mathbf{0},  \quad
m_1^I(\mathbf{w}_i, \boldsymbol{\vartheta}) \equiv \left[
  \begin{array}{l}
    (1 - z_i)(T_i - \alpha_0) \\
    (1 - z_i)(1 - T_i - \alpha_1) \\
    z_i(T - \alpha_0) \\
    z_i (1 - T_i - \alpha_1) 
  \end{array}
\right]
\end{equation}
The bounds derived in Theorem \ref{thm:sharpII} by imposing assumption \ref{assump:misclassification} (iii) are
\begin{equation*}
  \mu_k(\alpha_0) - \underline{\mu}_{tk}\big( \underline{q}_{tk}(\alpha_0, \alpha_1) \big) \geq 0, \quad \quad
  \overline{\mu}_{tk}\big( \overline{q}_{tk}(\alpha_0, \alpha_1) \big) - \mu_k(\alpha_0) \geq 0, \quad \mbox{for all } t,k
\end{equation*}
where $\mu_k, \underline{\mu}_{tk}, \overline{\mu}_{tk}, \underline{q}_{tk}$ and $\overline{q}_{tk}$ are defined in the statement of the Theorem.
Expressing these as unconditional moment inequalities, we have
\begin{equation}
  \mathbb{E}[m_2^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})] \geq \mathbf{0}, \quad 
  m_2^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) \equiv \left[
  \begin{array}{c}
    m_{2,00}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})  \\ 
    m_{2,10}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) \\
    m_{2,01}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})  \\ 
    m_{2,11}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) 
  \end{array}
\right] 
\end{equation}
where $\mathbf{q} \equiv ( \underline{q}_{00},\, \overline{q}_{00},\, \underline{q}_{10}, \,\overline{q}_{10},\, \underline{q}_{01}, \,\overline{q}_{01},\, \underline{q}_{11},\, \overline{q}_{11})$ and we define
\begin{align}
  m_{2,0k}^I\big(\mathbf{w}_i, \boldsymbol{\vartheta}, \mathbf{q}) \equiv \left[
  \begin{array}{r}
    y_i \mathbf{1}\left( z_i=k \right)\left\{(T_i - \alpha_0) - \mathbf{1}(y_i \leq \underline{q}_{0k})  (1 - T_i)\left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right)\right\} \\
    - y_i \mathbf{1}(z_i=k) \left\{ (T_i - \alpha_0) -  \mathbf{1}(y_i > \overline{q}_{0k}) (1 - T_i) \left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right) \right\} 
\end{array}
\right] \\\nonumber \\
  m_{2,1k}^I(\mathbf{w}_i, \boldsymbol{\vartheta}, \mathbf{q}) \equiv \left[
  \begin{array}{r}
    y_i \mathbf{1}\left( z_i=k \right)\left\{(T_i - \alpha_0) - \mathbf{1}(y_i \leq \underline{q}_{1k})  T_i\left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right)\right\} \\
    - y_i \mathbf{1}(z_i=k) \left\{ (T_i - \alpha_0) -  \mathbf{1}(y_i > \overline{q}_{1k}) T_i \left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right) \right\} 
\end{array}
\right].
\end{align}
Finally we define $m^I = (m_1^{I'}, m_2^{I'})'$.
Notice that the second set of inequalities, $m_2^I$, depends on unknown parameter $\mathbf{q}$ which is in turn a function of $(\alpha_0, \alpha_1)$.
In the next section we discuss how $\mathbf{q}$ can be estimated under a given null hypothesis for $(\alpha_0, \alpha_1)$. 


\subsection{Accounting for Preliminary Estimation}
\label{sec:prelim}


Let $\boldsymbol{\vartheta} = (\alpha_0, \alpha_1)$, $\boldsymbol{\gamma} = (\boldsymbol{\kappa}, \theta_1)$ and $\boldsymbol{\tau} =  (\boldsymbol{\gamma}, \mathbf{q})$, where $\theta_1$ is defined in Equation \ref{eq:theta1_def}, $\boldsymbol{\kappa}$ in Section \ref{sec:problem}, and $\mathbf{q}$ in Section \ref{sec:inequalities}.
Our moment conditions take the form 
\begin{equation}
  \mathbb{E}[m^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)] \geq \mathbf{0}, \quad 
  \mathbb{E}[m^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)] = \mathbf{0}
  \label{eq:MCs_ineq_eq}
\end{equation}
where $m^I = (m_1^{I'}, m_2^{I'})'$, defined in Section \ref{sec:inequalities}, and 
\begin{equation}
  m^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)= \left[
  \begin{array}{c}
 \left\{ \boldsymbol{\psi}_2'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \kappa_2 \right\}z_i \\
 \left\{ \boldsymbol{\psi}_3'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \kappa_3 \right\}z_i 
  \end{array}
\right].
\end{equation}
Notice that we now write $\boldsymbol{\psi}_2$ and $\boldsymbol{\psi}_3$, defined in Equation \ref{eq:psi_def}, as explicit functions of $\theta_1, \alpha_0, \alpha_1$, using the definitions of $(\theta_2, \theta_3)$ from Equations \ref{eq:theta2_def}--\ref{eq:theta3_def}.
To construct a GMS test of the null hypothesis $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta_0}$ based on Equation \ref{eq:MCs_ineq_eq}, we require a preliminary estimator $\widehat{\boldsymbol{\tau}}(\boldsymbol{\vartheta}_0)$ that is consistent and asymptotically normal \emph{under the null}.
We now provide full details of the construction and derive the associated adjustment to the asymptotic variance matrix.

Consider first the equality moment conditions $m^E$.
For these we require preliminary estimators of $\theta_1$, $\kappa_2$, and $\kappa_3$.
Recall that $\theta_1$ is simply the IV estimand: it can be consistently estimated directly from observations of $(y,T,z)$ without knowledge of $\alpha_0$ or $\alpha_1$.
Note, moreover, from Equation \ref{eq:MCs_endog} that $\boldsymbol{\kappa}$ is simply a vector of \emph{intercepts}.
These can be directly estimated from observations of $\mathbf{w}$ because $\boldsymbol{\Psi}(\theta_1, \alpha_0, \alpha_1)$ is consistently estimable under the $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$: the hypothesis specifies $\alpha_0$ and $\alpha_1$ and IV provides a consistent estimator of $\theta_1$.
Accordingly, define 
\begin{equation}
  h^E(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\gamma})  =  \left[
 \begin{array}{l}
   \boldsymbol{\Psi}'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \boldsymbol{\kappa}\\
   \left\{\boldsymbol{\psi}_1'(\theta_1)\mathbf{w}_i - \kappa_1\right\}z_i
 \end{array}
 \right].
\end{equation}
Under $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$, the  just-identified GMM-estimator based on $\mathbb{E}[h^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)] = \mathbf{0}$ yields a consistent and asymptotically normal estimator of $\boldsymbol{\gamma}_0$.

Now consider the inequality moment conditions $m^I$. 
From Section \ref{sec:inequalities} we see that $\boldsymbol{\tau}$ only enters $m^I$ through the second block of moment inequalities, $m_2^I$, which depend on $\mathbf{q}$.
The elements of $\mathbf{q}$ are the conditional quantiles $\overline{q}_{tk}$ and $\underline{q}_{tk}$ defined in Theorem \ref{thm:sharpII}.
Under the assumption that $y$ is continuous, these can be expressed as conditional moment equalities as follows:
\begin{align*}
  &\mathbb{E}\left[ \mathbf{1}(y \leq \underline{q}_{tk}) |T=t,z=k \right] - r_{tk}(\alpha_0, \alpha_1) = 0 \\
  &\mathbb{E}\left[ \mathbf{1}(y \leq \overline{q}_{tk}) |T=t,z=k \right] - \big(1 - r_{tk}(\alpha_0, \alpha_1)\big) = 0
\end{align*}
where $r_{tk}$ is defined in Theorem \ref{thm:sharpII} and $t,k = 0,1$.


\todo[inline]{Sample mean to estimate $p_k$ and under null this gives consistent estimator of $r_{tk}$. Plug-in estimator based on this an sample quantiles is consistent by slight modification of usual quantile argument. See e.g.\ Serfling. Asymptotic normality is slightly more involved since $h^I$ depends non-smoothly on $\mathbf{q}$. See Andrews and Newey \& McFaddden section 7.}

We can write these as 
\[
  \mathbb{E}[h^I(W,\boldsymbol{\alpha}, \mathbf{q})] = \mathbb{E}
  \left[
  \begin{array}{c}
    h^I_{0}(W,\boldsymbol{\alpha}, \mathbf{q})\\
    h^I_{1}(W,\boldsymbol{\alpha}, \mathbf{q})
  \end{array}
\right] = \mathbf{0}
\]
where
\[
  h_k^I(W,\boldsymbol{\alpha},\mathbf{q}) = \left[
  \begin{array}{c}
    \mathbf{1}(y \leq \underline{q}_{0k}) \mathbf{1}(z=k)(1 - T) 
    - \displaystyle \left( \frac{\alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z=k)(T-\alpha_0)\\ \\
    \mathbf{1}(y \leq \overline{q}_{0k}) \mathbf{1}(z=k)(1 - T)
    - \displaystyle \left( \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z=k)(1 - T-\alpha_1)\\ \\
    \mathbf{1}(y \leq \underline{q}_{1k}) \mathbf{1}(z=k)T
    - \displaystyle \left( \frac{1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z=k)(T-\alpha_0)\\ \\
    \mathbf{1}(y \leq \overline{q}_{1k}) \mathbf{1}(z=k)T 
    - \displaystyle \left( \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z=k)(1 - T-\alpha_1)\\ \\
  \end{array}
\right]
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Generic Version of the Expansion}

Let $g_1(\mathbf{w}_i,\boldsymbol{\vartheta}, \boldsymbol{\tau}_1)$ be a vector of functions that are continuously differentiable with respect to $\boldsymbol{\tau_1}$ and $g_2(\mathbf{w}_i,\boldsymbol{\vartheta}, \boldsymbol{\tau}_2)$ be a vector of functions that may not be continuous or differentiable in $\boldsymbol{\tau}_2$.


\[
  h  = \left[
  \begin{array}{c}
    h^I_0 \\ h^I_1 \\ h^E
  \end{array}
\right], \quad
  h^E  =  \left[
 \begin{array}{l}
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i - \kappa_1\\
   \boldsymbol{\psi}'_2(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_2\\
   \boldsymbol{\psi}'_3(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_3\\ 
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i z_i - \kappa_1 z_i
 \end{array}
 \right] = \left[
 \begin{array}{l}
   \boldsymbol{\Psi}'(\theta_1, \boldsymbol{\alpha}^0) \mathbf{w}_i - \boldsymbol{\kappa}\\
   \left\{\boldsymbol{\psi}_1'(\theta_1)\mathbf{w}_i - \kappa_1\right\}z_i
 \end{array}
 \right]
\]


\paragraph{Accounting for Preliminary Estimation}
\[
m = \left[
\begin{array}{c}
  m^I \\ m^E
\end{array}
\right], \quad
m^I = \left[
\begin{array}{c}
  m^I_1 \\ m^I_2
\end{array}
\right], \quad
m^E = \left[
  \begin{array}{cc}
    m_2^E \\ m_3^E 
  \end{array}
\right], \quad m_j^E(\mathbf{w}_i, \cdots) = \left[ \boldsymbol{\psi}_j'(\theta_1, \boldsymbol{\alpha}^0) \mathbf{w}_i - \kappa_j \right]z_i 
\]

\[
  m_1^I(\mathbf{w}_i, \alpha_0, \alpha_1) = \left[
  \begin{array}{l}
    (1 - z_i)(T_i - \alpha_0) \\
    (1 - z_i)(1 - T_i - \alpha_1) \\
    z_i(T - \alpha_0) \\
    z_i (1 - T_i - \alpha_1) \\
  \end{array}
\right], \quad
m^I_2(W, \boldsymbol{\alpha}, \mathbf{q}) = \left[
\begin{array}{c}
  m^I_{2,00}(W,\boldsymbol{\alpha},\mathbf{q})\\
  m^I_{2,10}(W,\boldsymbol{\alpha},\mathbf{q})\\
  m^I_{2,01}(W,\boldsymbol{\alpha},\mathbf{q})\\
  m^I_{2,11}(W,\boldsymbol{\alpha},\mathbf{q}) \\
\end{array}
\right]
\]
\[
  m_{2,tk}^I(W, \boldsymbol{\alpha}, \mathbf{q}) = \left[
  \begin{array}{c}
  \mu_k(\alpha_0) - \underline{\mu}_{tk}(\alpha_0, \alpha_1) \\
  \overline{\mu}_{tk}(\alpha_0, \alpha_1) - \mu_k(\alpha_0)  
  \end{array}
\right] 
\]



\[
  h  = \left[
  \begin{array}{c}
    h^I_0 \\ h^I_1 \\ h^E
  \end{array}
\right], \quad
  h^E  =  \left[
 \begin{array}{l}
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i - \kappa_1\\
   \boldsymbol{\psi}'_2(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_2\\
   \boldsymbol{\psi}'_3(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_3\\ 
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i z_i - \kappa_1 z_i
 \end{array}
 \right] = \left[
 \begin{array}{l}
   \boldsymbol{\Psi}'(\theta_1, \boldsymbol{\alpha}^0) \mathbf{w}_i - \boldsymbol{\kappa}\\
   \left\{\boldsymbol{\psi}_1'(\theta_1)\mathbf{w}_i - \kappa_1\right\}z_i
 \end{array}
 \right]
\]
Now, let $\boldsymbol{\gamma}' = (\mathbf{q}', \boldsymbol{\kappa}', \theta_1)$ where $\mathbf{q}' = ( \underline{q}_{00}, \overline{q}_{00}, \underline{q}_{10}, \overline{q}_{10}, \underline{q}_{01}, \overline{q}_{01}, \underline{q}_{11}, \overline{q}_{11})$, and $\boldsymbol{\kappa}' = (\kappa_1, \kappa_2, \kappa_3)$.
Now define,
\begin{align*}
  B^E &= -M^E (H^E)^{-1}\\
  M^E &= \mathbb{E}\left[
  \begin{array}{cc}
    \displaystyle \frac{\partial m^E}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial m^E}{\partial \theta_1}
  \end{array}
\right]\\
H^E &= \mathbb{E}\left[
  \begin{array}{cc}
    \displaystyle \frac{\partial h^E}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial h^E}{\partial \theta_1}
  \end{array}
\right] = \mathbf{R}
\end{align*}
where
\[
\mathbf{R} = \left[
\begin{array}{rrrr}
  -1 & 0 & 0 
  & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & -1 & 0 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & 0 & -1 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   -q & 0 & 0 &  \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}z]\\
\end{array}
\right]
\]
and
\[
\mathbb{E}\left[\frac{\partial m^E}{\partial \boldsymbol{\kappa}'}\right] = \left[
\begin{array}{ccc}
  0 & -q & 0 \\
  0 & 0 & -q
\end{array}
\right], \quad
\mathbb{E}\left[\frac{\partial m^E}{\partial \theta_1}\right] =
\left[
\begin{array}{c}
  \displaystyle\left( \frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1} \right)'\\ 
  \displaystyle\left( \frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1} \right)' 
\end{array}
\right] \mathbb{E}[\mathbf{w}_iz_i]
\]
Now, the moment functions that are not differentiable must be treated differently.
Define:
\[
  B^I = -{M}_2^I \left( H^I \right)^{-1}, \quad
  M_2^I = \frac{\partial\mathbb{E}[m_2^I]}{\partial \mathbf{q}'} , \quad
  H^I = \frac{\partial\mathbb{E}[h^I]}{\partial \mathbf{q}'} 
\]
Now, since
\begin{align*}
  \frac{\partial}{\partial q} \mathbb{E}\left[ y\mathbf{1}(y>q)\mathbf{1}(T=t)\mathbf{1}(z=k) \right] &= \mathbb{P}(T=t,z=k)\times -q f_{tk}(q)\\
  \frac{\partial}{\partial q} \mathbb{E}\left[ y\mathbf{1}(y\leq q)\mathbf{1}(T=t)\mathbf{1}(z=k) \right] &= \mathbb{P}(T=t, z=k) \times q f_{tk}(q)\\
  \frac{\partial}{\partial q} \mathbb{E}\left[ \mathbf{1}(y\leq q)\mathbf{1}(T=t)\mathbf{1}(z=k) \right] &= \mathbb{P}(T=t,z=k) \times f_{tk}(q)
\end{align*}
we see that
\[
  H^I_k = \mbox{diag} \left[
  \begin{array}{c}
    \mathbb{P}(T=0,z=k)\times f_{0k}(\underline{q}_{0k})\\
    \mathbb{P}(T=0,z=k)\times f_{0k}(\overline{q}_{0k})\\
    \mathbb{P}(T=1,z=k)\times f_{1k}(\underline{q}_{1k})\\
    \mathbb{P}(T=1,z=k)\times f_{1k}(\overline{q}_{1k})\\
  \end{array}
\right]
\]
while
\[
  M_{2k}^I = -(1 - \alpha_0 - \alpha_1)\times \mbox{diag} \left[
  \begin{array}{l}
    \mathbb{P}(T=0,z=k) \times \underline{q}_{0k} f_{0k}(\underline{q}_{0k}) / \alpha_1\\
    \mathbb{P}(T=0,z=k)\times \overline{q}_{0k} f_{0k}(\overline{q}_{0k}) / \alpha_1\\
    \mathbb{P}(T=1,z=k)\times \underline{q}_{1k} f_{1k}(\underline{q}_{1k}) / (1 - \alpha_1)\\
    \mathbb{P}(T=1,z=k)\times \overline{q}_{1k} f_{1k}(\overline{q}_{1k}) / (1 - \alpha_1)\\
  \end{array}
\right]
\]
and thus
\[
  B^I_k = (1 - \alpha_0 - \alpha_1) \times \mbox{diag} \left[
  \begin{array}{l}
    \underline{q}_{0k} / \alpha_1\\
    \overline{q}_{0k} / \alpha_1\\
    \underline{q}_{1k} / (1 - \alpha_1)\\
    \overline{q}_{1k} / (1 - \alpha_1)
  \end{array}
\right]
\]
and we just need to stack $B^I_0$ on top of $B^I_1$ to construct $B^I$.

\paragraph{Putting Everything Together}
Finally, accounting for preliminary estimation of $\boldsymbol{\kappa}$ and $\theta_1$, the variance matrix estimator is $\widehat{\Sigma}_n\left(\boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) = A \widehat{\mathcal{V}}_n \left(\boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) A'$ where 
\[
  \underset{(14 \times 26)}{A} = \left[
\begin{array}{cc}
  \underset{(14\times 14)}{\mathbf{I}} & \underset{(14 \times 12)}{B}
\end{array}
\right], \quad
  B = \left[
  \begin{array}{cc}
    \underset{(4\times 8)}{\mathbf{0}} & \underset{(4\times 4)}{\mathbf{0}} \\
    \underset{(8\times 8)}{B^I} & \underset{(8\times 4)}{\mathbf{0}} \\
    \underset{(2 \times 8)}{\mathbf{0}} & \underset{(2\times 4)}{B^E} \\
  \end{array}
\right]
\]
and $B^{E} = -M^E \mathbf{R}^{-1}$ with
\[
  M^E = \left[ 
    \begin{array}{cccc} 
      0 & -q & 0 & \displaystyle\left( \frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1} \right)' \mathbb{E}[\mathbf{w}_iz_i]\\ 
      0 & 0 & -q & \displaystyle\left( \frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1} \right)' \mathbb{E}[\mathbf{w}_iz_i] 
  \end{array} \right], \quad
\mathbf{R} = \left[
\begin{array}{rrrr}
  -1 & 0 & 0 
  & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & -1 & 0 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & 0 & -1 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   -q & 0 & 0 &  \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}z]\\
\end{array}
\right]
\]
while $B^I = -M_2^{I}\left( H^I \right)^{-1}$ which simplifies to
\[
  B^I = (1 - \alpha_0 - \alpha_1) \times \mbox{diag} \left[
  \begin{array}{llllllll}
    \displaystyle\frac{\underline{q}_{00}}{\alpha_1} &
    \displaystyle\frac{\overline{q}_{00}}{\alpha_1} &
    \displaystyle\frac{\underline{q}_{10}}{(1 - \alpha_1)} &
    \displaystyle\frac{\overline{q}_{10}}{(1 - \alpha_1)} &
    \displaystyle\frac{\underline{q}_{01}}{\alpha_1} &
    \displaystyle\frac{\overline{q}_{01}}{\alpha_1} &
    \displaystyle\frac{\underline{q}_{11}}{(1 - \alpha_1)} &
    \displaystyle\frac{\overline{q}_{11}}{(1 - \alpha_1)}
  \end{array}
\right]
\]
and
\[
  \widehat{\mathcal{V}}_n\left(\boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) = \frac{1}{n}\sum_{i=1}^n 
  \left[
  \begin{array}{c}
    m(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{m}_n(\alpha^0, \widehat{\gamma}(\alpha^0))\\
    h(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{h}_n(\alpha^0, \widehat{\gamma}(\alpha^0))
  \end{array}
\right]
  \left[
  \begin{array}{c}
    m(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{m}_n(\alpha^0, \widehat{\gamma}(\alpha^0))\\
    h(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{h}_n(\alpha^0, \widehat{\gamma}(\alpha^0))
  \end{array}
\right]'
\]
Where, 
\begin{align*}
  m_1^I(\mathbf{w}_i, \boldsymbol{\alpha}^0) &=
  \left[
  \begin{array}{r}
  (T_i - \alpha_0)(1 - z_i) \\
  (1 - T_i - \alpha_1)(1 - z_i) \\
  (T_i - \alpha_0) z_i \\
  (1 - T_i - \alpha_1) z_i
  \end{array}
\right]\\
  m^E\left(\mathbf{w}_i, \boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) &= 
  \left[
  \begin{array}{c}
 \widehat{\boldsymbol{\psi}}_2'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})z_i \\
   \widehat{\boldsymbol{\psi}}_3'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})z_i 
  \end{array}
\right]\\
  h^E\left(\mathbf{w}_i, \boldsymbol{\alpha}^0, \widehat{\gamma}(\boldsymbol{\alpha}^0)\right) &= \left[
  \begin{array}{l}
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})\\ 
    \widehat{\boldsymbol{\psi}}'_2(\boldsymbol{\alpha}^0) (\mathbf{w}_i - \bar{\mathbf{w}})\\ 
    \widehat{\boldsymbol{\psi}}'_3(\boldsymbol{\alpha}^0) (\mathbf{w}_i - \bar{\mathbf{w}})\\
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})z_i
  \end{array}
\right] = \left[
\begin{array}{l}
  \widehat{\boldsymbol{\Psi}}'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})\\
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})z_i
\end{array}
\right]
\end{align*}


\subsection{Details of Inference Procedure}
\label{sec:details}

\subsection{Incorporating Covariates}
\label{sec:covariates}
