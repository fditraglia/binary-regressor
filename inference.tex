%!TEX root = ./main.tex
\section{Identification-Robust Inference}
We now turn our attention to inference based on the identification results from above.
As we explain below, inference under binary mis-classification is complicated by problems of weak identification and parameters on the boundary.
For simplicity we fix the exogenous covariates at some specified level and suppress dependence on $\mathbf{x}$ in the notation.
This is appropriate if the covariates have a discrete support.
We discuss how to incorporate covariates more generally in Section \ref{sec:covariates}.

\subsection{The Non-standard Inference Problem}
\label{sec:problem}
Lemmas \ref{lem:wald}--\ref{lem:eta3} yield the following system of linear moment equalities in the reduced form parameters $\boldsymbol{\theta} = (\theta_1, \theta_2, \theta_3)$ from Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}:
\begin{align*}
  \mbox{Cov}(y,z) - \mbox{Cov}(T,z) \theta_1 &= 0\\
  \mbox{Cov}(y^2,z) - 2\mbox{Cov}(yT,z) \theta_1 + \mbox{Cov}(T,z)\theta_2 &= 0\\
  \mbox{Cov}(y^3,z) - 3 \mbox{Cov}(y^2T,z) \theta_1 + 3\mbox{Cov}(yT,z) \theta_2 - \mbox{Cov}(T,z) \theta_3 &= 0
\end{align*}
Non-linearity arises solely through the relationship between the reduced from parameters $\boldsymbol{\theta}$ and the structural parameters $(\alpha_0, \alpha_1, \beta)$.
To convert the preceding moment equations into unconditional moment equalities, we define the additional reduced form parameters $\boldsymbol{\kappa} = (\kappa_1, \kappa_2, \kappa_3)$ as follows:
\begin{align*}
\kappa_1 &= c - \alpha_0 \theta_1\\
  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]
\end{align*}
Building on this notation, let
\begin{equation}
  \boldsymbol{\psi}_1' = (-\theta_1, 1, 0, 0, 0, 0), \quad
  \boldsymbol{\psi}_2' = (\theta_2, 0, -2\theta_1, 1, 0, 0), \quad
  \boldsymbol{\psi}_3' = (-\theta_3, 0, 3\theta_2, 0, -3\theta_1, 1)
  \label{eq:psi_def}
\end{equation}
and collect these in the matrix
$\boldsymbol{\Psi} = \left[
  \begin{array}{ccc}
    \boldsymbol{\psi}_1 & \boldsymbol{\psi_2} & \boldsymbol{\psi_3}
\end{array}\right]$.
Defining the observed data vector $\mathbf{w}_i' = (T_i, y_i, y_iT_i, y_i^2, y_i^2 T_i, y_i^3)$ for observation $i$, we can re-write the moment equations as:
\begin{equation}
\mathbb{E}\left[
  \big(\boldsymbol{\Psi}'(\boldsymbol{\theta})\mathbf{w}_i - \boldsymbol{\kappa}\big) \otimes 
\left(
\begin{array}{c}
  1 \\ z_i
\end{array}\right)
\right] = \mathbf{0}.
\label{eq:MCs_endog}
\end{equation}

Equation \ref{eq:MCs_endog} is a just-identified, linear system of moment equalities in the reduced form parameters $(\boldsymbol{\theta},\boldsymbol{\kappa})$ and yields explicit GMM estimators $(\widehat{\boldsymbol{\kappa}},\widehat{\boldsymbol{\theta}})$.
From Theorem \ref{thm:main_ident}, knowledge of $\boldsymbol{\theta}$ suffices to identify $\beta$.
From the definitions of $\boldsymbol{\kappa}$ above and $\boldsymbol{\theta}$  in Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}, however, the moment equalities from Equation \ref{eq:MCs_endog} do not depend on $(\alpha_0, \alpha_1)$ if $\beta$ equals zero.
By continuity, they are \emph{nearly} uninformative about the mis-classification probabilities if $\beta$ is small.
But unless $\beta = 0$, knowledge of $(\alpha_0, \alpha_1)$ is necessary to recover $\beta$, via Lemma \ref{lem:wald}. 
Thus, we face a weak identification problem.\footnote{This is essentially equivalent to the problem of estimating mixture probabilities when the means of the component distributions are very similar to each other.}
Indeed, the GMM estimator of $\widehat{\beta}$ based on Equation \ref{eq:MCs_endog} may even fail to exist.
Using arguments from the proof of Theorem \ref{thm:main_ident}, this estimator is given by is
\[
  \widehat{\beta} = \mbox{sign}\big(\widehat{\theta}_1\big) \sqrt{3\left( \widehat{\theta}_2/\widehat{\theta}_1 \right)^2 - 2 \left(\widehat{\theta}_3/\widehat{\theta}_1 \right)}
\]
Under our assumptions, $3(\theta_2/\theta_1)^2 > 2 (\theta_3/\theta_1)$ provided that $\beta \neq 0$, but this may not be true of the sample analogue.
Indeed, because $\widehat{\theta}_1$ appears in the denominator, the terms within the square root will be highly variable if $\beta$ is small.
Even if the GMM estimator exists, it may violate the partial identification bounds for $(\alpha_0, \alpha_1)$ from Theorem \ref{thm:sharpII}, or imply that $(\alpha_0,\alpha_1)$ are not valid probabilities. 
Importantly, the partial identification bounds remain informative even if $\beta$ is small or zero: so long as Assumption \ref{assump:model} (ii) holds, the first-stage probabilities bound $\alpha_0$ and $\alpha_1$ from above.


Exactly the same inferential difficulties arise in the case where $T^*$ and $z$ are assumed to be jointly exogenous, as in \cite{KRS,BBS,FL,Mahajan,Lewbel}.\footnote{We provide details for \cite{FL} and \cite{Mahajan} in Appendix \ref{sec:FL_mahajan}.}
This issue, however, has received little attention in the literature. 
\cite{KRS} ensure that $(\alpha_0, \alpha_1)$ are valid probabilities by employing a logit specification.
Frazis and Loewenstein employ a pseudo-Bayesian approach to ensure that $\alpha_0$ and $\alpha_1$ are valid probabilities, and to impose partial identification bounds related to those from our Theorem \ref{thm:sharpI}, i.e.\ without using the non-differential measurement error restrictions.
Because they provide neither simulation evidence nor a theoretical justification for their procedure, however, it is unclear whether this method will yield valid Frequentist coverage.
We are unaware of any papers in the related literature that discuss the weak identification problem arising when $\beta$ is small.


\subsection{Overview of the Inference Procedure}
\label{sec:overview}
In the following sections we develop a procedure for uniformly valid inference in models with a mis-classified binary regressor.
%This procedure is computationally attractive, and can be applied to both the endogenous and exogenous regressor cases, under either point or partial identification.
Our purpose is to construct a confidence interval for $\beta$ that is robust to possible weak identification, respects the restricted parameter space for $(\alpha_0, \alpha_1)$, and incorporates both the information in the equality moment conditions from Equation \ref{eq:MCs_endog} along with the partial identification bounds from Theorem \ref{thm:sharpII}.\footnote{Note that $\beta=0$ if and only if $\theta_1 = 0$. Thus, if one is merely interested in testing $H_0\colon \beta=0$, one can ignore the mis-classification error problem and test $H_0\colon \theta_1 = 0$ using the standard IV estimator and standard error, provided that $z$ is a strong instrument.}
As argued in the preceding section, our partial identification bounds remain informative even when the equality moment conditions contain essentially no information about $\beta$.

To carry out identification-robust inference combining equality and inequality moment conditions, we adopt the generalized moment selection (GMS) approach of \cite{AndrewsSoares}.
This procedure provides a uniformly valid test of a \emph{joint} null hypothesis for the full parameter vector.
In our model, this includes the parameter of interest $\beta$ along various nuisance parameters: the mis-classification probabilities $\alpha_0$ and $\alpha_1$, the reduced form parameters $\boldsymbol{\kappa}$, defined in Section \ref{sec:problem}, and a vector $\mathbf{q}$ of parameters that enter the moment inequalities.\footnote{These are defined below in Section \ref{sec:inequalities}.}
Under a given joint null hypothesis for $(\beta, \alpha_0, \alpha_1)$, however, $\boldsymbol{\kappa}$ and $\mathbf{q}$ are strongly identified and lie on the interior their respective parameter spaces. 
Accordingly, in Section \ref{sec:prelim} we explain how to concentrate these parameters out of the GMS procedure, by deriving an appropriate correction to the asymptotic variance matrix for the test.\footnote{Note that we cannot take the same approach to concentrate out $\alpha_0$ and $\alpha_1$ because the mis-classification probabilities may be weakly identified or lie on the boundary of their parameter space.}

This leaves us with a uniformly valid test of any joint null hypothesis for $(\beta, \alpha_0, \alpha_1)$.
To construct a marginal confidence interval for $\beta$ we proceed as follows.
Suppose that $z$ is a strong instrument.
Then the usual IV estimator provides a valid confidence interval for the reduced from parameter $\theta_1$.
By Lemma \ref{lem:wald}, knowledge of $(1 - \alpha_0 - \alpha_1)$ suffices to determine $\beta$ from $\theta_1$.
Thus, a valid confidence interval for $(1 - \alpha_0 - \alpha_1)$ can be combined with the IV interval for $\theta_1$ to yield a corresponding interval for $\beta$, via a Bonferroni-type correction.
To construct the required interval for $(1 - \alpha_0 - \alpha_1)$, we note from Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}  that $\beta$ only enters the moment equality conditions in Equation \ref{eq:MCs_endog} through $\theta_1$.
But, again, inference for $\theta_1$ is standard provided that $z$ is a strong instrument.
We can thus pre-estimate $\theta_1$ along with $\boldsymbol{\kappa}$ and $\mathbf{q}$, yielding a uniformly valid GMS test of any joint null hypothesis for $(\alpha_0, \alpha_1)$.
By inverting this test, we construct a joint confidence set for $(\alpha_0, \alpha_1)$ which we then project to obtain a confidence interval $(1 - \alpha_0 - \alpha_1)$. 
Because the parameter space for $(\alpha_0, \alpha_1)$ is bounded and two-dimensional, the projection step is computationally trivial.\footnote{We considered two alternatives to the Bonferroni-based inference procedure described here.
  The first constructs a marginal confidence interval for $\beta$ by projecting a joint confidence set for $(\beta, \alpha_1, \alpha_0)$, i.e.\ \emph{without} preliminary estimation of $\theta_1$.
This method is more computationally demanding than our two-dimensional projection and involves a parameter space that is unbounded along the $\beta$-dimension.
From a practical perspective, the relevant question is whether the reduction in conservatism from projecting a lower dimensional set is outweighed by the additional conservatism induced by the Bonferroni correction. 
In our experiments, the full three-dimensional projection and Bonferroni procedure produced broadly similar results: neither reliably dominated in terms of confidence interval width.
Given its substantially lower computational burden, we prefer the Bonferroni procedure.
We also experimented with two recently proposed methods for sub-vector inference: \cite{kaido2016confidence} and \cite{BugniCanayShi}. In both cases we obtained significant size distortions, suggesting that our model may not satisfy the high-level regularity conditions required by these papers.}
If desired, one could also carry out a valid test of the null hypothesis that there is no mis-classification, $\alpha_0 = \alpha_1 = 0$, using the joint test for $(\alpha_0, \alpha_1)$. 
In the following sections we provide full details of our Bonferroni-based confidence interval procedure for $\beta$.


\subsection{Moment Inequalities} 
\label{sec:inequalities}
As noted above, the partial identification bounds from Theorems \ref{thm:sharpI} and \ref{thm:sharpII} remain informative about $(\alpha_0, \alpha_1)$ even when $\beta$ is small.
To incorporate them in our inference procedure, we first express them as unconditional moment inequalities.
The bounds from Theorem \ref{thm:sharpI} are given by
\begin{equation*}
  p_k - \alpha_0 \geq 0, \quad \quad 1 - p_k - \alpha_1 \geq 0, \quad \mbox{for all } k 
\end{equation*}
where the first-stage probabilities $p_k$ are defined in Equation \ref{eq:pk_def}.
We write these as 
\begin{equation}
  \mathbb{E}\left[ m_1^I(\mathbf{w}_i,\boldsymbol{\vartheta} ) \right] \geq \mathbf{0},  \quad
m_1^I(\mathbf{w}_i, \boldsymbol{\vartheta}) \equiv \left[
  \begin{array}{l}
    (1 - z_i)(T_i - \alpha_0) \\
    (1 - z_i)(1 - T_i - \alpha_1) \\
    z_i(T - \alpha_0) \\
    z_i (1 - T_i - \alpha_1) 
  \end{array}
\right]
\end{equation}
The bounds derived in Theorem \ref{thm:sharpII} by imposing assumption \ref{assump:misclassification} (iii) are
\begin{equation*}
  \mu_k(\alpha_0) - \underline{\mu}_{tk}\big( \underline{q}_{tk}(\alpha_0, \alpha_1) \big) \geq 0, \quad \quad
  \overline{\mu}_{tk}\big( \overline{q}_{tk}(\alpha_0, \alpha_1) \big) - \mu_k(\alpha_0) \geq 0, \quad \mbox{for all } t,k
\end{equation*}
where $\mu_k, \underline{\mu}_{tk}, \overline{\mu}_{tk}, \underline{q}_{tk}$ and $\overline{q}_{tk}$ are defined in the statement of the Theorem.
Expressing these as unconditional moment inequalities, we have
\begin{equation}
  \mathbb{E}[m_2^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})] \geq \mathbf{0}, \quad 
  m_2^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) \equiv \left[
  \begin{array}{c}
    m_{2,00}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})  \\ 
    m_{2,10}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) \\
    m_{2,01}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})  \\ 
    m_{2,11}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) 
  \end{array}
\right] 
\end{equation}
where $\mathbf{q} \equiv ( \underline{q}_{00},\, \overline{q}_{00},\, \underline{q}_{10}, \,\overline{q}_{10},\, \underline{q}_{01}, \,\overline{q}_{01},\, \underline{q}_{11},\, \overline{q}_{11})$ and we define
\begin{align}
  m_{2,0k}^I\big(\mathbf{w}_i, \boldsymbol{\vartheta}, \mathbf{q}) \equiv \left[
  \begin{array}{r}
    y_i \mathbf{1}\left( z_i=k \right)\left\{(T_i - \alpha_0) - \mathbf{1}(y_i \leq \underline{q}_{0k})  (1 - T_i)\left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right)\right\} \\
    - y_i \mathbf{1}(z_i=k) \left\{ (T_i - \alpha_0) -  \mathbf{1}(y_i > \overline{q}_{0k}) (1 - T_i) \left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right) \right\} 
\end{array}
\right] \\\nonumber \\
  m_{2,1k}^I(\mathbf{w}_i, \boldsymbol{\vartheta}, \mathbf{q}) \equiv \left[
  \begin{array}{r}
    y_i \mathbf{1}\left( z_i=k \right)\left\{(T_i - \alpha_0) - \mathbf{1}(y_i \leq \underline{q}_{1k})  T_i\left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right)\right\} \\
    - y_i \mathbf{1}(z_i=k) \left\{ (T_i - \alpha_0) -  \mathbf{1}(y_i > \overline{q}_{1k}) T_i \left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right) \right\} 
\end{array}
\right].
\end{align}
Finally we define $m^I = (m_1^{I'}, m_2^{I'})'$.
Notice that the second set of inequalities, $m_2^I$, depends on unknown parameter $\mathbf{q}$ which is in turn a function of $(\alpha_0, \alpha_1)$.
In the next section we discuss how $\mathbf{q}$ can be estimated under a given null hypothesis for $(\alpha_0, \alpha_1)$. 


\subsection{Accounting for Preliminary Estimation}
\label{sec:prelim}


Let $\boldsymbol{\vartheta} = (\alpha_0, \alpha_1)$ and $\boldsymbol{\gamma} = (\boldsymbol{\kappa}, \theta_1)$ where $\theta_1$ is defined in Equation \ref{eq:theta1_def} and $\boldsymbol{\kappa}$ in Section \ref{sec:problem}.
Our moment conditions take the form 
\begin{equation}
  \mathbb{E}[m^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \mathbf{q}_0)] \geq \mathbf{0}, \quad 
  \mathbb{E}[m^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)] = \mathbf{0}
  \label{eq:MCs_ineq_eq}
\end{equation}
where $m^I = (m_1^{I'}, m_2^{I'})'$, defined in Section \ref{sec:inequalities}, and 
\begin{equation}
  m^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)= \left[
  \begin{array}{c}
 \left\{ \boldsymbol{\psi}_2'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \kappa_2 \right\}z_i \\
 \left\{ \boldsymbol{\psi}_3'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \kappa_3 \right\}z_i 
  \end{array}
\right].
\end{equation}
Notice that we now write $\boldsymbol{\psi}_2$ and $\boldsymbol{\psi}_3$, defined in Equation \ref{eq:psi_def}, as explicit functions of $(\theta_1, \alpha_0, \alpha_1)$, using the definitions of $(\theta_2, \theta_3)$ from Equations \ref{eq:theta2_def}--\ref{eq:theta3_def}.
To construct a GMS test of the null hypothesis $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta_0}$ based on Equation \ref{eq:MCs_ineq_eq}, we require preliminary estimators $\widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ and $\widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$ that are consistent and asymptotically normal \emph{under the null}.
We now provide full details of the construction and derive the associated adjustment to the asymptotic variance matrix.

Consider first the equality moment conditions $m^E$.
For these we require preliminary estimators of $\theta_1$, $\kappa_2$, and $\kappa_3$.
Recall that $\theta_1$ is simply the IV estimand: it can be consistently estimated directly from observations of $(y,T,z)$ without knowledge of $\alpha_0$ or $\alpha_1$.
Note, moreover, from Equation \ref{eq:MCs_endog} that $\boldsymbol{\kappa}$ is simply a vector of \emph{intercepts}.
These can be directly estimated from observations of $\mathbf{w}$ because $\boldsymbol{\Psi}(\theta_1, \alpha_0, \alpha_1)$ is consistently estimable under the $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$: the hypothesis specifies $\alpha_0$ and $\alpha_1$ and IV provides a consistent estimator of $\theta_1$.
Accordingly, define 
\begin{equation}
  h^E(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\gamma})  =  \left[
 \begin{array}{l}
   \boldsymbol{\Psi}'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \boldsymbol{\kappa}\\
   \left\{\boldsymbol{\psi}_1'(\theta_1)\mathbf{w}_i - \kappa_1\right\}z_i
 \end{array}
 \right].
\end{equation}
Under $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$, the  just-identified GMM-estimator based on $\mathbb{E}[h^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)] = \mathbf{0}$ yields a consistent and asymptotically normal estimator of $\boldsymbol{\gamma}_0$ under the usual regularity conditions.

Now consider the inequality moment conditions $m^I$. 
From Section \ref{sec:inequalities} we see that $\boldsymbol{\tau}$ only enters $m^I$ through the second block of moment inequalities, $m_2^I$, which depend on $\mathbf{q}$.
The elements of $\mathbf{q}$ are the conditional quantiles $\overline{q}_{tk}$ and $\underline{q}_{tk}$ defined in Theorem \ref{thm:sharpII}.
Under the assumption that $y$ follows a continuous distribution, as maintained in Theorem \ref{thm:sharpII}, these can be expressed as conditional moment equalities as follows:
\begin{align*}
  &\mathbb{E}\left[ \mathbf{1}(y \leq \underline{q}_{tk}) |T=t,z=k \right] - r_{tk}(\alpha_0, \alpha_1) = 0 \\
  &\mathbb{E}\left[ \mathbf{1}(y \leq \overline{q}_{tk}) |T=t,z=k \right] - \big(1 - r_{tk}(\alpha_0, \alpha_1)\big) = 0
\end{align*}
where $r_{tk}$ is defined in Theorem \ref{thm:sharpII} and $t,k = 0,1$.
Under $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$, a consistent estimator $\widehat{r}_{tk}$ of $r_{tk}$ can be obtained directly from $\widehat{p}_k$, the sample analogue of $p_k$ based on iid observations of $\mathbf{w}_i$.
In turn, the $(\widehat{r}_{tk})$\textsuperscript{th} and $(1 - \widehat{r}_{tk})$\textsuperscript{th} sample conditional quantiles of $y$ provide consistent estimates of $\underline{q}_{tk}$ and $\overline{q}_{tk}$.\footnote{Consistency of the sample quantiles requires $0 < r_{tk} < 1$. If $r_{tk} = 0$ or $1$ for some $(t,k)$, however, then the associated moment inequality is trivially satisfied and we no longer require estimates of $\underline{q}_{tk}, \overline{q}_{tk}$.\label{foot:quantile}}
Collecting these for all $(t,k)$ gives $\widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$.
Now, define
\begin{equation}
h^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) = 
  \left[
  \begin{array}{c}
    h^I_{0}(\mathbf{w}, \boldsymbol{\vartheta}, \mathbf{q})\\
    h^I_{1}(\mathbf{w}, \boldsymbol{\vartheta}, \mathbf{q})
  \end{array}
\right] 
\end{equation}
where
\begin{equation}
  h_k^I(\mathbf{w}_i,\boldsymbol{\vartheta},\mathbf{q}) = \left[
  \begin{array}{l}
    \mathbf{1}(y_i \leq \underline{q}_{0k}) \mathbf{1}(z_i=k)(1 - T_i) 
    - \left( \frac{\alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(T_i-\alpha_0)\\ 
    \mathbf{1}(y_i \leq \overline{q}_{0k}) \mathbf{1}(z_i=k)(1 - T_i)
    - \left( \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(1 - T_i-\alpha_1)\\
    \mathbf{1}(y_i \leq \underline{q}_{1k}) \mathbf{1}(z_i=k)T_i
    - \left( \frac{1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(T_i-\alpha_0)\\ 
    \mathbf{1}(y_i \leq \overline{q}_{1k}) \mathbf{1}(z_i=k)T_i 
    - \left( \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(1 - T_i-\alpha_1)
  \end{array}
\right].
\end{equation}
Under the null $\widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$ converges in probability to $\mathbf{q}_0$, which satisfies the just-identified collection of moment equalities $\mathbb{E}[h^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0,\mathbf{q}_0)] = \mathbf{0}$.
Although $h^I$ is a discontinuous function of $\mathbf{q}$, it is bounded for any fixed $(\alpha_0, \alpha_1)$.
Moreover, since $y|(T=t,z=k)$ is a continuous random variable, $\mathbb{E}[h^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})]$ is continuously differentiable with respect to $\mathbf{q}$.
Hence, $\widehat{\mathbf{q}}$ is asymptotically normal under mild regularity conditions.\footnote{\todo[inline]{Cite Andrews/Newey-McFadden.}}
To account for the effect of preliminary estimation of $\mathbf{q}$ and $\boldsymbol{\gamma}$ on the asymptotic variance matrix used in the GMS test, we rely on the following Lemma.

\begin{lem}
  Let $\bar{m}^I_{1,n}(\boldsymbol{\vartheta}) = n^{-1}\sum_{i=1}^n m_{1,n}^I(\mathbf{w}_i,\boldsymbol{\vartheta})$ and define $\bar{m}_{2,n}^I, \bar{m}_n^E, \bar{h}_n^I,\bar{h}_n^E$ analogously.
  Further let $\widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0) = \arg \min_{\boldsymbol{\gamma}} \lVert \bar{h}_n^E(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma}) \rVert$ and 
    $\lVert h_n^I(\boldsymbol{\vartheta}_0, \widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0))\rVert \leq \inf_{\mathbf{q}} \lVert h_n^I(\boldsymbol{\vartheta}_0, \mathbf{q})\rVert + o_p(1)$.
Then, under standard regularity conditions
  \[
    \sqrt{n}\left[
    \begin{array}{l}
      \bar{m}_{1,n}^I\left( \boldsymbol{\vartheta}_0 \right)\\
      \bar{m}_{2,n}^I\big( \boldsymbol{\vartheta}_0, \widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)\big)\\
      \bar{m}_{n}^E\big( \boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0) \big)
    \end{array}
  \right] \rightarrow_p 
  \left[
  \begin{array}{ccccc}
    \mathbf{I}&\mathbf{0}&\mathbf{0} & \mathbf{0} & \mathbf{0} \\
    \mathbf{0}&\mathbf{I}&\mathbf{0} & B^I( \boldsymbol{\vartheta}_0, \boldsymbol{q}_0)& \mathbf{0} \\
    \mathbf{0}&\mathbf{0}&\mathbf{I} &\mathbf{0} & B^E(\boldsymbol{\vartheta}_0,\boldsymbol{\gamma}_0)\\
  \end{array}
\right] 
    \sqrt{n}\left[
    \begin{array}{l}
      \bar{m}_{1,n}^I\left( \boldsymbol{\vartheta}_0 \right)\\
      \bar{m}_{2,n}^I\left(\boldsymbol{\vartheta}_0,  \mathbf{q}_0\right)\\
      \bar{m}_n^E\left(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0  \right)\\
      \bar{h}^I_n\left(\boldsymbol{\vartheta}_0,  \mathbf{q}_0 \right)\\
      \bar{h}^E_n\left(\boldsymbol{\vartheta}_0,  \boldsymbol{\gamma}_0 \right)
    \end{array}
  \right] 
  \]
  where $B^I( \boldsymbol{\vartheta}, \boldsymbol{q}) = (1 - \alpha_0 - \alpha_1) \left[\mbox{diag}(\mathbf{a})  \right]^{-1} \mathbf{q}$ 
  with 
  \[
  \mathbf{a}' = (\alpha_1, \alpha_1, 1 - \alpha_1, 1 - \alpha_1, \alpha_1, \alpha_1, 1 - \alpha_1, 1 - \alpha_1)
\]
and $B^E(\boldsymbol{\vartheta},\boldsymbol{\gamma}) = -M^E (H^E)^{-1}$, with
\[
  M^E = \left[ 
    \begin{array}{cccc} 
      0 & -\mathbb{E}[z_i] & 0 & \left( \frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1} \right)' \mathbb{E}[\mathbf{w}_iz_i]\\ 
      0 & 0 & -\mathbb{E}[z_i] & \left( \frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1} \right)' \mathbb{E}[\mathbf{w}_iz_i] 
  \end{array} \right], \,
H^E = \left[
\begin{array}{cccc}
  -1 & 0 & 0 
  & \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}_i]\\
   0 & -1 & 0 &  \left(\frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}_i]\\
   0 & 0 & -1 & \left(\frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}_i]\\
   -\mathbb{E}[z_i] & 0 & 0 &  \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}_iz_i]
\end{array}
\right].
\]
\label{lem:prelim}
\end{lem}

Lemma \ref{lem:prelim} relates the sample analogues $\bar{m}_{2,n}^I$ and $\bar{m}_n^E$ evaluated at the preliminary estimators $\widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$ and $\widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ to their counterparts evaluated at the true parameter values $\mathbf{q}_0$ and $\boldsymbol{\gamma}_0$.
The estimator $\widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ exactly solves $h_n^E(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma})=0$ while $\widehat{q}(\boldsymbol{\vartheta}_0)$, constructed as described immediately before the statement of the Lemma, \emph{approximately} solves $\bar{h}_n^I(\boldsymbol{\vartheta}_0, \mathbf{q})=0$. 
%This relationship is determined by the sample analogues of the auxiliary moments, $\bar{h}_n^I$ and $\bar{h}_n^E$, via the matrices $B^I$ and $B^E$ defined in the Lemma.
A few lines of matrix algebra 
%using the definitions of $(\boldsymbol{\psi}_1,\boldsymbol{\psi}_2,\boldsymbol{\psi}_3)$ from Equation \ref{eq:psi_def} and $\mathbf{w}_i$ from Section \ref{sec:problem} 
show that the determinant of $H^E$ equals $\mbox{Cov}(z,T)$.
Hence, $B^E$ is well-defined if $z$ is a relevant instrument.
The matrix $B^I$ is likewise well-defined provided that $\alpha\neq 0$ and the elements of $\mathbf{q}_0$ are computed for probabilities strictly between zero and one.
If either of these conditions fails, however, some of the moment inequalities in $m_2^I$ are trivially satisfied and can be dropped (see Footnote \ref{foot:quantile}).
After removing the corresponding elements of $\mathbf{q}_0$ and $\mathbf{a}$, $B^I$ becomes well-defined.
The regularity conditions required for Lemma \ref{lem:prelim} are mild.
The result relies on a number of mean-value expansions: $\bar{h}_n^E(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)$ and $\bar{m}_n^E(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)$ are expanded around $\boldsymbol{\gamma} = \widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ while $\mathbb{E}[h^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \mathbf{q}_0)]$ and $\mathbb{E}[m^I_2(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \mathbf{q}_0)]$ are expanded around $\mathbf{q} = \widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$.
These expansions, in turn, rely on the fact that $\mathbf{q}$ and $\boldsymbol{\gamma}$ are interior to their respective parameter spaces and the relevant functions are all continuously differentiable in our example.


We now have all the ingredients required to construct an asymptotic variance matrix for the GMS test that accounts for preliminary estimation of $\boldsymbol{\gamma}$ and $\mathbf{q}$. 
Let $m' = (m_1^{I'}, m_{2}^{I'}, m^{E'})$, $h' = (h^{I'}, h^{E'})$, and define the shorthand $\boldsymbol{\tau}_0' = (\boldsymbol{\gamma}_0', \mathbf{q}_0')$ and 
$\widehat{\boldsymbol{\tau}}_0' = \big( \widehat{\boldsymbol{\gamma}}'(\boldsymbol{\vartheta}_0), \widehat{\mathbf{q}}'(\boldsymbol{\vartheta}_0) \big)$.
Given a collection of iid observations $(\mathbf{w}_1, \hdots, \mathbf{w}_n)$, we have
\begin{equation}
  \sqrt{n}\left[
  \begin{array}{c}
    \bar{m}_n(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\\
    \bar{h}_n(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)
  \end{array}
\right] \rightarrow_d N\big(\mathbf{0},  \mathcal{V}(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\big), \quad
  \mathcal{V}(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0) = \mbox{Var}\left[
  \begin{array}{c}
  m(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0) \\
  h(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\\
  \end{array}
\right]
\label{eq:clt}
\end{equation}
under $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$, by an appropriate central limit theorem.
What we require for the test, however, is the asymptotic variance matrix of $\sqrt{n}\, \bar{m}_n(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)$.
Combining Equation \ref{eq:clt} with Lemma \ref{lem:prelim}, we obtain 
\begin{equation}
  \mbox{Avar}\big(\sqrt{n}\, \bar{m}_n(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)\big) = \Xi(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\, \mathcal{V}(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\, \Xi'(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)
\end{equation}
with
\begin{equation}
  \Xi(\boldsymbol{\vartheta}, \boldsymbol{\tau}) = \left[
  \begin{array}{cc}
    \mathbf{I} & B(\boldsymbol{\vartheta}, \boldsymbol{\tau})
  \end{array}
\right], \quad
  B(\boldsymbol{\vartheta}, \boldsymbol{\tau})= \left[
\begin{array}{cc}
  \mathbf{0} & \mathbf{0}\\
  B^I(\boldsymbol{\vartheta}, \mathbf{q}) & \mathbf{0}\\
  \mathbf{0} & B^E(\boldsymbol{\vartheta}, \boldsymbol{\gamma})
\end{array}
\right]
\label{eq:Xi}
\end{equation}
where $B^I(\cdot, \cdot)$ and $B^E(\cdot, \cdot)$ are defined in Lemma \ref{lem:prelim}.
Finally, we construct a consistent estimator $\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0)$ of the asymptotic variance matrix of $\sqrt{n}\, \bar{m}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)$ under the null:
\begin{equation}
  \widehat{\Sigma}_n(\boldsymbol{\vartheta}_0) \equiv \Xi(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)\, \widehat{\mathcal{V}}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}})\,\Xi'(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)
  \label{eq:AvarEst}
\end{equation}
where
\begin{equation}
  \widehat{\mathcal{V}}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) \equiv \frac{1}{n} \sum_{i=1}^n
\left[
\begin{array}{c}
  m(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\tau}) - \bar{m}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) \\
  h(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\tau}) -  \bar{h}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) 
\end{array}
\right]
\left[
\begin{array}{c}
  m(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\tau}) - \bar{m}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) \\
  h(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\tau}) -  \bar{h}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) 
\end{array}
\right]'.
\label{eq:fullVar}
\end{equation}
In the following section we provide a step-by-step description of our inference procedure.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Details of Inference Procedure}
\label{sec:details}

\todo[inline]{First define notation, test statistic, etc. I cut and pasted the following from above, where it formerly appeared.}

Before proceeding, we introduce some notation.
Let
\begin{equation}
  \mathbb{E} \left[ m^I_j(\mathbf{w}_i,\boldsymbol{\vartheta}_0) \right]
  \geq \mathbf{0} \quad j = 1, \cdots, J_I; \quad \quad
 \mathbb{E} \left[ m^E_j(\mathbf{w}_i,\boldsymbol{\vartheta}_0) \right]
 = \mathbf{0} \quad  j = J_I + 1, \cdots,J  
   \label{eq:mdefs}
 \end{equation}
 where $\boldsymbol{\vartheta}_0$ is the true parameter vector, $m_j^I$ corresponds to the $j$th inequality moment condition, $m_j^E$ corresponds to the $j$th equality moment condition, $J$ denotes the total number of moment conditions, $J_I$ denotes the number of moment equalities, and $J_E = J - J_I$ denotes the number of equality moment conditions.
Further define 
\begin{equation}
  \bar{m}_n(\boldsymbol{\vartheta}) = \left[
  \begin{array}{c}
   \bar{m}_n^I(\boldsymbol{\vartheta}) \\
   \bar{m}_n^E(\boldsymbol{\vartheta}) \\
  \end{array}
\right], \quad
  \bar{m}_n^I(\boldsymbol{\vartheta}) = \left[
  \begin{array}{c}
    \bar{m}_{n,1}^I(\boldsymbol{\vartheta})\\
    \vdots \\
    \bar{m}_{n,J_I}^I(\boldsymbol{\vartheta})\\
  \end{array}
\right], \quad
  \bar{m}_n^E(\boldsymbol{\vartheta}) = \left[
  \begin{array}{c}
    \bar{m}_{n,J_I+1}^E(\boldsymbol{\vartheta})\\
    \vdots \\
    \bar{m}_{n,J}^E(\boldsymbol{\vartheta})\\
  \end{array}
\right]
\end{equation}
where 
$\bar{m}^I_{n,j}(\boldsymbol{\vartheta}) = n^{-1} \sum_{i=1}^{n} m_j^I(\mathbf{w}_i, \boldsymbol{\vartheta})$
and $\bar{m}^E_{n,j}$ is defined analogously.
Finally, let $\Sigma(\boldsymbol{\vartheta})$ denote the asymptotic variance matrix of $\sqrt{n}\; \bar{m}_n(\boldsymbol{\vartheta})$, with $j$th diagonal element $\sigma^2_j(\boldsymbol{\vartheta})$ along with the corresponding sample analogues $\widehat{\Sigma}_n(\boldsymbol{\vartheta})$ and $\widehat{\sigma}^2_j(\boldsymbol{\vartheta})$.

namely the modified method of moments (MMM) statistic 
\begin{equation}
  T_n(\boldsymbol{\vartheta}) = \sum_{j=1}^{J_1}\min\left\{ 0, \left( \frac{\sqrt{n}\; \bar{m}_n^I(\boldsymbol{\vartheta})}{\widehat{\sigma}_n(\boldsymbol{\vartheta})} \right)^2 \right\} + \sum_{j=J_1 + 1}^{J} \left( \frac{\sqrt{n}\; \bar{m}_n^E(\boldsymbol{\vartheta})}{\widehat{\sigma}_n(\boldsymbol{\vartheta})} \right)^2.
\end{equation}

In particular, when approximating the asymptotic distribution of $T_n$ under the null $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$, we drop any inequality moment condition $m_j^I$ for which $\sqrt{n}\big[\bar{m}_n(\boldsymbol{\vartheta}_0)/ \widehat{\sigma}_n(\boldsymbol{\vartheta}_0)\big] > \sqrt{\log(n)}$.\footnote{Full details appear in Section \ref{sec:details} below.}

\todo[inline]{Refer to the places where each of the $m$ functions is defined above}
\begin{equation}
  \bar{m}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)  = \frac{1}{n} \sum_{i=1}^n \left[
  \begin{array}{c}
    m^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \widehat{\mathbf{q}}_0)  \\
  m^E(\mathbf{w}_i,\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\gamma}}_0)  
  \end{array}
\right],\quad 
m^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \widehat{\mathbf{q}}) = \left[
\begin{array}{l}
  m_{1}^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0)\\
  m_{2}^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \widehat{\mathbf{q}}_0)
\end{array}
\right]
\label{eq:mbar_def}
\end{equation}

\todo[inline]{Explain about the counts of inequalities and equalities: $J_I, J_E$ etc.}
\begin{equation}
  S(\mathbf{x}, \mathbf{y}) = \sum_{j}\min\left\{ 0, 
  x_j^2 \right\} + \mathbf{y}'\mathbf{y} 
    \label{eq:MMM}
\end{equation}

\[
  \bar{\boldsymbol{\nu}}_n^I = , \quad
  \bar{\boldsymbol{\nu}}_n^E = 
\]

\begin{alg}[GMS Test for $\alpha_0$ and $\alpha_1$ ]
  \mbox{} \\
  Inputs: null hypothesis $H_0\colon\boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$ for $(\alpha_0, \alpha_1)$, normal draws $\zeta_1, \cdots, \zeta_R \sim \mbox{ iid } N(\mathbf{0}_J, \mathbf{I}_J)$
  \begin{enumerate}
    \item Calculate the variance matrix estimator: $\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0)$
      \begin{enumerate}[(i)]
        \item Calculate $\widehat{\boldsymbol{\tau}}_0 = (\widehat{\mathbf{q}}_0', \widehat{\boldsymbol{\gamma}}_0')'$ where $\widehat{\boldsymbol{\gamma}}_0 =  \widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ and $\widehat{\mathbf{q}}_0 =  \widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$ from Section \ref{sec:prelim}.
        \item Calculate $\Xi(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)$ using Equation \ref{eq:Xi}.
        \item Calculate $\widehat{\mathcal{V}}_n(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)$ using Equation \ref{eq:fullVar}.
        \item Set $\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0) = \Xi(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)\, \widehat{\mathcal{V}}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}})\,\Xi'(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)$
      \end{enumerate}
    \item Calculate the test statistic $T_n(\boldsymbol{\vartheta}_0)$: 
      \begin{enumerate}[(i)]
    \item Calculate $\bar{m}_n^I(\boldsymbol{\vartheta}_0,\widehat{\mathbf{q}}_0)$ and $\bar{m}_n^E(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\gamma}}_0)$ from Equation \ref{eq:mbar_def}. 
    %\item Calculate $\bar{m}_n(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)$ and  $\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)$ from Equations \ref{eq:mbar_def} and \ref{eq:AvarEst}.
    \item Set $T_n(\boldsymbol{\vartheta}_0) = S\left( \sqrt{n}\, \bar{m}_n\left(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0 \right), \widehat{\Sigma}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0) \right)$, using $S(\cdot, \cdot)$ from Equation \ref{eq:MMM}.
  \end{enumerate}
\item Determine which of the moment inequalities are far from binding: 
      \begin{enumerate}[(i)]
        \item For each $j=1, \hdots, J_I$ calculate the t-statistic $t_{n,j} = \sqrt{n}\, \bar{m}_j^I(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)/\widehat{\sigma}_{n,j}(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)$
        \item 
      \end{enumerate}
    \item Approximate the sampling distribution of $T_n(\boldsymbol{\vartheta}_0)$ under the null: 
      \begin{enumerate}[(i)]
        \item 
      \end{enumerate}
    \item Calculate the p-value of the test: 
      \[\widehat{p}(\boldsymbol{\vartheta}_0) = \frac{1}{R}\sum_{r=1}^R \mathbf{1}\left\{T_{n}^{*(r)}() > T_n \right\}
      \]
  \end{enumerate}
\end{alg}

\begin{alg}[Bonferroni Confidence Interval for $\beta$] \mbox{}
  \begin{enumerate}
    \item First get the joint set for $\alpha_0, \alpha_1$ by a grid search, using the GMS test described in the other algorithm
    \item Do projection to get the set for $1 - \alpha_0 - \alpha_1$
    \item Construct the Bonferroni interval
  \end{enumerate}
\end{alg}

\begin{thm}
Inference procedure gives valid coverage. First argue that GMS test gives uniformly valid inference and that we have a valid confidence interval for $\theta_1$. Then the Bonferroni interval gives us what we require for $\beta$. The statement of the result should refer to iid observations, the Lemma about preliminary estimation, and A3 of Andrews \& Soares.
\end{thm}

\todo[inline]{Say a few more things about Bonferroni such has how you can't reject $\beta=0$ unless the IV interval excludes this.}

\subsection{Incorporating Covariates}
\label{sec:covariates}

\todo[inline]{Just give a quick overview of how one can handle them more generally. There several routes: one is to be more restrictive and do what FL do. The other is to try to be fully nonparametric. There's a paper by Andrews that gives some more details. This is likely to be complicated in general. There may be a ``halfway house'' using a semiparametric model. Perhaps look at what Lewbel said about this. This Section should really just be a paragraph or so.}
