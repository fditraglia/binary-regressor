%!TEX root = ./main.tex
\section{Identification-Robust Inference}
\label{sec:inference}
We now turn our attention to inference based on the identification results from above.
As we explain below, inference under binary mis-classification is complicated by problems of weak identification and parameters on the boundary.
For simplicity we fix the exogenous covariates at some specified level and suppress dependence on $\mathbf{x}$ in the notation.
This is appropriate if the covariates have a discrete support.
We discuss how to incorporate covariates more generally in Section \ref{sec:covariates}.

\subsection{The Non-standard Inference Problem}
\label{sec:problem}
Lemmas \ref{lem:wald}--\ref{lem:eta3} yield the following system of linear moment equalities in the reduced form parameters $\boldsymbol{\theta} = (\theta_1, \theta_2, \theta_3)$ from Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}:
\begin{align*}
  \mbox{Cov}(y,z) - \mbox{Cov}(T,z) \theta_1 &= 0\\
  \mbox{Cov}(y^2,z) - 2\mbox{Cov}(yT,z) \theta_1 + \mbox{Cov}(T,z)\theta_2 &= 0\\
  \mbox{Cov}(y^3,z) - 3 \mbox{Cov}(y^2T,z) \theta_1 + 3\mbox{Cov}(yT,z) \theta_2 - \mbox{Cov}(T,z) \theta_3 &= 0
\end{align*}
Non-linearity arises solely through the relationship between the reduced from parameters $\boldsymbol{\theta}$ and the structural parameters $(\alpha_0, \alpha_1, \beta)$.
To convert the preceding moment equations into unconditional moment equalities, we define the additional reduced form parameters $\boldsymbol{\kappa} = (\kappa_1, \kappa_2, \kappa_3)$ as follows:
\begin{align*}
\kappa_1 &= c - \alpha_0 \theta_1\\
  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]
\end{align*}
Building on this notation, let
\begin{equation}
  \boldsymbol{\psi}_1' = (-\theta_1, 1, 0, 0, 0, 0), \quad
  \boldsymbol{\psi}_2' = (\theta_2, 0, -2\theta_1, 1, 0, 0), \quad
  \boldsymbol{\psi}_3' = (-\theta_3, 0, 3\theta_2, 0, -3\theta_1, 1)
  \label{eq:psi_def}
\end{equation}
and collect these in the matrix
$\boldsymbol{\Psi} = \left[
  \begin{array}{ccc}
    \boldsymbol{\psi}_1 & \boldsymbol{\psi_2} & \boldsymbol{\psi_3}
\end{array}\right]$.
Defining the observed data vector $\mathbf{w}_i' = (T_i, y_i, y_iT_i, y_i^2, y_i^2 T_i, y_i^3)$ for observation $i$, we can re-write the moment equations as:
\begin{equation}
\mathbb{E}\left[
  \big(\boldsymbol{\Psi}'(\boldsymbol{\theta})\mathbf{w}_i - \boldsymbol{\kappa}\big) \otimes 
\left(
\begin{array}{c}
  1 \\ z_i
\end{array}\right)
\right] = \mathbf{0}.
\label{eq:MCs_endog}
\end{equation}

Equation \ref{eq:MCs_endog} is a just-identified, linear system of moment equalities in the reduced form parameters $(\boldsymbol{\theta},\boldsymbol{\kappa})$ and yields explicit GMM estimators $(\widehat{\boldsymbol{\kappa}},\widehat{\boldsymbol{\theta}})$.
From Theorem \ref{thm:main_ident}, knowledge of $\boldsymbol{\theta}$ suffices to identify $\beta$.
From the definitions of $\boldsymbol{\kappa}$ above and $\boldsymbol{\theta}$  in Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}, however, the moment equalities from Equation \ref{eq:MCs_endog} do not depend on $(\alpha_0, \alpha_1)$ if $\beta$ equals zero.
By continuity, they are \emph{nearly} uninformative about the mis-classification probabilities if $\beta$ is small.
But unless $\beta = 0$, knowledge of $(\alpha_0, \alpha_1)$ is necessary to recover $\beta$, via Lemma \ref{lem:wald}. 
Thus, we face a weak identification problem.\footnote{This is essentially equivalent to the problem of estimating mixture probabilities when the means of the component distributions are very similar to each other.}
Indeed, the GMM estimator of $\widehat{\beta}$ based on Equation \ref{eq:MCs_endog} may even fail to exist.
Using arguments from the proof of Theorem \ref{thm:main_ident}, this estimator is given by is
\[
  \widehat{\beta} = \mbox{sign}\big(\widehat{\theta}_1\big) \sqrt{3\left( \widehat{\theta}_2/\widehat{\theta}_1 \right)^2 - 2 \left(\widehat{\theta}_3/\widehat{\theta}_1 \right)}
\]
Under our assumptions, $3(\theta_2/\theta_1)^2 > 2 (\theta_3/\theta_1)$ provided that $\beta \neq 0$, but this may not be true of the sample analogue.
Indeed, because $\widehat{\theta}_1$ appears in the denominator, the terms within the square root will be highly variable if $\beta$ is small.
Even if the GMM estimator exists, it may violate the partial identification bounds for $(\alpha_0, \alpha_1)$ from Theorem \ref{thm:sharpII}, or imply that $(\alpha_0,\alpha_1)$ are not valid probabilities. 
Importantly, the partial identification bounds remain informative even if $\beta$ is small or zero: so long as Assumption \ref{assump:model} (ii) holds, the first-stage probabilities bound $\alpha_0$ and $\alpha_1$ from above.


Exactly the same inferential difficulties arise in the case where $T^*$ and $z$ are assumed to be jointly exogenous, as in \cite{KRS,BBS,FL,Mahajan,Lewbel}.\footnote{We provide details for \cite{FL} and \cite{Mahajan} in Appendix \ref{sec:FL_mahajan}.}
This issue, however, has received little attention in the literature. 
\cite{KRS} ensure that $(\alpha_0, \alpha_1)$ are valid probabilities by employing a logit specification.
Frazis and Loewenstein employ a pseudo-Bayesian approach to ensure that $\alpha_0$ and $\alpha_1$ are valid probabilities, and to impose partial identification bounds related to those from our Theorem \ref{thm:sharpI}, i.e.\ without using the non-differential measurement error restrictions.
Because they provide neither simulation evidence nor a theoretical justification for their procedure, however, it is unclear whether this method will yield valid Frequentist coverage.
We are unaware of any papers in the related literature that discuss the weak identification problem arising when $\beta$ is small.


\subsection{Overview of the Inference Procedure}
\label{sec:overview}
In the following sections we develop a procedure for uniformly valid inference in models with a mis-classified binary regressor.
%This procedure is computationally attractive, and can be applied to both the endogenous and exogenous regressor cases, under either point or partial identification.
Our purpose is to construct a confidence interval for $\beta$ that is robust to possible weak identification, respects the restricted parameter space for $(\alpha_0, \alpha_1)$, and incorporates both the information in the equality moment conditions from Equation \ref{eq:MCs_endog} along with the partial identification bounds from Theorem \ref{thm:sharpII}.\footnote{Note that $\beta=0$ if and only if $\theta_1 = 0$. Thus, if one is merely interested in testing $H_0\colon \beta=0$, one can ignore the mis-classification error problem and test $H_0\colon \theta_1 = 0$ using the standard IV estimator and standard error, provided that $z$ is a strong instrument.}
As argued in the preceding section, our partial identification bounds remain informative even when the equality moment conditions contain essentially no information about $(\alpha_0, \alpha_1)$.

To carry out identification-robust inference combining equality and inequality moment conditions, we adopt the generalized moment selection (GMS) approach of \cite{AndrewsSoares}.
This procedure provides a uniformly valid test of a \emph{joint} null hypothesis for the full parameter vector.
In our model, this includes the parameter of interest $\beta$ along with various nuisance parameters: the mis-classification probabilities $\alpha_0$ and $\alpha_1$, the reduced form parameters $\boldsymbol{\kappa}$, defined in Section \ref{sec:problem}, and a vector $\mathbf{q}$ of parameters that enter the moment inequalities.\footnote{These are defined below in Section \ref{sec:inequalities}.}
Under a given joint null hypothesis for $(\beta, \alpha_0, \alpha_1)$, however, $\boldsymbol{\kappa}$ and $\mathbf{q}$ are strongly identified and lie on the interior their respective parameter spaces. 
Accordingly, in Section \ref{sec:prelim} we explain how to concentrate these parameters out of the GMS procedure, by deriving an appropriate correction to the asymptotic variance matrix for the test.\footnote{Note that we cannot take the same approach to concentrate out $\alpha_0$ and $\alpha_1$ because the mis-classification probabilities may be weakly identified or lie on the boundary of their parameter space.}

This leaves us with a uniformly valid test of any joint null hypothesis for $(\beta, \alpha_0, \alpha_1)$.
To construct a marginal confidence interval for $\beta$ we proceed as follows.
Suppose that $z$ is a strong instrument.
Then the usual IV estimator provides a valid confidence interval for the reduced from parameter $\theta_1$.
By Lemma \ref{lem:wald}, knowledge of $(1 - \alpha_0 - \alpha_1)$ suffices to determine $\beta$ from $\theta_1$.
Thus, a valid confidence interval for $(1 - \alpha_0 - \alpha_1)$ can be combined with the IV interval for $\theta_1$ to yield a corresponding interval for $\beta$, via a Bonferroni-type correction.
To construct the required interval for $(1 - \alpha_0 - \alpha_1)$, we note from Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}  that $\beta$ only enters the moment equality conditions in Equation \ref{eq:MCs_endog} through $\theta_1$.
But, again, inference for $\theta_1$ is standard provided that $z$ is a strong instrument.
We can thus pre-estimate $\theta_1$ along with $\boldsymbol{\kappa}$ and $\mathbf{q}$, yielding a uniformly valid GMS test of any joint null hypothesis for $(\alpha_0, \alpha_1)$.
By inverting this test, we construct a joint confidence set for $(\alpha_0, \alpha_1)$ which we then project to obtain a confidence interval for $(1 - \alpha_0 - \alpha_1)$. 
Because the parameter space for $(\alpha_0, \alpha_1)$ is bounded and two-dimensional, the projection step is computationally trivial.\footnote{We considered two alternatives to the Bonferroni-based inference procedure described here.
  The first constructs a marginal confidence interval for $\beta$ by projecting a joint confidence set for $(\beta, \alpha_1, \alpha_0)$, i.e.\ \emph{without} preliminary estimation of $\theta_1$.
This method is more computationally demanding than our two-dimensional projection and involves a parameter space that is unbounded along the $\beta$-dimension.
From a practical perspective, the relevant question is whether the reduction in conservatism from projecting a lower dimensional set is outweighed by the additional conservatism induced by the Bonferroni correction. 
In our experiments, the full three-dimensional projection and Bonferroni procedure produced broadly similar results: neither reliably dominated in terms of confidence interval width.
Given its substantially lower computational burden, we prefer the Bonferroni procedure.
We also experimented with two recently proposed methods for sub-vector inference: \cite{kaido2016confidence} and \cite{BugniCanayShi}. In both cases we obtained significant size distortions, suggesting that our model may not satisfy the regularity conditions required by these papers.\label{foot:alternatives}}
If desired, one could also carry out a valid test of the null hypothesis that there is no mis-classification, $\alpha_0 = \alpha_1 = 0$, using the joint test for $(\alpha_0, \alpha_1)$. 
In the following sections we provide full details of our Bonferroni-based confidence interval procedure for $\beta$.


\subsection{Moment Inequalities} 
\label{sec:inequalities}
As noted above, the partial identification bounds from Theorems \ref{thm:sharpI} and \ref{thm:sharpII} remain informative about $(\alpha_0, \alpha_1)$ even when $\beta$ is small.
To incorporate them in our inference procedure, we first express them as unconditional moment inequalities.
The bounds from Theorem \ref{thm:sharpI} are given by
\begin{equation*}
  p_k - \alpha_0 \geq 0, \quad \quad 1 - p_k - \alpha_1 \geq 0, \quad \mbox{for all } k 
\end{equation*}
where the first-stage probabilities $p_k$ are defined in Equation \ref{eq:pk_def}.
We write these as 
\begin{equation}
  \mathbb{E}\left[ m_1^I(\mathbf{w}_i,\boldsymbol{\vartheta} ) \right] \geq \mathbf{0},  \quad
m_1^I(\mathbf{w}_i, \boldsymbol{\vartheta}) \equiv \left[
  \begin{array}{l}
    (1 - z_i)(T_i - \alpha_0) \\
    (1 - z_i)(1 - T_i - \alpha_1) \\
    z_i(T - \alpha_0) \\
    z_i (1 - T_i - \alpha_1) 
  \end{array}
\right]
\label{eq:m1I}
\end{equation}
The bounds derived in Theorem \ref{thm:sharpII} by imposing assumption \ref{assump:misclassification} (iii) are
\begin{equation*}
  \mu_k(\alpha_0) - \underline{\mu}_{tk}\big( \underline{q}_{tk}(\alpha_0, \alpha_1) \big) \geq 0, \quad \quad
  \overline{\mu}_{tk}\big( \overline{q}_{tk}(\alpha_0, \alpha_1) \big) - \mu_k(\alpha_0) \geq 0, \quad \mbox{for all } t,k
\end{equation*}
where $\mu_k, \underline{\mu}_{tk}, \overline{\mu}_{tk}, \underline{q}_{tk}$ and $\overline{q}_{tk}$ are defined in the statement of the Theorem.
Expressing these as unconditional moment inequalities, we have
\begin{equation}
  \mathbb{E}[m_2^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})] \geq \mathbf{0}, \quad 
  m_2^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) \equiv \left[
  \begin{array}{c}
    m_{2,00}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})  \\ 
    m_{2,10}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) \\
    m_{2,01}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})  \\ 
    m_{2,11}^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) 
  \end{array}
  \label{eq:m2I}
\right] 
\end{equation}
where $\mathbf{q} \equiv ( \underline{q}_{00},\, \overline{q}_{00},\, \underline{q}_{10}, \,\overline{q}_{10},\, \underline{q}_{01}, \,\overline{q}_{01},\, \underline{q}_{11},\, \overline{q}_{11})$ and we define
\begin{align}
  m_{2,0k}^I\big(\mathbf{w}_i, \boldsymbol{\vartheta}, \mathbf{q}) \equiv \left[
  \begin{array}{r}
    y_i \mathbf{1}\left( z_i=k \right)\left\{(T_i - \alpha_0) - \mathbf{1}(y_i \leq \underline{q}_{0k})  (1 - T_i)\left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right)\right\} \\
    - y_i \mathbf{1}(z_i=k) \left\{ (T_i - \alpha_0) -  \mathbf{1}(y_i > \overline{q}_{0k}) (1 - T_i) \left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right) \right\} 
\end{array}
\right] \label{eq:m2I_0k} \\\nonumber \\
  m_{2,1k}^I(\mathbf{w}_i, \boldsymbol{\vartheta}, \mathbf{q}) \equiv \left[
  \begin{array}{r}
    y_i \mathbf{1}\left( z_i=k \right)\left\{(T_i - \alpha_0) - \mathbf{1}(y_i \leq \underline{q}_{1k})  T_i\left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right)\right\} \\
    - y_i \mathbf{1}(z_i=k) \left\{ (T_i - \alpha_0) -  \mathbf{1}(y_i > \overline{q}_{1k}) T_i \left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right) \right\} \label{eq:m2I_1k}
\end{array}
\right].
\end{align}
Finally we define $m^I = (m_1^{I'}, m_2^{I'})'$.
Notice that the second set of inequalities, $m_2^I$, depends on the unknown parameter $\mathbf{q}$ which is in turn a function of $(\alpha_0, \alpha_1)$.
In the next section we discuss how $\mathbf{q}$ can be estimated under a given null hypothesis for $(\alpha_0, \alpha_1)$. 


\subsection{Accounting for Preliminary Estimation}
\label{sec:prelim}


Let $\boldsymbol{\vartheta} = (\alpha_0, \alpha_1)$ and $\boldsymbol{\gamma} = (\boldsymbol{\kappa}, \theta_1)$ where $\theta_1$ is defined in Equation \ref{eq:theta1_def} and $\boldsymbol{\kappa}$ in Section \ref{sec:problem}.
Our moment conditions take the form 
\begin{equation}
  \mathbb{E}[m^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \mathbf{q}_0)] \geq \mathbf{0}, \quad 
  \mathbb{E}[m^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)] = \mathbf{0}
  \label{eq:MCs_ineq_eq}
\end{equation}
where $m^I = (m_1^{I'}, m_2^{I'})'$, defined in Section \ref{sec:inequalities}, and 
\begin{equation}
  m^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)= \left[
  \begin{array}{c}
 \left\{ \boldsymbol{\psi}_2'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \kappa_2 \right\}z_i \\
 \left\{ \boldsymbol{\psi}_3'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \kappa_3 \right\}z_i 
  \end{array}
\right].
\label{eq:mE}
\end{equation}
Notice that we now write $\boldsymbol{\psi}_2$ and $\boldsymbol{\psi}_3$, defined in Equation \ref{eq:psi_def}, as explicit functions of $(\theta_1, \alpha_0, \alpha_1)$, using the definitions of $(\theta_2, \theta_3)$ from Equations \ref{eq:theta2_def}--\ref{eq:theta3_def}.
To construct a GMS test of the null hypothesis $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta_0}$ based on Equation \ref{eq:MCs_ineq_eq}, we require preliminary estimators $\widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ and $\widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$ that are consistent and asymptotically normal \emph{under the null}.
We now provide full details of the construction and derive the associated adjustment to the asymptotic variance matrix.

Consider first the equality moment conditions $m^E$.
For these we require preliminary estimators of $\theta_1$, $\kappa_2$, and $\kappa_3$.
Recall that $\theta_1$ is simply the IV estimand: it can be consistently estimated directly from observations of $(y,T,z)$ without knowledge of $\alpha_0$ or $\alpha_1$.
Note, moreover, from Equation \ref{eq:MCs_endog} that $\boldsymbol{\kappa}$ is simply a vector of \emph{intercepts}.
These can be directly estimated from observations of $\mathbf{w}$ because $\boldsymbol{\Psi}(\theta_1, \alpha_0, \alpha_1)$ is consistently estimable under the null $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$: the hypothesis specifies $\alpha_0$ and $\alpha_1$ and IV provides a consistent estimator of $\theta_1$.
Accordingly, define 
\begin{equation}
  h^E(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\gamma})  =  \left[
 \begin{array}{l}
   \boldsymbol{\Psi}'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \boldsymbol{\kappa}\\
   \left\{\boldsymbol{\psi}_1'(\theta_1)\mathbf{w}_i - \kappa_1\right\}z_i
 \end{array}
 \right].
 \label{eq:hE}
\end{equation}
Under $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$, the  just-identified GMM-estimator based on $\mathbb{E}[h^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)] = \mathbf{0}$ yields a consistent and asymptotically normal estimator of $\boldsymbol{\gamma}_0$ under the usual regularity conditions.

Now consider the inequality moment conditions $m^I$. 
From Section \ref{sec:inequalities} we see that $m_2^I$ depends on $\mathbf{q}$,
the vector of conditional quantiles $\overline{q}_{tk}$ and $\underline{q}_{tk}$ defined in Theorem \ref{thm:sharpII}.
Under the assumption that $y$ follows a continuous distribution, as maintained in Theorem \ref{thm:sharpII}, these can be expressed as conditional moment equalities as follows:
\begin{align}
  &\mathbb{E}\left[ \mathbf{1}(y \leq \underline{q}_{tk}) |T=t,z=k \right] - r_{tk}(\alpha_0, \alpha_1) = 0 \label{eq:qunder}\\
  &\mathbb{E}\left[ \mathbf{1}(y \leq \overline{q}_{tk}) |T=t,z=k \right] - \big(1 - r_{tk}(\alpha_0, \alpha_1)\big) = 0 \label{eq:qover}
\end{align}
where $r_{tk}$ is defined in Theorem \ref{thm:sharpII} and $t,k = 0,1$.
Under $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$, a consistent estimator $\widehat{r}_{tk}$ of $r_{tk}$ can be obtained directly from $\widehat{p}_k$, the sample analogue of $p_k$ based on iid observations of $\mathbf{w}_i$.
In turn, the $(\widehat{r}_{tk})$\textsuperscript{th} and $(1 - \widehat{r}_{tk})$\textsuperscript{th} sample conditional quantiles of $y$ provide consistent estimates of $\underline{q}_{tk}$ and $\overline{q}_{tk}$.\footnote{Consistency of the sample quantiles requires $0 < r_{tk} < 1$. If $r_{tk} = 0$ or $1$ for some $(t,k)$, however, then the associated moment inequality is trivially satisfied and we no longer require estimates of $\underline{q}_{tk}, \overline{q}_{tk}$.\label{foot:quantile}}
Collecting these for all $(t,k)$ gives $\widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$.
Now, define
\begin{equation}
h^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) = 
  \left[
  \begin{array}{c}
    h^I_{0}(\mathbf{w}, \boldsymbol{\vartheta}, \mathbf{q})\\
    h^I_{1}(\mathbf{w}, \boldsymbol{\vartheta}, \mathbf{q})
  \end{array}
\right] 
\end{equation}
where
\begin{equation}
  h_k^I(\mathbf{w}_i,\boldsymbol{\vartheta},\mathbf{q}) = \left[
  \begin{array}{l}
    \mathbf{1}(y_i \leq \underline{q}_{0k}) \mathbf{1}(z_i=k)(1 - T_i) 
    - \left( \frac{\alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(T_i-\alpha_0)\\ 
    \mathbf{1}(y_i \leq \overline{q}_{0k}) \mathbf{1}(z_i=k)(1 - T_i)
    - \left( \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(1 - T_i-\alpha_1)\\
    \mathbf{1}(y_i \leq \underline{q}_{1k}) \mathbf{1}(z_i=k)T_i
    - \left( \frac{1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(T_i-\alpha_0)\\ 
    \mathbf{1}(y_i \leq \overline{q}_{1k}) \mathbf{1}(z_i=k)T_i 
    - \left( \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(1 - T_i-\alpha_1)
  \end{array}
  \label{eq:hkI}
\right].
\end{equation}
Equation \ref{eq:hkI} gives the unconditional version of Equations \ref{eq:qunder}--\ref{eq:qover}.
Now, under the null $\widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$ converges in probability to $\mathbf{q}_0$, which satisfies the just-identified collection of moment equalities $\mathbb{E}[h^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0,\mathbf{q}_0)] = \mathbf{0}$.
Although $h^I$ is a discontinuous function of $\mathbf{q}$, it is bounded for any fixed $(\alpha_0, \alpha_1)$.
Moreover, since $y|(T=t,z=k)$ is a continuous random variable, $\mathbb{E}[h^I(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q})]$ is continuously differentiable with respect to $\mathbf{q}$.
Hence, $\widehat{\mathbf{q}}$ is asymptotically normal under mild regularity conditions.\footnote{For details, see \cite{andrews1994empirical} and \cite{newey1994large} Section 7.}
To account for the effect of preliminary estimation of $\mathbf{q}$ and $\boldsymbol{\gamma}$ on the asymptotic variance matrix used in the GMS test, we rely on the following Lemma.

\begin{lem}
  Let $\bar{m}^I_{1,n}(\boldsymbol{\vartheta}) = n^{-1}\sum_{i=1}^n m_{1,n}^I(\mathbf{w}_i,\boldsymbol{\vartheta})$ and define $\bar{m}_{2,n}^I, \bar{m}_n^E, \bar{h}_n^I,\bar{h}_n^E$ analogously.
  Further let $\widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0) = \arg \min_{\boldsymbol{\gamma}} \lVert \bar{h}_n^E(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma}) \rVert$ and 
    $\lVert h_n^I(\boldsymbol{\vartheta}_0, \widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0))\rVert \leq \inf_{\mathbf{q}} \lVert h_n^I(\boldsymbol{\vartheta}_0, \mathbf{q})\rVert + o_p(1)$.
Then, under standard regularity conditions
  \[
    \sqrt{n}\left[
    \begin{array}{l}
      \bar{m}_{1,n}^I\left( \boldsymbol{\vartheta}_0 \right)\\
      \bar{m}_{2,n}^I\big( \boldsymbol{\vartheta}_0, \widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)\big)\\
      \bar{m}_{n}^E\big( \boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0) \big)
    \end{array}
  \right] \rightarrow_p 
  \left[
  \begin{array}{ccccc}
    \mathbf{I}&\mathbf{0}&\mathbf{0} & \mathbf{0} & \mathbf{0} \\
    \mathbf{0}&\mathbf{I}&\mathbf{0} & B^I( \boldsymbol{\vartheta}_0, \boldsymbol{q}_0)& \mathbf{0} \\
    \mathbf{0}&\mathbf{0}&\mathbf{I} &\mathbf{0} & B^E(\boldsymbol{\vartheta}_0,\boldsymbol{\gamma}_0)\\
  \end{array}
\right] 
    \sqrt{n}\left[
    \begin{array}{l}
      \bar{m}_{1,n}^I\left( \boldsymbol{\vartheta}_0 \right)\\
      \bar{m}_{2,n}^I\left(\boldsymbol{\vartheta}_0,  \mathbf{q}_0\right)\\
      \bar{m}_n^E\left(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0  \right)\\
      \bar{h}^I_n\left(\boldsymbol{\vartheta}_0,  \mathbf{q}_0 \right)\\
      \bar{h}^E_n\left(\boldsymbol{\vartheta}_0,  \boldsymbol{\gamma}_0 \right)
    \end{array}
  \right] 
  \]
  where we define $B^I( \boldsymbol{\vartheta}, \boldsymbol{q}) = (1 - \alpha_0 - \alpha_1) \left[\mbox{diag}(\mathbf{a})  \right]^{-1} \mathbf{q}$ and
$B^E(\boldsymbol{\vartheta},\boldsymbol{\gamma}) = -M^E (H^E)^{-1}$
  with $\mathbf{a}' = (\alpha_1, \alpha_1, 1 - \alpha_1, 1 - \alpha_1, \alpha_1, \alpha_1, 1 - \alpha_1, 1 - \alpha_1)$ 
and 
\[
  M^E = \left[ 
    \begin{array}{cccc} 
      0 & -\mathbb{E}[z_i] & 0 & \left( \frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1} \right)' \mathbb{E}[\mathbf{w}_iz_i]\\ 
      0 & 0 & -\mathbb{E}[z_i] & \left( \frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1} \right)' \mathbb{E}[\mathbf{w}_iz_i] 
  \end{array} \right], \,
H^E = \left[
\begin{array}{cccc}
  -1 & 0 & 0 
  & \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}_i]\\
   0 & -1 & 0 &  \left(\frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}_i]\\
   0 & 0 & -1 & \left(\frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}_i]\\
   -\mathbb{E}[z_i] & 0 & 0 &  \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}_iz_i]
\end{array}
\right].
\]
\label{lem:prelim}
\end{lem}

Lemma \ref{lem:prelim} relates the sample analogues $\bar{m}_{2,n}^I$ and $\bar{m}_n^E$ evaluated at the preliminary estimators $\widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$ and $\widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ to their counterparts evaluated at the true parameter values $\mathbf{q}_0$ and $\boldsymbol{\gamma}_0$.
The estimator $\widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ exactly solves $h_n^E(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma})=0$ while $\widehat{q}(\boldsymbol{\vartheta}_0)$, constructed as described immediately before the statement of the Lemma, \emph{approximately} solves $\bar{h}_n^I(\boldsymbol{\vartheta}_0, \mathbf{q})=0$. 
%This relationship is determined by the sample analogues of the auxiliary moments, $\bar{h}_n^I$ and $\bar{h}_n^E$, via the matrices $B^I$ and $B^E$ defined in the Lemma.
A few lines of matrix algebra 
%using the definitions of $(\boldsymbol{\psi}_1,\boldsymbol{\psi}_2,\boldsymbol{\psi}_3)$ from Equation \ref{eq:psi_def} and $\mathbf{w}_i$ from Section \ref{sec:problem} 
show that the determinant of $H^E$ equals $\mbox{Cov}(z,T)$.
Hence, $B^E$ is well-defined if $z$ is a relevant instrument.
The matrix $B^I$ is likewise well-defined provided that $\alpha_1\neq 0$ and the elements of $\mathbf{q}_0$ are computed for probabilities strictly between zero and one.
If either of these conditions fails, however, some of the moment inequalities in $m_2^I$ are trivially satisfied and can be dropped (see Footnote \ref{foot:quantile}).
After removing the corresponding elements of $\mathbf{q}_0$ and $\mathbf{a}$, $B^I$ becomes well-defined.
The regularity conditions required for Lemma \ref{lem:prelim} are mild.
The result relies on a number of mean-value expansions: $\bar{h}_n^E(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)$ and $\bar{m}_n^E(\boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)$ are expanded around $\boldsymbol{\gamma} = \widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ while $\mathbb{E}[h^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \mathbf{q}_0)]$ and $\mathbb{E}[m^I_2(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \mathbf{q}_0)]$ are expanded around $\mathbf{q} = \widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$.
These expansions, in turn, rely on the fact that $\mathbf{q}$ and $\boldsymbol{\gamma}$ are interior to their respective parameter spaces and the relevant functions are all continuously differentiable in our example.


We now have all the ingredients required to construct an asymptotic variance matrix for the GMS test that accounts for preliminary estimation of $\boldsymbol{\gamma}$ and $\mathbf{q}$. 
Let $m' = (m_1^{I'}, m_{2}^{I'}, m^{E'})$, $h' = (h^{I'}, h^{E'})$, and define the shorthand $\boldsymbol{\tau}_0' = (\boldsymbol{\gamma}_0', \mathbf{q}_0')$ and 
$\widehat{\boldsymbol{\tau}}_0' = \big( \widehat{\boldsymbol{\gamma}}'(\boldsymbol{\vartheta}_0), \widehat{\mathbf{q}}'(\boldsymbol{\vartheta}_0) \big)$.
Given a collection of iid observations $(\mathbf{w}_1, \hdots, \mathbf{w}_n)$, we have
\begin{equation}
  \sqrt{n}\left[
  \begin{array}{c}
    \bar{m}_n(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\\
    \bar{h}_n(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)
  \end{array}
\right] \rightarrow_d N\big(\mathbf{0},  \mathcal{V}(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\big), \quad
  \mathcal{V}(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0) = \mbox{Var}\left[
  \begin{array}{c}
  m(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0) \\
  h(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\\
  \end{array}
\right]
\label{eq:clt}
\end{equation}
under $H_0\colon \boldsymbol{\vartheta} = \boldsymbol{\vartheta}_0$, by an appropriate central limit theorem.
What we require for the test, however, is the asymptotic variance matrix of $\sqrt{n}\, \bar{m}_n(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)$.
Combining Equation \ref{eq:clt} with Lemma \ref{lem:prelim}, we obtain 
\begin{equation}
  \mbox{Avar}\big(\sqrt{n}\, \bar{m}_n(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)\big) = \Xi(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\, \mathcal{V}(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)\, \Xi'(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)
\end{equation}
with
\begin{equation}
  \Xi(\boldsymbol{\vartheta}, \boldsymbol{\tau}) = \left[
  \begin{array}{cc}
    \mathbf{I} & B(\boldsymbol{\vartheta}, \boldsymbol{\tau})
  \end{array}
\right], \quad
  B(\boldsymbol{\vartheta}, \boldsymbol{\tau})= \left[
\begin{array}{cc}
  \mathbf{0} & \mathbf{0}\\
  B^I(\boldsymbol{\vartheta}, \mathbf{q}) & \mathbf{0}\\
  \mathbf{0} & B^E(\boldsymbol{\vartheta}, \boldsymbol{\gamma})
\end{array}
\right]
\label{eq:Xi}
\end{equation}
where $B^I(\cdot, \cdot)$ and $B^E(\cdot, \cdot)$ are defined in Lemma \ref{lem:prelim}.
Finally, we construct a consistent estimator $\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0)$ of the asymptotic variance matrix of $\sqrt{n}\, \bar{m}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)$ under the null:
\begin{equation}
  \widehat{\Sigma}_n(\boldsymbol{\vartheta}_0) \equiv \Xi(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)\, \widehat{\mathcal{V}}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}})\,\Xi'(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)
  \label{eq:AvarEst}
\end{equation}
where
\begin{equation}
  \widehat{\mathcal{V}}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) \equiv \frac{1}{n} \sum_{i=1}^n
\left[
\begin{array}{c}
  m(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\tau}) - \bar{m}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) \\
  h(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\tau}) -  \bar{h}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) 
\end{array}
\right]
\left[
\begin{array}{c}
  m(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\tau}) - \bar{m}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) \\
  h(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\tau}) -  \bar{h}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) 
\end{array}
\right]'.
\label{eq:fullVar}
\end{equation}
In the following section we provide a step-by-step description of our inference procedure.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Details of the Inference Procedure}
\label{sec:details}
In this section we provide full details of our Bonferroni-based inference procedure.
We begin by defining some notation.
Let $J$ denote the total number of inequality moment conditions, $K$ denote the total number of equality moment conditions, and define
\begin{equation}
\bar{m}_n(\boldsymbol{\vartheta}, \boldsymbol{\tau}) = \left[
\begin{array}{c}
  \bar{m}_n^I(\boldsymbol{\vartheta}, \mathbf{q}) \\
  \bar{m}_n^E(\boldsymbol{\vartheta}, \boldsymbol{\gamma}) 
\end{array}
\right] = 
\left[
\begin{array}{l}
  \bar{m}_{n,1}^I(\boldsymbol{\vartheta})\\
  \bar{m}_{n,2}^I(\boldsymbol{\vartheta}, \mathbf{q}) \\
  \bar{m}_n^E(\boldsymbol{\vartheta}, \boldsymbol{\gamma}) 
\end{array}
\right] = 
\frac{1}{n} \sum_{i=1}^n
\left[\begin{array}{l}
    m^I_1(\mathbf{w}_i,\boldsymbol{\vartheta})\\
    m^I_2(\mathbf{w}_i,\boldsymbol{\vartheta}, \mathbf{q}) \\
    m^E(\mathbf{w}_i,\boldsymbol{\vartheta}, \boldsymbol{\gamma}) 
\end{array}\right]
\label{eq:mbar_def}
\end{equation}
with $m_1^I$ as defined in Equation \ref{eq:m1I}, $m_2^I$ in Equations \ref{eq:m2I}--\ref{eq:m2I_1k} and $m^E$ in Equation \ref{eq:mE}.\footnote{In our problem $K=2$ and $J$ is at most 12. Under certain nulls for $(\alpha_0, \alpha_1)$, however, we drop components of $m^I_2$ as they are trivially satisfied. See footnote \ref{foot:quantile} and Section \ref{sec:prelim} for further discussion.}
Now let $S$ be the function
\begin{equation}
  S(\mathbf{x}, \mathbf{y}) = \sum_{j}\min\left\{ 0, 
  x_j^2 \right\} + \mathbf{y}'\mathbf{y} 
    \label{eq:MMM}
\end{equation}
where $\mathbf{x}, \mathbf{y}$ are two arbitrary real vectors.
This function will be used to calculate the modified method of moments (MMM) test statistic as part of the GMS test below.
The argument $\mathbf{x}$ stands in for the moment inequalities, which only contribute to the test statistic when they are violated, i.e.\ take on a \emph{negative} value.
Using this notation, we now detail the first step of our inference procedure: a GMS test for $\boldsymbol{\vartheta} = (\alpha_0, \alpha_1)$ with preliminary estimation of $\mathbf{q}$ and $\boldsymbol{\gamma}$ under the null.


\begin{alg}[GMS Test for $\alpha_0$ and $\alpha_1$ ]
  \mbox{} \\
    Inputs: hypothesis $\boldsymbol{\vartheta}_0$; 
    iid dataset $\{\mathbf{w}_i\}_{i=1}^n$;
    simulations $\{\boldsymbol{\zeta}^{(r)}\}_{r=1}^R \sim \mbox{ iid } N_{J+K}(\mathbf{0}, \mathbf{I})$.
  \begin{enumerate}
    \item Calculate the variance matrix estimator $\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0)$.
      \begin{enumerate}[(i)]
        \item Calculate $\widehat{\boldsymbol{\tau}}_0 = (\widehat{\mathbf{q}}_0', \widehat{\boldsymbol{\gamma}}_0')'$ where $\widehat{\boldsymbol{\gamma}}_0 =  \widehat{\boldsymbol{\gamma}}(\boldsymbol{\vartheta}_0)$ and $\widehat{\mathbf{q}}_0 =  \widehat{\mathbf{q}}(\boldsymbol{\vartheta}_0)$ from Section \ref{sec:prelim}.
        \item Calculate $\Xi(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)$ using Equation \ref{eq:Xi}.
        \item Calculate $\widehat{\mathcal{V}}_n(\boldsymbol{\vartheta}_0,\widehat{\boldsymbol{\tau}}_0)$ using Equation \ref{eq:fullVar}.
        \item Set $\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0) = \Xi(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)\, \widehat{\mathcal{V}}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}})\,\Xi'(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)$.
      \end{enumerate}
    \item Calculate the test statistic $T_n(\boldsymbol{\vartheta}_0)$.
      \begin{enumerate}[(i)]
        \item Calculate $\sqrt{n}\, \bar{m}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0)$ using Equation \ref{eq:mbar_def}.
        \item Set $\boldsymbol{\nu}_n(\boldsymbol{\vartheta}_0) = \left[ \mbox{diag}\left\{\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0)\right\} \right]^{-1/2} \left[ \sqrt{n}\,\bar{m}_n(\boldsymbol{\vartheta}_0, \widehat{\boldsymbol{\tau}}_0) \right]$.
        \item Let $\boldsymbol{\nu}^I_n(\boldsymbol{\vartheta}_0)$ denote the first $J$ elements of $\boldsymbol{\nu}_n$ and  $\boldsymbol{\nu}_n^E(\boldsymbol{\vartheta}_0)$ the last $K$ elements.   
        \item Set $T_n(\boldsymbol{\vartheta}_0) = S\big( \boldsymbol{\nu}_n^I(\boldsymbol{\vartheta}_0),\,\boldsymbol{\nu}_n^E(\boldsymbol{\vartheta}_0) \big)$ using Equation \ref{eq:MMM}.
  \end{enumerate}
\item Construct the moment selection matrix $\Phi$.
      \begin{enumerate}[(i)]
        \item For $j = 1, \hdots, J$ set $\varphi_j^I = \mathbf{1}\left\{\nu_{n,j}^I(\boldsymbol{\vartheta}_0) \leq \sqrt{\log n} \right\}$ and let  $\widetilde{J} = \sum_{j=1}^J \varphi_j^I$.
        \item For $j = 1, \hdots, K$ set $\varphi_j^E = 1$.
        \item Set $\boldsymbol{\varphi} = (\varphi_{1}^I,\hdots, \varphi_{J}^I,\varphi_1^E,\hdots, \varphi_K^E)'$.
        \item Let $\Phi$ be the $(\widetilde{J}+K)\times(J+K)$ of zeros and ones that selects those elements $x_j$ of an arbitrary vector $\mathbf{x}$ that correspond to $\varphi_j = 1$. 
      \end{enumerate}
    \item Simulate the sampling distribution of $T_n(\boldsymbol{\vartheta}_0)$ under the null. 
      \begin{enumerate}[(i)]
        \item Let $\widehat{\Omega}$ be the correlation matrix that corresponds to $\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0)$.
        \item For each $r = 1, \hdots, R$ set $\boldsymbol{\xi}^{(r)} = \left[\Phi\,\widehat{\Omega}\,\Phi'\right]^{1/2}\,\Phi\boldsymbol{\zeta}^{(r)}$.
        \item Let $\boldsymbol{\xi}^{(r)}_I$ denote the first $\widetilde{J}$ and $\boldsymbol{\xi}_E^{(r)}$ the last $K$ elements of $\boldsymbol{\xi}^{(r)}$.  
        \item For each $r = 1, \hdots, R$ set $T^{(r)}_n(\boldsymbol{\vartheta}_0) = S\big( \boldsymbol{\xi}_{I}^{(r)}, \, \boldsymbol{\xi}^{(r)}_E\big)$ using Equation \ref{eq:MMM}.
      \end{enumerate}
    \item Calculate the p-value of the test: $\displaystyle \widehat{p}(\boldsymbol{\vartheta}_0) = \frac{1}{R}\sum_{r=1}^R \mathbf{1}\left\{T_{n}^{(r)}(\boldsymbol{\vartheta}_0) > T_n(\boldsymbol{\vartheta}_0)\right\}$.
  \end{enumerate}
  \label{alg:GMS}
\end{alg}

Algorithm \ref{alg:GMS} corresponds to the asymptotic version of the GMS test from \cite{AndrewsSoares}, based on the MMM test statistic -- $S_1$ in \cite{AndrewsSoares} -- and the ``BIC choice'' $\kappa_n = \sqrt{\log n}$ for the sequence of constants $\kappa_n$ used for moment selection.   
The procedure is as follows.
In Step 1, we compute a consistent estimator of the asymptotic variance matrix of the full set of moment conditions, under the null, accounting for preliminary estimation of $\mathbf{q}$ and $\boldsymbol{\gamma}$ as explained in Section \ref{sec:prelim}.
In step 2, we calculate the observed value of the MMM test statistic.
Note that this test statistic uses only the diagonal elements of $\widehat{\Sigma}_n(\boldsymbol{\vartheta}_0)$.
Moreover, the moment inequalities only contribute to $T_n$ if they are violated, i.e.\ if they take on a negative value.
In step 3 we determine which moment inequalities are ``far from binding,'' defined as having a t-ratio greater than $\sqrt{\log n}$.
These moment inequalities will be excluded when approximating the large-sample distribution of the test statistic.
The matrix $\Phi$ encodes the results of the moment selection step.
Pre--multiplying a $(J + K)$--vector $\mathbf{x}$ by $\Phi$ results in a $(\widetilde{J}\times K)$--vector $\widetilde{\mathbf{x}}$ whose last $K$ elements match the last $K$ elements of $\mathbf{x}$ but whose first $\widetilde{J}$ elements contain the subset of $(x_1, \dots, x_J)$ whose indices match those of the moment inequalities with t-ratios less than or equal to $\sqrt{\log n}$, i.e.\ those that are \emph{not} far from binding.\footnote{Although this does not affect the results of the procedure, notice that Algorithm \ref{alg:GMS} carries out moment selection in a slightly different way from the steps given by \cite{AndrewsSoares}. In particular, before carrying out any further calculations, we \emph{subset} the correlation matrix $\widehat{\Omega}$ and normal vectors $\boldsymbol{\zeta}^{(r)}$ to remove elements corresponding to moment inequalities deemed far from binding. In contrast, \cite{AndrewsSoares} carry along the full set of inequalities throughout, but add $+\infty$ to the appropriate elements when computing $T_n^{(r)}$ to ensure that only the moment inequalities that are not far from binding affect the results. Although it requires more notation to describe, sub-setting is substantially faster, as it avoids carrying out computations for inequalities that cannot affect the result.}
Step 4 uses a collection of iid normal draws, $\{\boldsymbol{\zeta}^{(r)}\}_{r=1}^{R}$, to approximate the large-sample distribution of $T_n$ under the null.
The appropriate multiplications by $\Phi$ ensure that this approximation includes all moment equalities, but excludes any moment inequality judged to be far from binding in step 3.
Finally, step 5 computes the p-value of the test by comparing the actual test statistic $T_n(\boldsymbol{\vartheta}_0)$ to the collection of simulated test statistics $\{T_n^{(r)}(\boldsymbol{\vartheta}_0)\}_{r=1}^R$ from step 4.
We now detail our Bonferroni-based confidence interval for $\beta$.

\begin{alg}[Bonferroni-based Confidence Interval for $\beta$] \mbox{}\\
    Inputs: significance levels $(\delta_1,\delta_2)$; 
    iid dataset $\{\mathbf{w}_i\}_{i=1}^n$;
    simulations $\{\boldsymbol{\zeta}^{(r)}\}_{r=1}^R \sim \mbox{ iid } N_{J+K}(\mathbf{0}, \mathbf{I})$.
    \begin{enumerate}
    \item Construct a $(1 - \delta_1)\times 100\%$ joint confidence set $\mathcal{C}(\delta_1)$ for $\boldsymbol{\vartheta} = (\alpha_0, \alpha_1)'$. 
      \begin{enumerate}[(i)]
        \item Let $\Lambda_N = \left\{0, \frac{1}{N}, \frac{2}{N}, \hdots, \frac{N-2}{N}, \frac{N-1}{N}\right\}$ where $N>1$ is a natural number. 
        \item Set $\Delta_N = \left\{(\alpha_0, \alpha_1) \in (\Lambda_N \times \Lambda_N) \colon \alpha_0 + \alpha_1 < 1 \right\}$.
        \item For each $\boldsymbol{\vartheta} \in \Delta_N$ calculate $\widehat{p}(\boldsymbol{\vartheta})$ using Algorithm \ref{alg:GMS}, holding $\{ \boldsymbol{\zeta}^{(r)} \}_{r=1}^R$ \emph{fixed}. 
        \item Set $\mathcal{C}(\delta_1) = \left\{ \boldsymbol{\vartheta} \in \Delta_N \colon \widehat{p}(\boldsymbol{\vartheta}) \geq \delta_1\right\}$.
      \end{enumerate}
    \item Construct a $(1 - \delta_1)\times 100\%$ confidence interval $\left[\underline{s}(\delta_1), \overline{s}(\delta_1) \right]$ for $s \equiv (1 - \alpha_0 - \alpha_1)$.
      \begin{enumerate}[(i)]
        \item Set $\underline{s}(\delta_1) = \min \left\{ (1 - \alpha_0 - \alpha_1) \colon (\alpha_0, \alpha_1) \in \mathcal{C}(\delta_1) \right\}$.
        \item Set $\overline{s}(\delta_1) = \max \left\{ (1 - \alpha_0 - \alpha_1) \colon (\alpha_0, \alpha_1) \in \mathcal{C}(\delta_1) \right\}$.
      \end{enumerate}
    \item Construct a $(1 - \delta_2)\times 100\%$ confidence interval $\left[\underline{\theta}_1(\delta_2), \overline{\theta}_1(\delta_2)\right]$ for $\theta_1$.
      \begin{enumerate}[(i)]
        \item Use the standard IV interval from a regression of $y$ on $T$ with instrument $z$.
      \end{enumerate}
    \item Construct the $(1 - \delta)\times 100\%$ Bonferroni-based confidence interval $\left[ \underline{\beta}(\delta), \overline{\beta}(\delta) \right]$ for $\beta$.
      \begin{enumerate}[(i)]
        \item Let $\delta = \delta_1 + \delta_2$.
        \item Set $\underline{\beta}(\delta) = \min \left\{ \underline{s}(\delta_1)\times \underline{\theta}_1(\delta_2),\, \overline{s}(\delta_1)\times \underline{\theta}_1(\delta_2) \right\}$.
        \item Set $\overline{\beta}(\delta) = \min \left\{ \underline{s}(\delta_1)\times \overline{\theta}_1(\delta_2),\, \overline{s}(\delta_1)\times \overline{\theta}_1(\delta_2) \right\}$.
      \end{enumerate}
  \end{enumerate}
  \label{alg:Bonferroni}
\end{alg}

Step 1 of Algorithm \ref{alg:Bonferroni} constructs a $(1 - \delta_1)\times 100\%$ joint confidence set $\mathcal{C}(\delta_1)$ for $\boldsymbol{\vartheta} = (\alpha_0, \alpha_1)$ by inverting the GMS test from Algorithm \ref{alg:GMS} over a discretized parameter space $\Delta_N$.
Because the parameter space for $(\alpha_0, \alpha_1)$ is bounded, this is computationally straightforward.
Note that the \emph{same} normal draws $\{\boldsymbol{\zeta}^{(r)}\}_{r=1}^R$ are used to test each null hypothesis contained in $\Delta_N$.
Step 2 projects $\mathcal{C}(\delta_1)$ to yield a $(1 - \delta_1)\times 100\%$ confidence interval for $s \equiv (1 - \alpha_0 - \alpha_1)$, simply taking the maximum and minimum values of $s$ in the discrete set  $\mathcal{C}(\delta_1)$.
Step 3 constructs the usual IV confidence interval for the reduced form parameter $\theta_1$, and step 4 combines the results of steps 2--3 with Bonferroni's inequality to yield a $(1 - \delta_1 - \delta_2) \times 100\%$ confidence interval for $\beta$.
For some discussion of alternatives to Algorithm \ref{alg:Bonferroni}, see Footnote \ref{foot:alternatives}.
Notice that, by construction, the Bonferroni interval for $\beta$ excludes zero if and only if the confidence interval for $\theta_1$ from step 3 of Algorithm \ref{alg:Bonferroni} excludes zero.
Under mild regularity conditions, the confidence interval from Algorithm \ref{alg:Bonferroni} is uniformly asymptotically valid.

\begin{thm}
  Let $\mathbf{w}_1, \hdots, \mathbf{w}_n$ be an iid collection of observations satisfying the conditions of Theorems \ref{thm:sharpII} and \ref{thm:main_ident}, and let $z$ be a strong instrument.
  Then, under standard regularity conditions, the confidence interval for $\beta$ from Algorithm \ref{alg:Bonferroni} has asymptotic coverage probability no less than $1 - (\delta_1 + \delta_2)$ as $R,N,n\rightarrow \infty$ uniformly over the parameter space.
  \label{thm:uniform}
\end{thm}

Theorem \ref{thm:uniform} is effectively a corollary of Theorem 1 from \cite{AndrewsSoares}, which establishes the uniform asymptotic validity of the GMS test, and Lemma \ref{lem:prelim}, which accounts for preliminary estimation of $\boldsymbol{\gamma}$ and $\mathbf{q}$.
Given iid observations $\mathbf{w}_i$, the only substantive condition required for Theorem \ref{thm:uniform} is the joint asymptotic normality of $\sqrt{n} \,\bar{m}_n(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)$ and $\sqrt{n}\bar{h}_n(\boldsymbol{\vartheta}_0, \boldsymbol{\tau}_0)$, where $\bar{h}_n$ denotes the sample analogues for full set of auxiliary moment conditions $(h^I, h^E)$ defined in Section \ref{sec:prelim}.
For further discussion of the regularity conditions required for the GMS procedure, see Appendix A3 of \cite{AndrewsSoares}.
For some discussion of the regularity conditions required for Lemma \ref{lem:prelim}, see Section \ref{sec:prelim}.

Theorem \ref{thm:uniform} invokes the higher-moment assumptions (Assumptions \ref{assump:2ndMoment}--\ref{assump:3rdMoment}) under which we establish global identification of $\beta$ in Theorem \ref{thm:main_ident}, and Algorithm \ref{alg:GMS} likewise incorporates the higher-moment equality conditions that arise from this result.
To proceed without these conditions, simply remove $m^E$ from the set of moment conditions used in the algorithm and leave the steps unchanged.
In this case $\beta$ is no longer point identified but the inference procedure provides valid inference for the points in the sharp identified set from Theorem \ref{thm:sharpII}.
Algorithm \ref{alg:Bonferroni} can likewise be used in the case of an exogenous $T^*$, as in \cite{Mahajan} and \cite{FL}.
As mentioned above in Section \ref{sec:problem}, the exogenous regressor case is subject to the same inferential difficulties and the endogenous case on which we focuse in this paper.
To accommodate an exogenous regressor,simply replace $m^E$ with the moment equalities described in Appendix \ref{sec:FL_mahajan}.


\subsection{Further Details Regarding Covariates}
\label{sec:covariates}

The inference procedure described in the preceding sections holds $\mathbf{x}$ fixed, and is thus appropriate for examples with discrete covariates.
To accommodate covariates more generally, there are several possible approaches.
At one extreme, suppose one were willing to assume that $(\alpha_0, \alpha_1)$ did not vary with $\mathbf{x}$ and that $y = c + \beta T^* + \mathbf{x}'\boldsymbol{\phi} + \varepsilon$, as in \cite{FL}.
In this case, the standard IV estimator identifies $\boldsymbol{\phi}$ and one could simply augment the moment equalities $m^E$ from Equation \ref{eq:mE} above to provide a preliminary estimator of $\boldsymbol{\phi}$ in Algorithm \ref{alg:GMS}.
At the other extreme, if one wished to remain fully non-parametric, one could adopt the approach of \cite{andrews2014nonparametric}, based on a Cram\'{e}r-von-Mises type test statistic.
As a compromise between these two extremes, one could alternatively specify a semi-parametric model, perhaps along the lines of Section 4 of \cite{Lewbel}, and follow the approach of \cite{andrews2013inference}, also based on a Cram\'{e}r-von-Mises type test statistic.
Both of these latter two possibilities could be an interesting extension of the method described above.
