\documentclass{beamer}  
\usepackage{../slides}
\usepackage{cancel}
\usepackage{appendixnumberbeamer}
\setbeameroption{hide notes}
\defbeamertemplate{description item}{align left}{\insertdescriptionitem\hfill}

%%%%%%%%%%%%%%%%%%%% Not needed at home!
\usepackage[compatibility=false]{caption}
\usepackage{subcaption}
%%%%%%%%%%%%%%%%%%%% Not needed at home!


\title[Binary Regressors]{Estimating the Effect of a Mis-measured, Endogenous, Binary Regressor}
\author[FJ DiTraglia]{Francis J.\ DiTraglia\\ Camilo Garc\'{i}a-Jimeno}
\institute{University of Pennsylvania}
\date{June 18th, 2017}
\begin{document} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[plain]
	\titlepage 
\end{frame} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Additively Separable Model}
\[
  y = h(T^*, \mathbf{x}) + \varepsilon
\]
\vspace{-1em}
    \begin{itemize}    
    \item $y$ -- Outcome of interest
    \item $h$ -- Known or unknown function 
    \item $T^*$ -- Unobserved, endogenous binary regressor
    \item $T$ -- Observed, mis-measured binary surrogate for $T^*$
    \item $\mathbf{x}$ -- Exogenous covariates
    \item $\varepsilon$ -- Mean-zero error term
    %\item $z$ -- Discrete (typically binary) instrumental variable
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is the Effect of $T^*$?}
 
  \begin{block}{Re-write the Model}
\begin{align*}
  y &= c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon\\
  &\beta(\mathbf{x}) = h(1,\mathbf{x}) - h(0,\mathbf{x})\\
  &c(\mathbf{x}) = h(0,\mathbf{x})
\end{align*}
  \end{block}

  \begin{alertblock}{This Paper:}
    \begin{itemize}
      \item Does a discrete instrument $z$ (typically binary) identify $\beta(\mathbf{x})$? 
      \item What assumptions are required for $z$ and the surrogate $T$?
      \item How to carry out inference for a mis-classified regressor?
    \end{itemize}
  \end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Example: Job Training Partnership Act (JPTA)}
\framesubtitle{Heckman et al.\ (2000, QJE)}
Randomized offer of job training, but about $30\%$ of those \emph{not} offered also obtain training and about $40\%$ of those offered training don't attend. Estimate causal effect of \emph{training} rather than \emph{offer} of training.

\begin{itemize}
  \item $y$ -- Log wage 
  \item $T^*$ -- True training attendence
  \item $T$ -- Self-reported training attendance
  \item $\mathbf{x}$ -- Individual characteristics
  \item $z$ -- Offer of job training
\end{itemize}
   
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Example: Smoking and Birthweight (SNAP Trial)}
%\framesubtitle{Coleman et al.\ (N Engl J Med, 2012)}
%  RCT with 1050 pregnant smokers in England: 521 given nicotine patches, the rest given placebo patches.
%\begin{itemize}
%  \item $y$ -- Birthweight 
%  \item $T^*$ -- True smoking behavior 
%  \item $T$ -- Self-reported smoking behavior
%  \item $\mathbf{x}$ -- Mother characteristics
%  \item $z$ -- Indicator of nicotine patch
%\end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Example: Schooling and Test Scores}
%\framesubtitle{Burde \& Linden (2013, AEJ Applied)}
%  RCT in Afghanistan: 32 villages divided into 11 clusters. Randomly choose 6 and set up school in each village of these clusters.
%
%\begin{itemize}
%  \item $y$ -- Girl's score on math and language test 
%  \item $T^*$ -- Girl's true school attendance
%  \item $T$ -- Parent's report of child's school attendance
%  \item $\mathbf{x}$ -- Child and household characteristics
%  \item $z$ -- School built in village
%\end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Example: Returns to Schooling} 
%\framesubtitle{Oreopoulos (2006, AER)}
%Fuzzy RD: minimum school-leaving age in UK increased from 14 to 15 in 1947 but some already stayed until 15 before the law and others failed to comply after it.
%\begin{itemize}
%  \item $y$ -- Log wage 
%  \item $T^*$ -- School attendance at age 15
%  \item $T$ -- Self-report of school attendance at age 15
%  \item $\mathbf{x}$ -- Individual characteristics
%  \item $z$ -- Indicator: born in or after 1933
%\end{itemize}
%   
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[label=MAHAJAN_BODY]
  \frametitle{Related Literature}
 
  \begin{block}{Continuous Treatment}
    \small
  Lewbel (1997, 2012), Schennach (2004, 2007), Chen et al. (2005), Hu \& Schennach (2008), Song (2015), Hu et al.\ (2015)\ldots 
  \end{block}

  \begin{block}{Binary, Exogenous Treatment}
    \small
   Aigner (1973), Bollinger (1996), Kane et al. (1999), Black et al. (2000), Frazis \& Loewenstein (2003), Mahajan (2006), Lewbel (2007), Hu (2008)
  \end{block}

  \begin{block}{Binary, Endogenous Treatment}
    \alert{Mahajan (2006)}, \small Shiu (2015), Ura (2015), Denteh et al.\ (2016)  
  \end{block}
    %\hyperlink{MAHAJAN_APPEND}{\beamergotobutton{Mahajan Details}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Baseline Assumptions -- Maintained Throughout}

  \small

  \begin{block}{Additively Separable Model}
    $y = c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon, \quad \mathbb{E}[ \varepsilon] = 0$ 
  \end{block}

  \begin{block}{Valid \& Relevant Instrument}
    $\mathbb{E}[\varepsilon|\mathbf{x},z] = 0, \quad \mathbb{E}\left[ T^*|\mathbf{x},z=k \right] \neq \mathbb{E}\left[ T^*|\mathbf{x},z=\ell \right]$
  \end{block}

  \begin{block}{Measurement Error Assumptions}
    \vspace{-0.5em}
  \begin{enumerate}[(i)] 
    \item  $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon|\mathbf{x},z, T^*]$
    \item $\alpha_0(\mathbf{x}) = \mathbb{P}(T=1|T^*=0,\mathbf{x},z), \quad \alpha_1(\mathbf{x}) = \mathbb{P}(T=0|T^*=1,\mathbf{x},z)$
    \item  $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) < 1 \quad$ ($T$ is positively correlated with $T^*$)
  \end{enumerate}
  \end{block}

  \vspace{-1em}

  \begin{alertblock}{Theorem}
    The baseline assumptions fail to identify $\beta(\mathbf{x})$, even if the instrument $z$ takes on an arbitrarily large finite number of distinct values.
  \end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Identification from Stronger Assumptions?}

  \begin{block}{Second Moment Assumption}
    \begin{enumerate}[(i)]
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^2|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z] = \mathbb{E}[\varepsilon^2|\mathbf{x}]$
  \end{enumerate}
  \end{block}

  \begin{block}{Third Moment Assumption}
    \begin{enumerate}[(i)]
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^3|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z] = \mathbb{E}[\varepsilon^3|\mathbf{x}]$
  \end{enumerate}
  \end{block}

  \begin{alertblock}{Sufficient Condition}
    \begin{enumerate}[(i)]
      \item $T$ is conditionally independent of $(\varepsilon,z)$ given $(T^*,\mathbf{x})$
      \item $z$ is conditionally independent of $\varepsilon$ given $\mathbf{x}$
    \end{enumerate}
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Identification Argument: Step I}
 
  \begin{block}{Reparameterization}
    \vspace{-1em}
\begin{align*}
  \theta_1(\mathbf{x}) &= \beta(\mathbf{x})/\left[ 1 - \alpha_0(\mathbf{x}) - \mathbf{\alpha}_1(\mathbf{x})  \right]\\
  \theta_2(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^2 \left[ 1 + \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})\right] \\
  \theta_3(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^3\left[ \left\{ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right\}^2 + 6\alpha_0(\mathbf{x})\left\{ 1 - \alpha_1(\mathbf{x}) \right\} \right]
\end{align*}
  \end{block}

  \begin{alertblock}{Theorem}
    If $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$ and $\theta_3(\mathbf{x})$ are identified and $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) <1$
    \begin{itemize}
      \item If $\theta_1(\mathbf{x})\neq 0$, then $\beta(\mathbf{x}), \alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$ are identified 
      \item If $\theta_1(\mathbf{x}) = 0$ then $\beta(\mathbf{x})$ is identified %but $\alpha_0(\mathbf{x})$ and $\mathbf{\alpha}_1(\mathbf{x})$ are not.
    \end{itemize}
    If $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) \neq 1$,  then $\beta(\mathbf{x})$ is identified up to sign.
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Identification Argument: Step II}

  \begin{block}{Notation}
    \vspace{-1em}
\[
    \pi(\mathbf{x}) = \mbox{Cov}(T,z|\mathbf{x}), \quad \eta_j(\mathbf{x}) = \mbox{Cov}(y^j,z|\mathbf{x}), \quad \tau_j(\mathbf{x}) = \mbox{Cov}(Ty^j,z|\mathbf{x})
\]
  \end{block}

  \begin{alertblock}{Theorem} 
    Baseline plus 2nd and 3rd Moment Assumptions imply 
\begin{align*}
 \eta_1(\mathbf{x}) &= \pi(\mathbf{x})\theta_1(\mathbf{x})\\
  \eta_2(\mathbf{x}) &=  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x}) \\
  \eta_3(\mathbf{x}) &=  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})
\end{align*}
so $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$ and $\theta_3(\mathbf{x})$ are identified if $\pi(\mathbf{x})\neq 0$.
  \end{alertblock}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Some Intuition: Suppress Dependence on Covariates} 
%
%  \begin{block}{Baseline Assumptions}
%    \footnotesize
%    \[\mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right) \mbox{Cov}(T,z) = 0\]
%    \hfill \alert{Identifies $\beta / (1 - \alpha_0 - \alpha_1)$}
%  \end{block}
%
%  \begin{block}{Second Moment Assumptions}
%    \scriptsize
%    \[\mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \left(\frac{\beta}{1 - \alpha_0 - \alpha_1}\right) \mbox{Cov}(T,z)\left(1 + \alpha_0 - \alpha_1\right)  \right\} = 0
%    \]
%
%    \vspace{0.5em}
%
%  \hfill \alert{Given $\beta/(1 - \alpha_0 - \alpha_1)$, identifies $\alpha_0 - \alpha_1$}
%  \end{block}
%
%  \begin{block}{Third Moment Assumptions}
%    \footnotesize
%    Messy, but can be shown yield two quadratics with \alert{identical coefficients}: one in $\alpha_0$ and the other in $1 - \alpha_1$.
%Roots are real provided that $\alpha_0 + \alpha_1 \neq 0$ but to tell which is $\alpha_0$ and which is $1 - \alpha_1$ we require $\alpha_0 + \alpha_1 < 1$.
%\end{block}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simple Special Case}


  \begin{block}{Suppose $\alpha_0 = 0$ and No Covariates}
  \begin{align*}
    \mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
      \mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z)\right\} &= 0
  \end{align*}
  \end{block}

  \begin{block}{Closed-Form Solution for $\beta$}
  \[
    \alert{\beta = \frac{2\mbox{Cov}(yT,z)}{\mbox{Cov}(T,z)} - \frac{\mbox{Cov}(y^2,z)}{\mbox{Cov}(y,z)}}
  \]
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Unconditional Moment Equalities ($\alpha_0 = 0$, No Covariates)}

\[
  \mathbf{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) = \left[
  \begin{array}{l}
  y_i - \kappa_1 - \theta_1 T_i\\
  y^2_i - \kappa_2  - \theta_1 2y_iT_i + \theta_2 T_i
  \end{array}
\right], \quad
 \mathbb{E}
  \left[
  \begin{array}{l}
    \boldsymbol{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) \\ \mathbf{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) z_i
  \end{array}
\right] = \mathbf{0}
\]

\small
\begin{align*}
    \theta_1 &= \beta / (1 - \alpha_1)\\
    \theta_2 &= \beta^2 / (1 - \alpha_1)\\
    \kappa_1 &= c\\
    \kappa_2 &= c^2 + \sigma_{\varepsilon}^2
  \end{align*}

  \alert{What happens if we try standard GMM inference?}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simulation DGP: $y = \beta T^* + \varepsilon$}
  \begin{block}{Errors}
      $(\varepsilon, \eta) \sim $ jointly normal, mean 0, variance 1, correlation 0.5.
  \end{block}
  \begin{block}{First-Stage}
      \begin{itemize}
        \item Half of individuals have $z=1$, the rest have $z=0$.
        \item $T^* = \mathbf{1}\left\{ \gamma_0 + \gamma_1 z + \eta > 0 \right\}$
        \item $\delta = \mathbb{P}(T^* = 0|z =1) = \mathbb{P}(T^*=1|z=0) = 0.15$
          %\begin{itemize}
        %\item $\gamma_0 = \Phi^{-1}(\delta)$
        %\item $\gamma_1 = \Phi^{-1}(1-\delta) - \Phi(\delta)$   
      %\end{itemize}
      \end{itemize}
  \end{block}
  \vspace{-1em}


  \begin{block}{Mis-classification}
      \begin{itemize}
        \item Set $\alpha_0 = 0$ 
        \item $T|T^*=1 \sim \mbox{Bernoulli}(1-\alpha_1)$
      \end{itemize}
  \end{block}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Coverage and Width of Nominal 95\% GMM CIs}
  \framesubtitle{$\alpha_1 = 0.1, \delta = 0.15, n = 1000, \rho = 0.5$, 5000 simulation replications}
\begin{table}[!tbp]
  \small
\centering
\begin{tabular}{ccc}
\hline\hline
$\beta$ & Coverage & Median Width \\
\hline
$2.00$&$0.95$&$0.23$\tabularnewline
$1.50$&$0.95$&$0.26$\tabularnewline
$1.00$&$0.95$&$0.32$\tabularnewline
$0.50$&$0.96$&$0.55$\tabularnewline
$0.25$&$0.98$&$1.08$\tabularnewline
$0.20$&$0.99$&$1.40$\tabularnewline
$0.15$&$0.99$&$1.86$\tabularnewline
$0.10$&$1.00$&$3.04$\tabularnewline
$0.05$&$1.00$&$4.76$\tabularnewline
$0.01$&$1.00$&$5.92$\tabularnewline
\hline
\end{tabular}
\end{table}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[plain,c]
%
%  \begin{figure}[h]
%    \centering
%    \includegraphics[width=\textwidth]{Rplot1}
%  \end{figure}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain,c]

  \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Rplot2}
  \end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain,c]

  \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Rplot4}
  \end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain,c]

  \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Rplot9}
  \end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Weak Identification Problem}
  \framesubtitle{Illustrated for $\alpha_0 = 0$ but holds generally}

  \small
\[
  \mathbf{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) = \left[
  \begin{array}{l}
  y_i - \kappa_1 - \theta_1 T_i\\
  y^2_i - \kappa_2  - \theta_1 2y_iT_i + \theta_2 T_i
  \end{array}
\right], \quad
 \mathbb{E}
  \left[
  \begin{array}{l}
    \boldsymbol{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) \\ \mathbf{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) z_i
  \end{array}
\right] = \mathbf{0}
\]

\[
  \alert{\theta_1 = \beta / (1 - \alpha_1),} \quad
  \alert{\theta_2 = \beta^2 / (1 - \alpha_1)}
\] 

\begin{itemize}
  \item $\beta$ small $\Rightarrow$ moment equalities uninformative about $\alpha_1$
  \item Same problem for other estimators from the literature but hasn't been pointed out.
  \item Identification robust inference: GMM Anderson-Rubin statistic
  \item \alert{But we can do better\ldots}
\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{``Weak'' Bounds for $\alpha_0, \alpha_1$}
  \framesubtitle{General Case $\alpha_0 \neq 0$}

  \small

  \begin{block}{\small Law of Total Probability}
  \[
    p_k^* = \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1}, \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1}\]
    where $p_k = \mathbb{P}(T=1|z=k), \quad p^*_k = \mathbb{P}(T^*=1|z=k)$ 
  \end{block}

  \begin{block}{\small $Cor(T,T^*)>0$}
   $\iff \alpha_0 + \alpha_1 < 1 \iff 1 - \alpha_0 - \alpha_1 > 0$ 
  \end{block}



  \begin{alertblock}{Implications}
    \vspace{-1em}
    \begin{itemize}
      \item $\alpha_0 < \min_k \left\{ p_k \right\}, \quad \alpha_1 < \min_k \left\{ 1 - p_k \right\}$
      \item $\beta$ is between $\beta_{RF}$ and $\beta_{IV}$ 
      \item $\beta_{IV}$ \emph{inflated} but has correct sign 
    \end{itemize}
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Second Moment Bounds for $\alpha_0, \alpha_1$}
  \small

  \begin{block}{Observables}
    $\sigma^2_{tk} = \mbox{Var}(y|T=t,z_k), \quad \mu_{tk} = \mathbb{E}[y|T=t, z_k], \quad p_k = \mathbb{P}(T=1|z_k)$
  \end{block}

  \begin{block}{Constraint on Unobservables}
    $\mbox{Var}(\varepsilon|T^*=t, z_k) > 0$ 

    \begin{block}{Equivalent To}
\begin{align*}
  (p_k - \alpha_0) \left[ \left(\frac{1 - \alpha_0}{1 - p_k}\right) \sigma^2_{1k} - \left(\frac{\alpha_0}{p_k}\right) \sigma_{0k}^2 \right] &> \alpha_0 (1 - \alpha_0)(\mu_{1k} - \mu_{0k})^2\\
  (1 - p_k - \alpha_1) \left[ \left(\frac{1 - \alpha_1}{p_k}\right) \sigma^2_{0k} - \left(\frac{\alpha_1}{1-p_k}\right) \sigma_{1k}^2 \right] &> \alpha_1 (1 - \alpha_1)(\mu_{1k} - \mu_{0k})^2
\end{align*}

    \end{block} 
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]
  \frametitle{Bounds can be very informative in practice\ldots}
  \framesubtitle{Figure based on data from Burde \& Linden (2013)}
\begin{columns}[c]
  \begin{column}{0.4\textwidth}
    \begin{block}{``Weak'' Bounds}
      $\beta \in [0.65 \times\beta_{IV},\; \beta_{IV}]$
    \end{block}
    \vspace{1em}
    \begin{alertblock}{Add 2nd Moments}
      $\beta \in [0.78 \times \beta_{IV},\;\beta_{IV}]$
    \end{alertblock}
  \end{column}
  \begin{column}{0.6\textwidth}
      \begin{figure}[h]
        \centering
        \begin{tikzpicture}[scale=16]
          \draw [fill = lightgray] (0,0) rectangle (0.15, 0.3);
          \draw [fill = cyan] (0,0) rectangle (0.10, 0.12);
          \draw [thick, <->] (0,0.35)
          node[above] {$\alpha_1$} -- (0,0) 
          node [below left] {$(0,0)$} -- (0.25,0) 
          node [right] {$\alpha_0$};
          \draw [thick] (0.15,0.01) -- (0.15,-0.01) node [below right] {$0.15$}; 
          \draw [thick] (0.1,0.01) -- (0.1,-0.01) node [below] {$0.10$}; 
          \draw [thick] (0.01,0.3) -- (-0.01,0.3) node [left] {0.30}; 
          \draw [thick] (0.01,0.12) -- (-0.01,0.12) node [left] {0.12}; 
          \end{tikzpicture}
      \end{figure}
  \end{column}
\end{columns}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adding Auxiliary Moment Inequalities}

  \begin{itemize}
    \item Bounds for $(\alpha_0, \alpha_1)$ immune to weak identification problem: remain informative if $\beta$ is small or zero.
    \item 2nd moment bounds strictly tighter, but still need weak bounds to determine which root of  quadratic is extraneous.
    \item Since $\beta = 1/(1 - \alpha_0 - \alpha_1)$ is identified by TSLS, we can always get meaningful restrictions on $\beta$.
    \item Inequalities that are far from binding sap power, so use Generalized Moment Selection (Andrews \& Soares, 2010) 
  \end{itemize}

  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Inference With Moment Equalities and Inequalities}
  \small

\begin{block}{Moment Conditions}
  $\mathbb{E} \left[ m_j(\mathbf{w}_i,\theta_0) \right] = 0, \quad j = 1, \cdots, p$\\
  $\mathbb{E} \left[ m_j(\mathbf{w}_i,\theta_0) \right]  \geq 0, \quad j = p+1, \cdots, p + v$
\end{block}




\begin{block}{Test Statistic}
  \vspace{-1em}
\[
  T_n(\theta) = \sum_{j=1}^p \left[\frac{\sqrt{n}\; \bar{m}_{n,j}(\theta)}{\widehat{\sigma}_{n,j}(\theta)}\right]^2_- + \sum_{j=p+1}^{p+v} \left[\frac{\sqrt{n}\; \bar{m}_{n,j}(\theta)}{\widehat{\sigma}_{n,j}(\theta)}\right]^2
\]
\footnotesize
\begin{align*}
[x]_- &= \min\left\{ x, 0 \right\}\\
\bar{m}_{n,j}(\theta) &= n^{-1} \sum_{i=1}^{n} m_j(\mathbf{w}_i, \theta)\\
\widehat{\sigma}^2_{n,j}(\theta) &=  \mbox{consistent est.\ of } \mbox{AVAR}\left[  \sqrt{n}\; \bar{m}_{n,j}(\theta)\right]
\end{align*}
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Generalized Moment Selection Critical Value}
  \framesubtitle{Andrews \& Soares (2010)}
  \small
\begin{enumerate}
  \item Calculate $\bar{m}_n(\theta_0)$, $\widehat{\Sigma}_n(\theta_0)$, and $T_n(\theta_0) = S\left( \sqrt{n}\; \bar{m}_n(\theta_0), \widehat{\Sigma}_n(\theta_0) \right)$
  \item Calculate the following quantities:
    \begin{align*}
      \widehat{\Omega}_n(\theta_0) &= \mbox{Diag}^{-1/2}\left(\widehat{\Sigma}_n(\theta_0)  \right)\left(\widehat{\Sigma}_n(\theta_0)\right)\mbox{Diag}^{-1/2}\left( \widehat{\Sigma}_n(\theta_0) \right)\\
      \xi_n(\theta_0) &= \mbox{Diag}^{-1/2}\left( \widehat{\Sigma}_n(\theta_0) \right) \sqrt{n}\; \bar{m}_n(\theta_0) / \sqrt{\ln n}\\
      \varphi\left( \xi_n(\theta_0), \widehat{\Omega}_n(\theta_0) \right) &= \left\{
      \begin{array}{cc}
        0, & \mbox{if } \xi_j \leq 1 \mbox{ and } j \leq p \\
        \infty & \mbox{if } \xi_j > 1 \mbox{ or } j > p\\
      \end{array}
      \right.
    \end{align*}
  \item Draw $Z_1^*, \hdots, Z_R^* \sim \mbox{ iid } N(0_k, I_k)$ for $R$ large.
  \item The critical value is the $1 - \alpha$ quantile of $\left\{S\left( \widehat{\Omega}^{1/2}(\theta_0) Z_r^* + \varphi\left( \xi_n(\theta_0), \widehat{\Omega}_n(\theta_0) \right), \widehat{\Omega}_n(\theta_0) \right)\right\}_{r=1}^R$
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{figure}
    \centering
    \includegraphics[scale=0.5]{GMS.pdf}
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Conclusion}
  %\singlespacing
  %\small

  \begin{itemize}
    \item Endogenous, mis-measured binary treatment.
    \item Important in applied work but no solution in the literature.
      \item Usual (1st moment) IV assumption fails to identify $\beta$
      \item Higher moment / independence restrictions identify $\beta$
      \item Identification-Robust Inference incorportating additional inequality moment conditions.
   \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\appendix
%
%\begin{frame}[label=MAHAJAN_APPEND]
%  \frametitle{Mahajan (2006, ECTA)}
%
%  \vspace{-1em}
%
%    \begin{columns}[c]
%    \column{.45\textwidth} 
%    \begin{exampleblock}{Regression Model}
%      $y = \mathbb{E}[y|T^*] + \nu$\\
%      {\small $\mathbb{E}[\nu|T^*]=0$ by construction}
%    \end{exampleblock}
%    \column{.45\textwidth}
%    \begin{exampleblock}{Causal Model}
%     $y = c + \beta T^* + \varepsilon$\\
%     {\small$\mathbb{E}[\varepsilon|T^*]\neq 0$}
%    \end{exampleblock}
%    \end{columns}
%
%    \vspace{1.5em}
%  
%  \begin{block}{Main Result (Correct) -- Exogenous Treatment}
%   Relevant binary instrument $z$ ($p^*_k \neq p^*_\ell$) identifies $\alpha_0, \alpha_1$ and $\mathbb{E}[y|T^*]$ provided that $\mathbb{E}[\nu|T^*,T,z]=0$ and $\alpha_0 + \alpha_1 < 1$. 
%  \end{block}
%
%  \begin{alertblock}{Extension (Incorrect) -- Endogenous Treatment}
%    $\mathbb{E}[\varepsilon|z]=0$, $p^*_k \neq p^*_\ell$, $\mathbb{E}[\varepsilon|T,T^*,z]=\mathbb{E}[\varepsilon|T^*] \implies$ $\beta$ identified.
%  \end{alertblock}
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{frame}
%  \frametitle{Mahajan (2006, ECTA)}
%    \begin{columns}[c]
%    \column{.45\textwidth} 
%    \begin{exampleblock}{Regression Model}
%      $y = \mathbb{E}[y|T^*] + \nu$\\
%      {\small $\mathbb{E}[\nu|T^*]=0$ by construction}
%    \end{exampleblock}
%    \column{.45\textwidth}
%    \begin{exampleblock}{Causal Model}
%     $y = c + \beta T^* + \varepsilon$\\
%     {\small$\mathbb{E}[\varepsilon|T^*]\neq 0$}
%    \end{exampleblock}
%    \end{columns}
%
%    \vspace{0.7em}
%
%    \begin{block}{Ingredients}
%      
%  \begin{enumerate}
%    \item If $p^*_k \neq p^*_\ell$, $\mathbb{E}[\varepsilon|z]=0$ then, since $\beta_{IV} = \beta/(1-\alpha_0-\alpha_1)$, knowledge of $\alpha_0,\alpha_1$ is sufficient to recover $\beta$. \textcolor{blue}{(Correct)}
%    \item If $p^*_k \neq p^*_\ell$, $\mathbb{E}[\nu|T^*,T,z]=0$, $\alpha_0, \alpha_1$ are identified. \textcolor{blue}{(Correct)}
%    \item[] \alert{\framebox{How to satisfy both 1 and 2 while allowing $\mathbb{E}[\varepsilon|T^*]\neq 0$?}}
%    \item[3.] Assume that $\mathbb{E}[\varepsilon|T^*,T,z]=\mathbb{E}[\varepsilon|T^*]$ \\ {\small (i.e.\ $m_{0k}^* = m_{0\ell}^*$ and $m_{1k}^*=m_{1\ell}^*$)}
%  \end{enumerate}
%    \end{block}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Flaw in the Argument}
%  \begin{block}{Proposition}
%    If $\mathbb{E}[\varepsilon|T^*]\neq 0$ then  $\mathbb{E}[\varepsilon|T^*,T,z]=\mathbb{E}[\varepsilon|T^*]$ combined with $\mathbb{E}[\varepsilon|z]=0$ implies $p^*_k = p^*_\ell$, i.e.\ $z$ is irrelevant for $T^*$.
%  \end{block}
%  \begin{block}{Proof}
%    $\mathbb{E}[\varepsilon|z]=0$ implies
%  \begin{align*}
%    (1-p_1^*) \textcolor{red}{m^*_{0k}} + p^*_1 \textcolor{blue}{m^*_{1k}}&=c\\
%    (1-p_2^*) \textcolor{red}{m^*_{0\ell}} + p^*_2 \textcolor{blue}{m^*_{1\ell}}&=c
%  \end{align*}
%  while Mahajan's assumption implies $m_{0k}^* = m_{0\ell}^*$ and $m_{1k}^*=m_{1\ell}^*$.
%  Therefore either $m_{0k}^*=m_{0\ell}^* = m_{1k}^* =m_{1\ell}^*=c$, which is ruled out by $E[\varepsilon|T^*]=0$, or $p^*_k = p^*_\ell$.
%  \end{block}
%    \hyperlink{MAHAJAN_BODY}{\beamergotobutton{Back}}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
