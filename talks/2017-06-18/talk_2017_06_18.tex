\documentclass{beamer}  
\usepackage{../slides}
\usepackage{cancel}
\usepackage{appendixnumberbeamer}
\setbeameroption{hide notes}
\defbeamertemplate{description item}{align left}{\insertdescriptionitem\hfill}

%%%%%%%%%%%%%%%%%%%% Not needed at home!
\usepackage[compatibility=false]{caption}
\usepackage{subcaption}
%%%%%%%%%%%%%%%%%%%% Not needed at home!


\title[Binary Regressors]{Estimating the Effect of a Mis-measured, Endogenous, Binary Regressor}
\author[FJ DiTraglia]{Francis J.\ DiTraglia\\ Camilo Garc\'{i}a-Jimeno}
\institute{University of Pennsylvania}
\date{June 18th, 2017}
\begin{document} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[plain]
	\titlepage 
\end{frame} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Additively Separable Model}
\[
  y = m(T^*, \mathbf{x}) + \varepsilon
\]
\vspace{-1em}
    \begin{itemize}    
    \item $y$ -- Outcome of interest
    \item $m$ -- Known or unknown function 
    \item $T^*$ -- Unobserved, endogenous binary regressor
    \item $T$ -- Observed, mis-measured binary surrogate for $T^*$
    \item $\mathbf{x}$ -- Exogenous covariates
    \item $\varepsilon$ -- Mean-zero error term
    %\item $z$ -- Discrete (typically binary) instrumental variable
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is the Effect of $T^*$?}
 
  \begin{block}{Re-write the Model}
\begin{align*}
  y &= c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon\\
  &\beta(\mathbf{x}) = m(1,\mathbf{x}) - m(0,\mathbf{x})\\
  &c(\mathbf{x}) = m(0,\mathbf{x})
\end{align*}
  \end{block}

  \begin{alertblock}{This Paper:}
    \begin{itemize}
      \item Does a discrete instrument $z$ (typically binary) identify $\beta(\mathbf{x})$? 
      \item What assumptions are required for $z$ and the surrogate $T$?
      \item How to carry out inference for a mis-classified regressor?
    \end{itemize}
  \end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Example: Job Training Partnership Act (JPTA)}
\framesubtitle{Heckman et al.\ (2000, QJE)}
Randomized offer of job training, but about $30\%$ of those \emph{not} offered also obtain training and about $40\%$ of those offered training don't attend. Estimate causal effect of \emph{training} rather than \emph{offer} of training.

\begin{itemize}
  \item $y$ -- Log wage 
  \item $T^*$ -- True training attendence
  \item $T$ -- Self-reported training attendance
  \item $\mathbf{x}$ -- Individual characteristics
  \item $z$ -- Offer of job training
\end{itemize}
   
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Example: Smoking and Birthweight (SNAP Trial)}
%\framesubtitle{Coleman et al.\ (N Engl J Med, 2012)}
%  RCT with 1050 pregnant smokers in England: 521 given nicotine patches, the rest given placebo patches.
%\begin{itemize}
%  \item $y$ -- Birthweight 
%  \item $T^*$ -- True smoking behavior 
%  \item $T$ -- Self-reported smoking behavior
%  \item $\mathbf{x}$ -- Mother characteristics
%  \item $z$ -- Indicator of nicotine patch
%\end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Example: Schooling and Test Scores}
%\framesubtitle{Burde \& Linden (2013, AEJ Applied)}
%  RCT in Afghanistan: 32 villages divided into 11 clusters. Randomly choose 6 and set up school in each village of these clusters.
%
%\begin{itemize}
%  \item $y$ -- Girl's score on math and language test 
%  \item $T^*$ -- Girl's true school attendance
%  \item $T$ -- Parent's report of child's school attendance
%  \item $\mathbf{x}$ -- Child and household characteristics
%  \item $z$ -- School built in village
%\end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Example: Returns to Schooling} 
%\framesubtitle{Oreopoulos (2006, AER)}
%Fuzzy RD: minimum school-leaving age in UK increased from 14 to 15 in 1947 but some already stayed until 15 before the law and others failed to comply after it.
%\begin{itemize}
%  \item $y$ -- Log wage 
%  \item $T^*$ -- School attendance at age 15
%  \item $T$ -- Self-report of school attendance at age 15
%  \item $\mathbf{x}$ -- Individual characteristics
%  \item $z$ -- Indicator: born in or after 1933
%\end{itemize}
%   
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[label=MAHAJAN_BODY]
  \frametitle{Related Literature}
 
  \begin{block}{Continuous Treatment}
    \small
  Lewbel (1997, 2012), Schennach (2004, 2007), Chen et al. (2005), Hu \& Schennach (2008), Song (2015), Hu et al.\ (2015)\ldots 
  \end{block}

  \begin{block}{Binary, Exogenous Treatment}
    \small
   Aigner (1973), Bollinger (1996), Kane et al. (1999), Black et al. (2000), Frazis \& Loewenstein (2003), Mahajan (2006), Lewbel (2007), Hu (2008)
  \end{block}

  \begin{block}{Binary, Endogenous Treatment}
    \alert{Mahajan (2006)}, \small Shiu (2015), Ura (2015), Denteh et al.\ (2016)  
  \end{block}
    %\hyperlink{MAHAJAN_APPEND}{\beamergotobutton{Mahajan Details}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Baseline Assumptions -- Maintained Throughout}

  \small

  \begin{block}{Additively Separable Model}
    $y = c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon, \quad \mathbb{E}[ \varepsilon] = 0$ 
  \end{block}

  \begin{block}{Valid \& Relevant Instrument}
    $\mathbb{E}[\varepsilon|\mathbf{x},z] = 0, \quad \mathbb{E}\left[ T^*|\mathbf{x},z=k \right] \neq \mathbb{E}\left[ T^*|\mathbf{x},z=\ell \right]$
  \end{block}

  \begin{block}{Measurement Error Assumptions}
    \vspace{-0.5em}
  \begin{enumerate}[(i)] 
    \item  $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon|\mathbf{x},z, T^*]$
    \item $\alpha_0(\mathbf{x}) = \mathbb{P}(T=1|T^*=0,\mathbf{x},z), \quad \alpha_1(\mathbf{x}) = \mathbb{P}(T=0|T^*=1,\mathbf{x},z)$
    \item  $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) < 1 \quad$ ($T$ is positively correlated with $T^*$)
  \end{enumerate}
  \end{block}

  \vspace{-1em}

  \begin{alertblock}{Theorem}
    The baseline assumptions fail to identify $\beta(\mathbf{x})$, even if the instrument $z$ takes on an arbitrarily large finite number of distinct values.
  \end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Identification from Stronger Assumptions?}

  \begin{block}{Second Moment Assumption}
    \begin{enumerate}[(i)]
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^2|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z] = \mathbb{E}[\varepsilon^2|\mathbf{x}]$
  \end{enumerate}
  \end{block}

  \begin{block}{Third Moment Assumption}
    \begin{enumerate}[(i)]
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^3|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z] = \mathbb{E}[\varepsilon^3|\mathbf{x}]$
  \end{enumerate}
  \end{block}

  \begin{alertblock}{Sufficient Condition}
    \begin{enumerate}[(i)]
      \item $T$ is conditionally independent of $(\varepsilon,z)$ given $(T^*,\mathbf{x})$
      \item $z$ is conditionally independent of $\varepsilon$ given $\mathbf{x}$
    \end{enumerate}
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Identification Argument: Step I}
 
  \begin{block}{Reparameterization}
    \vspace{-1em}
\begin{align*}
  \theta_1(\mathbf{x}) &= \beta(\mathbf{x})/\left[ 1 - \alpha_0(\mathbf{x}) - \mathbf{\alpha}_1(\mathbf{x})  \right]\\
  \theta_2(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^2 \left[ 1 + \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})\right] \\
  \theta_3(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^3\left[ \left\{ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right\}^2 + 6\alpha_0(\mathbf{x})\left\{ 1 - \alpha_1(\mathbf{x}) \right\} \right]
\end{align*}
  \end{block}

  \begin{alertblock}{Theorem}
    If $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$ and $\theta_3(\mathbf{x})$ are identified and $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) <1$
    \begin{itemize}
      \item If $\theta_1(\mathbf{x})\neq 0$, then $\beta(\mathbf{x}), \alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$ are identified 
      \item If $\theta_1(\mathbf{x}) = 0$ then $\beta(\mathbf{x})$ is identified %but $\alpha_0(\mathbf{x})$ and $\mathbf{\alpha}_1(\mathbf{x})$ are not.
    \end{itemize}
    If $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) \neq 1$,  then $\beta(\mathbf{x})$ is identified up to sign.
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Identification Argument: Step II}

  \begin{block}{Notation}
    \vspace{-1em}
\[
    \pi(\mathbf{x}) = \mbox{Cov}(T,z|\mathbf{x}), \quad \eta_j(\mathbf{x}) = \mbox{Cov}(y^j,z|\mathbf{x}), \quad \tau_j(\mathbf{x}) = \mbox{Cov}(Ty^j,z|\mathbf{x})
\]
  \end{block}

  \begin{alertblock}{Theorem} 
    Baseline plus 2nd and 3rd Moment Assumptions imply 
\begin{align*}
 \eta_1(\mathbf{x}) &= \pi(\mathbf{x})\theta_1(\mathbf{x})\\
  \eta_2(\mathbf{x}) &=  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x}) \\
  \eta_3(\mathbf{x}) &=  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})
\end{align*}
so $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$ and $\theta_3(\mathbf{x})$ are identified if $\pi(\mathbf{x})\neq 0$.
  \end{alertblock}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Some Intuition: Suppress Dependence on Covariates} 
%
%  \begin{block}{Baseline Assumptions}
%    \footnotesize
%    \[\mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right) \mbox{Cov}(T,z) = 0\]
%    \hfill \alert{Identifies $\beta / (1 - \alpha_0 - \alpha_1)$}
%  \end{block}
%
%  \begin{block}{Second Moment Assumptions}
%    \scriptsize
%    \[\mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \left(\frac{\beta}{1 - \alpha_0 - \alpha_1}\right) \mbox{Cov}(T,z)\left(1 + \alpha_0 - \alpha_1\right)  \right\} = 0
%    \]
%
%    \vspace{0.5em}
%
%  \hfill \alert{Given $\beta/(1 - \alpha_0 - \alpha_1)$, identifies $\alpha_0 - \alpha_1$}
%  \end{block}
%
%  \begin{block}{Third Moment Assumptions}
%    \footnotesize
%    Messy, but can be shown yield two quadratics with \alert{identical coefficients}: one in $\alpha_0$ and the other in $1 - \alpha_1$.
%Roots are real provided that $\alpha_0 + \alpha_1 \neq 0$ but to tell which is $\alpha_0$ and which is $1 - \alpha_1$ we require $\alpha_0 + \alpha_1 < 1$.
%\end{block}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simple Special Case}


  \begin{block}{Suppose $\alpha_0 = 0$ and No Covariates}
  \begin{align*}
    \mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
      \mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z)\right\} &= 0
  \end{align*}
  \end{block}

  \begin{block}{Closed-Form Solution for $\beta$}
  \[
    \alert{\beta = \frac{2\mbox{Cov}(yT,z)}{\mbox{Cov}(T,z)} - \frac{\mbox{Cov}(y^2,z)}{\mbox{Cov}(y,z)}}
  \]
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Unconditional Moment Equalities ($\alpha_0 = 0$, No Covariates)}

\[
  \mathbf{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) = \left[
  \begin{array}{l}
  y_i - \kappa_1 - \theta_1 T_i\\
  y^2_i - \kappa_2  - \theta_1 2y_iT_i + \theta_2 T_i
  \end{array}
\right], \quad
 \mathbb{E}
  \left[
  \begin{array}{l}
    \boldsymbol{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) \\ \mathbf{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) z_i
  \end{array}
\right] = \mathbf{0}
\]

\small
\begin{align*}
    \theta_1 &= \beta / (1 - \alpha_1)\\
    \theta_2 &= \beta^2 / (1 - \alpha_1)\\
    \kappa_1 &= c\\
    \kappa_2 &= c^2 + \sigma_{\varepsilon}^2
  \end{align*}

  \alert{What happens if we try standard GMM inference?}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simulation DGP: $y = \beta T^* + \varepsilon$}
  \begin{block}{Errors}
      $(\varepsilon, \eta) \sim $ jointly normal, mean 0, variance 1, correlation 0.5.
  \end{block}
  \begin{block}{First-Stage}
      \begin{itemize}
        \item Half of individuals have $z=1$, the rest have $z=0$.
        \item $T^* = \mathbf{1}\left\{ \gamma_0 + \gamma_1 z + \eta > 0 \right\}$
        \item $\delta = \mathbb{P}(T^* = 0|z =1) = \mathbb{P}(T^*=1|z=0) = 0.15$
          %\begin{itemize}
        %\item $\gamma_0 = \Phi^{-1}(\delta)$
        %\item $\gamma_1 = \Phi^{-1}(1-\delta) - \Phi(\delta)$   
      %\end{itemize}
      \end{itemize}
  \end{block}
  \vspace{-1em}


  \begin{block}{Mis-classification}
      \begin{itemize}
        \item Set $\alpha_0 = 0$ 
        \item $T|T^*=1 \sim \mbox{Bernoulli}(1-\alpha_1)$
      \end{itemize}
  \end{block}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Coverage and Width of Nominal 95\% GMM CIs}
  \framesubtitle{$\alpha_1 = 0.1, \delta = 0.15, n = 1000, \rho = 0.5$, 5000 simulation replications}
\begin{table}[!tbp]
  \small
\centering
\begin{tabular}{ccc}
\hline\hline
$\beta$ & Coverage & Median Width \\
\hline
$2.00$&$0.95$&$0.23$\tabularnewline
$1.50$&$0.95$&$0.26$\tabularnewline
$1.00$&$0.95$&$0.32$\tabularnewline
$0.50$&$0.96$&$0.55$\tabularnewline
$0.25$&$0.98$&$1.08$\tabularnewline
$0.20$&$0.99$&$1.40$\tabularnewline
$0.15$&$0.99$&$1.86$\tabularnewline
$0.10$&$1.00$&$3.04$\tabularnewline
$0.05$&$1.00$&$4.76$\tabularnewline
$0.01$&$1.00$&$5.92$\tabularnewline
\hline
\end{tabular}
\end{table}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[plain,c]
%
%  \begin{figure}[h]
%    \centering
%    \includegraphics[width=\textwidth]{Rplot1}
%  \end{figure}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain,c]

  \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Rplot2}
  \end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain,c]

  \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Rplot4}
  \end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain,c]

  \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Rplot9}
  \end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Weak Identification Problem}
  \framesubtitle{Illustrated for $\alpha_0 = 0$ but holds generally}

  \small
\[
  \mathbf{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) = \left[
  \begin{array}{l}
  y_i - \kappa_1 - \theta_1 T_i\\
  y^2_i - \kappa_2  - \theta_1 2y_iT_i + \theta_2 T_i
  \end{array}
\right], \quad
 \mathbb{E}
  \left[
  \begin{array}{l}
    \boldsymbol{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) \\ \mathbf{u}_i(\boldsymbol{\kappa}, \boldsymbol{\theta}) z_i
  \end{array}
\right] = \mathbf{0}
\]

\[
  \alert{\theta_1 = \beta / (1 - \alpha_1),} \quad
  \alert{\theta_2 = \beta^2 / (1 - \alpha_1)}
\] 

\begin{itemize}
  \item $\beta$ small $\Rightarrow$ moment equalities uninformative about $\alpha_1$
  \item Same problem for other estimators from the literature but hasn't been pointed out.
  \item Identification robust inference: GMM Anderson-Rubin Test
  \item \alert{But we can do better\ldots}
\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Auxiliary Moment Inequalities}
  \framesubtitle{General Case $\alpha_0 \neq 0$}

  \begin{block}{Law of Total Probability}
    \scriptsize
  \[
    \mathbb{P}(T^*=1|z=k) = \frac{\mathbb{P}(T=1|z=k) - \alpha_0}{1 - \alpha_0 - \alpha_1}, \quad
  \mathbb{P}(T^*=0|z=k) = \frac{\mathbb{P}(T=0|z=k) - \alpha_1}{1 - \alpha_0 - \alpha_1}\]
  \end{block}

  \begin{block}{$Cor(T,T^*)>0$}
   $\iff \alpha_0 + \alpha_1 < 1 \iff 1 - \alpha_0 - \alpha_1 > 0$ 
  \end{block}



  \begin{alertblock}{Implications}
    \vspace{-1em}
    \begin{itemize}
      \item $\alpha_0 < \min_k \mathbb{P}(T=1|z=k), \quad \alpha_1 < \min_k \mathbb{P}(T=0|z=k)$
      \item $\beta$ is between $\beta_{RF}$ and $\beta_{IV}$ 
      \item $\beta_{IV}$ \emph{inflated} but has correct sign 
    \end{itemize}
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[plain, c]
%  \frametitle{$\alpha_0 \leq \min_k \left\{p_k\right\}, \; \; \alpha_1 \leq \min_k \left\{1 - p_k\right\}$}
%\begin{figure}[h]
%  \centering
%  \begin{tikzpicture}[scale=5]
%    \draw [fill = lightgray] (0.66,0.75) rectangle (1,1);
%    \draw [fill = cyan] (0,0) rectangle (0.25, 0.34);
%    \draw [thick, <->] (0,1.1)
%    node[above] {$\alpha_1$} -- (0,0) 
%    node [below left] {$(0,0)$} -- (1.1,0) 
%    node [right] {$\alpha_0$};
%    \draw [thick] (0.25,0.02) -- (0.25,-0.02) node [below] {$p_\ell$}; 
%    \draw [thick] (0.02,0.75) -- (-0.02,0.75) node [left] {$1 - p_\ell$}; 
%    \draw [thick] (0.66,0.02) -- (0.66,-0.02) node [below] {$p_k$}; 
%    \draw [thick] (0.02,0.34) -- (-0.02,0.34) node [left] {$1 - p_k$}; 
%    \draw [thick, red] (0,1) to (1,0); 
%    \node [right, red] at (0.06,0.95) {$\alpha_0 + \alpha_1 = 1$};
%    \draw [thick] (0,1) -- (1,1) -- (1,0);
%    \node [above right] at (1,1) {$(1,1)$};
%    \end{tikzpicture}
%\end{figure}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Bounds for $\beta$}
%  \begin{block}{$\mathbb{E}[\varepsilon|z]=0$} 
%    $\implies \beta_{RF} = \mathbb{E}[y|z_k] - \mathbb{E}[y|z_\ell] =\beta (p_k^* - p_\ell^*)$
%  \end{block}
%
%  \begin{block}{Mis-classification} 
%  $\implies p_k^* - p_\ell^* = (p_k - p_\ell)/(1 - \alpha_0 - \alpha_1)$
%  \end{block}
%
%  \vspace{1em}
%
%  \begin{alertblock}{Combining: $\beta_{IV} = \beta / (1 - \alpha_0 - \alpha_1)$}
%  \end{alertblock}
%
%  \vspace{-1em}
%
%\begin{block}{$\alpha_0 + \alpha_1 < 1 \implies $}
%  \begin{itemize}
%    \item $\beta$ is between $\beta_{RF}$ and $\beta_{IV}$ 
%    \item $\beta_{IV}$ \emph{inflated} but has correct sign 
%    \item $\beta_{RF}$ bound equivalent to substituting $\alpha_0, \alpha_1$ bounds
%  \end{itemize}
%\end{block}
%  
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Even Tighter Bounds for $\alpha_0, \alpha_1$ from Conditional Variances}
  \begin{block}{Assume}
    $\mathbb{E}[\varepsilon^2|T^*,T,z] = \mathbb{E}[\varepsilon^2|T^*,z]$
  \end{block}

  \begin{block}{Observables}
    $\sigma^2_{tk} = \mbox{Var}(y|T=t,z=k)$ 
  \end{block}

  \begin{block}{Constrain Unobservables}
    $s^{*2}_{tk} = Var(u|T^*=t, z_k) > 0$ 

    \footnotesize
\begin{align*}
  (p_k - \alpha_0) \left[ (1 - \alpha_0)p_k \sigma^2_{1k} - \alpha_0 (1 - p_k)\sigma_{0k}^2 \right] &> \alpha_0 (1 - \alpha_0)p_k (1 - p_k)(\bar{y}_{1k} - \bar{y}_{0k})^2\\
  (1 - p_k - \alpha_1) \left[ (1 - \alpha_1)(1 - p_k) \sigma^2_{0k} - \alpha_1 p_k\sigma_{1k}^2 \right] &> \alpha_1 (1 - \alpha_1)p_k (1 - p_k)(\bar{y}_{1k} - \bar{y}_{0k})^2
\end{align*}
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{$(z \perp \varepsilon)$ and $(T\perp \varepsilon|T^*,z) \Rightarrow$ Continuum of MCs}
%  \framesubtitle{Figure depicts simulation DGP}
%
%%  \begin{block}{Characteristic Functions}
%%    \vspace{-1em}
%%\[
%%  e^{i\omega\beta}\left[(1 - \alpha_1) - \xi(\omega)\right] =  \alpha_0 - \xi(\omega) 
%%\]
%%\small
%%\[
%%  \xi(\omega) \equiv \frac{\varphi_k(\omega) - \varphi_\ell(\omega)}{p_k \varphi_{1k}(\omega) - p_\ell\varphi_{1\ell}(\omega)}
%%\]
%%\end{block}
%%\normalsize
%%  \begin{block}{Distribution Functions}
%    \vspace{-0.7em}
%  \begin{figure}[h]
%    \centering
%    \includegraphics[width=\textwidth]{Delta_sim}
%  \end{figure}
%    
%    \vspace{-4em}
%\begin{align*}
%    \widetilde{\Delta}_1(\tau+\beta) - \widetilde{\Delta}_1(\tau) &= \alpha_0 \Delta(\tau + \beta) - (1 - \alpha_1) \Delta(\tau)\\
%  \Delta(\tau) &= F_k(\tau) - F_\ell(\tau)\\
%  \widetilde{\Delta}_1(\tau) &= p_k F_{1k}(\tau) - p_\ell F_{1\ell}(\tau)
%\end{align*}
%
%%  \end{block}
%
%  \normalsize
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Identification-Robust Joint Inference for $(\alpha_0, \alpha_1, \beta)$}
  \begin{itemize}
    \item Auxiliary moment inequalities to bound $(\alpha_0, \alpha_1)$
    \item Joint CS for $(\alpha_0, \alpha_1, \beta)$ by inverting Anderson-Rubin Test 
    \item Marginal inference for $\beta$ by projection.
    \item Generalized Moment Selection (Andrews \& Soares, 2010) for tighter confidence sets.
    \item Results are preliminary (not exploiting full set of inequalities) but this approach seems to work extremely well. 
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{figure}
    \centering
    \includegraphics[scale=0.5]{GMS.pdf}
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Conclusion}
  %\singlespacing
  %\small

  \begin{itemize}
    \item Endogenous, mis-measured binary treatment.
    \item Important in applied work but no solution in the literature.
      \item Usual (1st moment) IV assumption fails to identify $\beta$
      \item Higher moment / independence restrictions identify $\beta$
      \item Identification-Robust Inference incorportating additional inequality moment conditions.
   \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\appendix
%
%\begin{frame}[label=MAHAJAN_APPEND]
%  \frametitle{Mahajan (2006, ECTA)}
%
%  \vspace{-1em}
%
%    \begin{columns}[c]
%    \column{.45\textwidth} 
%    \begin{exampleblock}{Regression Model}
%      $y = \mathbb{E}[y|T^*] + \nu$\\
%      {\small $\mathbb{E}[\nu|T^*]=0$ by construction}
%    \end{exampleblock}
%    \column{.45\textwidth}
%    \begin{exampleblock}{Causal Model}
%     $y = c + \beta T^* + \varepsilon$\\
%     {\small$\mathbb{E}[\varepsilon|T^*]\neq 0$}
%    \end{exampleblock}
%    \end{columns}
%
%    \vspace{1.5em}
%  
%  \begin{block}{Main Result (Correct) -- Exogenous Treatment}
%   Relevant binary instrument $z$ ($p^*_k \neq p^*_\ell$) identifies $\alpha_0, \alpha_1$ and $\mathbb{E}[y|T^*]$ provided that $\mathbb{E}[\nu|T^*,T,z]=0$ and $\alpha_0 + \alpha_1 < 1$. 
%  \end{block}
%
%  \begin{alertblock}{Extension (Incorrect) -- Endogenous Treatment}
%    $\mathbb{E}[\varepsilon|z]=0$, $p^*_k \neq p^*_\ell$, $\mathbb{E}[\varepsilon|T,T^*,z]=\mathbb{E}[\varepsilon|T^*] \implies$ $\beta$ identified.
%  \end{alertblock}
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{frame}
%  \frametitle{Mahajan (2006, ECTA)}
%    \begin{columns}[c]
%    \column{.45\textwidth} 
%    \begin{exampleblock}{Regression Model}
%      $y = \mathbb{E}[y|T^*] + \nu$\\
%      {\small $\mathbb{E}[\nu|T^*]=0$ by construction}
%    \end{exampleblock}
%    \column{.45\textwidth}
%    \begin{exampleblock}{Causal Model}
%     $y = c + \beta T^* + \varepsilon$\\
%     {\small$\mathbb{E}[\varepsilon|T^*]\neq 0$}
%    \end{exampleblock}
%    \end{columns}
%
%    \vspace{0.7em}
%
%    \begin{block}{Ingredients}
%      
%  \begin{enumerate}
%    \item If $p^*_k \neq p^*_\ell$, $\mathbb{E}[\varepsilon|z]=0$ then, since $\beta_{IV} = \beta/(1-\alpha_0-\alpha_1)$, knowledge of $\alpha_0,\alpha_1$ is sufficient to recover $\beta$. \textcolor{blue}{(Correct)}
%    \item If $p^*_k \neq p^*_\ell$, $\mathbb{E}[\nu|T^*,T,z]=0$, $\alpha_0, \alpha_1$ are identified. \textcolor{blue}{(Correct)}
%    \item[] \alert{\framebox{How to satisfy both 1 and 2 while allowing $\mathbb{E}[\varepsilon|T^*]\neq 0$?}}
%    \item[3.] Assume that $\mathbb{E}[\varepsilon|T^*,T,z]=\mathbb{E}[\varepsilon|T^*]$ \\ {\small (i.e.\ $m_{0k}^* = m_{0\ell}^*$ and $m_{1k}^*=m_{1\ell}^*$)}
%  \end{enumerate}
%    \end{block}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Flaw in the Argument}
%  \begin{block}{Proposition}
%    If $\mathbb{E}[\varepsilon|T^*]\neq 0$ then  $\mathbb{E}[\varepsilon|T^*,T,z]=\mathbb{E}[\varepsilon|T^*]$ combined with $\mathbb{E}[\varepsilon|z]=0$ implies $p^*_k = p^*_\ell$, i.e.\ $z$ is irrelevant for $T^*$.
%  \end{block}
%  \begin{block}{Proof}
%    $\mathbb{E}[\varepsilon|z]=0$ implies
%  \begin{align*}
%    (1-p_1^*) \textcolor{red}{m^*_{0k}} + p^*_1 \textcolor{blue}{m^*_{1k}}&=c\\
%    (1-p_2^*) \textcolor{red}{m^*_{0\ell}} + p^*_2 \textcolor{blue}{m^*_{1\ell}}&=c
%  \end{align*}
%  while Mahajan's assumption implies $m_{0k}^* = m_{0\ell}^*$ and $m_{1k}^*=m_{1\ell}^*$.
%  Therefore either $m_{0k}^*=m_{0\ell}^* = m_{1k}^* =m_{1\ell}^*=c$, which is ruled out by $E[\varepsilon|T^*]=0$, or $p^*_k = p^*_\ell$.
%  \end{block}
%    \hyperlink{MAHAJAN_BODY}{\beamergotobutton{Back}}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
