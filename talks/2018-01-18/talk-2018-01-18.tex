\documentclass{beamer}  
\usepackage{../slides}
\usepackage{cancel}
\usepackage{appendixnumberbeamer}
%\setbeameroption{show notes}
\setbeameroption{hide notes}
\defbeamertemplate{description item}{align left}{\insertdescriptionitem\hfill}

%%%%%%%%%%%%%%%%%%%% Not needed at home!
\usepackage[compatibility=false]{caption}
\usepackage{subcaption}
%%%%%%%%%%%%%%%%%%%% Not needed at home!


\title[Binary Regressors]{Mis-Classified, Binary, Endogenous Regressors: Identification and Inference}
\author[FJ DiTraglia]{Francis J.\ DiTraglia\\ Camilo Garc\'{i}a-Jimeno}
\institute{University of Pennsylvania}
\date{January 18th, 2018}
\begin{document} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[plain]
	\titlepage 
  \note{Thanks for inviting me etc. Very nice meeting with so many of you today. For those I haven't spoken with, let me introduce myself. Applied econometrician. Interested in issues related to model selection, instrumental variables, inference in non-standard settings, and measurement error. Also do applied work, particularly in structural empirical micro. This paper is part of a research agenda on measurement error. Joint work with my co-author Camilo.}
\end{frame} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is the effect of $T^*$?}
\[
  y = c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon
\]
\vspace{-1em}
    \begin{itemize}    
    \item $y$ -- Outcome of interest
    \item $T^*$ -- Unobserved, endogenous binary regressor
    \item $T$ -- Observed, mis-measured binary surrogate for $T^*$
    \item $\mathbf{x}$ -- Exogenous covariates
    \item $z$ -- Discrete (typically binary) instrumental variable
    %\item $\varepsilon$ -- Mean-zero error term
  \end{itemize}

  \vspace{2em}
  \alert{(Additively Separable $\varepsilon$ and binary $T^*$ $\Rightarrow$ linear model \alert{given $\mathbf{x}$})}

  \note{No loss of generality from conditionally linear model provided $\varepsilon$ is additively separable:
\begin{align*}
  y &= h(T^*,\mathbf{x}) + \varepsilon \\
  y &= c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon\\
  &\beta(\mathbf{x}) = h(1,\mathbf{x}) - h(0,\mathbf{x})\\
  &c(\mathbf{x}) = h(0,\mathbf{x})
\end{align*}
}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Using a discrete IV to learn about $\beta(\mathbf{x})$}
 
\[
  y = c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon
\]

\begin{alertblock}{Constributions of This Paper}
  \begin{enumerate}
    \item Show that only existing point identification result for mis-classified, endogenous $T^*$ is incorrect.
    \item Derive sharp identified set for $\beta(\mathbf{x})$ under standard assumptions.
    \item Prove point identification of $\beta(\mathbf{x})$ under slightly stronger assumptions.
    \item Point out problem of weak identification in mis-classification models, develop identification-robust inference for $\beta(\mathbf{x})$. 
  \end{enumerate}
\end{alertblock}

    \note{Questions this paper sets out to answer. Simple model. If $T^*$ were continuous and measurement error were classical, this would be trivially easy. But, as we'll see things are much more complicated when $T^*$ is binary. First some motivating examples.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Example: Job Training Partnership Act (JTPA)}
%\framesubtitle{Heckman et al.\ (2000, QJE)}
%Randomized offer of job training, but about $30\%$ of those \emph{not} offered also obtain training and about $40\%$ of those offered training don't attend. Estimate causal effect of \emph{training} rather than \emph{offer} of training.
%
%\begin{itemize}
%  \item $y$ -- Log wage 
%  \item $T^*$ -- True training attendence
%  \item $T$ -- Self-reported training attendance
%  \item $\mathbf{x}$ -- Individual characteristics
%  \item $z$ -- Offer of job training
%\end{itemize}
%   
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Example: Smoking and Birthweight (SNAP Trial)}
\framesubtitle{Coleman et al.\ (N Engl J Med, 2012)}
  RCT with pregnant smokers in England: half given nicotine patches, the rest given placebo patches.
  Some given nicotine fail to quit; some given placebo quit.
\begin{itemize}
  \item $y$ -- Birthweight 
  \item $T^*$ -- True smoking behavior 
  \item $T$ -- Self-reported smoking behavior
  \item $\mathbf{x}$ -- Mother characteristics
  \item $z$ -- Indicator of nicotine patch
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Example: Schooling and Test Scores}
\framesubtitle{Burde \& Linden (2013, AEJ Applied)}
  RCT in Afghanistan: schools built in randomly selected villages.
  In treatment villages only some girls attend school; in control villages some girls attend school elsewhere.
  %32 villages divided into 11 clusters. Randomly choose 6 and set up school in each village of these clusters.

\begin{itemize}
  \item $y$ -- Girl's score on math and language test 
  \item $T^*$ -- Girl's true school attendance
  \item $T$ -- Parent's report of child's school attendance
  \item $\mathbf{x}$ -- Child and household characteristics
  \item $z$ -- School built in village
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Related Literature}
 
  \begin{block}{Continuous Regressor}
    \small
  Lewbel (1997, 2012), Schennach (2004, 2007), Chen et al. (2005), Hu \& Schennach (2008), Song (2015), Hu et al.\ (2015)\ldots 
  \end{block}

  \begin{block}{Binary/Discrete, ``Exogenous''} 
    \small
   Aigner (1973), Bollinger (1996), Kane et al. (1999), Black et al. (2000), Frazis \& Loewenstein (2003), Mahajan (2006), Lewbel (2007), Hu (2008), Molinari (2008)
  \end{block}

  \begin{block}{Binary, Endogenous Regressor}
    \alert{Mahajan (2006)},\\ \small Shiu (2015), Denteh et al.\ (2016), Ura (2016), Calvi et al.\ (2017)
  \end{block}

  \note{Very large literature on measurement error. Can't summarize everything here. But I want to draw out a few themes that will be relevant later and point out one paper in particular:
    \begin{itemize}
      \item Continuous vs.\ discrete
      \item Classical vs.\ non-classical
      \item Exogenous vs.\ endogenous
    \end{itemize}
  Don't spend too much time on other papers. Just say there was almost no work on the endogenous case when we started this. Recently some work that complements but doesn't overlap with ours. Focus on Mahajan paper and give the capsule explanation.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{``Baseline'' Assumptions I -- Model \& Instrument}

  \begin{block}{Additively Separable Model}
    $y = c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon, \quad \mathbb{E}[ \varepsilon] = 0$ 
  \end{block}

  \begin{block}{Valid \& Relevant Instrument: $z \in \left\{ 0,1 \right\}$}
    \begin{itemize}
      \item $\mathbb{P}(T^*=1|\mathbf{x},z=1) \neq \mathbb{P}(T^*=1|\mathbf{x},z=0)$
      \item $\mathbb{E}[\varepsilon|\mathbf{x},z] = 0$
      \item $0 < \mathbb{P}(z=1|\mathbf{x}) < 1$
    \end{itemize}
  \end{block}

  \begin{alertblock}{If $T^*$ were observed, these conditions would identify $\beta$.}
  \end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{``Baseline'' Assumptions II -- Measurement Error}


  \begin{alertblock}{Notation: Mis-classification Rates}
    \begin{itemize}
      \item[``$\boldsymbol{\uparrow}$''] 
    $\alpha_0(\mathbf{x},z) \equiv \mathbb{P}\left(T=1|T^*=0,\mathbf{x},z  \right)$
  \item[``$\boldsymbol{\downarrow}$''] 
  $\alpha_1(\mathbf{x},z) \equiv \mathbb{P}\left(T=0|T^*=1,\mathbf{x},z  \right)$
    \end{itemize}
  \end{alertblock}

  \begin{block}{Mis-classification unaffected by $z$}
    $\alpha_0(\mathbf{x},z) = \alpha_0(\mathbf{x}), \quad   \alpha_1(\mathbf{x},z) = \alpha_1(\mathbf{x})$
  \end{block}

  \begin{block}{Extent of Mis-classification}
      $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) < 1 \quad$ ($T$ is positively correlated with $T^*$)
  \end{block}

  \begin{block}{Non-differential Mis-classification}
     $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon|\mathbf{x},z, T^*]$
  \end{block}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Identification Results from the Literature}

  \begin{block}{Mahajan (2006) Theorem 1, Frazis \& Loewenstein (2003)}
    $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*] = 0$, plus ``Baseline'' $\implies \beta(\mathbf{x})$ identified\\
   \hfill \alert{Requires $(T^*,z)$ jointly exogenous.}
  \end{block}

  \begin{block}{Mahajan (2006) A.2}
    $\mathbb{E}[\varepsilon|\mathbf{x}, z, T^*, T] = \mathbb{E}[\varepsilon|\mathbf{x},T^*]$, plus ``Baseline'' $\implies \beta(\mathbf{x})$ identified\\
    \hfill \alert{Allows $T^*$ endogenous, but we prove this claim is false.}
  \end{block}


  \begin{block}{Open Question}
    Can we identify $\beta(\mathbf{x})$ when $T^*$ is endogenous? If so, how?
  \end{block}

  \note{Next: show you two known results that will play a role in what comes later: (1) simple bounds for mis-classification using only assumption that $z$ doesn't change $\alpha_0, \alpha_1$; (2) effect of mis-classification on IV estimator.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{First-stage Probabilities \& Mis-classification Bounds} 

  \begin{table}[h]
    \centering
  \begin{tabular}[h]{|c|c|}
    \hline
   Unobserved & Observed \\
    $p^*_k(\mathbf{x}) \equiv \mathbb{P}(T^*=1|\mathbf{x}, z=k)$ & 
    $p_k(\mathbf{x}) \equiv \mathbb{P}(T=1|\mathbf{x}, z=k)$\\
    \hline
  \end{tabular}
\end{table}


  \begin{block}{Relationship}
    \vspace{-1em}
   \[
     p_k^*(\mathbf{x}) = \frac{p_k(\mathbf{x}) - \alpha_0(\mathbf{x})}{1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})}, \quad k = 0,1
   \]
   \hfill\alert{\footnotesize $z$ does not affect $(\alpha_0, \alpha_1)$; denominator $\neq 0$}
  \end{block}

  \normalsize
  \begin{block}{Bounds for Mis-classification}
    \vspace{-1em}
    \[
      \alpha_0(\mathbf{x}) \leq p_k(\mathbf{x}) \leq 1 - \alpha_1(\mathbf{x}), \quad k = 0,1
    \]
   \hfill \alert{\footnotesize$\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) <1$}
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain, c]
  \frametitle{$\alpha_0 \leq \min_k \left\{p_k\right\}, \; \; \alpha_1 \leq \min_k \left\{1 - p_k\right\}$}
\begin{figure}[h]
  \centering
  \begin{tikzpicture}[scale=5]
    \draw [fill = lightgray] (0.66,0.75) rectangle (1,1);
    \draw [fill = cyan] (0,0) rectangle (0.25, 0.34);
    \draw [thick, <->] (0,1.1)
    node[above] {$\alpha_1$} -- (0,0) 
    node [below left] {$(0,0)$} -- (1.1,0) 
    node [right] {$\alpha_0$};
    \draw [thick] (0.25,0.02) -- (0.25,-0.02) node [below] {$p_\ell$}; 
    \draw [thick] (0.02,0.75) -- (-0.02,0.75) node [left] {$1 - p_\ell$}; 
    \draw [thick] (0.66,0.02) -- (0.66,-0.02) node [below] {$p_k$}; 
    \draw [thick] (0.02,0.34) -- (-0.02,0.34) node [left] {$1 - p_k$}; 
    \draw [thick, red] (0,1) to (1,0); 
    \node [right, red] at (0.06,0.95) {$\alpha_0 + \alpha_1 = 1$};
    \draw [thick] (0,1) -- (1,1) -- (1,0);
    \node [above right] at (1,1) {$(1,1)$};
    \end{tikzpicture}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What does IV estimate under mis-classification?}
  \begin{block}{Unobserved}
  \[
    \beta(\mathbf{x}) = \frac{\mathbb{E}[y|\mathbf{x},z=1] - \mathbb{E}[y|\mathbf{x},z=0]}{p^*_1(\mathbf{x}) - p^*_0(\mathbf{x})} 
  \]
  \end{block}

  \begin{block}{Wald (Observed)}
    \vspace{-1em}
    \small
    \[
      \frac{\mathbb{E}[y|\mathbf{x},z=1] - \mathbb{E}[y|\mathbf{x},z=0]}{p_1(\mathbf{x}) - p_0(\mathbf{x})} = \beta(\mathbf{x})\left[ \frac{p_1^*(\mathbf{x}) - p_0^*(\mathbf{x})}{p_1(\mathbf{x}) - p_0(\mathbf{x})} \right] = \alert{\frac{\beta(\mathbf{x})}{1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})} }
    \]
   
    \vspace{2em}
    \scriptsize
    \[
      \boxed{p_1^*(\mathbf{x}) - p_0^*(\mathbf{x}) = \frac{p_1(\mathbf{x}) - \alpha_0(\mathbf{x})}{1 - \alpha_0 - \alpha_1(\mathbf{x})} -   \frac{p_0(\mathbf{x}) - \alpha_0(\mathbf{x})}{1 - \alpha_0 - \alpha_1(\mathbf{x})} = \frac{p_1(\mathbf{x}) - p_0(\mathbf{x})}{1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})}}
    \]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Partial Identification Bounds for $\beta(\mathbf{x})$}
%
%    \footnotesize
%    \[
%      \boxed{ \beta(\mathbf{x}) = \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right] 
%     \left[\frac{\mathbb{E}\left[y|\mathbf{x},z=1\right] - \mathbb{E}\left[y|\mathbf{x},z=0\right]}{p_1(\mathbf{x}) - p_0(\mathbf{x})}\right] }
%    \]
%
%    \footnotesize
%    \[
%      \boxed{ 0 \leq \alpha_0 \leq \min_k \{p_k(\mathbf{x})\}, \quad 0 \leq \alpha_1 \leq \min_k \{1 - p_k(\mathbf{x})\}}
%    \]
%
%    \normalsize
%    \begin{block}{No Mis-classification}
%      $\alpha_0(\mathbf{x}) =  \alpha_1(\mathbf{x}) = 0 \implies \alert{\beta(\mathbf{x}) = }$ \alert{Wald}
%    \end{block}
%
%    \begin{block}{Maximum Mis-classification}
%      $\alpha_0(\mathbf{x}) = p_{\min}(\mathbf{x}), \, \alpha_1(\mathbf{x}) = 1 - p_{\max}(\mathbf{x})$
%
%      \vspace{-0.5em}
%      \begin{align*}
%        \implies 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) = p_{\max}(\mathbf{x}) - p_{\min}(\mathbf{x})
%      = |p_1(\mathbf{x}) - p_0(\mathbf{x})|\\
%      \implies \alert{\beta(\mathbf{x}) =\mbox{sign}\left\{ p_1(\mathbf{x}) - p_0(\mathbf{x}) \right\}\times (\mbox{Reduced Form})}
%    \end{align*}
%      
%    \end{block}
%  
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Partial Identification Bounds for $\beta(\mathbf{x})$}
%
%    \begin{block}{No Mis-classification}
%      $\beta(\mathbf{x}) = $ Wald 
%    \end{block}
%
%    \begin{block}{Maximum Mis-classification}
%      \vspace{-1.5em}
%      \begin{align*}
%        \beta(\mathbf{x}) &=\mbox{sign}\left\{ p_1(\mathbf{x}) - p_0(\mathbf{x}) \right\}\times (\mbox{Reduced Form})\\
%        &=\mbox{sign}\left\{ \mbox{Wald} \right\} \times |\mbox{Reduced Form}|
%      \end{align*}
%      
%      \vspace{1em}
%    \footnotesize Wald $> 0\iff \mbox{sign}\left\{ p_1(\mathbf{x}) - p_0(\mathbf{x}) \right\} = \mbox{sign}\left\{ \mbox{Reduced Form} \right\}$ \\
%    \footnotesize Wald $< 0\iff \mbox{sign}\left\{ p_1(\mathbf{x}) - p_0(\mathbf{x}) \right\} \neq \mbox{sign}\left\{ \mbox{Reduced Form} \right\}$ 
%    \end{block}
%
%    \vspace{1em}
%    \begin{alertblock}{
%        $\beta(\mathbf{x})$ has the same sign as the Wald and its magnitude is between that of Wald and Reduced Form.}
%    \end{alertblock}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Partial Identification Bounds for $\beta(\mathbf{x})$}

  \begin{block}{Known Result}
    \begin{itemize}
      \item $\beta(\mathbf{x})$ is between Wald and Reduced form; same sign as Wald.
      \item Doesn't rely on non-differential assumption or additive sep.\
      \item Frazis \& Loewenstein (2003), Ura (2016), \ldots
    \end{itemize}
    \end{block}

    \begin{alertblock}{Non-differential Assumption}
      \begin{itemize}
        \item $\mathbb{E}[\varepsilon|\mathbf{x},T^*,T,z] = \mathbb{E}[\varepsilon|\mathbf{x},T^*,z]$
        \item Used in literature to identify $\beta(\mathbf{x})$ when $T^*$ is exogenous. 
        \item Does it restrict the identified set when $T^*$ is \alert{endogenous}? 
        %\item Is $\beta(\mathbf{x})$ identified under the baseline assumptions?
      \end{itemize}
    \end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
  \frametitle{Restrictions from Non-differential Mis-classification?}
  \framesubtitle{(Suppress $\mathbf{x}$ for simplicity)}

  \footnotesize

  \begin{block}{Notation}
    \begin{itemize}
      \item $r_{tk} \equiv \mathbb{P}(T^*=1|T=t,z=k)$, function of $(\alpha_0, \alpha_1)$ and observables only 
      \item $z_k$ is shorthand for $z =k$\\
    \end{itemize}
  \end{block}

  \begin{block}{Iterated Expectations over $T^*$}
    \vspace{-1em}
  \begin{align*}
    \mathbb{E}(y|T=0,z_k) &= (1 - r_{0k})\mathbb{E}(y|T^*=0,T=0,z_k) + r_{0k}\mathbb{E}(y|T^*=1,T=0,z_k)\\
    \mathbb{E}(y|T=1,z_k) &= (1 - r_{1k}) \mathbb{E}(y|T^*=0,T=1,z_k) + r_{1k}\mathbb{E}(y|T^*=1,T=1,z_k)
  \end{align*}
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t,noframenumbering]
  \frametitle{Restrictions from Non-differential Mis-classification?}
  \framesubtitle{(Suppress $\mathbf{x}$ for simplicity)}

  \footnotesize

  \begin{block}{Notation}
    \begin{itemize}
      \item $r_{tk} \equiv \mathbb{P}(T^*=1|T=t,z=k)$, function of $(\alpha_0, \alpha_1)$ and observables only 
      \item $z_k$ is shorthand for $z =k$\\
    \end{itemize}
  \end{block}
  
  \begin{block}{Adding Non-differential Assumption}
    \vspace{-1em}
  \begin{align*}
    \mathbb{E}(y|T=0,z_k) &= (1 - r_{0k})\textcolor{blue}{\mathbb{E}(y|T^*=0,z_k)}\phantom{,T=0} + r_{0k}\textcolor{red}{\mathbb{E}(y|T^*=1,z_k)}\phantom{,T=0}\\
    \mathbb{E}(y|T=1,z_k) &= (1 - r_{1k}) \textcolor{blue}{\mathbb{E}(y|T^*=0,z_k)}\phantom{,T=1} + r_{1k}\textcolor{red}{\mathbb{E}(y|T^*=1,z_k)}\phantom{,T=1}
  \end{align*}

  \vspace{1em}
  \fbox{2 equations in 2 unknowns $\Rightarrow$ solve for $\mathbb{E}(y|T^*=t^*,z=k)$ given $(\alpha_0, \alpha_1)$.}

  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Restrictions from Non-differential Mis-classification?}

  \begin{block}{Law of Total Probability}
    \vspace{-1.5em}
  \[
    \boxed{\textcolor{blue}{F_{tk}} = (1 - r_{tk}) \alert{F^{0}_{tk}} + r_{tk}\alert{F^{1}_{tk}}}
  \]
  \scriptsize

  \vspace{-1em}
    \begin{align*}
    \textcolor{blue}{F_{tk}} &\equiv \mbox{\textcolor{blue}{Observed} CDF: } y|(T=t,z=k) \\
    \alert{F^{t^*}_{tk}} &\equiv \mbox{\alert{Unobserved} CDF: } y|(T^*=t^*,T=t,z=k)
    \end{align*}
  \end{block}

  \begin{block}{Previous Slide}
    \begin{itemize} 
    \small
      \item $r_{tk}$ observable given $(\alpha_0, \alpha_1)$
      \item $\mathbb{E}(T^*,T,z) = \mathbb{E}(T^*,z)$ observable given $(\alpha_0, \alpha_1)$
    \end{itemize}
    
  \end{block}

  \begin{alertblock}{Key Question}
    \small
  Given $(\alpha_0, \alpha_1)$ can we always find $(F_{tk}^0, F_{tk}^1)$ to satisfy the mixture model? 
  \end{alertblock}
    



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Restrictions from Non-differential Mis-classification?}

  \begin{block}{Equivalent Problem}
    \small
    Given a specified CDF $F$, probability $p$ and mean $\mu$, do there exist valid CDFs $(G,H)$ with $F = (1 - p) G + p H$ and $\mu = \mbox{mean}(H)$? 
\end{block}

%  \begin{block}{Equivalent Problem}
%    Given a specified CDF $\textcolor{red}{F}$, probability $\textcolor{red}{p}$ and mean $\textcolor{red}{\mu}$, do there exist valid CDFs $\textcolor{red}{(G,H)}$ with $\textcolor{red}{F = (1 - p) G + p H}$ and $\textcolor{red}{\mu = \mbox{mean}(H)}$? 
%  \end{block}

%\begin{block}{Solve to Eliminate $G$}
%  \small
%  $G = (F - p H) / (1 - p)$
%\end{block}

\begin{block}{Valid CDFS}
  \vspace{-1em}
  \scriptsize
  \begin{align*}
    0 \leq H \leq 1 & \\
    0 \leq G \leq 1 & \quad \iff \quad [F - (1 - p)]/p \leq H \leq F/p
  \end{align*}

  \normalsize
  \[
    \alert{\boxed{\max\left\{ 0, \, \frac{F(x)}{p} - \frac{1-p}{p} \right\}\leq H(x) \leq \min\left\{ 1, \frac{F(x)}{p} \right\}}}
  \]
\end{block}

\note{Explain how we solve to eliminate $G$ and get conditions on $H$ only!}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Restrictions from Non-differential Mis-classification?}
  \begin{block}{Notation}
    \footnotesize
    \vspace{-1em}
    \[
      \overline{H} \equiv \max\left\{ 0, \, \frac{F(x)}{p} - \frac{1-p}{p} \right\}, \quad
      \underline{H} \equiv \min\left\{ 1, \frac{F(x)}{p} \right\}
    \]
  \end{block}
  
  \begin{alertblock}{1\textsuperscript{st} Order Stochastic Dominance}
    \vspace{-1em}
      \[
        \overline{H}(x) \leq H(x) \leq \underline{H}(x) \quad \mbox{for all } x
      \]

      \vspace{-1em}
      \[
        \alert{\implies \underbrace{\int_{\mathbb{R}} x\, \underline{H}(dx)}_{\underline{\mu}(p,F)} \leq \underbrace{\int_{\mathbb{R}} x\, H(dx)}_{\mu} \leq \underbrace{\int_{\mathbb{R}} x\, \overline{H}(dx)}_{\overline{\mu}(p,F)}}
      \]
  \end{alertblock}

  \note{Explain the somewhat odd-looking convention for $\underline{H}$ and $\overline{H}$. Constraint from preceding slide is a stochastic dominance relationship. It implies an inequality for means.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

\begin{figure}[h]
  \centering
\resizebox{0.85\textwidth}{!}{%
  \input{./densities_point1.tex}
}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[noframenumbering]

\begin{figure}[h]
  \centering
\resizebox{0.85\textwidth}{!}{%
  \input{./densities_point2.tex}
}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[noframenumbering]

\begin{figure}[h]
  \centering
\resizebox{0.85\textwidth}{!}{%
  \input{./densities_point3.tex}
}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[noframenumbering]

\begin{figure}[h]
  \centering
\resizebox{0.85\textwidth}{!}{%
  \input{./densities_point4.tex}
}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[noframenumbering]

\begin{figure}[h]
  \centering
\resizebox{0.85\textwidth}{!}{%
  \input{./densities_point5.tex}
}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[noframenumbering]

\begin{figure}[h]
  \centering
\resizebox{0.85\textwidth}{!}{%
  \input{./densities_point6.tex}
}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[noframenumbering]

\begin{figure}[h]
  \centering
\resizebox{0.85\textwidth}{!}{%
  \input{./densities_point7.tex}
}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

\begin{figure}[h]
  \centering
\resizebox{0.87\textwidth}{!}{%
  \input{./identified_set.tex}
}
\end{figure}

\note{As $p$ approaches 1, the mean of $H$ is more tightly constrained: must be close to the mean of the observed distribution $F$, namely $-0.8$. As $p$ approaches zero, it is less and less constrained: since it contributes very little to the overall mixture, it can take on nearly any mean.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Restrictions from Non-differential Mis-classification}

  \begin{block}{Equivalent Problem}
    \small
    Given a specified CDF $F$, probability $p$ and mean $\mu$, do there exist valid CDFs $(G,H)$ with $F = (1 - p) G + p H$ and $\mu = \mbox{mean}(H)$? 
\end{block}

  \begin{alertblock}{Necessary and Sufficient Condition if $F$ is Continuous}
    \small
   \[
     \int_{-\infty}^{F^{-1}(p)} \frac{x}{p}\; f(x) \; dx \leq \;  \mu \; \leq \int_{F^{-1}(1-p)}^{+\infty} \frac{x}{p} \; f(x)\; dx
   \]
  \end{alertblock}

  \begin{block}{Sharp Identified Set}
    \small
    Includes only those $(\alpha_0, \alpha_1)$ at which the preceding condition is satisfied jointly for the mixtures $F_{tk} = (1 - r_{tk})F_{tk}^0 + r_{tk} F_{tk}^1$.
  \end{block}

  \note{Explain that we have to step back and realize that we have several of these mixture decompositions, corresponding to different values of $T$ and $z$. For each the $p$ and $\mu$ are determined from observables and $(\alpha_0, \alpha_1)$. This yields restrictions on the mis-classification probabilities, and since we know how they are related to $\beta$ via the Wald estimator, we obtain restrictions on $\beta$ as well.}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Sharp Identified Set under Baseline Assumptions}

  \begin{alertblock}{Theorem}
    Under baseline assumptions, sharp identified set for $\beta(\mathbf{x})$ is never a singleton, regardless of how many (discrete) values $z$ takes on.
  \end{alertblock}

  \vspace{2em}
  \begin{block}{Point identification from stronger assumptions?}
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Point Identification: 1st Ingredient}


  \vspace{-1em}

  \begin{block}{Reparameterization}
    \vspace{-1em}
\begin{align*}
  \theta_1(\mathbf{x}) &= \beta(\mathbf{x})/\left[ 1 - \alpha_0(\mathbf{x}) - \mathbf{\alpha}_1(\mathbf{x})  \right]\\
  \theta_2(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^2 \left[ 1 + \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})\right] \\
  \theta_3(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^3\left[ \left\{ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right\}^2 + 6\alpha_0(\mathbf{x})\left\{ 1 - \alpha_1(\mathbf{x}) \right\} \right]
\end{align*}

\[\boxed{\beta(\mathbf{x}) = 0 \iff \theta_1(\mathbf{x}) = \theta_2(\mathbf{x}) = \theta_3(\mathbf{x}) = 0}\]
  \end{block}

  \vspace{-1.5em}

  \begin{block}{Lemma}
    Baseline Assumptions $\implies \alert{\mbox{Cov}(y,z|\mathbf{x}) = \theta_1(\mathbf{x}) \mbox{Cov}(z,T|\mathbf{x})}$.
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Point Identification: 2nd Ingredient}

  \begin{block}{Assumption (II)}
    $\mathbb{E}[\varepsilon^2|\mathbf{x},z] = \mathbb{E}[\varepsilon^2|\mathbf{x}]$
  \end{block}

  \begin{block}{Lemma}
    (Baseline) + (II) $\implies$ 
    \[
      \alert{\mbox{Cov}(y^2,z|\mathbf{x}) = 2\mbox{Cov}(yT,z|\mathbf{x}) \theta_1(\mathbf{x}) -\mbox{Cov}(T,z|\mathbf{x})\theta_2(\mathbf{x})}
    \]
  \end{block}

  \begin{block}{Corollary}
    (Baseline) + (II) + $[\beta(\mathbf{x})\neq 0] \implies \left[ \alpha_1(\mathbf{x}) - \alpha_0(\mathbf{x}) \right]$ is identified. Hence, $\beta(\mathbf{x})$ is identified if mis-classification is one-sided.
  \end{block}
    

  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Point Identification: 3rd Ingredient}

  \begin{block}{Assumption (III)}
    \begin{enumerate}[(i)]
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^2|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z] = \mathbb{E}[\varepsilon^3|\mathbf{x}]$
  \end{enumerate}
  \end{block}
 
  \begin{block}{Lemma}
    (Baseline) + (II) + (III) $\implies$ 
  \small
\[
  \alert{\mbox{Cov}(y^3,z|\mathbf{x}) = 3 \mbox{Cov}(y^2T,z|\mathbf{x}) \theta_1(\mathbf{x}) -3\mbox{Cov}(yT,z|\mathbf{x}) \theta_2(\mathbf{x}) + \mbox{Cov}(T,z|\mathbf{x}) \theta_3(\mathbf{x})}
\]
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Point Identification Result}

  \small 

  \begin{alertblock}{Theorem}
    (Baseline) + (II) + (III) $\implies \beta(\mathbf{x})$ is point identified.
    If $\beta(\mathbf{x}) \neq 0$, then $\alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$ are likewise point identified.
  \end{alertblock}

  \begin{block}{Proof Sketch}
    \vspace{-0.5em}
    \begin{enumerate}
      \item $\beta(\mathbf{x})=0 \iff \theta_1(\mathbf{x}) =0$ so suppose this is not the case.  
      \item Lemmas: full-rank linear system in $\theta_1(\mathbf{x}), \theta_2(\mathbf{x}), \theta_3(\mathbf{x})$ \& observables.
      \item Non-linear eqs.\ relating $\theta_1(\mathbf{x}),\theta_2(\mathbf{x}), \theta_3(\mathbf{x})$ to  $\beta(\mathbf{x})$ and $\alpha_0(\mathbf{x}), \alpha_1(\mathbf{x})$. 
        Show that solution exists and is unique.
    \end{enumerate}
  \end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{block}{Sufficient Conditions for (II) and (III)}
    \vspace{-0.5em}
    \begin{enumerate}[(i)]
      \item $T$ is conditionally independent of $(\varepsilon,z)$ given $(T^*,\mathbf{x})$
      \item $z$ is conditionally independent of $\varepsilon$ given $\mathbf{x}$
    \end{enumerate}
  \end{block}
  
  MENTION THE ANGRIST ARGUMENT\ldots

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Just-Identified System of Moment Equalities}
  \framesubtitle{Suppress dependence on $\mathbf{x}$ to simplify the notation from here on\dots}

  \begin{block}{Collect Lemmas from Above:}
\small
\vspace{-2em}
\begin{align*}
  \mbox{Cov}(y,z) - \mbox{Cov}(T,z) \theta_1 &= 0\\
  \mbox{Cov}(y^2,z) - 2\mbox{Cov}(yT,z) \theta_1 + \mbox{Cov}(T,z)\theta_2 &= 0\\
  \mbox{Cov}(y^3,z) - 3 \mbox{Cov}(y^2T,z) \theta_1 + 3\mbox{Cov}(yT,z) \theta_2 - \mbox{Cov}(T,z) \theta_3 &= 0
\end{align*}
\end{block}

\begin{block}{Notation: Observed Data Vector}
  \vspace{-1.5em}
  \[
\mathbf{w}_i' = (T_i, y_i, y_iT_i, y_i^2, y_i^2 T_i, y_i^3)
\]
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Just-Identified System of Moment Equalities}

\[
\boxed{\mathbb{E}\left[
  \big(\boldsymbol{\Psi}'(\boldsymbol{\theta})\mathbf{w}_i - \boldsymbol{\kappa}\big) \otimes 
\left(
\begin{array}{c}
  1 \\ z_i
\end{array}\right)
\right] = \mathbf{0}}
\]

\footnotesize
\begin{align*}
\boldsymbol{\Psi} &= \left[
  \begin{array}{ccc}
    \boldsymbol{\psi}_1 & \boldsymbol{\psi_2} & \boldsymbol{\psi_3}
\end{array}\right]
& \boldsymbol{\kappa} &= (\kappa_1, \kappa_2, \kappa_3)' \equiv \mbox{ ``Intercepts''}\\
  \boldsymbol{\psi}_1' &= \left[
  \begin{array}{cccccc}
-\theta_1 & 1 & 0 & 0 & 0 & 0
  \end{array}
\right] & \theta_1 &= \beta / (1 - \alpha_0 - \alpha_1)\\
  \boldsymbol{\psi}_2' &= \left[
  \begin{array}{cccccc}
\theta_2& 0& -2\theta_1& 1& 0& 0
  \end{array}
\right] & \theta_2 &= \theta_1^2 \left[ 1 + \alpha_0 - \alpha_1 \right]\\
  \boldsymbol{\psi}_3' &= \left[
  \begin{array}{cccccc}
-\theta_3& 0& 3\theta_2& 0& -3\theta_1& 1
  \end{array}
\right] & \theta_3 &= \theta_1^3 \left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0 (1 - \alpha_1) \right] 
\end{align*}

\begin{alertblock}{Weak Identification Problem}
  Moment conditions are uninformative about $(\alpha_0,\alpha_1)$ when $\beta$ is small.
  Moreover, $(\alpha_0,\alpha_1)$ could be on the boundary of the parameter space.
  LINK TO SIMULATION RESULTS!!!
\end{alertblock}

%\begin{align*}
%\kappa_1 &= c - \alpha_0 \theta_1\\
%  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
%  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]
%\end{align*}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Non-standard Inference Problem}

\begin{itemize}
  \item $\beta$ small $\Rightarrow$ moment equalities uninformative about $(\alpha_0, \alpha_1)$
  \item $(\alpha_0, \alpha_1)$ could be on the boundary of the parameter space
  \item Partial identification bounds remain informative even if $\beta$ is small or zero
  \item Same problem for other estimators from the literature but hasn't been pointed out\ldots
\end{itemize}

\begin{alertblock}{Our Approach}
 Identification-robust inference combining equality and inequality moment conditions based on generalized moment selection (GMS)
\end{alertblock}


SHOW FULL CONTINUUM OF MOMENTS? (TO ILLUSTRATE THAT IT'S NOT JUST A PROBLEM WITH THE MOMENTS WE HAPPEN TO USE)

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Inference With Moment Equalities and Inequalities}
  \small

\begin{block}{Moment Conditions}
  $\mathbb{E} \left[ m_j(\mathbf{w}_i,\vartheta_0) \right] \geq 0, \quad j = 1, \cdots, J$\\
  $\mathbb{E} \left[ m_j(\mathbf{w}_i,\vartheta_0) \right]  = 0, \quad j = J+1, \cdots, J + K$
\end{block}


\begin{block}{Test Statistic}
  \vspace{-1em}
\[
  T_n(\vartheta) = \sum_{j=1}^J \left[\frac{\sqrt{n}\; \bar{m}_{n,j}(\vartheta)}{\widehat{\sigma}_{n,j}(\vartheta)}\right]^2_- + \sum_{j=J+1}^{J+K} \left[\frac{\sqrt{n}\; \bar{m}_{n,j}(\vartheta)}{\widehat{\sigma}_{n,j}(\vartheta)}\right]^2
\]
\footnotesize
\begin{align*}
[x]_- &= \min\left\{ x, 0 \right\}\\
\bar{m}_{n,j}(\vartheta) &= n^{-1} \sum_{i=1}^{n} m_j(\mathbf{w}_i, \vartheta)\\
\widehat{\sigma}^2_{n,j}(\vartheta) &=  \mbox{consistent est.\ of } \mbox{AVAR}\left[  \sqrt{n}\; \bar{m}_{n,j}(\vartheta)\right]
\end{align*}
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Moment Inequalities: Part I}

  $\alpha_0(\mathbf{x}) \leq p_k \leq 1 - \alpha_1$ becomes $\alert{\mathbb{E}\left[ m_{1k}^I(\mathbf{w}_i,\boldsymbol{\vartheta} ) \right] \geq \mathbf{0}}$ for all $k$ where
\[
  m_{1k}^I(\mathbf{w}_i, \boldsymbol{\vartheta}) \equiv \left[
  \begin{array}{l}
    \mathbf{1}(z_i=k)(T - \alpha_0) \\
    \mathbf{1}(z_i = k) (1 - T_i - \alpha_1) 
  \end{array}
\right]
\]
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Moment Inequalities: Part II}

  \scriptsize

  For all $k$, we have $\alert{\mathbb{E}[m_{2k}^I\big(\mathbf{w}_i, \boldsymbol{\vartheta}, \mathbf{q}_k)]\geq 0}$ where
\[
  m_{2k}^I\big(\mathbf{w}_i, \boldsymbol{\vartheta}, \mathbf{q}_k) \equiv \left[
  \begin{array}{r}
    y_i \mathbf{1}\left( z_i=k \right)\left\{(T_i - \alpha_0) - \mathbf{1}(y_i \leq \underline{q}_{0k})  (1 - T_i)\left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right)\right\} \\
    - y_i \mathbf{1}(z_i=k) \left\{ (T_i - \alpha_0) -  \mathbf{1}(y_i > \overline{q}_{0k}) (1 - T_i) \left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right) \right\} \\
    y_i \mathbf{1}\left( z_i=k \right)\left\{(T_i - \alpha_0) - \mathbf{1}(y_i \leq \underline{q}_{1k})  T_i\left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right)\right\} \\
    - y_i \mathbf{1}(z_i=k) \left\{ (T_i - \alpha_0) -  \mathbf{1}(y_i > \overline{q}_{1k}) T_i \left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right) \right\} 
\end{array}
\right] 
\]
and $\alert{\mathbf{q}_k \equiv ( \underline{q}_{0k},\, \overline{q}_{0k},\, \underline{q}_{1k}, \,\overline{q}_{1k})'}$ defined by $\alert{\mathbb{E}[h_k^I(\mathbf{w}_i,\boldsymbol{\vartheta},\mathbf{q}_k)]=0}$ with
\[
  h_k^I(\mathbf{w}_i,\boldsymbol{\vartheta},\mathbf{q}_k) = \left[
  \begin{array}{l}
    \mathbf{1}(y_i \leq \underline{q}_{0k}) \mathbf{1}(z_i=k)(1 - T_i) 
    - \left( \frac{\alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(T_i-\alpha_0)\\ 
    \mathbf{1}(y_i \leq \overline{q}_{0k}) \mathbf{1}(z_i=k)(1 - T_i)
    - \left( \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(1 - T_i-\alpha_1)\\
    \mathbf{1}(y_i \leq \underline{q}_{1k}) \mathbf{1}(z_i=k)T_i
    - \left( \frac{1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(T_i-\alpha_0)\\ 
    \mathbf{1}(y_i \leq \overline{q}_{1k}) \mathbf{1}(z_i=k)T_i 
    - \left( \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z_i=k)(1 - T_i-\alpha_1)
  \end{array}
\right]
\]

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Inference via Generalized Moment Selection}
  \framesubtitle{Andrews \& Soares (2010)}

  \small

  \begin{block}{Moment Selection Step}
    If $\displaystyle\frac{\sqrt{n}\,\bar{m}_{n,j}(\vartheta_0)}{\widehat{\sigma}_{n,j}(\vartheta_0)} > \sqrt{\log n}$ then drop inequality $j$
  \end{block}

  \begin{block}{Critical Value} 
    \begin{itemize}
      \item $\sqrt{n}\, \bar{m}_n(\vartheta_0) \rightarrow_d$ normal limit with covariance matrix $\Sigma(\vartheta_0)$
      \item Use this to bootstrap the limit distribution of the test statistic.
    \end{itemize}
  \end{block}

  \begin{block}{Theoretical Guarantees}
    Uniformly valid test of $H_0\colon \vartheta = \vartheta_0$ \alert{regardless of whether $\vartheta_0$ is identified}.
    Not asymptotically conservative.
  \end{block}

  \begin{block}{Drawback}
   \emph{Joint test} for the whole parameter vector but we're only interested in $\beta$
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Moment Equalities}
%Let $\boldsymbol{\vartheta} = (\alpha_0, \alpha_1)$ and $\boldsymbol{\gamma} = (\boldsymbol{\kappa}, \theta_1)$ 
%
%\begin{equation}
%  \mathbb{E}[m^I(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \mathbf{q}_0)] \geq \mathbf{0}, \quad 
%  \mathbb{E}[m^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)] = \mathbf{0}
%  \label{eq:MCs_ineq_eq}
%\end{equation}
%where $m^I = (m_1^{I'}, m_2^{I'})'$ and 
%\begin{equation}
%  m^E(\mathbf{w}_i, \boldsymbol{\vartheta}_0, \boldsymbol{\gamma}_0)= \left[
%  \begin{array}{c}
% \left\{ \boldsymbol{\psi}_2'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \kappa_2 \right\}z_i \\
% \left\{ \boldsymbol{\psi}_3'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \kappa_3 \right\}z_i 
%  \end{array}
%\right].
%\label{eq:mE}
%\end{equation}
%
%\begin{equation}
%  h^E(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\gamma})  =  \left[
% \begin{array}{l}
%   \boldsymbol{\Psi}'(\theta_1, \alpha_0, \alpha_1) \mathbf{w}_i - \boldsymbol{\kappa}\\
%   \left\{\boldsymbol{\psi}_1'(\theta_1)\mathbf{w}_i - \kappa_1\right\}z_i
% \end{array}
% \right].
% \label{eq:hE}
%\end{equation}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Bonferroni-Based Inference Procedure}
  

  \begin{block}{Leverage Special Structure of Model}
    \begin{itemize}
      \item $\beta$ only enters MCs through $\theta_1 = \beta/ (1 - \alpha_0 - \alpha_1)$
      \item Inference for $\theta_1$ is standard if $z$ is a strong IV. 
      \item $(\boldsymbol{\kappa}, \mathbf{q})$ strongly identified under null for $(\alpha_0, \alpha_1)$
    \end{itemize}

    \begin{block}{Procedure}
      \begin{enumerate}
        \item Concentrate out $(\theta_1, \boldsymbol{\kappa}, \boldsymbol{q}) \implies$ joint GMS test for $(\alpha_0, \alpha_1)$ 
        \item Invert $\implies (1 - \delta_1)\times 100\%$ confidence set for $(\alpha_0, \alpha_1)$
        \item Project $\implies$ CI for $(1 - \alpha_0 - \alpha_1)$
        \item Construct standard $(1 - \delta_2)\times 100\%$ IV CI for $\theta_1$
        \item Bonferroni $\implies (1 - \delta - \delta_2)\times 100\%$ CI for $\beta$
      \end{enumerate}
    \end{block}
    
  \end{block}

  \note{Explain that the procedure works well in simulations etc. Possibly add link to simulation here.}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
Short empirical illustration using Burde \& Linden, including picture of joint confidence region for $(\alpha_0, \alpha_1)$ etc.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Conclusion}

  \begin{enumerate}
    \item Identification and inference for effect of binary, mis-classified, endogenous regressor.
    \item Show that only existing point identification result is incorrect. 
    \item Derive sharp identified set for $\beta(\mathbf{x})$ under standard assumptions.
    \item Prove point identification of $\beta(\mathbf{x})$ under slightly stronger assumptions.
    \item Point out problem of weak identification in mis-classification models, develop identification-robust inference for $\beta(\mathbf{x})$. 
  \end{enumerate}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Related Past and Current Research}
  Talk about how this paper fits into a research agenda concerning measurement error: the beliefs paper, this paper, returns to lying (with Arthur), and biased measurements of displacement in the paper with Camilo.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simulation DGP: $y = \beta T^* + \varepsilon$}
  \framesubtitle{Sample Size = 1000;  Simulation Replications = 2000} 
  \begin{block}{Errors}
      $(\varepsilon, \eta) \sim $ jointly normal, mean 0, variance 1, correlation 0.5.
  \end{block}
  \begin{block}{First-Stage}
      \begin{itemize}
        \item Half of observations have $z=1$, the rest have $z=0$.
        \item $T^* = \mathbf{1}\left\{ \gamma_0 + \gamma_1 z + \eta > 0 \right\}$
        \item $\mathbb{P}(T^* = 0|z =1) = \mathbb{P}(T^*=1|z=0) = 0.15$
      \end{itemize}
  \end{block}
  \vspace{-1em}


  \begin{block}{Mis-classification}
      \begin{itemize}
        \item $T|T^*=0 \sim \mbox{Bernoulli}(\alpha_0)$
        \item $T|T^*=1 \sim \mbox{Bernoulli}(1-\alpha_1)$
      \end{itemize}
  \end{block}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{table}[htbp]
  \scriptsize
  \centering
  \input{../../tab/GMM_CIs_na_1000.tex}
  \caption{Percentage of simulation replications for which the standard GMM CI fails to exist.}
  %\caption{Percentage of replications for which the standard GMM confidence interval based on Equation \ref{eq:MCs_endog} fails to exist, either because the point estimate is NaN or the asymptotic covariance matrix is numerically singular. Calculations are based on  2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
\end{table}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{table}[htbp]
  \scriptsize
  \centering
  \input{../../tab/GMM_CIs_cover_1000.tex}
  \caption{Coverge of nominal 95\% GMM CI, conditional on existence.} 
  %\caption{Coverage (\%) of the standard nominal 95\% GMM confidence interval for $\beta$ based on Equation \ref{eq:MCs_endog}. Coverage is calculated only for those simulation draws for which the interval exists. (See Table \ref{tab:GMM_na_1000}.) Calculations are based on 2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
\end{table}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

\begin{table}[htbp]
  \scriptsize
  \centering
  \input{../../tab/GMM_CIs_width_1000.tex}
  \caption{Median width of nominal 95\% GMM CI, conditional on existence.}
  %\caption{Median width of the standard nominal 95\% GMM confidence interval for $\beta$ based on Equation \ref{eq:MCs_endog}. Coverage is calculated only for those simulation draws for which the interval exists. Calculations are based on 2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
\end{table}

\end{frame}
\begin{frame}
\begin{table}
  \scriptsize
  \centering
  \input{../../tab/alphas_cover_97point5_1000.tex}
  \caption{Coverage (1 - size) of nominal 97.5\% GMS joint test for $(\alpha_0, \alpha_1)$.}
  %\caption{Coverage probability (1 - size) in percentage points of a 97.5\% GMS joint test for $\alpha_0$ and $\alpha_1$ using Algorithm \ref{alg:GMS} with $n = 1000$. Calculations are based on 10,000 replications of the DGP from Section \ref{sec:DGP}.}
\end{table}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{table}[htbp]
  \centering
  \scriptsize
  \input{../../tab/bonf_CIs_cover_1000.tex}
  \caption{Coverage of nominal $>95\%$ Bonferroni CI for $\beta$}
  %\caption{Coverage probability in percentage points of a nominal $>95\%$ Bonferroni confidence interval for $\beta$ using Algorithm \ref{alg:Bonferroni} with $n = 1000, R = 5000$ and $\delta_1 = \delta_2 = 0.025$. Calculations are based on 2000 replications of the DGP from Section \ref{sec:DGP}.}
\end{table}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{table}
  \scriptsize
  \centering
  \input{../../tab/bonf_CIs_width_1000.tex}
  \caption{Median width of nominal $>95\%$ Bonferroni CI for $\beta$.}
  %\caption{Median width of a nominal $>95\%$ Bonferroni confidence interval for $\beta$ using Algorithm \ref{alg:Bonferroni} with $n = 1000, R = 5000$ and $\delta_1 = \delta_2 = 0.025$. Calculations are based on 2000 replications of the DGP from Section \ref{sec:DGP}.}
  %\label{tab:bonf_width_1000}
\end{table}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\begin{figure}[h]
%  \centering
%\resizebox{0.75\textwidth}{!}{%
%  \input{../../fig/bonf_point5_1000.tex}
%}
%\caption{Coverage of nominal $>95\%$ Bonferroni CI for $\beta$: with and without non-differential assumption, $\beta = 0.5$} 
%  
%  %\caption{Coverage curves (1 - power) for $\beta$ when the truth is $\beta = 0.5$, from a nominal $>95\%$ Bonferroni confidence interval using Algorithm \ref{alg:Bonferroni}, with $n = 1000$ and $R = 5000$. The solid curve uses all moment inequalities from Section \ref{sec:inequalities} in the GMS step, while the dashed curve excludes $m_{2}^I$, those implied by non-differential measurement error. The dashed horizontal line gives the nominal coverage (95\%), while dashed vertical lines are the reduced form estimand (left) and the IV estimand (right). Calculations are based on 2000 replications of the DGP from Section \ref{sec:DGP}.}
%\end{figure}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\begin{figure}[h]
%  \centering
%\resizebox{0.75\textwidth}{!}{%
%  \input{../../fig/bonf_1_1000.tex}
%}
%\caption{Coverage of nominal $>95\%$ Bonferroni CI for $\beta$: with and without non-differential assumption, $\beta = 1$} 
%  
%  %\caption{Coverage curves (1 - power) for $\beta$ when the truth is $\beta = 1$, from a nominal $>95\%$ Bonferroni confidence interval using Algorithm \ref{alg:Bonferroni}, with $n = 1000$ and $R = 5000$. The solid curve uses all moment inequalities from Section \ref{sec:inequalities} in the GMS step, while the dashed curve excludes $m_{2}^I$, those implied by non-differential measurement error. The dashed horizontal line gives the nominal coverage (95\%), while dashed vertical lines are the reduced form estimand (left) and the IV estimand (right). Calculations are based on 2000 replications of the DGP from Section \ref{sec:DGP}.}
%\end{figure}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{table}
  \scriptsize
  \centering
  \input{../../tab/bonf_CIs_width_1000.tex}
  \caption{Median width of nominal $>95\%$ Bonferroni CI for $\beta$.}
  %\caption{Median width of a nominal $>95\%$ Bonferroni confidence interval for $\beta$ using Algorithm \ref{alg:Bonferroni} with $n = 1000, R = 5000$ and $\delta_1 = \delta_2 = 0.025$. Calculations are based on 2000 replications of the DGP from Section \ref{sec:DGP}.}
  %\label{tab:bonf_width_1000}
\end{table}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

\begin{table}[htbp]
  \scriptsize
  \centering
  \input{../../tab/GMM_CIs_width_1000.tex}
  \caption{Median width of nominal 95\% GMM CI, conditional on existence.}
  %\caption{Median width of the standard nominal 95\% GMM confidence interval for $\beta$ based on Equation \ref{eq:MCs_endog}. Coverage is calculated only for those simulation draws for which the interval exists. Calculations are based on 2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
\end{table}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

\begin{table}
  \scriptsize
  \centering
  \input{../../tab/twostep_CIs_cover_1000.tex}
  \caption{Coverage of hybrid CI constructed from nominal 95\% GMM and $>95\%$ Bonferroni intervals.}
  %\caption{Coverage probabilities (\%) of a hybrid confidence interval constructed from the nominal $95\%$ standard GMM interval and the $>95\%$ Bonferroni confidence interval for $\beta$ using Algorithm \ref{alg:Bonferroni} with $n = 1000, R = 5000$ and $\delta_1 = \delta_2 = 0.025$. The hybrid interval reports Bonferroni unless the GMM interval exists and is contained within the Bonferroni interval. Calculations are based on 2000 replications of the DGP from Section \ref{sec:DGP}.}
\end{table}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{table}
  \scriptsize
  \centering
  \input{../../tab/twostep_CIs_width_1000.tex}
  \caption{Median width of hybrid CI constructed from nominal 95\% GMM and $>95\%$ Bonferroni intervals.}
  %\caption{Median width of a hybrid confidence interval constructed from the nominal $95\%$ standard GMM interval and the $>95\%$ Bonferroni confidence interval for $\beta$ using Algorithm \ref{alg:Bonferroni} with $n = 1000, R = 5000$ and $\delta_1 = \delta_2 = 0.025$. The hybrid interval reports Bonferroni unless the GMM interval exists and is contained within the Bonferroni interval. Calculations are based on 2000 replications of the DGP from Section \ref{sec:DGP}.}
\end{table}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{figure}[h]
  \centering
%\resizebox{0.75\textwidth}{!}{%
%  \input{../../fig/bonf_vs_twostep_1_1000.tex}
%}
  
  \caption{Coverage of hybrid vs.\ $>95\%$ Bonferroni CIs: $\beta = 1$}
  %\caption{Comparison of Coverage curves (1 - power) for $\beta$ when the truth is $\beta = 1$: the solid curve corresponds the Bonferroni nominal $>95\%$ interval from Algorithm \ref{alg:Bonferroni} and the dashed curve to the hybrid interval from Tables \ref{tab:twostep_cover_1000}--\ref{tab:twostep_width_1000}. The dashed horizontal line gives the nominal coverage (95\%), while dashed vertical lines are the reduced form estimand (left) and the IV estimand (right). Results are based on 2000 simulation replications from the DGP in Section \ref{sec:DGP} with $n = 1000$.}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{figure}[h]
  \centering
%\resizebox{0.75\textwidth}{!}{%
%  \input{../../fig/bonf_vs_twostep_2_1000.tex}
%}
  
  \caption{Coverage of hybrid vs.\ $>95\%$ Bonferroni CIs: $\beta = 2$}
  %\caption{Comparison of Coverage curves (1 - power) for $\beta$ when the truth is $\beta = 2$: the solid curve corresponds the Bonferroni nominal $>95\%$ interval from Algorithm \ref{alg:Bonferroni} and the dashed curve to the hybrid interval from Tables \ref{tab:twostep_cover_1000}--\ref{tab:twostep_width_1000}. The dashed horizontal line gives the nominal coverage (95\%), while dashed vertical lines are the reduced form estimand (left) and the IV estimand (right). Results are based on 2000 simulation replications from the DGP in Section \ref{sec:DGP} with $n = 1000$.}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{figure}[h]
  \centering
%\resizebox{0.75\textwidth}{!}{%
%  \input{../../fig/bonf_vs_twostep_3_1000.tex}
%}
  \caption{Coverage of hybrid vs.\ $>95\%$ Bonferroni CIs: $\beta = 3$}
  %\caption{Comparison of Coverage curves (1 - power) for $\beta$ when the truth is $\beta = 3$: the solid curve corresponds the Bonferroni nominal $>95\%$ interval from Algorithm \ref{alg:Bonferroni} and the dashed curve to the hybrid interval from Tables \ref{tab:twostep_cover_1000}--\ref{tab:twostep_width_1000}. The dashed horizontal line gives the nominal coverage (95\%), while dashed vertical lines are the reduced form estimand (left) and the IV estimand (right). Results are based on 2000 simulation replications from the DGP in Section \ref{sec:DGP} with $n = 1000$.}
  
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
