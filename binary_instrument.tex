%!TEX root = ./main.tex
\section{Cleaner Proof of Mahajan's Main Result}

We can use arguments similar to those presented in \cite{Lewbel}, who studies a different version of the problem, to prove the main result of \cite{Mahajan}, the one with an exogenous regressor, much more easily.
The idea is to 

\cite{Lewbell} shows that under an exogenous but missclasified treatment,
and an instrument that takes on at least three values, the treatment
effect is identified. 
The model is 
\[
\mathbb{E}[Y|T^{*},T]=\alpha+\beta T^{*}
\]
Using iterated expectations over the distribution of $T^{*}$ given
$T$, 
\begin{eqnarray*}
\mathbb{E}[Y|T]&=& \mathbb{E}_{T^{*}|T}\left[\mathbb{E}[Y|T^{*},T]\right]\\
&=& \mathbb{P}(T^{*}=1|T)\mathbb{E}[Y|T^{*}=1]+\mathbb{P}(T^{*}=0|T)\mathbb{E}[Y|T^{*}=0]\\
&=& \mathbb{P}(T^{*}=1|T)(\alpha+\beta)+\mathbb{P}(T^{*}=0|T)\alpha\\
&=& \alpha+\mathbb{P}(T^{*}=1|T)\beta
\end{eqnarray*}
which implies that 
\[
\beta_{OLS}= \mathbb{E}[Y|T=1]-\mathbb{E}[Y|T=0]=\left[\mathbb{P}(T^{*}=1|T=1)-\mathbb{P}(T^{*}=1|T=0)\right]\beta
\]
Lewbel defines 
$$M(\alpha_{0},\alpha_{1},p)=\mathbb{P}(T^{*}=1|T=1)-\mathbb{P}(T^{*}=1|T=0)$$
implying that $\beta_{OLS} =\beta M(\alpha_{0},\alpha_{1},p)$.
It turns out that we can re-express $M(\alpha_0, \alpha_1,p)$ as
\[
M(\alpha_{0},\alpha_{1},p)=\frac{1}{1-\alpha_{0}-\alpha_{1}}\left[1-\frac{(1-\alpha_{1})\alpha_{0}}{p}-\frac{(1-\alpha_{0})\alpha_{1}}{1-p}\right]
\]
To see why this is the case first note that, by Bayes' Rule,
\begin{equation*}
M(\alpha_{0},\alpha_{1},p) = \frac{(1-\alpha_{1})p^{*}}{p}-\frac{\alpha_{1}p^{*}}{1-p}
=p^{*}\left[\frac{(1-\alpha_{1})(1-p)-\alpha_{1}p}{p(1-p)}\right]
\end{equation*}
Now, by the Law of Total Probability,
\begin{align*}
  p &= P(T=1|T^*=1)p^* + P(T=1|T^*=0)(1-p^*)\\
  &= (1 - \alpha_0)p^* + \alpha_0 (1-p^*)\\
  &= (1 - \alpha_0 - \alpha_1)p^* + \alpha_0
\end{align*}
Rearranging, we see that $p^* = (p - \alpha_0)/(1 - \alpha_0 - \alpha_1)$.
Substituting this into the expression for $M(\alpha_0, \alpha_1, p)$ and simplifying,
\begin{align*}
  M(\alpha_{0},\alpha_{1},p) &=\frac{p-\alpha_{0}}{1-\alpha_{0}-\alpha_{1}}\left[\frac{(1-\alpha_{1})(1-p)-\alpha_{1}p}{p(1-p)}\right]\\
&=\frac{1}{1-\alpha_{0}-\alpha_{1}}\left[\frac{(p-\alpha_{0})(1-\alpha_{1})(1-p)-(p-\alpha_{0})\alpha_{1}p}{p(1-p)}\right]\\
&=\frac{1}{1-\alpha_{0}-\alpha_{1}}\left[1-\frac{(1-p)(1-\alpha_{1})\alpha_{0}+p\alpha_{1}-\alpha_{0}\alpha_{1}p}{p(1-p)}\right]\\
&=\frac{1}{1-\alpha_{0}-\alpha_{1}}\left[1-\frac{(1-p)(1-\alpha_{1})\alpha_{0}}{p(1-p)}-\frac{p\alpha_{1}(1-\alpha_{0})}{p(1-p)}\right]\\
&=\frac{1}{1-\alpha_{0}-\alpha_{1}}\left[1-\frac{(1-\alpha_{1})\alpha_{0}}{p}-\frac{(1-\alpha_{0})\alpha_{1}}{1-p}\right]
\end{align*}

Now, the instrument $z$ is assumed to be discrete and to take on at least three distinct values.
Let $\beta_{OLS}^k$ denote the OLS estimator based only on observations for which $z = z_k$, where $z_k$ is a particular value in the support of $z_k$, that is
\begin{equation*}
  \beta_{OLS}^k = \frac{Cov(T,Y| z = z_k)}{Var(T|z=z_k)} 
\end{equation*}
and let $p_k = E(T|z=z_k)$.
The denominator of the expression for $\beta_{OLS}^k$ is simply $Var(T|z=z_k) = p_k(1-p_k)$.
For the numerator, note that
\begin{align*}
  Cov(T,y|z) &= E(Ty|z) - E(T|z)E(y|z)\\
  &= E_{T|z}\left[E\left( y|T,z \right)T  \right] - E(T|z) E_{T|z}\left[ E(y|T,z) \right]\\
  &= E(y|T=1,z)E(T|z)\\
  &\quad - E(T|z)\left\{ E(T|z)E(y|T=1,z) + [1 - E(T|z)]E(y|T=0,z) \right\}\\
  &= E(T|z)\left[ 1-E(T|z) \right]\left\{ E(y|T=1,z) - E(y|T=0,z) \right\}
\end{align*}
by iterated expectations over the distribution of $T^*$ given $T$ and $z$.
Thus, 
\begin{equation*}
  \beta_{OLS}^k = E\left( y | T=1,z=z_k \right) - E(y|T=0,z=z_k)
\end{equation*}
and finally, since $E(y|T,z) = \alpha + \beta P(T^*=1|T,z)$, we see that
\begin{equation*}
  \beta_{OLS}^k = \beta \left\{P\left( T^*=1|T=1,z=z_k \right) - P(T^*=1|T=0, z=z_k)  \right\}
\end{equation*}
Notice that this expression looks almost identical to $\beta_{OLS} = \beta M(\alpha_0, \alpha_1, p)$ from above.
The only difference is that we condition on $z=z_k$.
Because we assume that the mis-classification probabilities are independent of $z$, 
\begin{align*}
  P(T=1|T^*=1, z=z_k) &= P(T=1|T^*=1) = 1-\alpha_1\\
  P(T=1|T^*=0, z=z_k) &= P(T=1|T^*=0) = \alpha_0
\end{align*}
from which it follows by the Law of Total Probability that
\begin{equation*}
  p_k = P(T=1|z=z_k) = (1-\alpha_1) p_k^* + \alpha_0\left\{ 1 - p_k^* \right\}
\end{equation*}
where $p^*_k = P(T^*=1|z=z_k)$ and thus,
\begin{equation*}
  p^*_k = \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1}
\end{equation*}
Now, by Bayes' Rule and again using the fact that the mis-classification probabilities do not depend on $z$, so that conditioning on $z=z_k$ is superfluous given that we have already conditioned on $T^*$, we have
\begin{align*}
  \frac{\beta_{OLS}^k}{\beta} &= P\left( T^*=1|T=1,z=z_k \right) - P(T^*=1|T=0, z=z_k) \\
  &= \frac{P(T=1|T^*=1,z=z_k)p^*_k}{p_k} - \frac{P(T=1|T^*=1,z=z_k)p^*_k}{(1 - p_k)}\\
  &=
  \left( \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)\left[ \frac{(1-p_k)(1-\alpha_1) - p_k \alpha_0}{p_k(1-p_k)} \right]
\end{align*}
This is \emph{exactly} the same expression as $M(\alpha_0, \alpha_1,\cdot)$, only evaluated at $p_k$ rather than $p$.
This means that we can re-use the algebra from above:
\begin{equation}
  \beta_{OLS}^{k} = \beta M(\alpha_0, \alpha_1, p_k) = \beta\left\{ \frac{1}{1-\alpha_0 - \alpha_1}\left[ 1 - \frac{\alpha_0(1-\alpha_1)}{p_k} - \frac{(1-\alpha_0)\alpha_1}{1 - p_k} \right] \right\}
\end{equation}
which gives us an equation for \emph{each} value $z_k$ in the support of $z$.
Solving the expressions for $\beta_{OLS}$ and $\beta_{OLS}^k$ for $\beta$ and equating them yields an equation of the form
\begin{equation*}
  \beta_{OLS}^k M(\alpha_0, \alpha_1, p) = \beta_{OLS} M(\alpha_0, \alpha_1, p_k)
\end{equation*}
for \emph{each} value of $k$.
Using our expression for the function $M$ from above and multiplying through by $1 - \alpha_0 - \alpha_1$ gives
\[
\beta_{OLS}\left[1-\frac{(1-\alpha_{1})\alpha_{0}}{p_{k}}-\frac{(1-\alpha_{0})\alpha_{1}}{1-p_{k}}\right] = \beta^{k}_{OLS}\left[1-\frac{(1-\alpha_{1})\alpha_{0}}{p}-\frac{(1-\alpha_{0})\alpha_{1}}{1-p}\right]=0
\]
Rearranging this expression gives
\[
(1-\alpha_{1})\alpha_{0}\left[\frac{\beta^{k}_{OLS}}{p}-\frac{\beta_{OLS}}{p_{k}}\right]+(1-\alpha_{0})\alpha_{1}\left[\frac{\beta^{k}_{OLS}}{1-p}-\frac{\beta_{OLS}}{1-p_{k}}\right]=\beta^{k}_{OLS}-\beta_{OLS}
\]
which is an expression of the form
\[
B_{0}w_{0}^{k}+B_{1}w_{1}^{k}=w_{2}^{k}
\]
where the unknowns $B_0, B_1$ are defined as
\begin{align*}
  B_0 &= \alpha_0 (1- \alpha_1)\\
  B_1 &= \alpha_1(1- \alpha_0)
\end{align*}
and the observable constants $w_0^k, w_1^k, w_2^k$ are 
\begin{align*}
  w_0^k &= \frac{\beta_{OLS}^k}{p} - \frac{\beta_{OLS}}{p_k}\\
  w_1^k &= \frac{\beta_{OLS}^k}{1-p} - \frac{\beta_{OLS}}{1-p_k}\\
  w_2^k &= \beta_{OLS}^k - \beta_{OLS}
\end{align*}

Since we have an equation for each value of $k$, we have a linear system of $k$ equations in two unknowns.
One of these equations, however is redundant.
\todo[inline]{Need to prove this.}
Thus, $z$ must take at least \emph{three} values for the system to have a solution.
In matrix form, we have
\[
\left[\begin{array}{cc}
w_{0}^{1} & w{}_{1}^{1}\\
w_{0}^{2} & w_{1}^{2}
\end{array}\right]\left[\begin{array}{c}
B_{0}\\
B_{1}
\end{array}\right]=\left[\begin{array}{c}
w_{2}^{1}\\
w_{2}^{2}
\end{array}\right]
\]
and as long as $w_{0}^{1}w_{1}^{2}-w_{0}^{2}w_{1}^{1}\neq0$ (Lewbel's Assumption A5) the solution is
\[
\left[\begin{array}{c}
B_{0}\\
B_{1}
\end{array}\right]=\frac{1}{w_{0}^{1}w_{1}^{2}-w_{0}^{2}w_{1}^{1}}\left[\begin{array}{c}
w_{1}^{2}w_{2}^{1}-w_{1}^{1}w_{2}^{2}\\
w_{0}^{1}w_{2}^{2}-w_{0}^{2}w_{2}^{1}
\end{array}\right]
\]
Finally, given that $B_{0}=(1-\alpha_{1})\alpha_{0}$ and $B_{1}=(1-\alpha_{0})\alpha_{1}$, we can solve for the mis-classification rates as follows.
First, rearranging the definition of $B_1$ gives $\alpha_0 = 1 - B_1/\alpha_1$.
Substituting this into the definition of $B_0$, we see that $B_0 = (1 - B_1/\alpha_1)(1-\alpha_1)$, yielding the following quadratic equation
\begin{equation*}
  \alpha_1^2 - (1 - B_0 + B_1) \alpha_1 + B_1 = 0
\end{equation*}
Solving, we find that
\[
\alpha_{1}=\frac{1}{2}\left[1-B_{0}+B_{1}\pm\sqrt{(1-B_{0}+B_{1})^{2}-4B_{1}}\right]
\]
and since $\alpha_0 = B_0/(1- \alpha_1)$
\[
\alpha_{0}=\frac{B_{0}}{1-\frac{1}{2}\left[1-B_{0}+B_{1}\pm\sqrt{(1-B_{0}+B_{1})^{2}-4B_{1}}\right]}
\]
To determine the nature of these solutions, we re-express the discriminant: \begin{align*}
  (1 - B_0 + B_1)^2 - 4 B_1 &=\left[ 1 - \alpha_0\left( 1 - \alpha_1 \right) + \left( 1 - \alpha_0 \right)\alpha_1 \right]^2 - 4\alpha_1(1 - \alpha_0) \\
  &= \left( 1 - \alpha_0 + \alpha_1 \right)^2 - 4\alpha_1(1 - \alpha_0) \\
  &= \left[ (1 - \alpha_0 - \alpha_1) + 2\alpha_1 \right]^2 - 4\alpha_1(1 - \alpha_0)\\
  &= \left[(1 - \alpha_0 - \alpha_1)^2 - 4\alpha_1(1- \alpha_0)\right] - 
  4(\alpha_1 - \alpha_0 \alpha_1) \\
  &= (1 -\alpha_0 - \alpha_1)^2
\end{align*}
Since the discriminant is necessarily positive, the solutions are always real.
But more importantly, we have established that
\begin{equation*}
  1 - \alpha_0 - \alpha_1 = \pm \sqrt{(1 - B_0 + B_1)^2 - 4 B_1 }
\end{equation*}
Since $\beta = \beta_{IV} (1 - \alpha_0 - \alpha_1)$, this expression \emph{alone} is sufficient to identify $\beta$: there is no need to substitute the solutions for $(\alpha_0, \alpha_1)$ into $M(\alpha_0, \alpha_1, p)$.
Moreover, it clarifies the role of the assumption $1 - \alpha_0 - \alpha_1>0$: without this restriction we learn the \emph{magnitude} of $\beta$ but not the sign.


In page 544 Lewbel observes that if his instrument were binary (if
we only had one instrument in our case), identification could be achieved
with one additional restriction on the missclasification rates. One
such restriction is implied by homoskedasticity on the instrument,
which he does not mention. 

\todo[inline]{I like the way that Lewbel sets things up. I wonder whether a similar way of doing things would simplify some of our other calculations.}
