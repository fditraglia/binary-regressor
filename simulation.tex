%!TEX root = ./main.tex
\section{Simulation Study}

In this section we present results from a simulation study using the inference procedure described in Section \ref{sec:details} above.
Unless otherwise specified, all results are based on $2000$ simulation replications with $n = 1000$ using Algorithm \ref{alg:Bonferroni} with $R = 5000$ simulation draws.
Supplementary simulation results appear in Appendix \ref{sec:simulation_supplement}.

\subsection{Simulation DGP}
\label{sec:DGP}
Our simulation design generates $n$ iid observations of the observables $(y_i, T_i, z)$ as follows:
\begin{enumerate}
  \item Generate the instrumental variable $z$.
    \begin{enumerate}[(i)]
      \item For each $1 \leq i \leq n/2$ set $z_i = 0$.
      \item For each $n/2 < i \leq n$, set $z_i = 1$.
    \end{enumerate}
  \item Generate the error terms: 
    \[
      \left[
      \begin{array}{c}
        \eta_i \\ \varepsilon_i
      \end{array}
    \right] \sim \mbox{iid N}\left( \left[
\begin{array}{c}
  0 \\ 0
\end{array}
\right], \left[
\begin{array}{cc}
  1 & \rho \\
  \rho & 1
\end{array}
\right]\right).
\]
  \item Generate the unobserved regressor: $T^*_i = \mathbf{1}\left\{ d_0 + d_1 z_i + \eta_i > 0 \right\}$.
  \item Generate the outcome: $y_i = c + \beta T_i^* + \varepsilon_i$.  
  \item Generate the observed, mis-classified regressor $T$.
    \begin{enumerate}[(i)]
      \item For all $i$ with $T^*_i = 0$ draw $T_i \sim \mbox{iid Bernoulli}(\alpha_0)$. 
      \item For all $i$ with $T^*_i = 1$ draw $T_i \sim \mbox{iid Bernoulli}(1 - \alpha_1)$.
    \end{enumerate}
\end{enumerate}
This DGP generates random variables that satisfy the conditions of Theorems \ref{thm:sharpII} and \ref{thm:main_ident}.
Thus $\beta$ is point identified, and all moment equalities and inequalities from Section \ref{sec:inference} hold at the true parameter values of the DGP.
Note from step 1 that we hold \emph{condition} on the instrument $z$, holding it fixed in repeated samples.
Our simulation varies the parameters $(\alpha_0, \alpha_1, \beta, n)$ over a grid.
For simplicity we present results for $c = 0, d_0 = \Phi^{-1}(0.15)$, and  $d_1= \Phi^{-1}(0.85) - \Phi^{-1}(0.15)$, where $\Phi^{-1}(\cdot)$ denotes the quantile function of a standard normal random variable.
Using these values for $(d_0, d_1)$ holds the unobserved first stage probabilities fixed: $p^*_0 = 0.15$ and $p^*_1 = 0.85$.
In contrast the \emph{observed} first-stage probabilities $p_0$ and $p_1$ vary with $(\alpha_0, \alpha_1)$ according to Lemma \ref{lem:p_pstar}.


\subsection{Simulation Results}

\todo[inline]{Change from here down!}
\begin{itemize}
  \item GMM does a bad job for small $\beta$ but a good job for large $\beta$.
    Note that we have changed our convention so we report coverage and width conditional on the interval existing!
  \item GMS test for $(\alpha_0, \alpha_1)$ uniformly controls size except for a small distortion at $\beta=3,\alpha_0=0.1$ that we can't explain.
  \item Now show the Bonferroni results:
    \begin{itemize}
      \item Uniformly controls size
      \item There is basically no conservatism in the zero measurement error case
      \item This comes from using the non-diff bounds (refer to the picture). Notice how these turn out to be very informative. They can only be informative from below, however, since $\alpha_0 = \alpha_1 = 0$ is always in the sharp identified set without our higher moment assumptions.
      \item When measurement error isn't zero, the method is conservative. 
      \item In the small $\beta$ cases, the conservatism is really a ``feature'' rather than a bug because there you are effectively not identified so we're really looking at an identified set.
      \item The conservatism is severe when $\beta$ is large and there is a lot of measurement error.
        The widths of the intervals are huge, although they do shrink with sample size.
        In contrast, GMM is working fine here.
      \item GMS doesn't seem to be able to ``detect'' the case where inference is standard. 
        Interesting question that doesn't seem to have been studied much about transitioning between partial and point identification (or weak and strong identification).
        We think the source of the conservatism is that we have an ellipse with a small variance in the beta direction and a large one in the alpha directions, so any kind of joint interval that we project will be very conservative for $\beta$. Maybe try to make a picture of this?
      \item Propose two-step procedure to address the conservatism when $\beta$ is large. 
        Explain the basic idea and show that it works perfectly and is essentially a free lunch. 
        There is one slight size distortion at $n = 1000,\beta = 1, \alpha_0 = \alpha_1 = 0.3$ but it disappears when $n = 2000$. Even small distortion at $\beta = 3, \alpha_0 = \alpha_1 = 0$ for both $n =1000, 2000$.
    \end{itemize}
\end{itemize}
