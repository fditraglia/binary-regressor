%!TEX root = ./main.tex
\section{Simulation Study}

In this section we present results from a simulation study using the inference procedure described in Section \ref{sec:details} above.
Unless otherwise specified, all calculations are based on $2000$ simulation replications with $n = 1000$ using Algorithm \ref{alg:Bonferroni} with $R = 5000$ simulation draws.
Supplementary simulation results appear in Appendix \ref{sec:simulation_supplement}.

\subsection{Simulation DGP}
\label{sec:DGP}
Our simulation design generates $n$ iid observations of the observables $(y_i, T_i, z)$ as follows:
\begin{enumerate}
  \item Generate the instrumental variable $z$.
    \begin{enumerate}[(i)]
      \item For each $1 \leq i \leq n/2$ set $z_i = 0$.
      \item For each $n/2 < i \leq n$, set $z_i = 1$.
    \end{enumerate}
  \item Generate the error terms: 
    \[
      \left[
      \begin{array}{c}
        \eta_i \\ \varepsilon_i
      \end{array}
    \right] \sim \mbox{iid N}\left( \left[
\begin{array}{c}
  0 \\ 0
\end{array}
\right], \left[
\begin{array}{cc}
  1 & \rho \\
  \rho & 1
\end{array}
\right]\right).
\]
  \item Generate the unobserved regressor: $T^*_i = \mathbf{1}\left\{ d_0 + d_1 z_i + \eta_i > 0 \right\}$.
  \item Generate the outcome: $y_i = c + \beta T_i^* + \varepsilon_i$.  
  \item Generate the observed, mis-classified regressor $T$.
    \begin{enumerate}[(i)]
      \item For all $i$ with $T^*_i = 0$ draw $T_i \sim \mbox{iid Bernoulli}(\alpha_0)$. 
      \item For all $i$ with $T^*_i = 1$ draw $T_i \sim \mbox{iid Bernoulli}(1 - \alpha_1)$.
    \end{enumerate}
\end{enumerate}
This DGP generates random variables that satisfy the conditions of Theorems \ref{thm:sharpII} and \ref{thm:main_ident}.
Thus $\beta$ is point identified, and all moment equalities and inequalities from Section \ref{sec:inference} hold at the true parameter values of the DGP.
Note from step 1 that we hold \emph{condition} on the instrument $z$, holding it fixed in repeated samples.
Our simulation varies the parameters $(\alpha_0, \alpha_1, \beta, n)$ over a grid.
Because $\varepsilon$ has unit variance, values for $\beta$ are measured in standard deviations of the error.
For simplicity we present results for $c = 0, d_0 = \Phi^{-1}(0.15)$, and  $d_1= \Phi^{-1}(0.85) - \Phi^{-1}(0.15)$, where $\Phi^{-1}(\cdot)$ denotes the quantile function of a standard normal random variable.
Using these values for $(d_0, d_1)$ holds the unobserved first stage probabilities fixed: $p^*_0 = 0.15$ and $p^*_1 = 0.85$.
In contrast the \emph{observed} first-stage probabilities $p_0$ and $p_1$ vary with $(\alpha_0, \alpha_1)$ according to Lemma \ref{lem:p_pstar}.


\subsection{Simulation Results}

As explained in Section \ref{sec:problem} above, the just-identified, unconstrained GMM estimator based on Equation \ref{eq:MCs_endog} suffers from weak identification and boundary value problems.
Moreover, the estimator may not even exist in finite samples.
Even when the GMM estimator exists, its asymptotic variance matrix could be numerically singular, so that the standard GMM confidence interval is undefined.
Table \ref{tab:GMM_na_1000} reports the percentage of simulation draws for which the standard GMM confidence interval is undefined, while Table \ref{tab:GMM_cover_1000} reports the coverage probability of a nominal 95\% GMM confidence interval, conditional on its existence.

\begin{table}[htbp]
  \small
  \centering
  \input{./tab/GMM_CIs_na_1000.tex}
  \caption{Percentage of replications for which the standard GMM confidence interval based on Equation \ref{eq:MCs_endog} fails to exist, either because the point estimate is NaN or the asymptotic covariance matrix is numerically singular. Calculations are based on  2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
  \label{tab:GMM_na_1000}
\end{table}

\begin{table}[htbp]
  \small
  \centering
  \input{./tab/GMM_CIs_cover_1000.tex}
  \caption{Coverage (\%) of the standard nominal 95\% GMM confidence interval for $\beta$ based on the Equation \ref{eq:MCs_endog}. Coverage is calculated only for those simulation draws for which the interval exists. (See Table \ref{tab:GMM_na_1000}.) Calculations are based on 2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
  \label{tab:GMM_cover_1000}
\end{table}


We see from Table \ref{tab:GMM_na_1000} that when $\beta$ is small compared to the error variance, the GMM confidence interval fails to exist with high probability.
When $\beta = 0.5$, for example, the interval is undefined approximately 30\% of the time.
As $\beta$ increases, however, it becomes less likely that the GMM interval is undefined.
All else equal, larger amounts of mis-classification, i.e.\ higher values for $(\alpha_0, \alpha_1)$, increase the probability that the GMM interval fails to exist.
Turning our attention to the simulation draws for which it is well-defined, we see from Tables \ref{tab:GMM_cover_1000} and \ref{tab:GMM_width_1000} that the GMM confidence interval performs extremely poorly when $\beta$ is small.
Substantial size distortions persist until $\beta$ is 1.5 or larger.
All else equal, the size distortions are more severe the larger the amount of mis-classification error.
For sufficiently large $\beta$, however, standard GMM inference performs well.
As $\beta$ grows, the weak identification problem vanishes.
For large enough $\beta$ the inference problem in effect becomes standard.

We now examine the performance of the Bonferroni-based confidence interval from Algorithm \ref{alg:Bonferroni}, beginning with its first step: a joint GMS confidence set for $(\alpha_0, \alpha_1)$.
Table \ref{tab:alphas_cover_97point5_1000} presents coverage probabilities for a nominal 97.5\% GMS confidence set for $(\alpha_0, \alpha_1)$.
Because these results are extremely fast to compute, Table \ref{tab:alphas_cover_97point5_1000} is based on 10,000 simulation replications. 
Aside from some slight under-coverage at intermediate values of $(\alpha_0, \alpha_1)$ when $\beta = 3$, the GMS interval makes good on its promise of uniformly valid inference.
It tends, however, to be fairly conservative, particularly for larger values of $(\alpha_0, \alpha_1)$.
When there is no mis-classification error, the GMS confidence sets are very nearly exact.
Results for nominal 95\% and 90\% intervals, both with $n = 1000$ and $n=2000$ are given in Appendix \ref{sec:simulation_supplement}.
These are qualitatively similar to the results discussed here.

\begin{table}
  \small
  \centering
  \input{./tab/alphas_cover_97point5_1000.tex}
  \caption{Coverage (1 - size) of 97.5\% GMS joint test for $\alpha_0$ and $\alpha_1$: $n = 1000$}
  \label{tab:alphas_cover_97point5_1000}
\end{table}

We now present results for the Bonferroni interval from Algorithm \ref{alg:Bonferroni}, setting $\delta_1 = \delta_2 = 0.025$ to yield an interval with asymptotic coverage no less that $95\%$.\footnote{In principle, one could optimize the choice of $\delta_1$ subject to the constraint $\delta_1 + \delta_2 = 0.95$ to reduce the width of the resulting interval. In our experiments, there was no choice of $\delta_1$ that uniformly dominated for all values of $(\alpha_0, \alpha_1, \beta)$ so we report only results for $\delta_1 = \delta_2$ here.}
Table \ref{tab:bonf_cover_1000} presents coverage probabilities in percentage point for the full simulation grid, while Figures ???--??? plot coverage functions (1 - Power) at selected parameter values.
The solid curves in the figures correspond to using the full set of inequality moment conditions from \ref{sec:inequalities}, while the dashed curves use only $m_{1}^I$, i.e.\ they do not impose the restrictions implied by non-differential measurement error.

\begin{table}[htbp]
  \centering
  \input{./tab/bonf_CIs_cover_1000.tex}
  \caption{Coverage of nominal $>95\%$ Bonferroni Intervals with $n = 1000$}
  \label{tab:bonf_cover_1000}
\end{table}

\begin{table}[htbp]
  \small
  \centering
  \input{./tab/GMM_CIs_width_1000.tex}
  \caption{Median width of the standard nominal 95\% GMM confidence interval for $\beta$ based on the Equation \ref{eq:MCs_endog}. Coverage is calculated only for those simulation draws for which the interval exists. (See Table \ref{tab:GMM_na_1000}.) Calculations are based on 2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
  \label{tab:GMM_width_1000}
\end{table}

\todo[inline]{Change from here down!}
\begin{itemize}
  \item Now show the Bonferroni results:
    \begin{itemize}
      \item Uniformly controls size
      \item There is basically no conservatism in the zero measurement error case
      \item This comes from using the non-diff bounds (refer to the picture). Notice how these turn out to be very informative. They can only be informative from below, however, since $\alpha_0 = \alpha_1 = 0$ is always in the sharp identified set without our higher moment assumptions.
      \item When measurement error isn't zero, the method is conservative. 
      \item In the small $\beta$ cases, the conservatism is really a ``feature'' rather than a bug because there you are effectively not identified so we're really looking at an identified set.
      \item The conservatism is severe when $\beta$ is large and there is a lot of measurement error.
        The widths of the intervals are huge, although they do shrink with sample size.
        In contrast, GMM is working fine here.
      \item GMS doesn't seem to be able to ``detect'' the case where inference is standard. 
        Interesting question that doesn't seem to have been studied much about transitioning between partial and point identification (or weak and strong identification).
        We think the source of the conservatism is that we have an ellipse with a small variance in the beta direction and a large one in the alpha directions, so any kind of joint interval that we project will be very conservative for $\beta$. Maybe try to make a picture of this?
      \item Propose two-step procedure to address the conservatism when $\beta$ is large. 
        Explain the basic idea and show that it works perfectly and is essentially a free lunch. 
        There is one slight size distortion at $n = 1000,\beta = 1, \alpha_0 = \alpha_1 = 0.3$ but it disappears when $n = 2000$. Even small distortion at $\beta = 3, \alpha_0 = \alpha_1 = 0$ for both $n =1000, 2000$.
    \end{itemize}
\end{itemize}
