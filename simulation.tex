%!TEX root = ./main.tex
\section{Simulation Study}

In this section we present results from a simulation study using the inference procedure described in Section \ref{sec:details} above.
Unless otherwise specified, all calculations are based on $2000$ simulation replications with $n = 1000$ using Algorithm \ref{alg:Bonferroni} with $R = 5000$ simulation draws.
Supplementary simulation results appear in Appendix \ref{sec:simulation_supplement}.

\subsection{Simulation DGP}
\label{sec:DGP}
Our simulation design generates $n$ iid observations of the observables $(y_i, T_i, z)$ as follows:
\begin{enumerate}
  \item Generate the instrumental variable $z$.
    \begin{enumerate}[(i)]
      \item For each $1 \leq i \leq n/2$ set $z_i = 0$.
      \item For each $n/2 < i \leq n$, set $z_i = 1$.
    \end{enumerate}
  \item Generate the error terms: 
    \[
      \left[
      \begin{array}{c}
        \eta_i \\ \varepsilon_i
      \end{array}
    \right] \sim \mbox{iid N}\left( \left[
\begin{array}{c}
  0 \\ 0
\end{array}
\right], \left[
\begin{array}{cc}
  1 & \rho \\
  \rho & 1
\end{array}
\right]\right).
\]
  \item Generate the unobserved regressor: $T^*_i = \mathbf{1}\left\{ d_0 + d_1 z_i + \eta_i > 0 \right\}$.
  \item Generate the outcome: $y_i = c + \beta T_i^* + \varepsilon_i$.  
  \item Generate the observed, mis-classified regressor $T$.
    \begin{enumerate}[(i)]
      \item For all $i$ with $T^*_i = 0$ draw $T_i \sim \mbox{iid Bernoulli}(\alpha_0)$. 
      \item For all $i$ with $T^*_i = 1$ draw $T_i \sim \mbox{iid Bernoulli}(1 - \alpha_1)$.
    \end{enumerate}
\end{enumerate}
This DGP generates random variables that satisfy the conditions of Theorems \ref{thm:sharpII} and \ref{thm:main_ident}.
Thus $\beta$ is point identified, and all moment equalities and inequalities from Section \ref{sec:inference} hold at the true parameter values of the DGP.
Note from step 1 that we hold \emph{condition} on the instrument $z$, holding it fixed in repeated samples.
Our simulation varies the parameters $(\alpha_0, \alpha_1, \beta, n)$ over a grid.
Because $\varepsilon$ has unit variance, values for $\beta$ are measured in standard deviations of the error.
For simplicity we present results for $c = 0, d_0 = \Phi^{-1}(0.15)$, and  $d_1= \Phi^{-1}(0.85) - \Phi^{-1}(0.15)$, where $\Phi^{-1}(\cdot)$ denotes the quantile function of a standard normal random variable.
Using these values for $(d_0, d_1)$ holds the unobserved first stage probabilities fixed: $p^*_0 = 0.15$ and $p^*_1 = 0.85$.
In contrast the \emph{observed} first-stage probabilities $p_0$ and $p_1$ vary with $(\alpha_0, \alpha_1)$ according to Lemma \ref{lem:p_pstar}.


\subsection{Simulation Results}

As explained in Section \ref{sec:problem} above, the just-identified, unconstrained GMM estimator based on Equation \ref{eq:MCs_endog} suffers from weak identification and boundary value problems.
Moreover, the estimator may not even exist in finite samples.
Even when the GMM estimator exists, its asymptotic variance matrix could be numerically singular, so that the standard GMM confidence interval is undefined.
Table \ref{tab:GMM_na_1000} reports the percentage of simulation draws for which the standard GMM confidence interval is undefined, while Table \ref{tab:GMM_cover_1000} reports the coverage probability of a nominal 95\% GMM confidence interval, conditional on its existence.

\begin{table}[htbp]
  \small
  \centering
  \input{./tab/GMM_CIs_na_1000.tex}
  \caption{Percentage of replications for which the standard GMM confidence interval based on Equation \ref{eq:MCs_endog} fails to exist, either because the point estimate is NaN or the asymptotic covariance matrix is numerically singular. Calculations are based on  2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
  \label{tab:GMM_na_1000}
\end{table}

\begin{table}[htbp]
  \small
  \centering
  \input{./tab/GMM_CIs_cover_1000.tex}
  \caption{Coverage (\%) of the standard nominal 95\% GMM confidence interval for $\beta$ based on the Equation \ref{eq:MCs_endog}. Coverage is calculated only for those simulation draws for which the interval exists. (See Table \ref{tab:GMM_na_1000}.) Calculations are based on 2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
  \label{tab:GMM_cover_1000}
\end{table}


We see from Table \ref{tab:GMM_na_1000} that when $\beta$ is small compared to the error variance, the GMM confidence interval fails to exist with high probability.
When $\beta = 0.5$, for example, the interval is undefined approximately 30\% of the time.
As $\beta$ increases, however, it becomes less likely that the GMM interval is undefined.
All else equal, larger amounts of mis-classification, i.e.\ higher values for $(\alpha_0, \alpha_1)$, increase the probability that the GMM interval fails to exist.
Turning our attention to the simulation draws for which it is well-defined, we see from Tables \ref{tab:GMM_cover_1000} and \ref{tab:GMM_width_1000} that the GMM confidence interval performs extremely poorly when $\beta$ is small.
Substantial size distortions persist until $\beta$ is 1.5 or larger.
All else equal, the size distortions are more severe the larger the amount of mis-classification error.
For sufficiently large $\beta$, however, standard GMM inference performs well.
As $\beta$ grows, the weak identification problem vanishes.
For large enough $\beta$ the inference problem in effect becomes standard.

We now examine the performance of the Bonferroni-based confidence interval from Algorithm \ref{alg:Bonferroni}, beginning with its first step: a joint GMS confidence set for $(\alpha_0, \alpha_1)$.
Table \ref{tab:alphas_cover_97point5_1000} presents coverage probabilities for a nominal 97.5\% GMS confidence set for $(\alpha_0, \alpha_1)$.
Because these results are extremely fast to compute, Table \ref{tab:alphas_cover_97point5_1000} is based on 10,000 simulation replications. 
Aside from some slight under-coverage at intermediate values of $(\alpha_0, \alpha_1)$ when $\beta = 3$, the GMS interval makes good on its promise of uniformly valid inference.
As shown in Appendix \ref{sec:simulation_supplement}, the under-coverage problem appears to be a finite-sample artifact: if we increase $n$ to 2000, the maximum size distortion becomes negligible.
The GMS test tends, however, to be fairly conservative, particularly for larger values of $(\alpha_0, \alpha_1)$.
When there is no mis-classification error, the GMS confidence sets are very nearly exact.
Results for nominal 95\% and 90\% intervals are qualitatively similar: see Appendix \ref{sec:simulation_supplement}.

\begin{table}
  \small
  \centering
  \input{./tab/alphas_cover_97point5_1000.tex}
  \caption{Coverage (1 - size) of 97.5\% GMS joint test for $\alpha_0$ and $\alpha_1$: $n = 1000$}
  \label{tab:alphas_cover_97point5_1000}
\end{table}

We now present results for the Bonferroni interval from Algorithm \ref{alg:Bonferroni}, setting $\delta_1 = \delta_2 = 0.025$ to yield an interval with asymptotic coverage no less that $95\%$.\footnote{In principle, one could optimize the choice of $\delta_1$ subject to the constraint $\delta_1 + \delta_2 = 0.95$ to reduce the width of the resulting interval. In our experiments, there was no choice of $\delta_1$ that uniformly dominated for all values of $(\alpha_0, \alpha_1, \beta)$ so we report only results for $\delta_1 = \delta_2$ here.}
Table \ref{tab:bonf_cover_1000} presents coverage probabilities in percentage points and Table \ref{tab:bonf_cover_1000} median widths.


\begin{table}[htbp]
  \centering
  \small
  \input{./tab/bonf_CIs_cover_1000.tex}
  \caption{Coverage of nominal $>95\%$ Bonferroni Intervals with $n = 1000$}
  \label{tab:bonf_cover_1000}
\end{table}

\begin{table}
  \small
  \centering
  \input{./tab/bonf_CIs_width_1000.tex}
  \caption{Median Width of nominal $>95\%$ Bonferroni Intervals with $n = 1000$}
  \label{tab:bonf_width_1000}
\end{table}

The Bonferroni interval achieves its stated minimum coverage uniformly over the parameter space.
When there is no mis-classification, $\alpha_0 = \alpha_1$, its actual coverage is close or equal to $95\%$.
In the presence of mis-classification, however, the interval can be quite conservative, particularly for larger values of $\beta$.
In spite of this conservatism, the Bonferroni interval is informative, as we see from the median widths in Table \ref{tab:bonf_width_1000}. 
Because median widths provide only a limited pictures of the behavior of a confidence interval, Figures \ref{fig:bonf_point5_1000}--\ref{fig:bonf_3_1000} present further evidence in the form of coverage functions (1 - power) for selected parameter values.
Each figure holds the true value of $\beta$ fixed and varies $(\alpha_0, \alpha_1)$ over a grid. 
The plots within each Figure give coverage in percentage points as a function of the specified alternative for $\beta$.
Solid curves are computed using the full set of inequality moment conditions from \ref{sec:inequalities}, while dashed curves use only $m_{1}^I$, i.e.\ they do not impose the restrictions implied by non-differential measurement error.
In each figure, the dashed horizontal line gives the nominal coverage probability, $95\%$, while the dashed vertical lines are the reduced from  and instrumental variables estimands: for $\beta \geq 0$ the reduced form is always smaller than the IV.
Coverage curves for additional parameter values appear in Appendix \ref{sec:simulation_supplement}. 


\begin{figure}[h]
  \centering
  %\input{./fig/bonf_point5_1000.tex}
  \caption{$\beta = 0.5, n = 1000$}
  \label{fig:bonf_point5_1000}
\end{figure}

\begin{figure}[h]
  \centering
  %\input{./fig/bonf_1_1000.tex}
  \caption{$\beta = 1, n = 1000$}
  \label{fig:bonf_1_1000}
\end{figure}

\begin{figure}[h]
  \centering
  %\input{./fig/bonf_3_1000.tex}
  \caption{$\beta = 3, n = 1000$}
  \label{fig:bonf_3_1000}
\end{figure}

As seen from Figures \ref{fig:bonf_point5_1000}--\ref{fig:bonf_3_1000}, and their counterparts in Appendix \ref{sec:simulation_supplement}, the Bonferroni procedure has power against the alternative that $\beta = 0$, even when the true value of $\beta$ is small.
As described in Section \ref{sec:details}, the Bonferroni interval excludes zero if and only if the confidence interval for $\theta_1$ from which it is constructed also excludes zero.
These figures also indicate the gains from including $m_2^I$, the moment inequalities that emerge from assuming non-differential measurement error: substantial increases in power against alternatives between the true parameter value and zero, particularly for larger values of $\beta$.
Note moreover that the excellent performance of Bonferroni in the zero mis-classification case $(\alpha_0, \alpha_1)$ depends crucially on imposing the assumption of non-differential measurement error.
As the true value of $\beta$ increases, the Bonferroni interval begins to have power against both the reduced form and IV estimands.

\begin{table}[htbp]
  \small
  \centering
  \input{./tab/GMM_CIs_width_1000.tex}
  \caption{Median width of the standard nominal 95\% GMM confidence interval for $\beta$ based on the Equation \ref{eq:MCs_endog}. Coverage is calculated only for those simulation draws for which the interval exists. (See Table \ref{tab:GMM_na_1000}.) Calculations are based on 2000 replications of the DGP from \ref{sec:DGP} with $n = 1000$.} 
  \label{tab:GMM_width_1000}
\end{table}

\todo[inline]{Change from here down!}
\begin{itemize}
  \item Now show the Bonferroni results:
    \begin{itemize}
      \item There is basically no conservatism in the zero measurement error case
      \item This comes from using the non-diff bounds (refer to the picture). Notice how these turn out to be very informative. They can only be informative from below, however, since $\alpha_0 = \alpha_1 = 0$ is always in the sharp identified set without our higher moment assumptions.
      \item When measurement error isn't zero, the method is conservative. 
      \item In the small $\beta$ cases, the conservatism is really a ``feature'' rather than a bug because there you are effectively not identified so we're really looking at an identified set.
      \item The conservatism is severe when $\beta$ is large and there is a lot of measurement error.
        The widths of the intervals are huge, although they do shrink with sample size.
        In contrast, GMM is working fine here.
      \item GMS doesn't seem to be able to ``detect'' the case where inference is standard. 
        Interesting question that doesn't seem to have been studied much about transitioning between partial and point identification (or weak and strong identification).
        We think the source of the conservatism is that we have an ellipse with a small variance in the beta direction and a large one in the alpha directions, so any kind of joint interval that we project will be very conservative for $\beta$. Maybe try to make a picture of this?
      \item Propose two-step procedure to address the conservatism when $\beta$ is large. 
        Explain the basic idea and show that it works perfectly and is essentially a free lunch. 
        There is one slight size distortion at $n = 1000,\beta = 1, \alpha_0 = \alpha_1 = 0.3$ but it disappears when $n = 2000$. Even small distortion at $\beta = 3, \alpha_0 = \alpha_1 = 0$ for both $n =1000, 2000$.
    \end{itemize}
\end{itemize}
