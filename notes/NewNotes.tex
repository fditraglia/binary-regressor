\documentclass[12pt]{article}
\usepackage{../frankstyle}

\title{Notes for Paper on Mis-measured, Binary, Endogenous Regressors}
\author{Francis J.\ DiTraglia \& Camilo Garc\'{i}a-Jimeno}

\begin{document}

\maketitle

\section{Model and Notation}

\paragraph{Probabilities}
\begin{eqnarray*}
p^*_{tk} &=& P(T^*=t, Z=k)\\
p_{tk} &=& P(T=t, Z=k)\\
p^*_k &=& P(T^* = 1|Z = k)\\
p_k &=& P(T = 1|Z = k)\\
q &=& P(Z = 1)
\end{eqnarray*}

\begin{eqnarray*}
  p^*_{00} &=& P(T^* = 0|Z=0)P(Z=0) = (1 - p_0^*)(1 - q) =  \left( \frac{1 - p_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{10} &=& P(T^* = 1|Z=0)P(Z=0) = p_0^*(1 - q) =  \left( \frac{p_0 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{01} &=& P(T^* = 0|Z=1)P(Z=1) = (1 - p_1^*)q =  \left( \frac{1 - p_1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) q\\
  p^*_{11} &=& P(T^* = 1|Z=1)P(Z=1) = p_1^*(1 - q)  =  \left( \frac{p_1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)q
\end{eqnarray*}

\paragraph{CDFs}
For $t, Z \in \left\{ 0,1 \right\}$ define
\begin{eqnarray*}
F_{tk}^*(\tau) &=&  P(Y \leq \tau|T^* = t, Z = k) \\
F_{tk}(\tau) &=&  P(Y \leq \tau|T = t, Z = k)\\
F_k(\tau) &=& P(Y \leq \tau | Z=k) 
\end{eqnarray*}
Note that the second two are observed for all $t,k$ while the first is never observed since it depends on the unobserved RV $T^*$.


\section{Weakest Bounds on $\alpha_0, \alpha_1$}
Assume that $\alpha_0 + \alpha_1 < 1$ that $T$ is independent of $Z$ conditional on $T^*$.
These standard assumptions turn out to yield informative bounds on $\alpha_0$ and $\alpha_1$ without \emph{any further restrictions of any kind}.
In particular, we assume nothing about the validity of the instrument $Z$ and nothing about the relationship between the mis-classification error and the outcome $Y$: we impose only that the mis-classification error rates do not depend on $z$ and that the mis-classification is not so bad that $1 - T$ is a better measure of $T^*$ than $T$. 

By the Law of Total Probability and the assumption that $T$ is conditionally independent of $Z$ given $T^*$,
\begin{eqnarray*}
  p_k &=& P(T=1|Z=k,T^*=0) (1 - p_k^*) + P(T=1|Z=k,T^*=1)p_k^*\\
  &=& P(T=1|T^*=0)(1 - p_k^*) + P(T=1|T^*=1)p_k^*\\
  &=& \alpha_0 (1 - p_k^*) + (1 - \alpha_1) p_k^*\\
  &=& \alpha_0 +(1 - \alpha_0 - \alpha_1) p_k^* 
\end{eqnarray*}
and similarly 
\begin{eqnarray*}
  1 - p_k &=& P(T=0|Z=k,T^*=0) (1 - p_k^*) + P(T=0|Z=k,T^*=1)p_k^*\\
  &=& P(T=0|T^*=0)(1 - p_k^*) + P(T=0|T^*=1)p_k^*\\
  &=& (1 - \alpha_0)(1 - p_k^*) + \alpha_1 p_k^*\\
  &=& \alpha_1 + (1 - p_k^*)(1 - \alpha_0 - \alpha_1)
\end{eqnarray*}
and hence
\begin{eqnarray*}
  p_k - \alpha_0 &=& (1 - \alpha_0 - \alpha_1)p_k^*\\
  (1 - p_k) - \alpha_1 &=& (1 - \alpha_0 - \alpha_1)(1 - p_k^*)
\end{eqnarray*}
Now, since $p_k^*$ and $(1 - p_k^*)$ are probabilities they are between zero and one which means that the sign of $p_k - \alpha_0$ as well as that of $(1 - p_k) - \alpha_1$ are both determined by that of $1 - \alpha_0 - \alpha_1$.
Accordingly, provided that $1 - \alpha_0 - \alpha_1 < 1$, we have
\begin{eqnarray*}
  \alpha_0 &<& p_k\\
  \alpha_1 &<& (1 - p_k)
\end{eqnarray*}
so long as $p_k^*$ does not equal zero or one, which is not a realistic case for any example that we consider.
Since these bounds hold for all $k$, we can take the tightest bound over all values of $Z$.

\todo[inline]{Important: using these to bound $\beta$ gives $\beta \in [\mbox{ITT}, \mbox{Wald}]$.}

\section{Stronger Bounds for $\alpha_0, \alpha_1$}
Now suppose we add the assumption that $T$ is conditionally independent of $Y$ given $T^*$. 
This is essentially the non-differential measurement error assumption although it is slightly stronger than the version used by Mahajan (2006) who assumes only conditional mean independence.
This assumption allows us to considerably strengthen the bounds from the preceding section by exploiting information contained in the conditional distribution of $Y$ given $T$ and $Z$.
The key ingredient is a relationship that we can derive between the unobservable distributions $F_{tk}^*$ and the observable distributions $F_{tk}$ using this new conditional independence assumption.
To begin, note that by Bayes' rule we have
\begin{eqnarray*}
  P(T^*=1|T=1, Z=k) &=& P(T=1 | T^*=1) \left(\frac{p_k^*}{p_k}\right) = (1 - \alpha_1)\left( \frac{p_k^*}{p_k} \right)\\
  P(T^*=1|T=0, Z=k) &=& P(T=0 | T^*=1) \left(\frac{p_k^*}{1 - p_k}\right) = \alpha_1 \left( \frac{p_k^*}{1 - p_k} \right)\\
  P(T^*=0|T=1, Z=k) &=& P(T=1 | T^*=0) \left(\frac{1 - p_k^*}{p_k}\right) = \alpha_0 \left( \frac{1 - p_k^*}{p_k} \right)\\
  P(T^*=0|T=0, Z=k) &=& P(T=0 | T^*=0) \left(\frac{1 - p_k^*}{1 - p_k}\right) = (1 - \alpha_0)\left( \frac{1 - p_k^*}{1 - p_k} \right)
\end{eqnarray*}
Now, by the conditional independence assumption
\begin{eqnarray*}
  P(Y\leq \tau|T^* = 0, T=t , Z = k) = P(Y \leq \tau|T^*=0, Z =k) = F_{0k}^*(\tau)\\
  P(Y\leq \tau|T^* = 1, T=t , Z = k) = P(Y \leq \tau|T^*=1, Z =k) = F_{1k}^*(\tau)
\end{eqnarray*}
Finally, putting everything together using the Law of Total Probability, we find that
\begin{eqnarray*}
  (1 - p_k) F_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  p_k F_{1k}(\tau) = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$.
Defining the shorthand 
\begin{eqnarray*}
  \widetilde{F}_{0k}(\tau)&\equiv& (1 - p_k) F_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\equiv& p_k F_{1k}(\tau) 
\end{eqnarray*}
this becomes
\begin{eqnarray}
  \label{eq:F0kTilde}
  \widetilde{F}_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  \label{eq:F1kTilde}
  \widetilde{F}_{1k}(\tau)  = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray}
Now, solving Equation \ref{eq:F0kTilde} for $p_k^* F_{1k}^*(\tau)$ we have
\[
  p_{k}^* F_{1k}^*(\tau) = \frac{1}{\alpha_1}\left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) (1 - p_k^*) F_{0k}^*(\tau)\right]
\]
Substituting this into Equation \ref{eq:F1kTilde},
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{\alpha_1} \left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) ( 1 - p_k^*)F_{0k}^*(\tau) \right]\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \alpha_0 - \frac{(1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \frac{\alpha_0 \alpha_1 - (1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ (1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ 1 - \alpha_1 -  \alpha_0 }{\alpha_1} \right]\left( \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) F_{0k}^*(\tau)\\
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) -  \frac{1 - p_k - \alpha_1}{\alpha_1}  F_{0k}^*(\tau)
  \label{eq:F1kTildeAlpha1}
\end{equation}
Equation \ref{eq:F1kTildeAlpha1} relates the observable $\widetilde{F}_{1k}(\tau)$ to the mis-classification error rate $\alpha_1$ and the unobservable CDF $F_{0k}^*\left( \tau \right)$.
Since $F_{0k}^*(\tau)$ is a CDF, however, it lies in the interval $\left[ 0,1 \right]$.
Accordingly, substituting $0$ in place of $F^*_{0k}(\tau)$ gives 
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_leq_a1}
\end{equation}
while substituting $1$ gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau) - \frac{1 - p_k - \alpha_1}{\alpha_1}
  \label{eq:F1ktilde_F0kTilde_geq_a1}
\end{equation}
Rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a1}
\begin{eqnarray*}
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau)\\
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& \widetilde{F}_{0k}(\tau) - \alpha_1 \widetilde{F}_{0k}(\tau)\\
 \alpha_1 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right]&\leq& \widetilde{F}_{0k}(\tau) 
\end{eqnarray*}
since $\alpha_1 \in [0,1]$ and therefore
\begin{equation}
  \alpha_1  \leq \frac{\widetilde{F}_{0k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = (1 - p_k) \left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
  \label{eq:Alpha1_Bound1}
\end{equation}
since $\widetilde{F}_{1k}(\tau) + \widetilde{F}_{1k}(\tau) \geq 0$.
Proceeding similarly for Equation \ref{eq:F1ktilde_F0kTilde_geq_a1},
\begin{eqnarray*}
  \alpha_1 \widetilde{F}_{1k}(\tau) &\geq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau) - (1 - p_k - \alpha_1)\\
  \alpha_1 \left[\widetilde{F}_{1k}(\tau) + \widetilde{F}_{0k}(\tau) - 1\right] &\geq& \widetilde{F}_{0k}(\tau) - (1 - p_k)\\
  -\alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\geq& -\left[1 - \widetilde{F}_{0k}(\tau) - p_k \right]\\
  \alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\leq& 1 - \widetilde{F}_{0k}(\tau) - p_k 
\end{eqnarray*}
Now since $\widetilde{F}_{1k}(\tau) = p_k F_{1k}(\tau) \leq p_k$ and $\widetilde{F}_{0k}(\tau) = (1 - p_k) F_{0k}(\tau) \leq (1 - p_k)$ it follows that $1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \geq 0$ and hence
\begin{equation}
  \alpha_1 \leq \frac{1 - \widetilde{F}_{0k}(\tau) - p_k}{1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau)} = (1 - p_k) \left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha1_Bound2}
\end{equation}
The bounds given in Equations \ref{eq:Alpha1_Bound1} and \ref{eq:Alpha1_Bound2} relate $\alpha_1$ to observable quantities \emph{only} and hold for all values of $\tau$ for which their respective denominators are non-zero.
Moreover, these bounds hold for any value $k$ that the instrument takes on.

We can proceed similarly for $\alpha_0$.
First solve Equation \ref{eq:F0kTilde} for $(1 - p_k^*)F^*_{0k}(\tau)$:
\[
  (1 - p_k^*)F^*_{0k}(\tau) = \frac{1}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right]
\]
and then substitute into Equation \ref{eq:F1kTilde}:
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \frac{\alpha_0}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right] + (1 - \alpha_1) p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ (1 - \alpha_1) - \frac{\alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{(1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_0}   \right] \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} F_{1k}^*(\tau) 
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) +  \frac{p_k - \alpha_0}{1 - \alpha_0} F_{1k}^*(\tau) 
\end{equation}
Now we can again obtain two bounds by substituting the smallest and largest possible values of $F_{1k}^*(\tau)$.
Substituting zero gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_geq_a0}
\end{equation}
while substituting one gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \frac{p_k - \alpha_0}{1 - \alpha_0}
  \label{eq:F1ktilde_F0kTilde_leq_a0}
\end{equation}
Now, rearranging Equation \ref{eq:F1ktilde_F0kTilde_geq_a0}, 
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \widetilde{F}_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] 
\end{eqnarray*}
since $1 - \alpha_0 \geq 0$.
Therefore,
\begin{equation}
  \alpha_0 \leq \frac{\widetilde{F}_{1k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{F_{1k}(\tau)}{F_{k}(\tau)}\right]
  \label{eq:Alpha0_Bound1}
\end{equation}
since $\left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] \geq 0$.
Similarly, rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a0}
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\leq& \alpha_0\widetilde{F}_{0k}(\tau) + p_k - \alpha_0\\
  \widetilde{F}_{1k}(\tau) - p_k &\leq& \alpha_0\left[\widetilde{F}_{0k}(\tau)  + \widetilde{F}_{1k}(\tau) - 1 \right] \\
  -\left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\leq& -\alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] \\
  \left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\geq& \alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] 
\end{eqnarray*}
Therefore
\begin{equation}
\alpha_0 \leq \frac{1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)}{1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha0_Bound2}
\end{equation}

\paragraph{Putting Everything Together} 
For all $k$ we have
\begin{equation}
  \alpha_0 \leq p_k \min_\tau\left\{\left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{1k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq p_k 
\end{equation}
\begin{equation}
  \alpha_1 \leq (1 - p_k) \min_\tau \left\{\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{0k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq (1 - p_k) 
\end{equation}
Note that these bounds can only improve upon those derived in the previous section since the ratio of CDFs tends to one as $\tau \rightarrow \infty$.
To derive these tighter bounds we have made no assumption regarding the relationship between $Z$ and the error term $\varepsilon$.
These bounds use only the assumption that $\alpha_0 + \alpha_1 < 1$, and the assumption that $T$ is conditionally independent of $Z,Y$ given $T^*$.
Notice that that the bounds are related.
In particular,
\[
  p_k \left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
\]
and 
\[
p_k \left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
\]


\section{Even Stronger Bounds on $\alpha_0, \alpha_1$}
Try applying the stochastic dominance conditions from our simulation study.

\section{Independent Instrument}
Assume that $Z \perp U$.  
The model is $Y = \beta T^* + U$ and
\[ F_{U}(\tau) = P(U \leq\tau) = P(Y - \beta T^* \leq \tau)\]
but if $Z$ is independent of $U$ then it follows that
\begin{eqnarray*}
F_U(\tau) &=&  F_{U|Z=k}(\tau) = P(U\leq \tau |Z=k) = P(Y  - \beta T^* \leq \tau |Z=k)\\
&=&  P(Y \leq \tau |T^* = 0, Z = k)(1 - p_k^*) + P(Y\leq \tau + \beta| T^* = 1, Z = k)p_k^* \\
&=& (1 - p_k^*) F^*_{0k}(\tau) + p_k^* F^*_{1k}(\tau + \beta)
\end{eqnarray*} 
for all $k$ by the Law of Total Probability.
Similarly, 
\[ F_k(\tau) = (1 - p_k^*) F_{0k}^*(\tau)  + p_k^* F_{1k}^*(\tau)\]
and rearranging
\[  (1 - p_k^*) F_{0k}^*(\tau)  = F_k(\tau) - p_k^* F_{1k}^*(\tau)\]
Substituting this expression into the equation for $F_U(\tau)$ from above, we have
\[F_U(\tau) = F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]\]
for all $k$ and all $\tau$.
Evaluating at two values $k$ and $\ell$ in the support of $Z$ and equating 
\[ F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right] =  F_\ell(\tau) + p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right]\]
or equivalently
\begin{equation}
 F_k(\tau) - F_\ell(\tau) =  p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right] - p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]  
 \label{eq:CDFs1}
\end{equation}
for all $\tau$.
Now we simply need to re-express all of the ``star'' quantities, namely $p_k^*, p_\ell^*$ and $F_{1k}^*, F_{1\ell}^*$ in terms of $\alpha_0, \alpha_1$ and the \emph{observable} probability distributions $F_{1k}$ and $F_{1\ell}$ and observable probabilities $p_k, p_\ell$.
To do this, we use the fact that
\begin{eqnarray*}
  F_{0k}(\tau) &=& \frac{1 - \alpha_0}{1 - p_k} (1 - p^*_k)F_{0k}^*(\tau) + \frac{\alpha_1}{1 - p_k}p_k^* F_{1k}^*(\tau)\\ \\
  F_{1k}(\tau) &=& \frac{ \alpha_0}{p_k}(1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{p_k}p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$ by Bayes' rule.
Solving these equations,
\begin{equation*}
  p_k^* F_{1k}^*(\tau) = \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} p_k F_{1k}(\tau) - \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} (1 - p_k) F_{0k}(\tau) 
\end{equation*}
for all $k$.
Combining this with Equation \ref{eq:CDFs1}, we find that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Now, define
\[
  \Delta^\tau_{tk}(\beta) = F_{tk}(\tau + \beta) - F_{tk}(\tau) = E\left[ \frac{\mathbf{1}\left\{ T = t, Z = k \right\}}{p_{tk}}\left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right) \right]
\]
and note that we can express $F_k(\tau) - F_\ell(\tau)$ similarly as 
\[
  F_k(\tau)  - F_{\ell}(\tau) = E\left[ \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right) \right]
\]
Using this notation, we can write the preceding as
\begin{equation*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_{\ell}(\tau) \right] = \alpha_0\left[ (1 - p_k) \Delta^\tau_{0k}(\beta) - (1 - p_\ell) \Delta^\tau_{0\ell}(\beta) \right] - (1 - \alpha_0)\left[ p_k \Delta^\tau_{1k}(\beta) - p_\ell \Delta^\tau_{1\ell}(\beta) \right]
\end{equation*}
or in moment-condition form
\begin{align*}
   E\Bigg[ &(1 - \alpha_0 - \alpha_1) \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right)  - 
   \left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right)\Bigg\{ \\
   &\alpha_0 \bigg((1 - p_k)\frac{\mathbf{1}\left\{ T = 0, Z = k \right\}}{p_{0k}} - 
    (1 - p_\ell)\frac{\mathbf{1}\left\{ T = 0, Z = \ell \right\}}{p_{0\ell}}\bigg)\\
   &-(1 - \alpha_0) \bigg( p_k\frac{\mathbf{1}\left\{ T = 1, Z = k \right\}}{p_{1k}} - 
 p_\ell \frac{\mathbf{1}\left\{ T = 1, Z = \ell \right\}}{p_{1\ell}}\bigg) \Bigg\}\Bigg] = 0
\end{align*}
Each value of $\tau$ yields a moment condition.

\section{Special Case: $\alpha_0 = 0$}
In this case the expressions from above simplify to
\begin{align}
  (1 - \alpha_1)\left[ F_k(\tau) - F_\ell(\tau)\right] = \left[ p_\ell F_{1\ell}(\tau + \beta) 
 - p_k  F_{1k}(\tau+ \beta) 
 - p_\ell F_{1\ell}(\tau) 
 + p_k F_{1k}(\tau) \right]
 \label{eq:specialCDF}
\end{align}
for all $\tau$.
Now, provided that all of the CDFs are differentiable we have\footnote{There must be a way to generalize this using Lebesgue.}
\begin{align*}
  e^{i\omega \tau}(1 - \alpha_1)\left[f_k(\tau) - f_\ell(\tau)\right] = e^{i\omega \tau}\left[ p_\ell f_{1\ell}(\tau + \beta) - p_k  f_{1k}(\tau+ \beta) - p_\ell f_{1\ell}(\tau) + p_k f_{1k}(\tau) \right]
\end{align*}
where we have pre-multiplied both sides by $e^{i\omega \tau}$.
Finally, integrating both sides with respect to $\tau$ over $(-\infty, \infty)$, we have
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] = \left\{  \int_{-\infty}^{\infty} e^{i\omega \tau} \left[p_\ell f_{1\ell}(\tau + \beta) - p_k f_{1k}(\tau+ \beta)\right] \; d\tau - p_\ell \varphi_{1\ell}(\omega) + p_k \varphi_{1k}(\omega) \right\}
\end{align*}
where $\varphi_k$ is the conditional characteristic function of $Y$ given $Z=k$ and $\varphi_{1k}$ is the conditional characteristic function of $Y$ given $T=1, Z=k$.
Finally, 
\begin{align*}
  \int_{-\infty}^{\infty} e^{i\omega \tau} p_\ell f_{1\ell}(\tau + \beta) \; d\tau &=  e^{ i\omega \beta } p_\ell \int_{u = -\infty + \beta}^{u = \infty + \beta} e^{ i\omega u }f_{1\ell}(u)\; du \\
  &= e^{-i\omega \beta } p_\ell \varphi_{1\ell}(\omega)
\end{align*}
using the substitution $u = \tau + \beta$.
Changing subscripts, the same holds for $k$ and thus
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  e^{-i\omega \beta}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] +  \left[p_k \varphi_{1k}(\omega) -  p_\ell \varphi_{1\ell}(\omega)\right]
\end{align*}
which, after collecting terms, simplifies to
\begin{align}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  \left(e^{-i\omega \beta} - 1\right)\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] 
  \label{eq:CharacteristicSpecial}
\end{align}
for all $\omega$.  
Equation \ref{eq:CharacteristicSpecial} contains exactly the same information as Equation \ref{eq:specialCDF} but gives us a more convenient way to prove identification since $\beta$ enters in a simpler way.
Leibniz's formula for the $r$th derivative of a product of two functions $f$ and $g$ is:
\begin{align*}
  (fg)^{(r)} = \sum_{s=0}^r {r \choose s} f^{(s)}g^{(r-s)}
\end{align*}
where $f^{(r)}$ denotes the $r$th derivative of the function $f$ and $g^{(r-s)}$ denotes the $(r-s)$th derivative of the function $g$.
Applying this to the RHS, $R(\omega)$ of Equation \ref{eq:CharacteristicSpecial} gives
\begin{align*}
  \frac{d}{d\omega^r}R(\omega)
  &=  \sum_{s=0}^r {r \choose s} \frac{d}{d\omega^s}\left( e^{-i\omega\beta} - 1\right)\frac{d}{d\omega^{r - s}}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] \\
  &= \left( e^{-i\omega \beta} - 1 \right) \left[ p_\ell \varphi_{1\ell}^{(r)}(\omega) - p_k \varphi_{1k}^{r}(\varphi) \right] + e^{-i\omega\beta} \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(\omega) - p_k \varphi^{(r-s)}_{1k}(\omega) \right] 
\end{align*}
where we split off the $s=0$ term because our generic expression for the $s$th derivative of $(e^{-i\omega\beta} - 1)$ only applies for $s\geq 1$.
Evaluating at zero:
\begin{align*}
  \frac{d}{d\omega^r}R(0)
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Combining this with the LHS of Equation \ref{eq:CharacteristicSpecial}, also differentiated $r$ times and evaluated at zero, we have
\begin{align*}
  (1 - \alpha_1) \left[ \varphi_{k}^{(r)}(0) - \varphi_{\ell}^{(r)}(0) \right] 
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Now, recall that if $\varphi(\omega)$ is the characteristic function of $Y$ then $\varphi^{(r)}(0) = i^r E[Y^r]$ provided that the expectation exists where $\varphi^{(r)}$ denotes the $r$th derivative of $\varphi$.
The same applies for the conditional characteristic functions we consider here.
Hence, provided that the $r$th moments exist, 
\footnotesize
\begin{align*}
  i^r(1 - \alpha_1)\left\{ E[Y^r|Z=k] - E[Y^r|Z=\ell]\right\} = \sum_{s=1}^r {r \choose s} (-i\beta)^s i^{r-s}\left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
After simplifying the terms involving $i$ and cancelling them from both sides, 
\small
\begin{align*}
  (1 - \alpha_1)\left(E[Y^r|Z=k] - E[Y^r|Z=\ell]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
again provided that the moments exist.
Abbreviating the conditional expectations according to $E[Y^r|Z=k] = E_k[Y^r]$ and $E[Y^r|T=t,Z=k] = E_{tk}[Y^r]$, this becomes
\begin{equation}
  (1 - \alpha_1)\left(E_k[Y^r] - E_\ell[Y^r]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{r-s}\right] - p_k E_{1k}\left[ Y^{r-s}\right] \right)
  \label{eq:MomentsSpecial}
\end{equation}
Equation \ref{eq:MomentsSpecial} can be used to generate moment equations that are implied by the Equation \ref{eq:CharacteristicSpecial} and the equivalent representation in terms of CDFs: Equation \ref{eq:specialCDF}.
Assuming that the conditional first moments exist, we can evaluate Equation \ref{eq:MomentsSpecial} at $r=1$, yielding
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y] - E_\ell[Y]\right) &= \sum_{s=1}^1 {1 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{1-s}\right] - p_k E_{1k}\left[ Y^{1-s}\right] \right)\\
  &=  - \beta\left( p_\ell - p_k \right) 
\end{align*}
Rearranging, this gives us the expression for the probability limit of the Wald estimator
\begin{equation}
  \mathcal{W} \equiv \frac{E_{k}[Y]- E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_1} 
  \label{eq:WaldSpecial}
\end{equation}
Evaluating Equation \ref{eq:MomentsSpecial} at $r = 2$, we have
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y^2] - E_\ell[Y^2]\right) &= \sum_{s=1}^2 {2 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{2-s}\right] - p_k E_{1k}\left[ Y^{2-s}\right] \right)\\
  &= 2\beta\left( p_k E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta^2\left( p_k - p_{\ell} \right)
\end{align*}
Rearranging, we have
\begin{equation}
  E_k[Y^2] - E_\ell[Y^2] 
  =  \frac{\beta}{1 - \alpha_1}\left[2\left( p_k  E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta(p_k - p_\ell)\right]
  \label{eq:SpecialSquared}
\end{equation}
Substituting Equation \ref{eq:WaldSpecial}, we can replace $\beta/(1-\alpha_1)$ with a function of observables only, namely $\mathcal{W}$.
Solving, we find that 
\begin{align}
  \beta &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} 
  \label{eq:BetaSpecial}
\end{align}
This allows us to state low-level sufficient conditions for identification:
\begin{enumerate}[(a)]
  \item $\alpha_1 < 1$
  \item $p_k \neq p_\ell$ 
  \item $E_k[Y] \neq E_\ell[Y]$ 
  \item $E_{1k}[|Y|], E_{1\ell}[|Y|], E_k[|Y^2|], E_\ell[|Y^2|] < \infty$.
\end{enumerate}
Note that, although $\beta = 0$ is always a solution of Equation \ref{eq:specialCDF} this solution is ruled out by the assumption that $E_k[Y] \neq E_\ell[Y]$ via Equation \ref{eq:WaldSpecial}.
The mis-classification error rate $\alpha_1$ is likewise uniquely identified under these assumptions.
Substituting $\beta/\mathcal{W} = 1-\alpha_1$ into Equation \ref{eq:BetaSpecial}
\begin{align*}
  (1 - \alpha_1) &= \left\{ \frac{p_k - p_\ell}{E_k[Y] - E_\ell[Y]} \right\}\left\{\frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} \right\}\\
  &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} - (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\}
\end{align*}
and thus
\begin{align*}
  \alpha_1
  &= 1 + (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\} - \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} 
\end{align*}

\section{Identification in the General Case}

\section{Characteristic Functions}
Recall from above that in the general case an independent instrument combined with non-differential measurement error implies that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Using the same steps as in the preceding section, we can convert this expression into characteristic function form by differentiating each side, multiplying by $e^{i\omega\tau}$ and then integrating with respect to $\tau$, yielding
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left[ \varphi_k(\omega) - \varphi_{\ell}(\omega) \right] &= \alpha_0 \left\{ (1 - p_k)\left(e^{-i\omega\beta} - 1\right)\varphi_{0k}(\omega) - (1 - p_\ell)\left( e^{-i\omega\beta} - 1\right) \varphi_{0\ell}(\omega)  \right\}\\
  &\quad - (1 - \alpha_0) \left\{ p_k\left(e^{-i\omega\beta} - 1 \right)\varphi_{1k}(\omega) - p_\ell \left( e^{-i\omega\beta} - 1\right) \varphi_{1\ell}(\omega) \right\}
\end{align*}
which simplifies to
\begin{align*}
  \varphi_k(\omega) - \varphi_{\ell}(\omega) &= \left( e^{-i\omega\beta} - 1 \right)\left(\frac{\alpha_0\left[ (1 - p_k)\varphi_{0k}(\omega) - (1-p_\ell)\varphi_{0\ell}(\omega) \right]  - (1 - \alpha_0)\left[ p_k \varphi_{1k}(\omega) - p_\ell \varphi_{1\ell}(\omega) \right]}{1 - \alpha_0 - \alpha_1}\right).
\end{align*}
As above, we will differentiate both sides of this expression $r$ times and evaluate at $\omega = 0$.
Steps nearly identical to those given above yield
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left(  E_k[Y^r] - E_\ell[Y^r]\right) 
  &= \alpha_0 \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{ (1 - p_k) E_{0k}[Y^{r-s}] - (1 - p_\ell) E_{0\ell}[Y^{r-s}] \right\}\\
  &\quad - (1 - \alpha_0) \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{p_k E_{1k}[Y^{r-s}] - p_\ell E_{1\ell}[Y^{r-s}] \right\}
\end{align*}

\paragraph{First Moments}
Taking $r = 1$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left( E_k[Y] - E_{\ell}[Y] \right) = \beta (p_k - p_\ell)
\end{align*}
Simplifying,
\begin{equation}
  \mathcal{W} \equiv \frac{E_k[Y] - E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_0 - \alpha_1}
  \label{eq:Wald}
\end{equation}

\paragraph{Second Moments}
Now, taking $r = 2$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left( E_{k}[Y^2] - E_{\ell}[Y^2] \right) &=
  \alpha_0\left\{ \left[ (1 - p_k) E_{0k}[Y] - (1 - p_\ell) E_{0\ell} \right] - \beta^2\left( p_k - p_\ell \right) \right\}\\
  &\quad  -(1 - \alpha_0)\left\{ -2\beta\left( p_k E_{1k}[Y] - p_{\ell}E_{1\ell}[Y] \right) + \beta^2\left( p_k - p_\ell \right) \right\}\\
  &= -2\beta \alpha_0\left\{ (1 - p_k)E_{0k}[Y] - (1 - p_\ell) E_{0\ell}[Y] p_k E_{1k}[Y] + p_{\ell}E_{1\ell}[Y]\right\} \\ 
  &\quad +2\beta \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) 
  - (p_k - p_\ell)\beta^2\left( \alpha_0 + 1 - \alpha_0 \right)\\
  &= -2\beta\left\{ \alpha_0 \left( E_k[Y] - E_\ell[Y] \right) - \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) \right\} - \beta^2(p_k - p_\ell)
\end{align*}
Now, simplifying
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\beta \alpha_0 \left(\frac{E_k[Y]-E_k[Y]}{p_k - p_\ell}\right) + 2\beta \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) - \beta^2
\end{align*}
and substituting Equation \ref{eq:Wald} to eliminate $\beta$, this becomes
\small
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 (1 - \alpha_0 - \alpha_1)\mathcal{W}^2 + 2\mathcal{W}(1 - \alpha_0 - \alpha_1) \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) \\
  &\quad \quad - (1 - \alpha_0 - \alpha_1)^2 \mathcal{W}^2\\
  \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 \mathcal{W}^2 + 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2
\end{align*}
\normalsize
And thus, simplifying
\begin{align*}
  -2\alpha_0 \mathcal{W}^2 - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2 &= \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)- 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  \\
  \alpha_1  - \alpha_0   &= 1 +  \left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\mathcal{W}^2(p_k - p_\ell)} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{\mathcal{W}(p_k - p_\ell)} \right] 
\end{align*}
and therefore
\begin{equation}
  \alpha_1  - \alpha_0  = 1 +  (p_k - p_\ell)\left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\left( E_k[Y] - E_\ell[Y] \right)^2} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{E_k[Y] - E_\ell[Y]} \right] 
\end{equation}

\paragraph{``Product'' Moments}
Recall that in our initial draft of the paper we worked with moments such as $E[TY|Z=k], E[TY|Z=\ell]$ and $E[TY^2|Z=k], E[TY^2|Z=\ell]$.
In the notation of this document, we can express these quantities as follows:
\begin{align*}
  E[TY^r|z=k] &= E[TY^r|T=1,z=k]p_k + E[TY^r|T=0,z=k](1 - p_k)\\
  &= p_k E[Y^r|T=1,z=k] + 0\\
  &= p_k E_{1k}[Y^r]
\end{align*}
for any $r$. 
We will use this relationship to motivate some shorthand notation below.

\paragraph{Some Shorthand}
The notation above is becoming very cumbersome and we haven't even looked at the third moments yet! 
To make life easier, define the following: 
\begin{align*}
  \widetilde{y^r_{1k}} &= p_k E_{1k}[Y^r] \\
  \widetilde{y^r_{0k}} &= (1 - p_k) E_{1k}[Y^r] \\
  \Delta \overline{y^r} &= E_k[Y^r] - E_\ell[Y^r]\\
  \Delta \overline{Ty^r} &= p_k E_{1k}[Y^r] - p_\ell E_{1\ell}[Y^r] = \widetilde{y^r_{1k}} - \widetilde{y^r_{1k}}\\
  \mathcal{W} &= (E_k[Y] - E_\ell[Y]) / (p_k - p_\ell)
\end{align*}
for all $r$.
When no $r$ superscript is given this means $r=1$.
Note, moreover, that when $r =0$ we have $\widetilde{y_{1k}^0} = p_k$ and $\widetilde{y_{0k}^0} = (1 - p_k)$.
Thus $\Delta \overline{Ty^0} = p_k - p_\ell$.
In contrast, $\Delta y^0 = 0$.

Among other things, this notation will make it easier for us to link the derivations here to our earlier derivations from the first draft of the paper that used slightly different notation and did not work explicitly with the independence of the instrument.

\paragraph{Simplifying the Moment Equalities}
Using the final two pieces of notation defined in the preceding section, we can re-rewrite the collection of moment equalities arising from the characteristic function equations as
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left[\alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right) \right]
\end{align*}
Now, simplifying the terms in the square brackets,
\begin{align*}
  \alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)
  &= \alpha_0\left[ \left( \widetilde{y_{0k}^{r-s}} + \widetilde{y_{1k}^{r-s}} \right) - \left( \widetilde{y_{0\ell}^{r-s}} + \widetilde{y_{1\ell}^{r-s}} \right)  \right] - \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)\\
  &= \alpha_0\left( E_k[Y^{r-s}] - E_\ell[Y^{r-s}] \right) - \Delta \overline{Ty^{r-s}}\\
  &= \alpha_0 \Delta \overline{y^{r-s}} - \Delta\overline{Ty^{r-s}}
\end{align*}
and hence
\begin{align}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{r-s}} - \Delta\overline{Ty^{r-s}} \right) 
  \label{eq:MomentEqualitiesSimplified}
\end{align}

\paragraph{Third Moments}
Evaluating Equation \ref{eq:MomentEqualitiesSimplified} at $r=3$ 
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^3} 
  &= \sum_{s=1}^3 {3 \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{3-s}} - \Delta\overline{Ty^{3-s}} \right) \\
  &= -3\beta\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta^2\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^3 (p_k - p_\ell)
\end{align*}

\paragraph{Solving the System}
Using $\mathcal{W} = \beta/(1 - \alpha_0 - \alpha_1)$ we can re-write the third moment expression as follows
\begin{align*}
  \Delta \overline{y^3} &= -3\mathcal{W}\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta \mathcal{W}\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^2 \mathcal{W} (p_k - p_\ell)\\
  \frac{\Delta \overline{y^3}}{\mathcal{W} (p_k - p_\ell)} 
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} }{p_k - p_\ell}\right) \\
  \frac{\Delta \overline{y^3} - 3\mathcal{W}\Delta\overline{y^2T}}{\mathcal{W}(p_k - p_\ell)}
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2}  }{p_k - p_\ell}\right) 
\end{align*}
Now, translating the second moment equation into the shorthand notation defined above, we have


\paragraph{Simplifying the Characteristic Function Equation}
From above, we have
\begin{align*}
  \varphi_k(\omega) - \varphi_{\ell}(\omega) &= \left( e^{-i\omega\beta} - 1 \right)\left(\frac{\alpha_0\left[ (1 - p_k)\varphi_{0k}(\omega) - (1-p_\ell)\varphi_{0\ell}(\omega) \right]  - (1 - \alpha_0)\left[ p_k \varphi_{1k}(\omega) - p_\ell \varphi_{1\ell}(\omega) \right]}{1 - \alpha_0 - \alpha_1}\right).
\end{align*}
Using the fact that $\varphi_{k} = p_k \varphi_{1k} + (1 - p_k) \varphi_{0k}$, we can simplify this further, yielding
\[
(1 - \alpha_0 - \alpha_1) = \left( e^{-i\omega \beta} - 1 \right)\left[ \alpha_0 - \xi(\omega)\right] 
\]
where we define
\[
  \xi(\omega) \equiv \frac{\varphi_k(\omega) - \varphi_\ell(\omega)}{p_k \varphi_{1k}(\omega) - p_\ell\varphi_{1\ell}(\omega)}
\]
Now, re-arranging
\[
  (1 - \alpha_1) - \xi(\omega) = e^{-i\omega\beta}\left[ \alpha_0 - \xi(\omega) \right]  
\]
or equivalently
\[
  e^{i\omega\beta}\left[(1 - \alpha_1) - \xi(\omega)\right] =  \alpha_0 - \xi(\omega) 
\]
or
\[
  e^{i\omega \beta} = \frac{\alpha_0 - \xi(\omega)}{(1 - \alpha_1) - \xi(\omega)}
\]
provided the denominator does not vanish.
By taking differences or ratios evaluated at $\omega_1$ and $\omega_2$ we can eliminate $\beta$, $\alpha_0$ or $\alpha_1$ but it's not clear how or if we can prove identification in terms of a restriction on the characteristic functions.

Suppose we consider three values $\omega_1, \omega_2$ and $\omega_3$ for which that yield to distinct, non-zero values $\xi_1, \xi_2$ and $\xi_3$ of $\xi(\omega)$.
\begin{align*}
  e^{i\omega_1\beta}\left[ (1 - \alpha_1) - \xi_1 \right] - e^{i\omega_2\beta}\left[ (1 - \alpha_1) - \xi_2 \right] = \xi_2 - \xi_1
\end{align*}

\subsection{Simplifying the Characteristic CDF Equation}
Recall from above that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
We can simplify the RHS as follows
\begin{align*}
  \mbox{RHS} &= \alpha_0 \left\{ \left[ F_k(\tau + \beta) - F_\ell(\tau + \beta) \right] - \left[ F_k(\tau) - F_\ell(\tau) \right] \right\}\\
  &- \left\{ \left[ p_k F_{1k}(\tau + \beta) - p_\ell F_{1\ell}(\tau + \beta) \right]  - \left[ p_k F_{1k}(\tau) - p_\ell F_{1\ell}(\tau) \right]\right\}
\end{align*}
Now, define
\begin{align*}
  \Delta(\tau) &= F_k(\tau) - F_\ell(\tau)\\
  \widetilde{\Delta}_1(\tau) &= p_k F_{1k}(\tau) - p_\ell F_{1\ell}(\tau)
\end{align*}
Using this notation, our equation becomes
\[
  (1 - \alpha_0 - \alpha_1) \Delta(\tau) = \alpha_0 \left[ \Delta(\tau + \beta) - \Delta(\tau)\right] - \left[ \widetilde{\Delta}_1(\tau+\beta) - \widetilde{\Delta}_1(\tau)\right]
\]
which simplifies to
\[
    \widetilde{\Delta}_1(\tau+\beta) - \widetilde{\Delta}_1(\tau) = \alpha_0 \Delta(\tau + \beta) - (1 - \alpha_1) \Delta(\tau)
\]

\paragraph{Suppose $\alpha_0 = 0$:}
In this case we obtain
\[
  (1 - \alpha_1)  = \frac{ \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}
\]
Now, evaluating at two values of $\tau$ and taking differences, we find
\[
  \frac{ \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)} - 
  \frac{ \widetilde{\Delta}_1(\tau') - \widetilde{\Delta}_1(\tau' + \beta)}{\Delta(\tau')} = 0
\]

\paragraph{Suppose $\alpha_1 = 0$:}
In this case we obtain
\[
  \alpha_0 = \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + \Delta(\tau)}{\Delta(\tau + \beta)}
\]
Again, taking differences evaluated at two values of $\tau$,
\[
  \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + \Delta(\tau)}{\Delta(\tau + \beta)} - 
  \frac{\widetilde{\Delta}_1(\tau' + \beta) - \widetilde{\Delta}_1(\tau') + \Delta(\tau')}{\Delta(\tau' + \beta)} = 0
\]

\paragraph{Some Equations to Check Numerically}
We can use the same basic idea when either $\alpha_0$ or $\alpha_1$ is known but nonzero.
This isn't realistic in practice, but can be used to check our equations:
\begin{align*}
  \alpha_0 &= \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + (1 - \alpha_1) \Delta(\tau)}{\Delta(\tau + \beta)}\\ \\
  (1 - \alpha_1) &= \frac{\alpha_0 \Delta(\tau + \beta) + \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}
\end{align*}
As above, after substituting the true value of either $\alpha_1$ or $\alpha_0$, we can eliminate the remaining mis-classification probability by evaluating at two quantiles $\tau$, $\tau'$ and taking differences.
\todo[inline]{These appear to work just fine!}

\paragraph{What if $\alpha_0$ and $\alpha_1$ are both unknown?}
Suppose we take differences at two quantiles $\tau$ and $\nu$ to eliminate $\alpha_1$:
\begin{align*}
  \left[\frac{\alpha_0 \Delta(\tau + \beta) + \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}\right]
  - \left[\frac{\alpha_0 \Delta(\nu + \beta) + \widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0 \\ 
  \alpha_0 \left[ \frac{\Delta(\tau+ \beta)}{\Delta(\tau)} - \frac{\Delta(\nu + \beta)}{\Delta(\nu)} \right] - \left[ \frac{\widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau+ \beta)}{\Delta(\tau)} - \frac{\widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0
 \end{align*}

 \paragraph{The Equation that Didn't Work\ldots}
\[
  \frac{[\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau)] - [\widetilde{\Delta}_1(\tau' + \beta) - \widetilde{\Delta}_1(\tau')]}{\Delta(\tau + \beta) - \Delta(\tau' + \beta)}
- \frac{[\widetilde{\Delta}_1(\nu + \beta) - \widetilde{\Delta}_1(\nu)] - [\widetilde{\Delta}_1(\nu' + \beta) - \widetilde{\Delta}_1(\nu')]}{\Delta(\nu + \beta) - \Delta(\nu' + \beta)} = 0
 \]
 where $\Delta(\nu) = \Delta(\nu')$ and $\Delta(\tau) = \Delta(\tau')$.
%Now, recall that $\Delta(\tau)$ is a difference of CDFs. 
%This means that it its limits as $\tau \rightarrow +\infty$ and as $\tau \rightarrow -\infty$ both equal zero.
%If $Y$ is continuous, then it follows that for any $\tau$ we can always find a $\tau' \neq \tau$ such that $\Delta(\tau) = \Delta(\tau')$.
%Now if we take \emph{another} difference, between pairs  $(\tau, \nu)$ and $(\tau', \nu')$ such that $\Delta(\tau) = \Delta(\tau')$ and $\Delta(\nu) = \Delta(\nu')$, the $\alpha_0$ term disappears:
%\begin{align*}
%  \left[ \frac{\widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau+ \beta)}{\Delta(\tau)} - \frac{\widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0
% \end{align*}
%But so long as $\widetilde{\Delta}_1(\tau) \neq \widetilde{\Delta}_1(\tau')$ and  $\widetilde{\Delta}_1(\nu) \neq \widetilde{\Delta}_1(\nu')$ the equation itself does not vanish and can be used to solve for $\beta$.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{New Results from September 2016}

\subsubsection{Relationship between observed and unobserved CDFs}
Let
\begin{align*}
F^*_{tk}(\tau) &= P(Y \leq \tau|T^*=t, z_k)\\
F_{tk}(\tau) &= P(Y \leq \tau|T=t, z_k)
\end{align*}
Now, by the assumption of non-differential measurement error,
\begin{align*}
  p_k F_{1k}(\tau) &= (1 - \alpha_1) p_k^* F_{1k}^*(\tau) + \alpha_0 (1 - p_k^*)F_{0k}^*(\tau)\\
  (1 - p_k) F_{0k}(\tau) &= \alpha_1 p_k^* F_{1k}^*(\tau) + (1 - \alpha_0) (1 - p_k^*)F_{0k}^*(\tau)
\end{align*}
Solving the linear system as above, we find that
\begin{align*}
  F_{0k}^*(\tau) &= F_{0k}(\tau) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ F_{0k}(\tau) - F_{1k}(\tau) \right]\\
  F_{1k}^*(\tau) &= F_{1k}(\tau) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ F_{1k}(\tau) - F_{0k}(\tau) \right]\\
\end{align*}

\subsection{Can we relax the measurement error assumptions?}
Suppose that we continue to assume that $P(Y|T^*,T,z) = P(Y|T^*,z)$ but relax the assumption that $P(T|T^*,z) = P(T|T^*)$.
Define:
\begin{align*}
  \alpha_{0k} &= P\left( T=1|T^*=1, z_k \right)\\
  \alpha_{1k} &= P\left( T=1|T^*=0, z_k \right)
\end{align*}
As before, the Wald estimator converges in probability to
\[
  \mathcal{W} = \frac{E[Y|z_k]-E[Y|z_\ell]}{p_k - p_\ell}
\]
but the relationship between $p_1 - p_0$ and the unobserved $p^*_1 - p^*_0$ changes.
By the law of total probability
\begin{align*}
  p_k &= P(T=1|z_k) = P(T=1|T^*=1,z_k)P(T^*=1|z_k) + P(T=1|T^*=0,z_k)P(T^*=0|z_k)\\
  &= (1 - \alpha_{1k})p_k^* + \alpha_{0k}(1 - p^*_k) = p_k^*(1 - \alpha_{0k} - \alpha_{1k}) + \alpha_{0k}
\end{align*}
and thus
\[
  p_k^* = \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}},
  \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_{1k}}{1 - \alpha_{0k} - \alpha_{1k}}.
\]
Thus, we have
\begin{align*}
  p^*_k - p^*_\ell &= \left( \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}} \right) - \left( \frac{p_0 - \alpha_{0\ell}}{1 - \alpha_{0\ell} - \alpha_{1\ell}} \right)\\
  &= \frac{\left( p_k - \alpha_{0k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right) - \left( p_0 - \alpha_{0\ell} \right)\left( 1 - \alpha_{0k} - \alpha_{1k} \right)}{\left( 1 - \alpha_{0k} - \alpha_{1k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right)}
\end{align*}



\subsection{Is there a LATE interpretation of our results?}
Let $J \in \left\{ a, c, d, n \right\}$ index an individual's \emph{type}: always-taker, complier, defier, or never-taker.
Let $\pi_a, \pi_c, \pi_d, \pi_n$ denote the population proportions of always-takers, compliers, defiers, and never-takers.
The unconfounded type assumption is $P(J=j|z=1) = P(J=j|z=0)$.
Combined with the law of total probability, this gives
\begin{align*}
  p^*_1 &= P(T^*=1|z=1) = \pi_a + \pi_c \\
  1 - p^*_1 &= P(T^*=0|z=1) = \pi_d + \pi_n \\
  p^*_0 &= P(T^*=1|z=0) = \pi_d + \pi_a \\
  1-p^*_0 &= P(T^*=0|z=0) = \pi_n + \pi_c 
\end{align*}
Imposing no-defiers, $\pi_d = 0$, these expressions simplify to
\begin{align*}
  p^*_1 &=  \pi_a + \pi_c \\
  1 - p^*_1 &=  \pi_n \\
  p^*_0 &=  \pi_a \\
  1-p^*_0 &=  \pi_n + \pi_c 
\end{align*}
Solving for $\pi_c$, we see that
\begin{align*}
  \pi_c &= p_1^* - p_0^*\\
  \pi_a &= p_0^*\\
  \pi_n &= 1 - p_1^*
\end{align*}

Now, let $Y(1)$ indicate the potential outcome when $T^*=1$ and $Y(0)$ indicate the potential outcome when $T^*=0$.
The standard LATE assumptions (no defiers, mean exclusion, unconfounded type) imply
\begin{align*}
  \mathbb{E}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{E}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{E}\left[ Y(1)|J=c \right] \\
  \mathbb{E}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=n \right]\\
  \mathbb{E}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{E}\left[ Y(1)|J=a \right]\\
  \mathbb{E}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{E}\left[ Y(0)|J=n \right]
\end{align*}



\subsubsection{LATE Version of Theorem 2 from the Draft}
\begin{align*}
  \Delta\overline{yT} &= \mathbb{E}\left( yT|z=1 \right) - \mathbb{E}\left( yT|z=0 \right) \\
  &= (1 - \alpha_1) \left[ p_1^* \mathbb{E}\left( y|T^*=1, z=1 \right) - p_0^* \mathbb{E}\left(y|T^*=1, z=0\right) \right] \\
  & \; \; \quad \quad + \alpha_0 \left[ (1 - p_1^*)\mathbb{E}\left( y|T^*=0, z=1\right) - (1 - p_0^*)\mathbb{E}\left(y|T^*,z=0 \right) \right]
\end{align*}
So we find that
\begin{align*}
  \Delta\overline{yT} &= (p_1^* - p_0^*)\left\{ (1 - \alpha_1) \mathbb{E}\left[ Y(1)|J=c \right] - \alpha_0\mathbb{E}\left[ Y(0)|J=c \right] \right\}\\
  &= (1 - \alpha_1) \left\{ \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1} (p_1 - p_0) \right\} + (p_1  - p_0) \mathbb{E}\left[ Y(0)|J=c \right]
\end{align*}
Recall that the analogous expression in the homogeneous treatment effect case is
\begin{align*}
  \Delta\overline{yT} &= (1 - \alpha_1) \mathcal{W} (p_1 - p_0) + \mu_{10}^*\\
  &= (1 - \alpha_1) \left(\frac{\beta}{1 - \alpha_0 - \alpha_1}\right) (p_1 - p_0) + (p_1 - \alpha_0)m_{11}^* - (p_0 - \alpha_0)m_{10}^*
\end{align*}
while the expression for the difference of variances is 
\begin{align*}
  \Delta\overline{y^2} &= \beta \mathcal{W}(p_1 - p_0) + 2\mathcal{W} \mu_{10}^*
\end{align*}
From above we see that the analogue of $\mu_{10}^*$ in the heterogeneous treatment effects setting is $(p_1 - p_0)E\left[ Y(0)|J=c \right]$ and since the LATE is $\mathbb{E}\left[ Y(1) - Y(0) |J=c\right]$, the analogue of $\mathcal{W}$ is
\[
  \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1}
\]
so \emph{if} we could establish that 
\[
  \Delta\overline{y^2} =  \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0) |J=c \right]
\]
in the heterogeneous treatment effects case, the proof of Theorem 2 would go through immediately.
Now, if we assume an exclusion restriction on the \emph{second} moment of $y$ an argument almost identical to the standard LATE derivation gives
\[
  \Delta\overline{y^2} = \frac{\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right]}{p_1^* - p_0^*} = \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right] 
\]
so we see that the necessary and sufficient condition for our proof to go through is 
\[
  \mathbb{E}\left[ Y^2(1) - Y^2(0)|J=c \right] = \mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0)|J=c \right]
\]
Rearranging, this in turn is equivalent to
\[
  \mbox{Var}\left[ Y(1)|J=c \right] = \mbox{Var}\left[ Y(0)|J=c \right]
\]


\subsection{Partial Identification Under Independence Assumption}
Suppose we only make the LATE independence assumption $Y(T^*,z) = Y(T^*)$ rather than the conditional independence assumption $P(Y<\tau|T^*,z_k) = P(Y<\tau|T^*,z_\ell)$.
Then we still obtain
\begin{align*}
  \mathbb{P}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{P}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{P}\left[ Y(1)|J=c \right] \\
  \mathbb{P}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=n \right]\\
  \mathbb{P}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{P}\left[ Y(1)|J=a \right]\\
  \mathbb{P}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{P}\left[ Y(0)|J=n \right]
\end{align*}
From above, we also know that
\begin{align*}
  P(Y|T^*=0,z_k) &= P(Y|T=0, z_k) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ P(Y|T=0,z_k) - P(Y|T=1,z_k) \right]\\
  P(Y|T^*=1,z_k) &= P(Y|T=1, z_k) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ P(Y|T=1,z_k) - P(Y|T=0,z_k) \right]
\end{align*}
The notation is getting a bit unwieldy so let $\pi^*_{tk}(y)= P(Y=y|T^*=t,z_k)$ and similarly define $\pi_{tk}(y) = P(Y=y|T=t,z_k)$.
Using this new notation, we have
\begin{align*}
  (1 - p_k - \alpha_1) \pi^*_{0k}(y) &= (1 - p_k - \alpha_1) \pi_{0k}(y) + \alpha_1 p_k \left[ \pi_{0k}(y) - \pi_{1k}(y) \right]\\
  (p_k - \alpha_0) \pi_{1k}^*(y) &= (p_k - \alpha_0) \pi_{1k}(y) + \alpha_0 (1 - p_k)\left[ \pi_{1k}(y) - \pi_{0k}(y) \right]
\end{align*}
Writing these out for all values of $k$,
\begin{align*}
  (p_1 - \alpha_0) \pi_{11}^*(y) &= (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  (1 - p_0 - \alpha_1) \pi^*_{00}(y) &= (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0) \pi_{10}^*(y) &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) \pi^*_{01}(y) &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
Similarly, using the fact that $p_k^* = (p_k - \alpha_0)/(1 - \alpha_0 - \alpha_1)$,  
\begin{align*}
  \pi^*_{11}(y) &= \left( \frac{p_0 - \alpha_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=a \right] + \left( \frac{p_1 - p_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=c \right]\\
  \pi^*_{00}(y) &= \left( \frac{p_1 - p_0}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1 - \alpha_1}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=n \right]\\
  \pi^*_{10}(y) &= P\left[ Y(1)|J=a \right]\\
  \pi^*_{01}(y) &= P\left[ Y(0)|J=n \right] 
\end{align*}
or equivalently,
\begin{align*}
  (p_1 - \alpha_0)\pi^*_{11}(y) &= \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right]\\
  (1 - p_0 - \alpha_1)\pi^*_{00}(y) &=\left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right]\\
  (p_0 - \alpha_0)\pi^*_{10}(y) &= (p_0 - \alpha_0)P\left[ Y(1)|J=a \right]\\
  (1 - p_1 - \alpha_1)\pi^*_{01}(y) &= (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] 
\end{align*}
Equating,
\begin{align*}
  \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
and substituting the third and fourth equalities into the first and second we obtain
\footnotesize
\begin{align*}
   (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Simplifying and re-arranging,
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y) + (1 - p_1)\pi_{01}(y) - (1 - p_0) \pi_{00}(y) }{p_1 - p_0} \right] \\ 
  P\left[Y(0) =y|J=c \right] &= \left[ \frac{(1 - p_0)\pi_{00}(y) - (1 - p_1)\pi_{01}(y)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{(1 - p_0) \pi_{00}(y) - (1 - p_1)\pi_{01}(y) + p_0 \pi_{10}(y) - p_1 \pi_{11}(y)}{p_1 - p_0} \right] \\
  P\left[Y(1) = y|J=a \right] &=  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  P\left[ Y(0) = y|J=n \right] &=  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Notice that the first two equations can be simplified as follows
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{P(Y = y,T=1|z=1) - P(Y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y =y|z=0)}{p_1 - p_0} \right] \\ 
  P\left[Y(0) = y|J=c \right] &= \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] 
\end{align*}
\normalsize
Now, since probabilities must be between zero and one, we obtain the bounds
\begin{align*}
  0 &\leq \left[ \frac{P(Y = y,T=1|z=1) - P(Y = y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y = y|z=0)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] \leq 1 
\end{align*}
\normalsize
which we abbreviate 
\begin{align*}
  0 &\leq \left[ \frac{\Delta P(Y=y,T=1)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \alpha_1 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] - \left[\frac{ \Delta P(Y=y,T=0)}{p_1 - p_0} \right] \leq 1 
\end{align*}
where
\begin{align*}
  \Delta P(Y=y) &= P(Y=y|z=1) - P(Y=y|z=0)\\
  \Delta P(Y=y, T=t) &= P(Y=y, T=t|z=1) - P(Y=y,T=t|z=0).
\end{align*}
To manipulate these bounds, we need to know the sign of $R = \Delta P(Y=y)/(p_1 - p_0)
$. 
Presumably this will be positive for most values of $y$, but it could be negative.
\paragraph{Case I: $R$ is positive.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)}\\
  \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)}
\end{align*}

\paragraph{Case II: $R$ is negative.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} \\
  \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)}
\end{align*}
Note that we \emph{two-sided} bounds for the misclassification probabilities.
These may be trivial in some cases, but I don't think it's obvious that they always will be.
\todo[inline]{Do these bounds have anything to do with the testability of the LATE assumptions? That is, do we get a lower bound for measurement error \emph{precisely when} we would otherwise violate a testable LATE assumption?}

Note that we also obtain bounds from the potential outcome distributions of always-takers and never-takers, namely 
\begin{align*}
  0 &\leq  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right] \leq 1\\
  0&\leq  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right] \leq 1
\end{align*}
but these are redundant.
From the assumption of non-differential measurement error, we already have 
\begin{align*}
  \pi_{0k}^* &= \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \\
  \pi_{1k}^* &= \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) 
\end{align*}
for all $k$ as given at the beginning of this section.
These expressions imply
\begin{align*}
 0 &\leq \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \leq 1 \\
  0 &\leq \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) \leq 1
\end{align*}
Re-arranging, we have
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \pi_{0k} + \alpha_1 p_k \left( \pi_{0k} - \pi_{1k} \right) \leq 1 - p_k - \alpha_1 \\
 0 &\leq p_k \pi_{1k}- \alpha_0\pi_{1k} + \alpha_0 (1 - p_k) \left( \pi_{1k} - \pi_{0k} \right) \leq p_k - \alpha_0
\end{align*}
and thus
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \left[(1 - p_k)\pi_{0k} + p_k \pi_{1k} \right] \leq 1 - p_k - \alpha_1 \\
  0 &\leq p_k \pi_{1k} - \alpha_0\left[p_k\pi_{1k} + (1 - p_k)\pi_{0k}\right] \leq p_k - \alpha_0
\end{align*}
Now consider the first inequality.
Re-arranging the right-hand side we obtain
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)(1 - \pi_{0k})}{1 - \left[ (1 - p_k)\pi_{0k} + p_k \pi_{1k} \right]} = (1 - p_k) \left[ \frac{P(Y=0|T=0, z=k)}{P(Y=0|z=k)} \right]
\end{align*}
and re-arranging the left-hand side we find
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)\pi_{0k}}{(1 - p_k)\pi_{0k} + p_k \pi_{1k}} = (1 - p_k) \left[ \frac{P(Y=1|T=0,z=k)}{P(Y=1|z=k)} \right]
\end{align*}
For the second inequality, the left-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k \pi_{1k}}{p_k \pi_{1k} + (1 - p_k)\pi_{0k}} = p_k \left[ \frac{P(Y=1|T=1,z=k)}{P(Y=1|z_k)} \right] 
\end{align*}
while the right-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k (1 - \pi_{1k})}{1 - \left[ p_k \pi_{1k} + (1 - p_k) \pi_{0k} \right]} = p_k \left[ \frac{P(Y=0|T=1,z=k)}{P(Y=0|z=k)} \right]
\end{align*}
These are analogous to our CDF bounds from above although they may not be tighter than the bounds 
\[
  \alpha_0 \leq p_k, \quad \alpha_1 \leq (1 - p_k)
\]
because we cannot argue, as we did above, about a limit in which the ratio of CDFs approaches one.
As before, however, we can take the tightest bound over $k = 0, 1$.

\subsection{Bounding the LATE}
Even if we didn't know anything about $\alpha_0$ and $\alpha_1$ beyond the fact that they are probabilities, it looks like we could still bound the LATE.
I think we can do this without using the independence of the instrument, that is only using the mean exclusion restriction.
Write out the LATE expressions with the $\alpha_0$ and $\alpha_1$ in them and them just plug in zero and one.
Could then tighten the bounds by imposing additional assumptions to get bounds for $\alpha_0$ and $\alpha_1$, from weakest to strongest.
If you have an independent instrument, you also get bounds for the outcome distributions.
Need to think some more about this\ldots


\subsection{Stochastic Dominance Conditions}
What if we imposed a stochastic ordering, e.g.\ $Y(1) > Y(0)$ for compliers?
Presumably this would give joint bounds for $\alpha_0$ and $\alpha_1$ from the LATE expressions from above.
Alternatively, perhaps one would choose to impose an ordering on the $Y(0)$ distributions for compliers versus never-takers or the $Y(1)$ distributions for the compliers versus always-takers.
This might be interesting in situations where one is concerned that the assumption we need for identification does not in fact hold and should give additional bounds.


\section{Outline For New Draft}
\begin{enumerate}
  \item Introduction / Literature Review
    \begin{enumerate}
      \item Why is this an important question?
        \begin{itemize}
          \item Treatments of interest in economics usually endogenous and often binary.
          \item Randomized encouragement designs are common in applied work.
          \item Treatment status is often self-reported.
          \item This problem is much more challenging that people realize.
        \end{itemize}
      \item Why are we different from Ura?
        \begin{itemize}
          \item Main difference is that we, in line with the existing literature, study the case of non-differential measurement error. This allows us to obtain point identification under certain assumptions.
          \item In contrast, Ura considers arbitray forms of mis-classification but as a consequence presents only partial identification results.
          \item Second, while we do provide results for LATE in Section blah, we mainly focus on additively separable model in which heterogeneity is captured by observed covariates while Ura considers only a LATE model. (And also doesn't allow for covariates.) 
        \end{itemize}
    \end{enumerate}
  \item Mahajan/Lewbell-style Assumptions 
    \begin{enumerate}
      \item Setup and Assumptions:
        \begin{itemize}
          \item Homogenous treatment effect model (additively separable)
          \item Conditional mean version of non-differential measurement error assumption.
          \item Conditional mean independence for IV.
        \end{itemize}
      \item Show that the model is not identified, regardless of (discrete) support of IV.
      \item Derive sharp bounds for $\alpha_0, \alpha_1$ and treatment effect.
      \item Show that second and third-moment independence for IV identifies this model? Maybe this isn't interesting in and of itself?
    \end{enumerate}
  \item Independence Assumption
    \begin{enumerate}
      \item Motivation
    \begin{itemize}
      \item Showed above that stronger assumptions are needed for identification, but the additional moment restrictions seem a bit artificial.
      \item When instruments are derived from economic theory that yields conditional mean independence only, we wouldn't want to use them.
      \item They would make sense, however, in an an RCT or natural experiment.
      \item The whole point in these settings is \emph{not} to rely on functional form assumptions. It would be strange to say that $z$ is an instrument for $y$ but not $\log y$.
      \item This points towards an \emph{independence} assumption for the instrument.
      \item Can make a similar argument for measurement error: seems strange to assume that $T$ is non-differential for $y$ but not $\log y$.
    \end{itemize}
      \item Sharp Bounds for $\alpha_0$ and $\alpha_1$ without valid instrument  
        \begin{itemize}
          \item Assume ``independence'' version of non-differential measurement error.
          \item Derive CDF bounds.
        \end{itemize}
      \item Conditional Independence for Instrument
        \begin{itemize}
          \item Exactly what assumptions do we need here? 
          \item Characteristic functions.
          \item Identification conditions?
          \item Overidentifying restrictions? Test model?
        \end{itemize}
    \end{enumerate}
  \item LATE Model
    \begin{enumerate}
      \item Introduction
        \begin{itemize}
          \item Most of the existing mis-classification literature focuses on a homogeneous treatment effects model.
          \item What if we don't have an additively separable model?
          \item These results complement Ura because we work under the assumption of non-differential measurement error while he asks what can be learned when one is unwilling to make any assumptions about the form of the mis-classification.
        \end{itemize}
      \item Mahajan/Lewbel Setup
        \begin{itemize}
          \item Presumably the partial identification results go through for a LATE.
          \item The second and third moment conditions would require restrictions on form of heterogeneity.
            These would seem to be satisfied by a generalized Roy model.
        \end{itemize}
      \item Independence Assumptions
        \begin{itemize}
          \item Presumably the CDF bounds go through as before but need to state exact form of independence assumption in terms of potential outcomes.
          \item Kitagawa-style independence assumption for IV: $Y(T^*,z) = Y(T^*)$. This gives bounds for all quantile treatment effects.
          \item Stochastic dominance conditions?
        \end{itemize}
    \end{enumerate}
  \item Estimation / Inference
  \item Simulation Study
  \item Empirical Examples
    \begin{itemize}
      \item Try to look at a number of examples under different assumptions to illustrate both point and partial identification results. Don't forget about Oreopoulous: the sample size is so huge that inference isn't a major concern.
    \end{itemize}
\end{enumerate}

\section{Weak Identification}

\subsection{Moment Equations}
First we write the moment equations in a more familiar GMM-style form.
\paragraph{First Moment Condition:}
This is simply the IV moment condition: 
\[
  \mbox{Cov}(y,z)/ \mbox{Cov}(T,z) = \beta/(1 - \alpha_0 - \alpha_1)
\]
Rearranging gives a more ``canonical'' GMM form:
\[
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right) \mbox{Cov}(T,z) = 0
\]

\paragraph{Second Moment Condition:}
The equations used to identify $(\alpha_0 - \alpha_1)$ in the paper are 
\begin{align*}
  \mu_{k\ell}^* &= (p_k - \alpha_0) m_{1k}^* - (p_\ell - \alpha_0) m_{1\ell}^* \\
  \Delta \overline{y^2} &= \beta \mathcal{W} (p_k - p_\ell) + 2 \mathcal{W} \mu_{k\ell}^*\\
  \Delta \overline{yT} &= (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) + \mu_{k\ell}^*
\end{align*}
Re-arranging the third equation,
$\mu_{k\ell}^* = \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell)$.
Substituting into the second equation, 
\begin{align*}
  \Delta\overline{y^2} &= \mathcal{W}\left[ \beta(p_k - p_\ell) + 2 \mu_{k\ell}^* \right]\\
  &= \mathcal{W}\left\{ \beta(p_k - p_\ell) + 2\left[ \Delta\overline{yT} - (1 - \alpha_1)\mathcal{W}(p_k - p_\ell) \right]  \right\}\\
  &= \mathcal{W} \left\{ (p_k - p_\ell)\left[ \beta - 2(1 - \alpha_1)\mathcal{W} \right] + 2\Delta\overline{yT} \right\}
\end{align*}
Now, substituting $\mathcal{W} = \beta/(1 - \alpha_0 - \alpha_1)$
\begin{align*}
  \Delta\overline{y^2}&= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{\beta (p_k - p_\ell)\left[ 1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right] + 2 \Delta\overline{yT} \right\}\\
  &= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\Delta\overline{yT} - \beta(p_k - p_\ell)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\}
\end{align*}
We now write this in a more standard form.
Let $w$ be any random variable. 
Then,
\begin{align*}
  \mbox{Cov}(w,z) &= E(wz) - E(w)E(z) = \left[ 1 \times E(w|z=1)q + 0 \times E(w|z=0)(1 - q) \right] - E(w)q\\
  &= q E(w|z=1) - qE(w) = q E(w|z=1) - q\left[ E(w|z=1)q + E(w|z=0)(1 - q) \right]\\
  &= q\left[ E(w|z=1)(1 - q) + E(w|z=0)(1 - q)\right]\\
  &= q(1-q)\left[ E(w|z=1) - E(w|z=0) \right]
\end{align*}
Using this fact, we can express the quantities that appear in the second moment equality in terms of covariances as follows
\[
  \Delta\overline{y^2} = \frac{\mbox{Cov}(y^2,z)}{q(1 - q)}, \quad
  \Delta\overline{yT} = \frac{\mbox{Cov}(yT,z)}{q(1 - q)}, \quad
  (p_k - p_\ell) = \frac{\mbox{Cov}(T,z)}{q(1 - q)}
\]
leading to
\[\frac{\mbox{Cov}(y^2,z)}{q(1-q)}= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{\frac{2\mbox{Cov}(yT,z)}{q(1-q)} - \beta \frac{\mbox{Cov}(T,z)}{q(1-q)}\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\}
\]
Or, multiplying through by $q(1-q)$ and re-arranging,
\[
  \mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\} = 0
\]

\paragraph{Third Moment Condition:}
The third and final set of moment conditions is
\begin{align*}
    \Delta\overline{y^3} &= \beta^2 [\mathcal{W} (p_k - p_\ell)]  + 3 \beta [\mathcal{W} \mu_{k\ell}^*] + 3 \mathcal{W} \lambda^*_{k\ell}\\
    \Delta\overline{y^2T} &=  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) + 2(1-\alpha_1)\mathcal{W}\mu_{k\ell}^* + \lambda_{k\ell}^*
\end{align*}
  To put this into a more familiar format, we first eliminate $\mu_{k\ell}^*$ using 
\[
  \mu_{k\ell}^* = \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell)
\]
from the derivation of the second moment equation from above, yielding
\begin{align*}
  \Delta\overline{y^3} &= \beta^2 [\mathcal{W} (p_k - p_\ell)]  + 3 \beta \mathcal{W}\left[ \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) \right]  + 3 \mathcal{W} \lambda^*_{k\ell}\\
    \Delta\overline{y^2T} &=  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) + 2(1-\alpha_1)\mathcal{W}\left[ \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) \right] + \lambda_{k\ell}^*
\end{align*}
Re-arranging and factoring the first equation gives
\[
  \Delta\overline{y^3} = \mathcal{W}\left( p_k - p_\ell \right)
  \left\{ \beta^2 + \frac{3\beta \Delta\overline{yT}}{p_k - p_\ell} - 3\beta \mathcal{W} (1 - \alpha_1) + \frac{3\lambda^*_{k\ell}}{p_k - p_\ell} \right\}
\]
Now, by re-arranging the second equation we find that
\begin{align*}
  \lambda_{k\ell}^* &= \Delta\overline{y^2T} -  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) - 2(1-\alpha_1)\mathcal{W}\left[ \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) \right] \\
  &= \Delta\overline{y^2T} -  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) - 2(1-\alpha_1)\mathcal{W} \Delta \overline{yT} + 2(1-\alpha_1)^2\mathcal{W}^2(p_k - p_\ell)
\end{align*}
and thus
\begin{align*}
  \frac{3 \lambda^*_{k\ell}}{p_k - p_\ell} &= 
  3\left(\frac{\Delta\overline{y^2T}}{p_k - p_\ell}\right) -  3\beta(1-\alpha_1)\mathcal{W} - 6(1-\alpha_1)\mathcal{W} \left(\frac{\Delta \overline{yT}}{p_k - p_\ell}\right) + 6(1-\alpha_1)^2\mathcal{W}^2
\end{align*}
so that 
\small
\begin{align*}
  \frac{\Delta\overline{y^3}}{\mathcal{W}(p_k - p_\ell)} &=
  \left\{ \beta^2 - 6\beta\mathcal{W}(1 - \alpha_1) + 6\mathcal{W}^2(1 - \alpha_1)^2 + \left( \frac{\Delta\overline{yT}}{p_k - p_\ell} \right)\left[ 3\beta - 6\mathcal{W}(1 - \alpha_1) \right] + 3\left( \frac{\Delta\overline{y^2T}}{p_k - p_\ell} \right)  \right\}\\
  &= \left\{ \beta^2\left[1 - \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}  + \frac{6(1 - \alpha_1)^2}{(1 - \alpha_0 - \alpha_1)^2}\right]   + 3\beta\left[ 1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right] \left( \frac{\Delta\overline{yT}}{p_k - p_\ell} \right)+ 3\left( \frac{\Delta\overline{y^2T}}{p_k - p_\ell} \right) \right\}
\end{align*}
\normalsize
Simplifying, we find that
\[
  \left[1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}\right] 
  = \frac{(1 - \alpha_0 - \alpha_1) - 2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}
  =\frac{1 - \alpha_0 - \alpha_1 - 2 + 2\alpha_1}{1 - \alpha_0 - \alpha_1}
  = \frac{\alpha_1 - \alpha_0 - 1}{1 - \alpha_0 - \alpha_1}
\]
and
\begin{align*}
\left[1 - \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}  + \frac{6(1 - \alpha_1)^2}{(1 - \alpha_0 - \alpha_1)^2}\right] 
&= 1 - \left[ \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right]\left[ 1 - \frac{1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right]\\
&= 1 - \left[ \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right]\left[  \frac{(1 - \alpha_0 - \alpha_1) - (1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right]\\
&= 1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2}
\end{align*}
so that
\begin{align*}
  \frac{\Delta\overline{y^3}}{\mathcal{W}(p_k - p_\ell)} 
  &= \left\{ \beta^2\left[1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2} \right] - 3\beta\left[ \frac{1 - (\alpha_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] \left( \frac{\Delta\overline{yT}}{p_k - p_\ell} \right)+ 3\left( \frac{\Delta\overline{y^2T}}{p_k - p_\ell} \right) \right\}
\end{align*}
Therefore, re-arranging and multiplying through by $q(1 - q)$,
\footnotesize
\begin{align*}
  \mbox{Cov}(z,y^3) 
  &= \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right)\left\{ \beta^2\left[1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2} \right] \mbox{Cov}(z,T) - 3\beta\left[ \frac{1 - (\alpha_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] \mbox{Cov}(z,yT) + 3\mbox{Cov}(z,y^2T) \right\}
\end{align*}
\normalsize
\paragraph{Full Set of Moment Conditions}
\footnotesize
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\} &= 0\\
  \mbox{Cov}(y^3,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right)\left\{ \beta^2\left[1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2} \right] \mbox{Cov}(T,z) - 3\beta\left[ \frac{1 - (\alpha_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] \mbox{Cov}(yT,z) + 3\mbox{Cov}(y^2T,z) \right\} &= 0
\end{align*}
\normalsize

\subsection{Simple Special Case: $\alpha_0 = 0$}
Suppose that $\alpha_0$.
Then the model is identified using the first and second moment equalities, which simplify to
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 -\alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \left(\frac{\beta}{1 - \alpha_1}\right)\left[2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z) \right] &= 0
\end{align*}
In this simple special case, it is easy to solve for $\beta$ by substituting the first moment condition into the second:
\[
  \beta = \frac{2 \mbox{Cov}(yT,z)}{\mbox{Cov}(T,z)} - \frac{\mbox{Cov}(y^2,z)}{\mbox{Cov}(y,z)}
\]
\todo[inline]{I checked this equation in our simulation experiment and it is indeed correct} 
Notice that if $\beta \approx 0$ then both $\mbox{Cov}(y^2,z)$ and $\mbox{Cov}(y,z)$ are close to zero so their ratio becomes extremely noisy.

\paragraph{Standard GMM form:}
To express this system in the standard GMM form, we need to agument these moment equalities with expressions for the means of $z, y, y^2, T,$ and $yT$ as follows.
Let $\mathbf{w}_i = (y_i, z_i, T_i)'$, $\theta = (\beta, \alpha_1)'$ and $\gamma = (q, p, \mu, s, r)'$ where
\begin{align*}
  q &= \mathbb{E}\left[z \right] \\
  p &= \mathbb{E}\left[T \right] \\
  \mu &= \mathbb{E}\left[y \right] \\
  s &= \mathbb{E}\left[y^2 \right] \\
  r &= \mathbb{E}\left[yT \right].
\end{align*}
We can express our problem in terms of two blocks of moment conditions, namely
\[
  f(\mathbf{w}; \theta, \gamma) = \left[
  \begin{array}{c}
    g(\mathbf{w}; \theta, \gamma)\\
    h(\mathbf{w}; \gamma)
  \end{array}
\right]
\]
where
\[
  g(\mathbf{w}; \theta, \gamma) = \left[
  \begin{array}{c}
    (zy - q\mu) - \displaystyle\left( \frac{\beta}{1 - \alpha_1} \right)(zT - qp) \\
    (zy^2 - qs) - 2\displaystyle\left( \frac{\beta}{1 -\alpha_1}\right) (zyT - qr) + \left(\frac{\beta^2}{1 - \alpha_1}\right)(zT - qp)  
  \end{array}
\right]
\]
and
\[
  h(\mathbf{w}; \gamma) = 
  \left[
  \begin{array}{c}
    z - q \\ T - p \\ y - \mu \\ y^2 - s \\ yT - r
  \end{array}
\right]
\]
We can view this as a two-step or ``plug-in'' GMM estimation problem where $\widehat{\gamma}$ solves the sample moment condition
\[
  \frac{1}{n}\sum_{i=1}^n h(\mathbf{w}_i; \gamma) = 0
\]
and $\widehat{\theta}$ solves
\[
  \frac{1}{n}\sum_{i=1}^n g(\mathbf{w}_i; \theta, \widehat{\gamma}) = 0.
\]
Unfortunately, in our example the first-step estimation affects the asymptotic variance of the second since an inconsistent estimator of $\gamma$ yields an inconsistent estimator of $\theta$.\footnote{See Newey \& McFadden (1994), Section 6.}
This means that we will have to proceed ``the hard way.''

Under standard regularity conditions, a GMM estimator based on the sample analogue $f_n(\theta, \gamma)$ of $\mathbb{E}[f(\mathbf{w};\theta,\gamma)]=0$ using a weighting matrix $\widehat{W}\rightarrow_p W$ converges in distribution to
\[
  -(F'WF)^{-1}F'W M, \quad M\sim N(0, \Omega) 
\]
where $\sqrt{n}f_n(\theta_0, \gamma_0) \rightarrow_d M$ and $F = \mathbb{E}[\nabla_\theta' f(\mathbf{w};\theta_0, \gamma_0), \nabla_\gamma' f(\mathbf{w}; \theta_0, \gamma_0)]$.
The present example, however, is just-identified which means that $F$ is square and hence
\[
  -(F'WF)^{-1}F'W = F^{-1}W^{-1}(F')^{-1}F'W = -F^{-1}
\]
Now, given the special structure of our example,
\begin{align*}
  F &= \left[
    \begin{array}{cc}
      \mathbb{E}\left\{\nabla'_\theta g(\mathbf{w};\theta_0, \gamma_0)\right\} & 
      \mathbb{E}\left\{\nabla'_\gamma g(\mathbf{w};\theta_0, \gamma_0)\right\}\\
\mathbf{0}& -\mathbf{I}
    \end{array}
  \right] \equiv
  \left[
  \begin{array}{cc}
    G_\theta & G_\gamma\\
    \mathbf{0} & - \mathbf{I}
  \end{array}
\right]
\end{align*}
becuase $h$ does not involve $\theta$ and $\nabla_\gamma' h(\mathbf{w},\gamma) = -\mathbf{I}$.
Inverting, we have
\[
  -F^{-1} = 
  \left[
  \begin{array}{cc}
    -G_\theta & -G_\gamma \\
    \mathbf{0} & \mathbf{I}
  \end{array}
\right]^{-1} =
\left[
\begin{array}{cc}
  -G_{\theta}^{-1} & -G_{\theta}^{-1}G_{\gamma}\\
  \mathbf{0} & \mathbf{I}
\end{array}
\right]
\]
We see from this expression that if $G_\gamma$ were zero, the first step-estimation would not affect the limit distribution of $\widehat{\theta}$.
Differentiating,
\[ 
  \left[
  \begin{array}{cc}
    \nabla g_\beta & \nabla g_{\alpha_1}
  \end{array}
\right] = 
  \left[
  \begin{array}{cc}
    \displaystyle -\left(\frac{zT - qp}{1 - \alpha_1}\right) & \displaystyle -\left\{\frac{\beta(zT - qp)}{(1 - \alpha_1)^2}\right\} \\ \\
    \displaystyle 2\left\{ \frac{\beta(zT - qp) - (zyT - qr)}{1 - \alpha_1}\right\} & \displaystyle \frac{\beta^2(zT - qp) - 2\beta (zyT - qr)}{(1 - \alpha_1)^2}
  \end{array}
\right]
\]
and thus, taking expectations,
\[
  G_{\theta} = 
  \left[
  \begin{array}{cc}
    \displaystyle \frac{-\mbox{Cov}(z,T)}{1 - \alpha_1} & \displaystyle  \frac{-\beta \mbox{Cov}(z,T)}{(1 - \alpha_1)^2} \\ \\
    \displaystyle 2\left\{ \frac{\beta\mbox{Cov}(z,T) - \mbox{Cov}(yT,z) }{1 - \alpha_1} \right\} & \displaystyle \frac{ \beta^2\mbox{Cov}(z,T) - 2\beta \mbox{Cov}(yT,z)}{(1 - \alpha_1)^2} 
  \end{array}
\right]
\]
Now, for $G_\gamma$ we have
\begin{align*}
  G_\gamma &= \mathbb{E}
  \left[
  \begin{array}{ccccc}
    \nabla_q g & \nabla_p \mu & \nabla_\mu g & \nabla_s g & \nabla_r g
  \end{array}
\right] \\
&=
\left[
\begin{array}{ccccc}
  \displaystyle \left( \frac{p\beta}{1 - \alpha_1}  - \mu \right)  & \displaystyle\left(\frac{q\beta}{1 - \alpha_1}\right)& -q & 0 & 0 \\ \\
  \displaystyle\left( \frac{\beta}{1 - \alpha_1} \right)(2r - \beta p) - s &\displaystyle \frac{-q\beta^2}{1 - \alpha_1} & 0 & -q & \displaystyle \frac{2\beta q}{1 - \alpha_1}
\end{array}
\right]
\end{align*}
The next step is to invert $G_\theta$.
First we calculate the determinant.
For the purposes of this calculation, use the shorthand $C = \mbox{Cov}(z,T)$ and $D = \mbox{Cov}(yT,z)$.
We have:
\begin{align*}
  |G_\theta| &= \left[ \frac{-C}{1 - \alpha_1} \right]\left[ \frac{\beta^2 C - 2\beta D}{(1 - \alpha_1)^2} \right] - \left[ \frac{-\beta C}{(1 - \alpha_1)^2} \right]\left[ \frac{2\beta C - 2D}{1 - \alpha_1} \right]\\
  &= \left( \frac{1}{1 - \alpha_1} \right)^3 \left[ 2\beta C D - \beta^2 C^2 + 2\beta^2 C^2 - 2\beta CD  \right]\\
  &= \frac{\beta^2 \mbox{Cov}(z,T)^2}{(1 - \alpha_1)^3} 
\end{align*}
Thus,
\begin{align*}
  G_\theta^{-1} &= \frac{(1 - \alpha_1)^3}{\beta^2 \mbox{Cov}(z,T)^2} 
  \left[
  \begin{array}{cc}
    \displaystyle \frac{ \beta^2\mbox{Cov}(z,T) - 2\beta \mbox{Cov}(yT,z)}{(1 - \alpha_1)^2}  & \displaystyle  \frac{\beta \mbox{Cov}(z,T)}{(1 - \alpha_1)^2} \\ \\
    \displaystyle -2\left\{ \frac{\beta\mbox{Cov}(z,T) - \mbox{Cov}(yT,z) }{1 - \alpha_1} \right\} & \displaystyle\frac{-\mbox{Cov}(z,T)}{1 - \alpha_1}
  \end{array}
\right] \\ \\
  &=\left[
  \begin{array}{cc}
    \displaystyle \left\{\frac{1 - \alpha_1}{\mbox{Cov}(z,T)}\right\}\left\{ 1 - \frac{2\mbox{Cov}(yT,z)}{\beta\mbox{Cov}(z,T)} \right\}& \displaystyle  \frac{1 - \alpha_1}{\beta \mbox{Cov}(z,T)} \\ \\
    \displaystyle \frac{2(1 - \alpha_1)^2}{\beta \mbox{Cov}(z,T)}\left\{ \frac{\mbox{Cov}(yT,z)}{\beta\mbox{Cov}(z,T)} - 1 \right\} & \displaystyle \frac{-(1 - \alpha_1)^2}{\beta^2 \mbox{Cov}(z,T)} 
  \end{array}
\right] \\
\end{align*}
The next step is to calculate $\Omega$:
\begin{align*}
  \Omega = \lim_{n\rightarrow \infty}\mbox{Var}\left[\sqrt{n}f_n(\theta_0, \gamma_0)\right] 
  = \lim_{n\rightarrow \infty}\mbox{Var}\left[\frac{1}{\sqrt{n}} \sum_{i=1}^n f(\mathbf{w}_i; \theta_0, \gamma_0)  \right]
\end{align*}
If $\mathbf{w}_i$ is an iid sequence of RVs, then
\begin{align*}
  \Omega = \lim_{n\rightarrow \infty}\frac{1}{n}\mbox{Var}\left[ \sum_{i=1}^n f(\mathbf{w}_i; \theta_0, \gamma_0)\right] = \lim_{n\rightarrow \infty} \frac{1}{n}\sum_{i=1}^n \mbox{Var}\left[ f(\mathbf{w}_i;\theta_0, \gamma_0) \right] = \mbox{Var}\left[ f(\mathbf{w}_i;\theta_0, \gamma_0) \right]  
\end{align*}
And assuming that our model is correctly specified, so that $\mathbb{E}\left[ f(\mathbf{w}_i;\theta_0, \gamma_0) \right]=0$,
\begin{align*}
  \mbox{Var}\left[ f(\mathbf{w}_i; \theta_0, \gamma_0) \right] 
  &=\mathbb{E}
  \left[
  \begin{array}{cc}
    g(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    g(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)' \\
    h(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    h(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)'
  \end{array}
\right]\\ &\equiv
\left[
\begin{array}{cc}
  \Omega_{gg} & \Omega_{gh}\\
  \Omega_{gh} & \Omega_{hh}
\end{array}
\right]
\end{align*}
We now calculate each block.
\todo[inline]{I don't think this is actually going to give us anything interpretable. The expressions are quite involved and it seems unlikely that they'll cancel in a useful way. This doesn't matter for implementation, of course, since it's easy to calculate the estimate of $\widehat{\Omega}$ by plugging the GMM estimates into the sample moment conditions, taking the outer product, and averaging.}

We are only interested in the asymptotic variance matrix $V_\theta$ of our parameters of interest $\theta = (\beta, \alpha_1)$.
We calculate this as follows:
\begin{align*}
  V_\theta &= \left[
  \begin{array}{cc}
    G_\theta^{-1} & G_{\theta}^{-1}G_\gamma
  \end{array}
\right]
\left[
\begin{array}{cc}
  \Omega_{gg} & \Omega_{gh}\\
  \Omega_{hg} & \Omega_{hh}\\
\end{array}
\right]
\left[
\begin{array}{c}
  \left( G_\theta^{-1} \right)'\\
  \left( G_\theta^{-1}G_{\gamma} \right)'\\
\end{array}
\right] \\
&= 
  \left[\begin{array}{cc}
      G_\theta^{-1}\left(\Omega_{gg} + G_\gamma \Omega_{gh}  \right) & 
      G_{\theta}^{-1}\left( \Omega_{gh} + G_\gamma \Omega_{hh} \right)
  \end{array}
\right]
\left[
\begin{array}{c}
  \left( G_\theta^{-1} \right)'\\
  G_{\gamma}' \left( G_\theta^{-1}\right)'\\
\end{array}
\right]\\
&= G_\theta^{-1}\left(\Omega_{gg} + G_\gamma \Omega_{hg}  \right)\left( G_{\theta}^{-1} \right)' + G_\theta^{-1}\left( \Omega_{gh} + G_\gamma \Omega_{hh}\right)G_\gamma' \left( G_\theta^{-1} \right)'\\
&= G_\theta^{-1}\left(\Omega_{gg} + G_\gamma \Omega_{hg} + \Omega_{gh}G_\gamma' + G_\gamma \Omega_{hh} G_\gamma'\right)\left( G_\theta^{-1} \right)'
\end{align*}

\subsection{Easier(?) Derivation of Simple Special Case: $\alpha_0 = 0$}
Recall that we could eliminate $\alpha_1$ from the moment conditions, yielding,
\[
  \beta = \frac{2 \mbox{Cov}(yT,z)}{\mbox{Cov}(T,z)} - \frac{\mbox{Cov}(y^2,z)}{\mbox{Cov}(y,z)}
\]
We can treat this as our $g$ block of moment conditions with a parameter vector $\theta$ that is simply $\beta$.
This gives
\[
  g(\mathbf{w}; \beta, \gamma) = \left[ 2\left(\frac{zTy - qr}{zT - qp}\right) - \frac{zy^2 - qs}{zy - q\mu} - \beta \right]
\]
The $h$ block of moment conditions is unchanged.
Now, we have
\begin{align*}
  F &= \left[
    \begin{array}{cc}
      \mathbb{E}\left\{\nabla'_\beta g(\mathbf{w};\beta_0, \gamma_0)\right\} & 
      \mathbb{E}\left\{\nabla'_\gamma g(\mathbf{w};\beta_0, \gamma_0)\right\}\\
\mathbf{0}& -\mathbf{I}
    \end{array}
  \right] \equiv
  \left[
  \begin{array}{cc}
    G_\beta & G_\gamma\\
    \mathbf{0} & - \mathbf{I}
  \end{array}
\right] = \left[
\begin{array}{cc}
  -1 & G_\gamma \\
    \mathbf{0} & - \mathbf{I}
\end{array}
\right]
\end{align*}
since the derivative of $g$ with respect to $\beta$ is -1 and that of $h$ with respect to $\gamma$ is $-\mathbf{I}$.
Inverting, 
\[
  -F^{-1} = 
  \left[
  \begin{array}{cc}
    -G_\beta & -G_\gamma \\
    \mathbf{0} & \mathbf{I}
  \end{array}
\right]^{-1} =
\left[
\begin{array}{cc}
  -G_{\beta}^{-1} & -G_{\beta}^{-1}G_{\gamma}\\
  \mathbf{0} & \mathbf{I}
\end{array}
\right] = \left[
\begin{array}{cc}
  1 & G_\gamma \\
  \mathbf{0} & \mathbf{I}
\end{array}
\right]
\]
We calculate $G_\gamma$ as follows: 
\begin{align*}
  G_\gamma &= \mathbb{E}
  \left[
  \begin{array}{ccccc}
    \nabla_q g & \nabla_p g & \nabla_\mu g & \nabla_s g & \nabla_r g
  \end{array}
\right] \\ \\
  \nabla_q g(\mathbf{w};\beta_0, \gamma_0) &= \nabla_q \left[ 2\left(\frac{zTy - qr}{zT - qp}\right) - \frac{zy^2 - qs}{zy - q\mu} - \beta \right]\\
  &= 2\left[ \frac{-r(zT - qp) + p(zTy - qr)}{(zT - qp)^2} \right] - \frac{-s(zy - q\mu) + \mu (zy^2 - qs)}{(zy - q\mu)^2}\\
  &=  \frac{2zT(py - r)}{(zT - qp)^2}  - \frac{zy (\mu y - s)  }{(zy - q\mu)^2}\\ \\
  \nabla_p g(\mathbf{w};\beta_0, \gamma_0) &= \frac{2q(zTy - qr)}{(zT - qp)^2} \\ \\
  \nabla_\mu g(\mathbf{w};\beta_0, \gamma_0) &= \frac{-q(zy^2 - qs)}{(zy - q\mu)^2}\\ \\
  \nabla_s g(\mathbf{w};\beta_0, \gamma_0) &= \frac{q}{zy - q\mu} \\ \\
  \nabla_r g(\mathbf{w};\beta_0, \gamma_0) &= \frac{-2q}{zT - qp}
\end{align*}

The next step is to calculate $\Omega$:
\begin{align*}
  \Omega &= \mbox{Var}\left[ f(\mathbf{w}_i; \theta_0, \gamma_0) \right] 
  =\mathbb{E}
  \left[
  \begin{array}{cc}
    g(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    g(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)' \\
    h(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    h(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)'
  \end{array}
\right]\\ &\equiv
\left[
\begin{array}{cc}
  \Omega_{gg} & \Omega_{gh}\\
  \Omega_{gh} & \Omega_{hh}
\end{array}
\right]
\end{align*}
We now calculate each block.
\todo[inline]{Still need to do this!}


\subsection{Two-Step Inference Idea}
If $\beta$ is small, then confidence intervals based on the GMM limit distribution from above will perform badly.
But even in this case, inference for the \emph{identified set} $\left[ \beta_{RF}, \beta_{IV} \right]$ should still be well-behaved, so long as the instrument is strong.
The idea of this section is to explore a two-step procedure that chooses between reporting the GMM confidence interval or inference for the identified set based on a pre-test of $\beta_{RF}$.
Presumably conducting valid inference based on such a procedure will require a Bonferroni correction.
The first step, however, is to determine the joint limiting behavior of the reduced form, IV, and GMM estimators.

\paragraph{Reduced Form Estimator}
The reduced form is given by
\begin{align*}
  y &= \gamma_0 + \gamma_1 z + \eta \\
  \gamma_0 &= \mathbb{E}[y|z=0]\\
  \gamma_1 &= \mathbb{E}[y|z=0] - \mathbb{E}[y|z=1]
\end{align*}
Now, we need to write $\eta$ in terms of the ``primitives'' of our model.
The first stage and main equation are
\begin{align*}
  y &= c + \beta T^* + \varepsilon\\
  T^* &= \pi_0 + \pi_1 z + v\\
  \pi_1 &= p_1^* - p_0^*
\end{align*}
which implies
\[
  y = (c + \beta \pi_0) + (\beta \pi_1) z + (\varepsilon + \beta v)
\]
so that
\begin{align*}
\gamma_1 &= \beta(p_1^* - p_0^*)\\
\eta &= \varepsilon + \beta v
\end{align*}
Now, define $W = (\mathbf{1}, \mathbf{z})$ and $\boldsymbol{\gamma} = (\gamma_0, \gamma_1)'$.
Then the reduced form estimator is 
\[
  \widehat{\boldsymbol{\gamma}} = \left(W'W\right)^{-1}  W'\mathbf{y} 
  = \left( W'W \right)^{-1}W'\left( W\boldsymbol{\gamma} + \boldsymbol{\eta} \right) = \boldsymbol{\gamma} + \left( W'W \right)^{-1}W'\boldsymbol{\eta}
\]
and hence
\[
  \sqrt{n}\left( \widehat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}\right) = \left( \frac{W'W}{n} \right)^{-1}\frac{W'\boldsymbol{\eta}}{\sqrt{n}}
\]
Now,
\[
  \left(\frac{W'W}{n}\right)^{-1} = \left[
  \begin{array}{cc}
    1 & \bar{\mathbf{z}} \\
    \bar{\mathbf{z}} & \mathbf{z}'\mathbf{z}/n
  \end{array}
\right]^{-1} \rightarrow_p 
\left[
\begin{array}{cc}
  1 & q \\
  q & q
\end{array}
\right]^{-1}
= \frac{1}{q(1-q)}\left[
\begin{array}{cc}
  q & -q \\
  -q & 1
\end{array}
\right]
\]
and by the Central Limit Theorem,
\[
  \frac{W'\boldsymbol{\eta}}{\sqrt{n}} = \frac{1}{\sqrt{n}}\sum_{i=1}^n \eta_i\left[
  \begin{array}{c}
    1 \\ z_i 
  \end{array}
\right] 
= \frac{1}{\sqrt{n}}\sum_{i=1}^n (\varepsilon_i + \beta v_i)\left[
  \begin{array}{c}
    1 \\ z_i 
  \end{array}
\right] \rightarrow_d N\left(\mathbf{0}, \Sigma \right)
\]
where
\begin{align*}
  \Sigma &= 
  \mathbb{E}\left[
  \begin{array}{cc}
   \eta_i^2 & z_i \eta_i^2\\
   z_i \eta_i^2 & z_i^2 \eta_i^2
  \end{array}
\right]
=
  \mathbb{E}\left[
  \begin{array}{cc}
   \left( \varepsilon_i + \beta v_i  \right)^2 & 
   z_i \left( \varepsilon_i + \beta v_i  \right)^2\\ 
   z_i \left( \varepsilon_i + \beta v_i  \right)^2 &
   z_i^2\left( \varepsilon_i + \beta v_i  \right)^2  
  \end{array}
\right]\\
&=
  \mathbb{E}\left[
  \begin{array}{cc}
   \varepsilon_i^2 + 2\beta \varepsilon_i v_i + \beta^2 v_i^2  & 
 z_i^2\varepsilon_i + 2\beta z_i \varepsilon_i v_i + \beta^2 z_i v_i^2 \\
   z_i^2\varepsilon_i + 2\beta z_i \varepsilon_i v_i + \beta^2 z_i v_i^2 &
   z_i^2\varepsilon_i^2 + 2\beta z_i^2 \varepsilon_i v_i + \beta^2 z_i^2 v_i^2  
  \end{array}
\right]
\end{align*}
The next step is to work out the joint distribution of $v$ and the other primitives of our model:
\begin{align*}
  T^*=0, z = 0 &\implies 0 = \pi_0 + v \implies v = -\pi_0 = -p_0^*\\
  T^*=1, z = 0 &\implies 1 = \pi_0 + v \implies v = 1-\pi_0 = 1-p_0^*\\
  T^*=0, z = 1 &\implies 0 = \pi_0 + \pi_1 + v \implies v = -(\pi_0 + \pi_1) = -p_1^*\\
  T^*=1, z = 1 &\implies 1 = \pi_0 + \pi_1 + v \implies v = 1-(\pi_0 + \pi_1) = 1-p_1^*
\end{align*}
Thus,
\begin{align*}
  \mathbb{P}(T^*=0, z = 0) &= \mathbb{P}(v = -p_0^*) = (1-p_0^*)(1-q)\\
  \mathbb{P}(T^*=1, z = 0) &= \mathbb{P}(v = 1-p_0^*) = p_0^*(1-q)\\
  \mathbb{P}(T^*=0, z = 1) &= \mathbb{P}(v = -p_1^*) = (1-p_1^*)q\\
  \mathbb{P}(T^*=1, z = 1) &= \mathbb{P}(v = 1-p_1^*) = p_1^* q 
\end{align*}
Notice that, as must be true \emph{by construction}, $v$ is mean zero:
\begin{align*}
  \mathbb{E}[v] &= -p_0^*(1-p_0^*)(1 - q) + (1 - p_0^*)p_0^*(1-q) - p_1^*(1-p_1^*)q + (1-p_1^*)p_1^*q  \\
  &= (1-q)(1 - p_0^*) (p_0^* - p_0^*) + q(1-p_1^*)(p_1^* - p_1^*) = 0
\end{align*}
and uncorrelated with $v$:
\begin{align*}
  \mathbb{E}[zv] &= q\mathbb{E}[v|z=1] = q\left\{ p_1^*  \mathbb{E}[v|T^*=1,z=1] + (1 - p_1^*)\mathbb{E}[v|T^*=0,z=1]\right\} \\
  &= q \left[ p_1^* (1 - p_1^*) - p_1^* (1 - p_1^*)  \right] = 0
\end{align*}
Now, to calculate $\Sigma$, we need $\mathbb{E}[v^2], \mathbb{E}[\varepsilon v], \mathbb{E}[z\varepsilon v], \mathbb{E}[z^2\varepsilon v], \mathbb{E}[zv^2]$, and $\mathbb{E}[z^2v^2]$:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[v^2] &=p_0^{*2}(1-p_0^*)(1 - q) + (1 - p_0^*)^2 p_0^*(1-q) + p_1^{*2}(1-p_1^*)q + (1-p_1^*)^2p_1^*q  \\ 
  &= (1-q)p_0^*(1 - p_0^*)\left[p_0^* + (1 - p_0^*)  \right] + qp_1^*(1 - p_1^*)\left[ p_1^* + (1 - p_1^*) \right]\\
  &= (1-q)p_0^*(1 - p_0^*) + qp_1^*(1 - p_1^*)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[\varepsilon v] &= \mathbb{E}\left[ v \mathbb{E}\left( \varepsilon|v \right) \right]\\
  &= -p_0^*\mathbb{P}(v = -p_0^*)\mathbb{E}(\varepsilon|v = -p_0^* ) - p_1^* \mathbb{P}(v = -p_1^* )\mathbb{E}(\varepsilon|v = -p_1^*) \\
  & \qquad {} + (1 - p_0^*)\mathbb{P}(v = 1-p_0^*)\mathbb{E}(\varepsilon|v = 1-p_0^*) + (1 -p_1^*)\mathbb{P}(v = 1- p_1^*)\mathbb{E}(\varepsilon|v = 1-p_1^*) \\
   &= -p_0^*(1-p_0^*)(1-q)\mathbb{E}[\varepsilon|T^*=0,z=0] - p_1^* (1-p_1^*)q\mathbb{E}[\varepsilon|T^*=0,z=1]\\
   &\qquad {} + (1 - p_0^*) p_0^*(1-q)\mathbb{E}[\varepsilon|T^*=1,z=0] + (1 - p_1^*)p_1^*q \mathbb{E}[\varepsilon|T^*=1,z=1]\\
   &= -p_0^*(1-p_0^*)(1-q)(m^*_{00}-c) -p_1^* (1-p_1^*)q(m^*_{01} - c)\\
   &\qquad {} + (1-p_0^*)p_0^*(1-q)(m^*_{10}-c) + (1 - p_1^*)p_1^*q (m^*_{11}-c)\\
   &= (1-q)p_0^*(1-p_0^*)(m_{10}^* - m_{00}^*) + qp_1^*(1-p_1^*)(m_{11}^* - m_{01}^*)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[z\varepsilon v] &= \mathbb{E}[z^2\varepsilon v] = \mathbb{E}\left[ z^2 \mathbb{E}\left[ \varepsilon v|z \right] \right] = q \mathbb{E}\left[ \varepsilon v|z=1 \right] = q \mathbb{E}_{T^*|z=1}\left[ \mathbb{E}\left[ \varepsilon v |T^*=1,z=1 \right] \right]\\
  &= q\left\{p_1^* \mathbb{E}\left[ \varepsilon v \right|T^*=1,z=1] + (1 - p_1^*)\mathbb{E}[\varepsilon v|T^*=0,z=1]\right\}\\
  &= q\left\{p_1^*(1 - p_1^*) \mathbb{E}\left[ \varepsilon  \right|T^*=1,z=1] - p_1^* (1 - p_1^*)\mathbb{E}[\varepsilon |T^*=0,z=1]\right\}\\
  &= q p_1^*(1-p_1^*)\left[m^*_{11} - m^*_{01} \right]
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[z v^2] &= \mathbb{E}[z^2v^2] =  \mathbb{E}\left[ z^2 \mathbb{E}\left[ v^2|z \right]  \right] = q\mathbb{E}[v^2|z=1] = q \mathbb{E}_{T^*|z=1}\left[ \mathbb{E}\left[ v^2|T^*,z=1 \right] \right]\\
  &= q\left\{ p_1^* \mathbb{E}[v^2|T^*=1,z=1] + (1 - p_1^*) \mathbb{E}\left[ v^2|T^*=0,z=1 \right] \right\}\\
  &= q\left\{ p_1^*(1-p_1^*)^2 + p_1^{*2} (1 - p_1^*) \right\}\\
  &= qp_1^*(1 - p_1^*)\left[ (1-p_1^*)+ p_1^* \right]\\
  &= qp_1^*(1 - p_1^*)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Using these calculations, we find the elements of $\Sigma$ as follows:
\begin{align*}
  \mathbb{E}[\eta^2] &= \mathbb{E}[\varepsilon^2 + 2\beta \varepsilon v + \beta^2 v^2]  \\
  &= \sigma_\varepsilon^2 + 2 \beta \left[ (1-q)p_0^*(1-p_0^*)(m_{10}^* - m_{00}^*) + qp_1^*(1-p_1^*)(m_{11}^* - m_{01}^*)\right]\\
  &\qquad {} + \beta^2\left[(1-q)p_0^*(1 - p_0^*) + qp_1^*(1 - p_1^*)\right]\\
  &= \sigma_\varepsilon^2 + (1-q)p_0^*(1-p_0^*)\beta\left[ \beta + 2 \left( m_{10}^* - m_{00}^* \right) \right] + q p_1^*(1 - p_1^*)\beta\left[ \beta + 2\left( m_{11}^* - m_{01}^* \right) \right]\\
  \mathbb{E}[z^2 \eta^2] =  \mathbb{E}[z \eta^2] &= \mathbb{E}[ z \varepsilon^2 + 2\beta z \varepsilon v + \beta^2 z v^2 ] = \sigma_\varepsilon^2 + 2\beta \mathbb{E}[z \varepsilon v] + \beta^2 \mathbb{E}[zv^2]\\
  &= q\sigma_\varepsilon^2 + 2\beta q p_1^*(1-p_1^*)\left( m_{11}^* - m_{01}^* \right) + \beta^2 q p_1^*(1-p_1^*) \\
  &= q\left\{\sigma_\varepsilon^2 + p_1^*(1-p_1^*)\beta\left[ \beta + 2 \left( m_{11}^* - m_{01}^* \right) \right]\right\}
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Now, using the fact that $\mathbb{E}[z^2\eta] = \mathbb{E}[z\eta]$, the asymptotic variance of the reduced form estimator can be written as:
\begin{align*}
  \mbox{AVAR}\left[ \sqrt{n}\left( \widehat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}\right) \right] &= \frac{1}{q^2(1-q)^2}
  \left[
  \begin{array}{cc}
    q & -q \\ -q & 1
  \end{array}
\right]
\left[
\begin{array}{cc}
  \mathbb{E}(\eta^2) & \mathbb{E}(z\eta^2)\\
  \mathbb{E}(z\eta^2) & \mathbb{E}(z\eta^2)
\end{array}
\right]
\left[
\begin{array}{cc}
  q & -q \\ -q & 1
\end{array}
\right]\\
&= 
\frac{1}{q^2(1-q)^2}
\left[
\begin{array}{cc}
  q & -q \\ -q & 1
\end{array}
\right]
\left[
\begin{array}{cc}
  q\left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} &
  \mathbb{E}(z\eta^2) - q \mathbb{E}(\eta^2)\\
  0 & (1-q)\mathbb{E}(z\eta^2)
\end{array}
\right] \\ 
&= 
\frac{1}{q^2(1-q)^2}
\left[
\begin{array}{cc}
  q^2 \left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} & 
  -q^2 \left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} \\
  -q^2 \left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} &
  q^2 \mathbb{E}(\eta^2) + (1-2q) \mathbb{E}(z\eta^2)
\end{array}
\right]
\end{align*}
Now, we are only interested in the reduced form slope coefficient $\gamma_1$.
The asymptotic variance of its OLS estimator is:
\begin{align*}
  \mbox{AVAR}\left[ \sqrt{n}(\widehat{\gamma}_1 - \gamma_1) \right] &= 
  \frac{1}{q^2(1-q)^2}\left[q^2 \mathbb{E}(\eta^2) + (1-2q) \mathbb{E}(z\eta^2)\right]\\
  &= \frac{\mathbb{E}(\eta^2)}{(1-q)^2} + \left[ \frac{1-2q}{q^2(1-q)^2} \right]\mathbb{E}(z\eta^2)
\end{align*}
In general, this will lead to quite a complicated expression.
In our simulation design, however, 
\begin{align*}
  q &= 1/2\\ 
  p^*_0(1-p^*_0) &= \delta(1-\delta)\\
  p^*_1(1-p^*_1) &= (1 -\delta) \delta\\
  (m^*_{10} - m^*_{00}) &= (m^*_{11} - m_{01}^*) > 0 
\end{align*}
leading to the following simplifications:
\small
\begin{align*}
  \mbox{AVAR}\left[ \sqrt{n}(\widehat{\gamma}_1 - \gamma_1) \right] &= 
  4 \mathbb{E}(\eta^2)\\
  &= 4\left\{ \sigma_\varepsilon^2 + 1/2 \times \delta(1 - \delta)\beta\left[ \beta + 2 \left( m_{10}^* - m_{00}^* \right) \right] + 1/2 \times \delta(1 - \delta)\beta\left[ \beta + 2\left( m_{11}^* - m_{01}^* \right) \right]\right\}\\
  &= 4\left\{ \sigma_\varepsilon^2 + \delta(1 - \delta)\beta\left[ \beta + 2\left( m_{11}^* - m_{01}^* \right) \right]\right\}
\end{align*}
\normalsize
Notice that, since $(m_{11}^* - m_{01}^*)$ is positive, as in the simulation design, the asymptotic variance is \emph{smallest} when $\beta = 0$ so that there is no treatment effect.

\paragraph{Robust Standard Errors} 
When implementing the reduced form estimator in practice we base our inference on sample residuals:
\[
  \widehat{\eta}_i = y_i - \widehat{\gamma}_0 - \widehat{\gamma}_1 z_i
\]
As we saw above, however, the errors $\eta$ are heteroskedastic: $\mathbb{E}(z\eta^2) \neq \mathbb{E}(z)\mathbb{E}(\eta^2)$ etc.
For this reason, we must use robust standard errors:
\[
  \widehat{\Sigma} = \frac{1}{n} \sum_{i=1}^n \left[
  \begin{array}{cc}
    \widehat{\eta}_i^2 & z_i \widehat{\eta}_i^2\\
    z_i \widehat{\eta}_i^2 & z_i^2 \widehat{\eta}_i^2\\
  \end{array}
\right]
\]
leading to
\[
  \widehat{\mbox{AVAR}}\left[ \sqrt{n}(\widehat{\gamma}_1 - \gamma_1) \right] = \frac{\widehat{\mathbb{E}}(\eta^2)}{(1-\widehat{q})^2} + \left[ \frac{1-2\widehat{q}}{\widehat{q}^2(1-\widehat{q})^2} \right]\widehat{\mathbb{E}}(z\eta^2)
\]
Note that $\widehat{q}$ is fixed and equal to $1/2$ in our simulation design, so this becomes $4 \widehat{\sigma}^2_{\eta}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Values of $m^*_{tk}$ in the Simulation}
To calculate $\Sigma$ in our simulation design, we'll need to know the values of $m_{tk}^*$ in the threshold-crossing model with bivariate normal errors:
\begin{align*}
  \left[
  \begin{array}{c}
    \varepsilon \\ \xi
  \end{array}
  \right] &\sim N\left( \left[
  \begin{array}{c}
    0 \\ 0
  \end{array}
\right], \left[
\begin{array}{cc}
  1 & \rho \\
  \rho & 1
\end{array}
\right]\right)\\ \\
  T^* &= \mathbf{1}\left\{ \kappa_0 + \kappa_1 z + \xi > 0 \right\}\\
  \kappa_0 &= \Phi^{-1}(\delta)\\
  \kappa_1 &= \Phi^{-1}(1-\delta) - \Phi^{-1}(\delta)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  m_{00}^* - c &= \mathbb{E}[\varepsilon|T^*=0,z=0] = \mathbb{E}[\varepsilon|\xi \leq -\kappa_0]\\
  m_{01}^* - c &= \mathbb{E}[\varepsilon|T^*=0,z=1] = \mathbb{E}[\varepsilon|\xi\leq -(\kappa_0 + \kappa_1)]\\
  m_{10}^* - c &= \mathbb{E}[\varepsilon|T^*=1,z=0] = \mathbb[\varepsilon|\xi > -\kappa_0]\\
  m_{11}^* - c &= \mathbb{E}[\varepsilon|T^*=1,z=1] = \mathbb{E}[\varepsilon|\xi > -(\kappa_0 + \kappa_1)]
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
  \xi|\varepsilon \sim N(\rho \varepsilon, 1 - \rho^2)
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
\mathbb{P}(\varepsilon \leq x|\xi \leq a) &= \frac{\mathbb{P}(\varepsilon \leq x,\xi \leq a)}{\mathbb{P}(\xi \leq a)} = \frac{\int_{-\infty}^x \int_{-\infty}^a f(\xi|\varepsilon)f(\varepsilon) d \xi \, d \varepsilon }{\Phi(a)}\\
&=\frac{\int_{-\infty}^x F_{\xi|\varepsilon}(a)f(\varepsilon) d\varepsilon}{\Phi(a)} = \frac{\displaystyle\int_{-\infty}^x \varphi(\varepsilon)\Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)d\varepsilon }{\Phi(a)} \\ \\
  f(\varepsilon|\xi\leq a) &= \frac{\varphi(\varepsilon)}{\Phi(a)}\Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right) \\ \\
  \mathbb{E}\left[ \varepsilon|\xi \leq a \right] & = \frac{1}{\Phi(a)} \int_{-\infty}^\infty \varepsilon \varphi(\varepsilon) \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right) d\varepsilon
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
\mathbb{P}(\varepsilon \leq x|\xi > a) &= \frac{\mathbb{P}(\varepsilon \leq x,\xi > a)}{\mathbb{P}(\xi > a)} = \frac{\int_{-\infty}^x \int_a^\infty f(\xi|\varepsilon)f(\varepsilon) d \xi \, d \varepsilon }{1 - \Phi(a)}\\
&=\frac{\int_{-\infty}^x [1 - F_{\xi|\varepsilon}(a)]f(\varepsilon) d\varepsilon}{1- \Phi(a)} = \frac{\displaystyle\int_{-\infty}^x \varphi(\varepsilon)\left[1 - \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)\right]d\varepsilon }{1 - \Phi(a)} \\ \\
f(\varepsilon|\xi > a) &= \frac{\varphi(\varepsilon)}{1-\Phi(a)}\left[1 - \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)\right] \\ \\
\mathbb{E}\left[ \varepsilon|\xi > a \right] & = \frac{1}{1-\Phi(a)} \int_{-\infty}^\infty \varepsilon \varphi(\varepsilon)\left[1 -  \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)\right] d\varepsilon
\end{align*}

\todo[inline]{I've checked all of these integrals in R and they're definitely correct.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{IV Estimator}
First we calculate the probability limits of the IV estimators of $\beta$ and $c$.
Notice that \emph{both} are inconsistent:
\begin{align*}
  \widehat{\beta}_{IV} &= \left( \widetilde{Z}\widetilde{T}/n \right)^{-1}\left(\widetilde{Z}'\mathbf{y}/n\right) =  \left( \widetilde{Z}\widetilde{T}/n \right)^{-1}\widetilde{Z}' \left( \widetilde{T}^* \widetilde{\beta} + \boldsymbol{\varepsilon} \right)/n\\
&= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \left[\widetilde{Z}'\widetilde{T}^*/n \right] \widetilde{\beta} + \widetilde{Z}'\boldsymbol{\varepsilon}/n \right) \\ \\
  \widetilde{Z}'\boldsymbol{\varepsilon}/n &\rightarrow_p \mathbb{E}\left[
  \begin{array}{c}
    \varepsilon \\ z \varepsilon
  \end{array}
\right] = \mathbf{0}\\ \\ 
\left(\widetilde{Z}'\widetilde{T}/n\right)^{-1} &= \left[
\begin{array}{cc}
  1 & \bar{T} \\ \bar{z} & \mathbf{z}'\mathbf{T}/n
\end{array}
\right] \rightarrow_p \left[
\begin{array}{cc}
  1 & p \\ q & p_1 q
\end{array}
\right] = \frac{1}{q(p_1 - p)}\left[
\begin{array}{cc}
  p_1 q & -p \\ -q & 1
\end{array}
\right] \\ \\ 
\widetilde{Z}'\widetilde{T}^*/n &= \left[
\begin{array}{cc}
  1 & \bar{T^*} \\ \bar{z} & \mathbf{z}'\mathbf{T}^*/n
\end{array}
\right] \rightarrow_p \left[
\begin{array}{cc}
  1 & p^* \\ q & p_1^* q
\end{array}
\right] \\ \\ 
\left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\widetilde{T}^*/n \right) &\rightarrow_p \frac{1}{q(p_1 -p)}\left[
\begin{array}{cc}
  qp_1 & - p\\
  -q & 1 
\end{array}
\right]
\left[
\begin{array}{cc}
  1 & p^*\\
  q & qp_1^*
\end{array}
\right]= 
\left[
\begin{array}{cc}
  1 & (p_1 p^* - p_1^* p)/(p_1 - p)\\
  0 & (p_1^* - p^*) / (p_1 - p)
\end{array}
\right] \\ \\ 
\frac{p_1^* - p^*}{p_1 - p} &= \frac{1}{p_1 - p}\left( \frac{p_1 - \alpha_0}{1 - \alpha_0 - \alpha_1} - \frac{p - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) = \frac{1}{1 - \alpha_0 - \alpha_1}\\ \\
\frac{p_1 p^* - p_1^*p}{p_1 - p} &= \frac{1}{p_1 - p}\left[ \frac{p_1(p - \alpha_0)}{1 - \alpha_0 - \alpha_1} -  \frac{p(p_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] = \frac{-\alpha_0}{1 - \alpha_0 - \alpha_1}\\ \\
\boldsymbol{\beta}_{IV} &\rightarrow_p 
\left[
\begin{array}{cc}
  1 & (p_1 p^* - p_1^* p)/(p_1 - p)\\
  0 & (p_1^* - p^*) / (p_1 - p)
\end{array}
\right]\left[
\begin{array}{c}
  c \\ \beta
\end{array}
\right] = \frac{1}{1 - \alpha_0 - \alpha_1}\left[
\begin{array}{c} 
  c - \alpha_0 \beta \\
  \beta
\end{array}
\right] \equiv
\left[
\begin{array}{c}
  c_{IV}\\ \beta_{IV}
\end{array}
\right]
\end{align*}
Now that we have the probability limit of the IV estimator, we can work out the error term $\omega$ that corresponds to it as a function of the ``primitives'' of our model.
We have:
\begin{align*}
  \zeta &= y - (c_{IV} + \beta_{IV} T) = (c + \beta T^* + \varepsilon) - \frac{1}{1 - \alpha_0 - \alpha_1}\left[ \left(c - \alpha_0 \beta\right) + \beta T \right] \\
  &= \varepsilon + c - \left(\frac{c - \alpha_0 \beta}{1 - \alpha_0 - \alpha_1} \right) + \beta\left( T^* - \frac{T}{1 - \alpha_0 - \alpha_1} \right)\\
  &= \varepsilon + \left[\frac{\alpha_0 (\beta - c) - \alpha_1 c}{1 - \alpha_0 - \alpha_1} \right] + \beta\left[ \frac{(T^* - T) - (\alpha_0 + \alpha_1)T^*}{1 - \alpha_0 - \alpha_1} \right]\\
  &= \varepsilon + \left[\frac{\alpha_0 (\beta - c) - \alpha_1 c}{1 - \alpha_0 - \alpha_1} \right] + \beta\left[ \frac{w - (\alpha_0 + \alpha_1)(\pi_0 + \pi_1 z + v)}{1 - \alpha_0 - \alpha_1} \right]
\end{align*}
\todo[inline]{This looks pretty complicated. I'm also not sure we need to work this out analytically. All we really need is to write out the joint limit distribution of the IV and RF\ldots}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Joint Distribution of IV and RF}
To carry out inference for the identified set, we need to work out the joint distribution of the reduced form and IV estimators:
\begin{align*}
  \widehat{\boldsymbol{\beta}}_{IV} &= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\mathbf{y}/n \right)\\
  \widehat{\boldsymbol{\gamma}} &= \left( \widetilde{Z}'\widetilde{Z} \right)^{-1}\left( \widetilde{Z}'\boldsymbol{y}/n \right)
\end{align*}
The probability limits of each estimator define an associated error term, each of which depends in a complicated way on our ``primitive'' model parameters:
\begin{align*}
  y &= c_{IV} + \beta_{IV} T + \zeta \\
  y &= \gamma_0 + \gamma_1 z + \eta 
\end{align*}
But because these two estimators depend only on observable quantities, we can use their residuals to work out the covariance matrix of $(\zeta, \eta)$. 
Accordingly, we proceed as follows:
\begin{align*}
  \widehat{\boldsymbol{\beta}}_{IV} &= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\left[ \widetilde{T}\boldsymbol{\beta}_{IV} + \boldsymbol{\zeta} \right]/n \right) = \boldsymbol{\beta}_{IV} +\left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\boldsymbol{\zeta}/n\right) \\
  \widehat{\boldsymbol{\gamma}} &= \left( \widetilde{Z}'\widetilde{Z} \right)^{-1}\left( \widetilde{Z}'\left[ \widetilde{Z}\boldsymbol{\gamma} + \boldsymbol{\eta} \right]/n \right) = \boldsymbol{\gamma} + \left( \widetilde{Z}'\widetilde{Z}/n \right)^{-1}\left( \widetilde{Z}\boldsymbol{\eta}/n \right)
\end{align*}
yielding
\begin{align*}
  \sqrt{n}\left(\widehat{\boldsymbol{\beta}}_{IV} - \boldsymbol{\beta}_{IV}\right) &= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\boldsymbol{\zeta}/\sqrt{n}\right) \\
  \sqrt{n}\left(\widehat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}\right) &=  \left( \widetilde{Z}'\widetilde{Z}/n \right)^{-1}\left( \widetilde{Z}'\boldsymbol{\eta}/\sqrt{n} \right)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other Stuff\ldots}

\begin{itemize}
  \item Manski was interested in a heterogenous treatment effect model and whether we could bound \emph{ATE} rather than \emph{LATE}.
  \item Would it help in the performance of the GMM estimator if we enforced the bounds on $\alpha_0$ and $\alpha_1$?
\end{itemize}



\paragraph{Exogenous Covariates in a Linear Model:} These should be very easy to handle because we can just stack the GMM moment conditions to include an IV estimator for the parameter on the exogenous covariates in the main equation.
Recall that the usual IV estimator for these parameters is unaffected by measurement error.
We should write this out since it's the case that many people will use in practice given the extreme sample size demands of fully non-parametric estimation!

\paragraph{More About Weak Identification:}
Sophocles pointed out in an email exchange that I had been assuming (incorrectly) that $\mbox{Cov}(y,z)$ is always well-behaved.
This is not the case if $z$ is a weak instrument.
I don't think we can simply assume we have a strong instrument and consider the weak identification that arises from $\beta \approx 0$ in isolation.
I think the two problems of $\beta \approx 0$ and weak $z$ interact in an important way since, as we saw from above, the determinant $|G_\theta|$ that measures the strength of identification depends on the \emph{product} of $\beta$ and $Cov(z,T)$.
I think it the correct interpretation of this is that the magnitude of $\beta$ that gives strong identification should is always relative to the strength of $z$.
If $z$ is very strong, then $\beta$ can be smaller without causing problems.
But if $z$ is weak then I think $\beta$ needs to be really large to get strong identification.
If I recall correctly, we uncovered something in our simulations that appears to agree with this intuition but I need to go back and check.

To see why $\mbox{Cov}(y,z)$ is badly behaved when $z$ is weak, write out an explicit first-stage equation for our model as follows:
\[
T^* = \pi_0 + \pi_1 z + v
\]
where
\begin{align*}
\pi_0 &= \mathbb{E}[T^*|z=0] = p^*_0\\
\pi_1 &= \mathbb{E}[T^*=1|z=1] - \mathbb{E}[T^*|z=0] = p^*_1 - p^*_0
\end{align*}
and $\mathbb{E}[zv]=0$ by construction.
Now,
\begin{align*}
  \mbox{Cov}(z,y) &= \mathbb{E}(zy) - \mathbb{E}(z)\mathbb{E}(y)\\
  &=\mathbb{E}\left[z\left( c + \beta T^* + \varepsilon \right) \right] - q \mathbb{E}\left( c + \beta T^* + \varepsilon \right)\\
  &=\mathbb{E}\left[z\left\{ c + \beta \left( \pi_0 + \pi_1 z + v \right) + \varepsilon \right\} \right] - q \mathbb{E}\left[ c + \beta \left( \pi_0 + \pi_1 z + v \right) + \varepsilon \right]\\
  &= q (c + \beta \pi_0) + \beta \pi_1 \mathbb{E}(z^2) - q\left( c + \beta \pi_0 + \beta \pi_1 \mathbb{E}[z] \right) \\
  &= q (c + \beta \pi_0 + \beta \pi_1)  - q\left( c + \beta \pi_0 + \beta \pi_1 q \right) \\
  &= q(\beta \pi_1  - \beta \pi_1 q) = \beta \pi_1 q(1 - q)\\
  &= \beta (p^*_1 - p^*_0) q(1-q)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Auxiliary Moment Inequalities}
Notice that if $\beta=0$, then the preceding moment equalities do \emph{not} identify $\alpha_1$.
However, we do have auxiliary moment \emph{inequalities} that partially identify $\alpha_1$ regardless of the value of $\beta$.
The simplest of these comes from the relationship  
\[
  p_k^* = \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1}, \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1}
\]
where $p_k = P(T=1|z_k)$ and $p_k^* = P(T^*=1|z_k)$.
(This follows from the Law of Total Probability and our assumption that the mis-classification probabilities rates depend only on $T^*$, not $z$.)
Under our assumption that $\alpha_0 + \alpha_1 < 1$, we obtain $\alpha_0 < \min_k p_k$ and $\alpha_1 < \min_k (1 - p_k)$.
If $\alpha_0 = 0$, as we assume in the present special case, then without any assumption on the true value of $\alpha_1$ we have 
\[0 \leq \alpha_1 < \min_k (1 - p_k) = 1 - \max_k p_k.\] 
Is there some way to use these moment inequalities in estimation?



\paragraph{Under Normality}
  In our simulation for the CDF bounds on $\alpha_0$ and $\alpha_1$, we found that the upper bounds were in fact equal to the true parameter values.
  This is very surprising and is very likely comes from the specific parametric model from which we simulated.
  This happens to have been a model with normally distributed errors.
  Can we say anything about such a model theoretically?
  Perhaps try to write down the likelihood function?
  This could also be a useful way to look at the weak identification problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{April 2017 -- New GMM Formulation}
  Consider the special case in which $\alpha_0 = 0$ so the model is identified from
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 -\alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \left(\frac{\beta}{1 - \alpha_1}\right)\left[2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z) \right] &= 0
\end{align*}
Above we wrote this in a standard GMM form by adding auxiliary moment equations to identify $\mathbb{E}[y^2]$, $\mathbb{E}[yT]$, etc.\
But there's a simpler and more transparent way to do this.
Under our assumptions and $\alpha_0 = 0$, some algebra shows that 
\todo[inline]{Add the algebra from the whiteboard notes later}
\[
  \mathbb{E}\left[ y - \frac{\beta}{1 - \alpha_1}T \right] = c
\]
and that
\[
  \mathbb{E}\left[ y^2 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha} T \right] = \sigma_{\varepsilon\varepsilon} + c^2
\]
where $c$ is the intercept from the regression model and $\sigma_{\varepsilon\varepsilon} = \mbox{Var}(\varepsilon)$.
Now, re-writing the first covariance equation using the linearity of expectation,
\begin{align*}
  \mathbb{E}\left[ yz - \frac{\beta}{1 - \alpha_1} Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ y - \frac{\beta}{1 - \alpha} T\right] &=  0\\
  \mathbb{E}\left[ yz - \frac{\beta}{1 - \alpha_1} Tz \right] - \mathbb{E}[z] c&=  0\\
  \mathbb{E}\left[ \left\{y - c -  \frac{\beta}{1 - \alpha_1} T\right\}z \right] &=  0
\end{align*}
and proceeding similarly for the second covariance equation
\begin{align*}
  \mathbb{E}\left[ y^2z - \frac{\beta}{1 - \alpha_1} 2yTz + \frac{\beta^2}{1 - \alpha_1} Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ y - \frac{\beta}{1 - \alpha_1}yT + \frac{\beta^2}{1 - \alpha_1} T \right] &= 0\\
  \mathbb{E}\left[ y^2z - \frac{\beta}{1 - \alpha_1} 2yTz + \frac{\beta^2}{1 - \alpha_1} Tz \right] - \mathbb{E}[z]\left( \sigma_{\varepsilon\varepsilon} + c^2 \right) &= 0\\
  \mathbb{E}\left[ \left\{y^2 - \sigma_{\varepsilon\varepsilon} - c^2 -\frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right\}z \right] = 0 
\end{align*}
Thus, we can express our estimator in terms of the following four moment equations:
\begin{align*}
  \mathbb{E}\left[ y - c -  \frac{\beta}{1 - \alpha_1} T \right] &= 0\\
  \mathbb{E}\left[ y^2 - \sigma_{\varepsilon\varepsilon} - c^2 -\frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right] &= 0 \\
  \mathbb{E}\left[ \left\{y - c -  \frac{\beta}{1 - \alpha_1} T\right\}z \right] &= 0\\
  \mathbb{E}\left[ \left\{y^2 - \sigma_{\varepsilon\varepsilon} - c^2 -\frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right\}z \right] &= 0 
\end{align*}
To simplify the notation, let $\boldsymbol{\theta} = (\alpha_1, \beta, c, \sigma_{\varepsilon\varepsilon})'$ and define
\begin{align*}
  u(\boldsymbol{\theta}) &= y - c - \frac{\beta}{1 - \alpha_1} T\\
  v(\boldsymbol{\theta}) &= y^2 - \sigma_{\varepsilon\varepsilon} - c^2 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1}
\end{align*}
Then we can express the four moment equalities as 
\[
  \mathbb{E}\left[ g_1(\mathbf{x}, \boldsymbol{\theta}) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    u(\boldsymbol{\theta})\\ v(\boldsymbol{\theta})
  \end{array}
\right] = \mathbf{0}, \quad
\mathbb{E}\left[ g_2(\mathbf{x}, \boldsymbol{\theta}) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    u(\boldsymbol{\theta}) z\\ v(\boldsymbol{\theta}) z
  \end{array}
\right] = \mathbf{0}
\]
We also have two moment inequalities, namely $\alpha_1 \leq 1 - p_1$ and $\alpha_1 \leq 1 - p_0$.
After some algebra (see the whiteboard), we can show that these are equivalent to
\[
  \mathbb{E}\left[ h(\mathbf{x},\theta) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    (1 - \alpha_1) - T(1 - z)/(1 - q)\\
    (1 - \alpha_1) - Tz/q
  \end{array}
\right] \geq \mathbf{0}
\]
where $q = \mathbb{E}[z]$.
We will \emph{condition} on $z$, i.e.\ hold it fixed in repeated samples, so we will not add an extra moment condition for $q$.
Instead we will simply substitute the sample analogue.

To formulate the GMM estimator with moment inequalities as in Moon and Schorfheide (2009), we introduce some further notation.
Let $\boldsymbol{\lambda}$ denote the slack in $h(\mathbf{x}, \theta)$, so that
\[
  \mathbb{E}[h(\mathbf{x},\theta)] = \boldsymbol{\lambda} \geq \mathbf{0} \iff \mathbb{E}[h(\mathbf{x}, \theta) - \boldsymbol{\lambda}] = \mathbf{0}
\]
Further define
\[
  f(\mathbf{x}) = \left[
  \begin{array}{c}
    g(\mathbf{x},\theta) \\ h(\mathbf{x},\boldsymbol{\theta})
  \end{array}
\right], \quad
\psi(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda}) = 
\left[
\begin{array}{c}
  g(\mathbf{x}, \boldsymbol{\theta})\\
  h(\mathbf{x}, \boldsymbol{\theta}) - \boldsymbol{\lambda}
\end{array}
\right]
\]
and let $\Theta = \left\{ \boldsymbol{\theta}\colon \alpha_1 \geq 0, \sigma_{\varepsilon\varepsilon}\geq 0 \right\}$.
Then, the GMM estimator based on our moment equalities and inequalities is
\[
  (\widehat{\boldsymbol{\theta}}, \widehat{\boldsymbol{\lambda}}) = \underset{\boldsymbol{\theta} \in \Theta, \boldsymbol{\lambda} \geq \mathbf{0}}{\arg\min} \quad Q_n(\boldsymbol{\theta}, \boldsymbol{\lambda})
\]
where
\[ 
  Q_n(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \frac{1}{2}\bar{\psi}_n(\mathbf{x},\boldsymbol{\theta}, \boldsymbol{\lambda})' \mathbf{W}_n \bar{\psi}_n(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda})
\]
\[
  \bar{\psi}_n(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \frac{1}{n}\sum_{i=1}^n \psi(\mathbf{x}_i, \boldsymbol{\theta}, \boldsymbol{\lambda}) 
\]
and $\mathbf{W}_n$ is a weighting matrix that should be irrelevant in our case because we're just identified.
We scale the criterion by $1/2$ so that the derivative looks nice.
\todo[inline]{Is this still the case with the moment inequalities? Mechanically we have introduced two new parameters to match the two extra conditions.}

At various points we will need the derivatives of the moment equations.
Let
\[
  \mathbf{G}(\boldsymbol{\theta}) = \mathbb{E}[\nabla_{\boldsymbol{\theta}'} g(\mathbf{x},\boldsymbol{\theta})], \quad \mathbf{H}(\boldsymbol{\theta}) = \mathbb{E}[\nabla_{\boldsymbol{\theta}'} h(\mathbf{x},\boldsymbol{\theta})]
\]
and define 
\begin{align*}
  \boldsymbol{\Psi}(\boldsymbol{\theta}, \boldsymbol{\lambda}) &= \mathbb{E}
  \left[
  \begin{array}{cc}
    \nabla_{\boldsymbol{\theta}'} \psi(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda}) &
    \nabla_{\boldsymbol{\lambda}'} \psi(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda}) 
  \end{array}
\right]\\
&= \mathbb{E}\left[
\begin{array}{cc}
 fill & this \\
 in & later
\end{array}
\right]
\end{align*}

\section{April 1--15, 2017 -- Andrews \& Soares (2010)}
Our moment equalities from above do not identify $\alpha$ when $\beta=0$.
More generally, the estimator based on them performs poorly when $\beta$ is relatively small compared to the error variance.
Continue to assume that $\alpha_0 = 0$ so the moment conditions simplify.

We now consider an inference procedure following Andrews \& Soares.
The basic idea is to ``isolate'' the problematic parameters, in our case $\alpha$ and $\beta$, and carry out joint inference for these using the Anderson-Rubin test statistic.
This is constructed by substituting a null hypothesis $H_0\colon \theta = \theta_0$ into the sample analogue of the GMM moment conditions and relying on the fact that this sample analogue remains ``well-behaved'' even in situations where inference for the GMM \emph{parameter estimator} breaks down.
Examples include parameters on the boundary, and parameters that may not be identified, e.g.\ $\alpha_1$ if $\beta = 0$

\subsection{Simple Example: $\alpha_0 = 0$, $c=0$, and $\sigma_{\varepsilon\varepsilon}=1$}
In our problem $c$ and $\sigma_{\varepsilon\varepsilon}$ are essentially nuisance parameters.
Fortunately, they are \emph{always identified} from our moment conditions regardless of the values of $\beta$ and $\alpha$, as we will discuss further below.
For the moment, we will suppose that $c$ is known to equal zero and $\varepsilon_{\varepsilon\varepsilon}$ is known to equal one as is the case in our baseline simulation.
Later we will estimate them which requires only a small modification of the procedure we know outline.
With the simplifications $\alpha_0, c=0, \sigma_{\varepsilon\varepsilon}=1$ the equality moment conditions become
\begin{align*}
  \mathbb{E}\left[ \left\{y - \frac{\beta}{1 - \alpha_1} T\right\}z \right] &= 0\\
  \mathbb{E}\left[ \left\{y^2 - 1 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right\}z \right] &= 0 
\end{align*}
since we no longer need the $g_1$ block of moment conditions to identify the ``intercepts'' $c$ and $\sigma_{\varepsilon\varepsilon}$.
For this simplified set of moment conditions, our parameter vector is $\boldsymbol{\theta} = (\alpha_1, \beta)'$ and the residuals are given by
\begin{align*}
  u(\boldsymbol{\theta}) &= y - \frac{\beta}{1 - \alpha_1} T\\
  v(\boldsymbol{\theta}) &= y^2 - 1 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1}
\end{align*}
and we can write the equality moment conditions as
\[
\mathbb{E}\left[ g(\mathbf{x}, \boldsymbol{\theta}) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    u(\boldsymbol{\theta}) z\\ v(\boldsymbol{\theta}) z
  \end{array}
\right] = \mathbf{0}
\]
The inequality moment conditions are unchanged from above, namely
\[
  \mathbb{E}\left[ h(\mathbf{x},\theta) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    (1 - \alpha_1) - T(1 - z)/(1 - q)\\
    (1 - \alpha_1) - Tz/q
  \end{array}
\right] \geq \mathbf{0}
\]
where $q = \mathbb{E}[z]$.
As above we will \emph{condition} on $z$, i.e.\ hold it fixed in repeated samples, so we will not add an extra moment condition for $q$.
Instead we will simply substitute the sample analogue.
Note that this means we should hold $q$ \emph{fixed} when bootstrapping below.
We now introduce some notation from Andrews and Soares (2010).

\paragraph{Population Moment Conditions}
\[
  \mathbb{E} \left[ m_j(\mathbf{w}_i,\theta_0) \right]
  \left\{
  \begin{array}{cc}
    \geq 0 & \mbox{for } j = 1, \cdots, p\\
    = 0 & \mbox{for } j = p + 1, \cdots,k \mbox{ where } k = p + v
  \end{array}
  \right.
\]
where $p$ is the number of inequality moment conditions (in our case $p = 2$), $v$ is the number of equality moment conditions (in our case $v = 2$), $\theta_0$ is the true parameter vector, and $\mathbf{w}_i$ is the vector of observations for individual $i$ (in our case $\mathbf{w}_i = (T_i, z_i, y_i)$).

\paragraph{Sample Moment Functions, etc.}
\[
  \bar{m}_n(\theta) = \left[
  \begin{array}{c}
    \bar{m}_{n,1}(\theta)\\
    \vdots \\
    \bar{m}_{n,k}(\theta)\\
  \end{array}
\right], \quad
\bar{m}_{n,j} = \frac{1}{n} \sum_{i=1}^{n} m_j(\mathbf{w}_i, \theta) \mbox{ for } j = 1, \cdots, k
\]
Now, let $\Sigma(\theta_0)$ denote the asymptotic variance of $\sqrt{n}\; \bar{m}_n(\theta)$.
We estimate this quantity using $\widehat{\Sigma}_n(\theta)$.
For iid observations, as in our example, the estimator is
\[
  \widehat{\Sigma}(\theta) = \frac{1}{n} \sum_{i=1}^n \left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]\left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]', \quad 
  m(\mathbf{w}_i, \theta) = \left[
  \begin{array}{c}
    m_1(\mathbf{w}_i, \theta)\\
    \vdots \\
    m_k(\mathbf{w}_i, \theta)\\
  \end{array}
\right]
\]

\paragraph{Test Statistic}
The test statistic takes the form $T_n(\theta) = S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right)$ for some real-valued function $S$.
The example we will use is $S_1$, defined by
\[
  S_1(m, \Sigma) = \sum_{j=1}^p [m_j/\sigma_j]^2_- + \sum_{j=p+1}^{p+v} (m_j/\sigma_j)^2
\]
where $m = (m_1, \cdots, m_k)'$,
\[
  [x]_- = \left\{
  \begin{array}{cc}
    x, & \mbox{if } x <0\\
    0, & \mbox{if } x \geq 0
  \end{array}
\right.
\]
and $\sigma_j^2$ is the $j$th diagonal element of $\Sigma$.
Notice that $S_1$ only gives weight to inequality moment conditions that are \emph{violated}.

\paragraph{Basic Idea of the Test}
Let's return to our specific example for a moment.
The idea is essentially to plug a null hypothesis $\theta^* = (\alpha_1^*, \beta^*)'$ into the sample analogue:
\[
  \sqrt{n}\; \bar{m}_n(\alpha_1^*, \beta^*) = \frac{1}{\sqrt{n}}\sum_{i=1}^n
  \left[
  \begin{array}{r}
    (1 - \alpha_1^*) - T_i (1 - z_i)/(1-q)\\ \\
    (1 - \alpha_1^*) - T_i z_i/q\\ \\
    \left(y_i - \displaystyle\frac{\beta^*}{1 - \alpha_1^*}T_i\right) z_i \\ \\
    \left( y_i^2 - 1 - \displaystyle \frac{\beta^*}{1 - \alpha_1^*} 2y_i T_i + \displaystyle\frac{\beta^{*2}}{1 - \alpha_1^*}T_i\right)z_i 
  \end{array}
\right]
\]
and see if the result is ``large'' after standardizing and squaring the individual elements.
We only give weight to an inequality if it is violated.
The variance matrix of the sample analogue is calculated \emph{under the null}, i.e.\ assuming that $\theta = \theta^*$.
Note that we also use the \emph{centered variance matrix estimator}.

We reject the null if the test statistic is too large.
This gives us \emph{joint inference} for $\alpha$ and $\beta$ \emph{simultaneously}.
To construct a joint confidence region, we need to test pairs $(\alpha_1, \beta)$.
Of course, we restrict $\alpha_1$ to lie in $[0, 1)$.
The resulting confidence region \emph{need not be convex}.
In fact it could even be disconnected!
However, in our particular example, it might be possible to prove that one gets a connected or even convex region.
This is something we should think about since it would reduce the computational burden substantially.
To get \emph{marginal} inference, say for $\beta$ only, one projects the joint confidence set.
This is necessarily conservative, but may not be too bad in practice. 
We'll have to see\ldots

A particularly salient null hypothesis is $\beta = 0$.
Imposing this yields
\[
  \sqrt{n}\; \bar{m}_n(\alpha_1^*, 0) = \frac{1}{\sqrt{n}}\sum_{i=1}^n
  \left[
  \begin{array}{r}
    (1 - \alpha_1^*) - T_i (1 - z_i)/(1-q)\\ 
    (1 - \alpha_1^*) - T_i z_i/q\\ 
    y_i z_i \\ 
    \left( y_i^2 - 1 \right)z_i 
  \end{array}
\right]
\]
We see that this function depends on $\alpha_1^*$ \emph{only via the moment inequalities}.
What is more, the test statistic based on $S_1$ does \emph{not} depend on $\alpha_1^*$ unless the inequality constraints are violated.

\paragraph{Calculating the Critical Value}
The test statistic we will use to test $\theta = \theta^*$ is fairly simple to compute: we simply substitute into the GMM sample analogue.
The critical value for the test, however, is much more complicated.
Following Andrews \& Soares (2010), we use the following bootstrap procedure.
First we define some additional notation.
All of the test statistics considered in Andrews \& Soares (2010) satisfy
\[
  T_n = S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right) = S\left( \widehat{D}^{-1/2}(\theta) \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Omega}_n(\theta) \right)
\]
where
\[
  \widehat{D}_n(\theta) = \mbox{diag}\left( \widehat{\Sigma}\left( \theta \right) \right), \quad \widehat{\Omega}_n(\theta) = \widehat{D}_n^{-1/2}(\theta)\, \widehat{\Sigma}(\theta)\, \widehat{D}_n^{-1/2}(\theta)
\]
Now, let $\left\{ \mathbf{w}_i^* \right\}_{i=1}^n$ denote a bootstrap sample and define the associated bootstrap quantities
\begin{align*}
  M_n^*(\theta) &=  \sqrt{n} \left( \widehat{D}^*\left(\theta\right) \right)^{-1/2} \left( \bar{m}^*_n\left( \theta \right) - \bar{m}_n\left( \theta) \right) \right)\\
  \widehat{\Omega}^*(\theta) &= \left( \widehat{D}^*\left( \theta \right) \right)^{-1/2} \widehat{\Sigma}_n^*(\theta)\left( \widehat{D}^*\left( \theta \right) \right)^{-1/2} \\
  \widehat{D}^*\left( \theta \right) &= \mbox{diag}\left( \widehat{\Sigma}_n^*\left( \theta \right) \right)\\
  m_n^*(\theta) &= \frac{1}{n} \sum_{i=1}^n m(\mathbf{w}_i^*,\theta)\\
  \widehat{\Sigma}(\theta)^* &= \frac{1}{n} \sum_{i=1}^n \left[ m(\mathbf{w}_i^*, \theta) - \bar{m}_n^*(\theta) \right]\left[ m(\mathbf{w}_i^*, \theta) - \bar{m}_n^*(\theta) \right]'
\end{align*}
Note that $M_n^*(\theta)$ is centered around the \emph{non-bootstrap}  sample analogue $\bar{m}_n(\theta)$: this is \emph{very important!}
Now we describe the procedure for calculating the bootstrap critical value:
\begin{enumerate}
  \item Calculate $\sqrt{n}\; \bar{m}_n\left( \theta_0 \right)$ and $\widehat{\Sigma}(\theta_0)$ under the null hypothesis $H_0\colon \theta = \theta_0$.
  \item Determine which inequality moment conditions are ``far from binding'' as follows:
    \begin{itemize}
      \item Let $j \in J = \{ 1, \cdots, p\}$ index the inequality moment conditions.
      \item Let $\widehat{\sigma}_{n,j}(\theta_0)^2$ denote the $(j,j)$ element of $\widehat{\Sigma}(\theta_0)$
      \item For each $j\in J$ calculate the ``t-statistic'' $ t_{n,j} = \sqrt{n}\; \bar{m}_j(\theta_0)/\widehat{\sigma}_{n,j}(\theta_0)$
      \item Let $\mathcal{FB}$ denote the subset of $J$ for which $t_{n,j} >\sqrt{\log n}$.
        These are the inequality moment conditions that are ``far from binding'' under $H_0\colon\theta = \theta_0$. 
    \end{itemize}
  \item Calculate the test statistic $T_n = S_1\left(\sqrt{n}\; \bar{m}_n\left( \theta_0 \right), \widehat{\Sigma}\left( \theta_0 \right)  \right)$.
  \item Calculate the bootstrap critical value for the test as follows:
    \begin{itemize}
      \item Draw $R$ bootstrap samples -- each with sample size $n$. 
      \item For each bootstrap sample, $r$, calculate $M^{**}_{n,r}(\theta_0)$ and $\widehat{\Omega}^{**}_{n,r}(\theta_0)$ -- the bootstrap versions of $M_n(\theta_0)$ and $\widehat{\Omega}(\theta_0)$, defined above but with a slight change: \emph{drop} any moment inequality $j \in \mathcal{FB}$.
        That is, drop any inequality that we determined was far from binding \emph{on the basis of the real data} (i.e.\ \emph{not} this bootstrap sample!)
      \item For each bootstrap sample $r$ calculate $T_{n,r}^{**} = S_1\left(M^{**}_{n,r}\left( \theta_0 \right), \widehat{\Omega}^{**}_{n,r}\left( \theta_0 \right)  \right)$.
      \item Set $\widehat{c}_n(\theta_0, 1-\delta)$ equal to the $1-\delta$ sample quantile of the $\left\{ T_{n,r}^{**} \right\}_{r=1}^R$
    \end{itemize}
  \item Reject $H_0\colon \theta = \theta_0$ if $T_n > \widehat{c}_n(\theta_0, 1-\delta)$
  \item To construct a $(1 - \delta)\times 100\%$ confidence set, invert the test of $H_0\colon \theta = \theta_0$ for $\theta_0 \in \Theta$.
\end{enumerate}

\section{April 15--18, 2017}
 
\subsection*{Second Moment Inequalities}

\paragraph{Step 1: Second Moment Bounds}
Our model has $\varepsilon = y - c - bT^*$ where $\mathbb{E}[\varepsilon]=0$.
Conditional on $T^*=0$, $\varepsilon = y-c$ and conditional on $T^*=1$, $\varepsilon = y - c -\beta$.
Hence,
\begin{align*}
  \mathbb{E}\left[ \varepsilon^2| T^*=0, z=k\right] &= \mathbb{E}\left[ y^2 - 2cy|T^*=0, z=k \right] + c^2\\
  \mathbb{E}\left[ \varepsilon^2| T^*=1, z=k\right] &= \mathbb{E}\left[ y^2 - 2(\beta + c)y|T^*=1, z=k \right] + (\beta + c)^2
\end{align*}
The bounds will impose that $\mbox{Var}(\varepsilon|T^*,z)>0$ which is equivalent to $\mathbb{E}\left[ \varepsilon^2|T^*,z \right]>0$ since $\varepsilon$ is mean zero:
\begin{align*}
  \mathbb{E}\left[ y^2|T^*=0, z=k \right]&> 2c \mathbb{E}\left[ y|T^*=0, z=k \right] - c^2\\
  \mathbb{E}\left[ y^2|T^*=1, z=k \right]&> 2(\beta + c) \mathbb{E}\left[ y|T^*=1, z=k \right] - (\beta + c)^2
\end{align*}

\todo[inline]{These are not in fact the bounds we want! I messed up since $\varepsilon$ is \emph{not} mean zero \emph{conditional} on $T^*$ and $z$. These bounds are still correct, but they're not the best we can do and may not in fact be better than the simple first-moment bounds\ldots}

\paragraph{Step 2: Relate $\mathbb{E}[y^r|T^*,z]$ to $\mathbb{E}[y^r|T,z]$}
By iterated expectations and the assumption that $\mathbb{E}\left[ y^r|T,T^*,z \right]= \mathbb{E}\left[ y^r|T^*,z \right]$,
\begin{align*}
  \mathbb{E}\left[ y^r|T=0, z_k \right] &= \mathbb{E}\left[ y^r|T^*=0, z_k \right]\mathbb{P}(T^*=0|T=0, z_k) + \mathbb{E}\left[ y^r|T^*=1, z_k \right]\mathbb{P}(T^*=1|T=0, z_k)\\
  \mathbb{E}\left[ y^r|T=1, z_k \right] &= \mathbb{E}\left[ y^r|T^*=0, z_k \right]\mathbb{P}(T^*=0|T=1, z_k) + \mathbb{E}\left[ y^r|T^*=1, z_k \right]\mathbb{P}(T^*=1|T=1, z_k)
\end{align*}
The preceding is a linear system of the form
\begin{align*}
  a &= \pi x + (1 - \pi)y \\
  b &= (1 - \delta) x + \delta y
\end{align*}
and hence its solution is
\begin{align*}
  x &= \left[ \frac{\delta}{\pi\delta - (1 - \pi)(1 - \delta)} \right] a + \left[ \frac{-(1 - \pi)}{\pi \delta - (1 - \pi)(1 - \delta)} \right] b\\
  y &= \left[ \frac{-(1 - \delta)}{\pi\delta - (1 - \pi)(1 - \delta)} \right] a + \left[ \frac{\pi}{\pi \delta - (1 - \pi)(1 - \delta)} \right] b\\
\end{align*}
by Bayes' rule, as we show in the appendix to sick-instruments,
\begin{align*}
  \pi &= \mathbb{P}(T^*=0|T=0, z_k) =(1 - \alpha_0)(1 - p_k^*)/(1 - p_k)\\
  1 - \pi &= \mathbb{P}(T^*=1|T=0, z_k) =\alpha_1p_k^*/(1 - p_k)\\
  \delta &= \mathbb{P}(T^*=1|T=1, z_k) =(1 - \alpha_1)p_k^*/p_k\\
  1-\delta &= \mathbb{P}(T^*=0|T=1, z_k) = \alpha_0 (1 - p_k^*)/p_k
\end{align*}
Some algebra shows that 
\[
  \pi\delta - (1 - \pi)(1 - \delta) = \frac{(p_k - \alpha_0)(1 - p_k - \alpha_1)}{1 - \alpha_0 - \alpha_1}
\]
Hence, after simplifying and rearranging, it follows that 
\begin{align*}
  p_k(1 - p_k)(1 - p_k - \alpha_1)\mathbb{E}\left[ y^r|T^*=0, z_k \right] &= (1 - \alpha_1)(1 - p_k) \mathbb{E}[y^r|T=0, z_k] - \alpha_1 p_k \mathbb{E}[y^r|T=1,z_k]\\
  p_k(1 - p_k)(p_k - \alpha_0)\mathbb{E}\left[ y^r|T^*=1, z_k \right] &= (1 - \alpha_0)p_k\mathbb{E}[y^r|T=1,z_k] - \alpha_0(1 - p_k) \mathbb{E}[y^r|T=0,z_k]
\end{align*}


\paragraph{Step 3: Convert Conditional to Unconditional Moments}
By iterated expectations and the assumption that $\mathbb{E}\left[ y^r|T,T^*,z \right]= \mathbb{E}\left[ y^r|T^*,z \right]$
\begin{align*}
  p_k &= \mathbb{E}[T|z=k] = \mathbb{E}\left[ T \mathbf{1}(z=k) \right]/\mathbb{P}(z=k)\\
  \mathbb{E}\left[ y^r |T=0, z=k\right] &= \mathbb{E}\left[ y^r(1 - T) \mathbf{1}(z=k) \right]/\left[ (1 - p_k) \mathbb{P}(z=k)\right]\\
  \mathbb{E}\left[ y^r |T=1, z=k\right] &= \mathbb{E}\left[ y^r T \mathbf{1}(z=k) \right]/\left[p_k \mathbb{P}(z=k)\right]
\end{align*}

\paragraph{Step 4: Substitute Steps 2--3 into Step 1}
After some algebra, we find that
\begin{align*}
  \mathbb{E}\left[ \mathbf{1}(z=k)\left\{ T - (1 - \alpha_1) \right\}\left(y^2 - 2cy \right) \right] &< c^2 \mathbb{P}(z=k)p_k(1 - p_k)(1 - p_k - \alpha_1)\\
  \mathbb{E}\left[ \mathbf{1}(z=k)(\alpha_0 - T)\left\{ y^2 - 2(\beta + c)y \right\} \right] &< (\beta + c)^2 \mathbb{P}(z=k) p_k(1 - p_k)(p_k - \alpha_0)
\end{align*}

\paragraph{Step 5: Complete the Square}
\begin{align*}
  \mathbb{E}\left[ \mathbf{1}(z=k)\left\{ 1 - \alpha_1 - T \right\}\left(y - c\right)^2 \right] &> c^2 \mathbb{P}(z=k)(1 - p_k - \alpha_1)\left[ 1 - p_k(1 - p_k) \right]\\
  \mathbb{E}\left[ \mathbf{1}(z=k)(T - \alpha_0)\left\{ y - (\beta + c) \right\}^2 \right] &> (\beta + c)^2 \mathbb{P}(z=k) (p_k - \alpha_0)\left[ 1 - p_k(1 - p_k) \right]
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{April 21--23}
\subsection*{The Full Set of Moment Inequalities}
Above we considered the special case in which $\alpha_0= 0$ so that first and second moments were sufficient to identify the model.
We now derive the GMM-style moment conditions for the general case in which $\alpha_0$ may not equal zero and we need to use third moments for identification.

\todo[inline]{Note that we have checked all of these equalities numerically in our simulation study and they are indeed correct!}

\paragraph{Notation and Identification} Define the following re-parameterization
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right]
\end{align*}
Using this notation, the covariance equations from above become
\begin{align*}
  \mbox{Cov}(y,z) - \theta_1 \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \theta_1 2\mbox{Cov}(yT,z) + \theta_2 \mbox{Cov}(T,z)&= 0\\
  \mbox{Cov}(y^3,z) - \theta_1 3 \mbox{Cov}(y^2T,z) + \theta_2 3\mbox{Cov}(yT,z) - \theta_3\mbox{Cov}(T,z) &= 0
\end{align*}
Note that it is trivial to prove that $\theta_1, \theta_2$ and $\theta_3$ are identified.
To show that $\beta, \alpha_0$ and $\alpha_1$ are identified, write
\begin{align*}
  \theta_2/\theta_1^2 &= 1 + \left( \alpha_0 - \alpha_1 \right)\\
  \theta_3/\theta_1^3 &= (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0 (1 - \alpha_1) 
\end{align*}
which we can always do provided that $\theta_1\neq 0$ which is equivalent to $\beta \neq 0$.
Re-arranging the first equation allows us to solve for $\alpha_0$ as a function of $\alpha_1$.
Substituting this into the second gives us a quadratic in $\alpha_1$ only.
This should be identical to the quadratic we obtained in our original identification proof.

\paragraph{First Moment Equalities}
Expanding,
\begin{align*}
  \mbox{Cov}(y,z) - \theta_1 \mbox{Cov}(T,z) &= 0\\
  \mathbb{E}[yz - \theta_1 Tz] - \mathbb{E}[z]\mathbb{E}[y - \theta_1 T]&= 0
\end{align*}
and
\begin{align*}
  \mathbb{E}[y - \theta_1 T] &= c + \beta \mathbb{E}\left[ T^* - T/(1 - \alpha_0 - \alpha_1) \right]\\
  &= c + \beta\left\{ \frac{\mathbb{E}[T] - \alpha_0}{1 - \alpha_0 - \alpha_1} - \frac{\mathbb{E}[T]}{1 - \alpha_0 - \alpha_1} \right\}\\
&= c - \alpha_0 \theta_1 
\end{align*}
Thus,
\[
  \mathbb{E}\left[ \left\{ y - \left( c - \alpha_0 \theta_1 \right) - \theta_1 T\right\}z \right] = 0
\]
Now define 
\begin{align*}
\kappa_1 &= c - \alpha_0 \theta_1\\
u_1 &= y - \kappa_1 - \theta_1 T
\end{align*}
Using this notation, we obtain the unconditional moment equalities
\[
  \mathbb{E}\left[
  \begin{array}{l}
    u_1(\kappa_1, \theta_1)\\ u_1(\kappa_1, \theta_1) z
  \end{array}
\right] = \mathbf{0}
\]

\paragraph{Second Moment Equalities}

\begin{align*}
  \mbox{Cov}(y^2,z) - \theta_1 2\mbox{Cov}(yT,z) + \theta_2 \mbox{Cov}(T,z)&= 0\\
  \mathbb{E}\left[ y^2z - \theta_1 2yTz + \theta_2 Tz \right] -\mathbb{E}\left[ z \right]\mathbb{E}\left[ y^2 - \theta_1 2yT + \theta_2 T \right] &= 0
\end{align*}

Now, using some lemmas from the notes in our sick-instruments paper,
\begin{align*}
  \mathbb{E}[T\varepsilon] &= \mbox{Cov}(T,\varepsilon) \\
  &= \mbox{Cov}(T^*,\varepsilon) + \mbox{Cov}(w,\varepsilon)\\
  &= \mbox{Cov}(T^*,\varepsilon) - \mbox{Cov}(T^*,\varepsilon)(\alpha_0 + \alpha_1)\\
  &= \mathbb{E}(T^*\varepsilon)(1 - \alpha_0 - \alpha_1)
\end{align*}
hence,
\begin{align*}
  \mathbb{E}[yT] &= c \mathbb{E}[T] + \beta \mathbb{E}[TT^*] + \mathbb{E}\left[T,\varepsilon \right] \\
  &= cp + \beta \mathbb{P}(T=1,T^*=1) + \mathbb{E}[T^*\varepsilon](1 - \alpha_0 - \alpha_1)\\
  &= cp + \beta (1 - \alpha_1)p^* + \mathbb{E}[T^*\varepsilon](1 - \alpha_0 - \alpha_1)
\end{align*}
Combining this with
\[
  \mathbb{E}[y^2] = c^2 + \beta^2 p^* + \sigma_{\varepsilon\varepsilon} + 2 c\beta p^* + 2 \beta \mathbb{E}[T^*\varepsilon]
\]
we find that 
\[
  \mathbb{E}\left[ y^2 - \theta_1 2yT + \theta_2 T \right] = \cdots = c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0(\theta_2 - 2c\theta_1)
\]
\todo[inline]{For the steps, see our whiteboard notes from 2017-04-21 17.06.32}

Now, define
\begin{align*}
  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
  u_2 &= y^2 - \kappa_2 - \theta_1 2 yT + \theta_2 T
\end{align*}
Using this notation, we obtain the unconditional moment equalities
\[
  \mathbb{E}\left[
  \begin{array}{l}
    u_2(\kappa_2, \theta_1, \theta_2)\\ 
    u_2(\kappa_2, \theta_1, \theta_2) z
  \end{array}
\right] = \mathbf{0}
\]


\paragraph{Third Moment Equalities} 
\begin{align*}
  \mbox{Cov}(y^3,z) - \theta_1 3 \mbox{Cov}(y^2T,z) + \theta_2 3\mbox{Cov}(yT,z) - \theta_3\mbox{Cov}(T,z) &= 0\\
  \mathbb{E}\left[ y^3 z - \theta_1 3 y^2 Tz + \theta_2 3 yTz - \theta_3 Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ y^3 - \theta_1 3 y^2T + \theta_2 3 yT - \theta_3 T \right] &= 0
\end{align*}

\[
  \mathbb{E}[y^3] = \cdots = c^3 + \beta p^* \left( 3 c^2 + 3 c\beta + \beta^2 \right) + 3 \beta \mathbb{E}[\varepsilon T^*] (2c + \beta) + 3 c \sigma_{\varepsilon\varepsilon} + 3\beta \mathbb{E}[\varepsilon^2 T^*] + \mathbb{E}[\varepsilon^3]
\]

\todo[inline]{For derivations of the following, see our whiteboard notes from April 21st and 22nd, 2017}
\[
  \mathbb{E}\left[ y^2T \right] = \cdots = c^2 p + \beta(1 - \alpha_1)p^* + \mathbb{E}[T\varepsilon^2] + 2 c \beta(1 - \alpha_1) p^* + 2 c \mathbb{E}[T\varepsilon] + 2 \beta \mathbb{E}[TT^* \varepsilon] 
\]
and 
\begin{align*}
  \mathbb{E}[T\varepsilon^2] &= \cdots = \alpha_0 \sigma_{\varepsilon\varepsilon} + (1 - \alpha_0 - \alpha_1) \mathbb{E}[\varepsilon^2 T^*]\\
  \mathbb{E}[TT^*\varepsilon] &= \cdots = (1 - \alpha_1) \mathbb{E}[T^*\varepsilon] 
\end{align*}

We then need to calculate $\mathbb{E}[y^3 - \theta_1 3 y^2T + \theta_2 3yT - \theta_3 T]$.
Using the preceding expressions, we can show after some algebra that the $\mathbb{E}[T^*\varepsilon]$ and $\mathbb{E}[T^*\varepsilon^2]$ terms drop out, hence
\begin{align*}
\mathbb{E}[y^3 - \theta_1 3 y^2T + \theta_2 3yT - \theta_3 T]&=
\left\{ c^3 + \beta p^* \left( 3c^2 + 3 c\beta + \beta^2 \right) \right\} + 3c \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \theta_1 3 \alpha_0 \sigma_{\varepsilon\varepsilon} - \theta_3 p\\
&- \theta_1 3 \left\{ c^2p + \beta^2(1 - \alpha_1)p^* + 2 c \beta (1 - \alpha_1) p^* \right\} + \theta_2 3 \left\{ cp + \beta (1 - \alpha_1)p^* \right\}
\end{align*}
After \emph{even more algebra} we can show that this expression depends neither on $p$ nor on $p^*$:
\small
\begin{align*}
\mathbb{E}[y^3 - \theta_1 3 y^2T + \theta_2 3yT - \theta_3 T]&=
c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_2 \frac{1 - \alpha_1}{1 + \alpha_0 - \alpha_1} \right]\\
&= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]\\
\end{align*}
\normalsize
Now let 
\begin{align*}
  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]\\
  u_3 &= y^3 - \kappa_3 - \theta_1 3y^2 T + \theta_2 3yT - \theta_3 T
\end{align*}
we obtain the following moment equalities
\[
  \mathbb{E}\left[
  \begin{array}{l}
    u_3(\kappa_3, \theta_1, \theta_2, \theta_3)\\ 
    u_3(\kappa_3, \theta_1, \theta_2, \theta_3) z
  \end{array}
\right] = \mathbf{0}
\]

\paragraph{Putting Everything Together} Define
\begin{align*}
\boldsymbol{\kappa} &= (\kappa_1, \kappa_2, \kappa_3)' \\
\boldsymbol{\theta} &= (\theta_1, \theta_2, \theta_3)'\\
\mathbf{u}(\boldsymbol{\kappa}, \boldsymbol{\theta}) &= 
\left[
\begin{array}{ccc}
  u_1(\kappa_1, \theta_1) & 
  u_2(\kappa_2, \theta_2) &
  u_3(\kappa_3, \theta_3)
\end{array}
\right]'
\end{align*}
where
\begin{align*}
\kappa_1 &= c - \alpha_0 \theta_1\\
  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]
\end{align*}
and
\begin{align*}
u_1(\kappa_1, \theta_1) &= y - \kappa_1 - \theta_1 T\\
  u_2(\kappa_2, \theta_1, \theta_2) &= y^2 - \kappa_2 - \theta_1 2 yT + \theta_2 T\\
  u_3(\kappa_3, \theta_1, \theta_2, \theta_3) &= y^3 - \kappa_3 - \theta_1 3y^2 T + \theta_2 3yT - \theta_3 T
\end{align*}
Then the full system of moment equalities is given by
\[
  \mathbb{E}\left[
  \begin{array}{l}
    \mathbf{u}\left( \boldsymbol{\kappa}, \boldsymbol{\theta} \right)\\
    \mathbf{u}\left( \boldsymbol{\kappa}, \boldsymbol{\theta} \right)z
  \end{array}
\right] = \mathbf{0}
\]
where
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right]
\end{align*}


\paragraph{What about exogenous covariates?}
At one extreme, we could simply carry out the above \emph{conditional} on $\mathbf{x}$.
This would entail treating $\boldsymbol{\kappa}$ and $\boldsymbol{\theta}$ as functions of $\mathbf{x}$ such that $\alpha_0, \alpha_1$ and $\beta$ would be allowed to depend on $\mathbf{x}$.
To implement this, one would need to estimate a number of conditional mean functions: $\mathbb{E}[y|\mathbf{x}]$, $\mathbb{E}[T|\mathbf{x}]$, $\mathbb{E}[yT|\mathbf{x}]$, $\mathbb{E}[y^3|\mathbf{x}]$, and $\mathbb{E}[y^2T|\mathbf{x}]$.
It would appear that this leads to a standard two-step estimation problem although clearly you'd need a lot of data to have any chance!
Another idea would be to impose some restrictions on the way in which $\mathbf{x}$ affects $\alpha_0$ etc, leading to a semi-parametric estimator.
At the other extreme, we could impose the assumption that $\mathbf{x}$ affects $y$ linearly and $\alpha_0, \alpha_1$ do not vary with $\mathbf{x}$:
\[
  y = c + \beta T^* + \mathbf{x}'\boldsymbol{\gamma} + \varepsilon
\]
Under this assumption the result of Frazis \& Loewenstein (2003) holds -- the IV estimator would recover $\kappa_1, \theta_1, \boldsymbol{\gamma}$.
Since $(y - \mathbf{x}'\boldsymbol{\gamma} ) = c + \beta T^* + \varepsilon$, all of the preceding moment conditions should still hold only with $(y - \mathbf{x}'\boldsymbol{\gamma})$ in place of $y$.
To identify $\boldsymbol{\gamma}$ we add an extra moment condition in which $\mathbf{x}$ multiplies the $u_1$ error term, re-defined to subtract $\mathbf{x}'\boldsymbol{\gamma}$.

\subsection*{Correcting the Second Moment Inequalities}
In the derivation from above I mistakenly claimed that since $\varepsilon$ is mean zero, it is sufficient to work with the conditional means of $\varepsilon^2$ given $T^*$ and $z$. 
This is not correct since the \emph{conditional} means of $\varepsilon$ are not zero.
The bounds from above are not wrong, but they can be improved by looking at the conditional variance of $\varepsilon$ rather than simply $\mathbb{E}[\varepsilon^2]$.
Steps 2--3 from above are still usable, but we need to change step 1.

Our inequalities come from the fact that
\[
  \mbox{Var}(\varepsilon|T^*,z) > 0 \iff \mathbb{E}[\varepsilon^2 |T^*,z] > \left\{\mathbb{E}[\varepsilon|T^*,z]\right\}^2
\]
We've already done the calculations for $\varepsilon^2|T^*,z$.
Now we simply need to do the same for $\varepsilon|T^*,z$, namely
\begin{align*}
  \mathbb{E}[\varepsilon|T^*=0,z=k] &= \mathbb{E}\left[ y |T^*=0, z=k \right] - c\\
  \mathbb{E}[\varepsilon|T^*=1,z=k] &= \mathbb{E}\left[ y |T^*=1, z=k\right] - (c + \beta)
\end{align*}
Now we relate $\mathbb{E}[y|T^*,z]$ to $\mathbb{E}[y|T,z]$ using the calculations from above:
\begin{align*}
  \mathbb{E}\left[ y|T^*=0, z_k \right] &= \frac{(1 - \alpha_1)}{p_k (1 - p_k - \alpha_1)} \mathbb{E}[y|T=0, z_k] - \frac{\alpha_1 }{(1 - p_k)(1 - p_k - \alpha_1)}\mathbb{E}[y|T=1,z_k]\\
  \mathbb{E}\left[ y|T^*=1, z_k \right] &= \frac{(1 - \alpha_0)}{( 1 - p_k)(p_k - \alpha_0)}\mathbb{E}[y|T=1,z_k] - \frac{\alpha_0}{p_k (p_k - \alpha_0)} \mathbb{E}[y|T=0,z_k]
\end{align*}
After lots of algebra (see our handwritten notes), we obtain:
\small
\begin{align*}
  p_k (1 - p_k)(1 - p_k - \alpha_1) \left\{ (1  - \alpha_1)(1 - p_k)\mathbb{E}\left[ y^2|T=0,z_k \right] - \alpha_1 p_k \mathbb{E}[y^2|T=1,z_k \right\}  \\
    > \left\{ (1 - \alpha_1)(1 - p_k)\mathbb{E}[y|T=0,z_k] - \alpha_1 p_k \mathbb{E}[y|T=1,z_k] \right\}^2
\end{align*}
and
\begin{align*}
  p_k (1 - p_k)(p_k - \alpha_0) \left\{ (1  - \alpha_0)p_k\mathbb{E}\left[ y^2|T=1,z_k \right] - \alpha_0 (1-p_k) \mathbb{E}[y^2|T=0,z_k \right\}  \\
    > \left\{ (1 - \alpha_0)p_k\mathbb{E}[y|T=1,z_k] - \alpha_0 (1-p_k) \mathbb{E}[y|T=0,z_k] \right\}^2
\end{align*}
Finally, we need to convert these into expressions involving unconditional moments via
\begin{align*}
  p_k &= \mathbb{E}[T|z=k] = \mathbb{E}\left[ T \mathbf{1}(z=k) \right]/\mathbb{P}(z=k)\\
  \mathbb{E}\left[ y^r |T=0, z=k\right] &= \mathbb{E}\left[ y^r(1 - T) \mathbf{1}(z=k) \right]/\left[ (1 - p_k) \mathbb{P}(z=k)\right]\\
  \mathbb{E}\left[ y^r |T=1, z=k\right] &= \mathbb{E}\left[ y^r T \mathbf{1}(z=k) \right]/\left[p_k \mathbb{P}(z=k)\right]
\end{align*}
These appear to match our expressions from the sick-instruments paper.


\subsection*{Estimated Parameters in Andrews \& Soares}
In our simple example from above applying the Andrews and Soares (2010) generalized moment selection procedure to the case where $\alpha_0 = 0$, we assumed for simplicity that $c$ and $\sigma_{\varepsilon\varepsilon}$ were known.
Without $\alpha_0 = 0$, the analogous assumption would be that $\boldsymbol{\kappa}$ is known.
In practice, however, this parameter is estimated from the data. 
Accounting for this fact requires a change in the procedure for calculating the variance matrix $\Sigma$ of the moment conditions under the null hypothesis. 
We know explain how this works.

\section{May 5th, 2017 -- Todo for Revised Paper}

\begin{enumerate}
  \item New notation to accomodate covariates etc.\ as in Mahajan and Lewbel
  \item Move Mahajan stuff into appendix; possibly convert back to his notation later -- I have handwritten notes on this.
  \item Identification results: identification from higher moments, lack of identification from conditional means alone
  \item Explain briefly how to do fully non-parametric estimation, specialize to some simple cases, e.g.\ linear model
  \item Explain about inference versus estimation.
  \item Possibly show that Mahajan, Lewbel, etc.\ also suffer from a weak identification problem. This might be helpful for selling the paper.
    Would need to think about what inequalities to use in their case.
    Presumably we could still use the ``weak'' bounds, but there might be others we could exploit.
  \item Testing $\beta = 0$ should be very easy: don't need Andrews \& Soares at all. I think we can either just use the Stock \& Wright GMM-AR or even a plain old-fashioned GMM inference for testing a linear restriction.
  \item Canay et al.\ projection inference for $\beta$? Need to figure out if this actually gives a speed-up in our case.
    If so, could be very useful for simulations.
    Would be nice to show that we have power to reject the probability limit of the IV and reduced form.
    Is it easier to handle the strongly-identified parameters using their method?
  \item Could also talk briefly about the continuum of moment conditions idea and how one obtains identification from quantiles. There's also a question of using more inequalities by imposing the independence assumption for the measurement error.
    Could also proceed without our higher moment assumptions and just get inference for the identified set.
  \item Is there any way to handle treatment effect heterogeneity? Should be ok if it's modeled heterogeneity but maybe there are some other special cases we can handle? What about that quantile treatment effect idea?
  \item Simulations: want to show that we can actually learn something useful. Things we want to show are that we have more power than just the RF for testing $\beta=0$, that we have power to reject the RF and IV plims in certain cases.
  \item Need to show at least in simulations that our confidence regions aren't insane. Would be good if we could formally argue that they must be convex and connected, for example.
    Maybe the key is to show that our inequalities are always well-behaved and that adding the equalities cann't make the problem suddenly become badly behaved.
  \item Andrews \& Soares with estimates of the strongly identified parameters to get a joint region for $\alpha_0, \alpha_1$, and $\beta$. But this is slow, so we probably can't do simulations although it would be interesting in the empirical examples.
  \item Try to get the projection inference for $\beta$ working 
  \item Empirical example or examples: Oreopolous? Maybe the Heckman JPTA dataset? Petra suggested emailing someone about this but I forget who it was\ldots

\end{enumerate}

\section{Lewbel (2007)}

\paragraph{Notation}
Observe $Y,Z$ and $T$ where $T$ is a proxy for $T^*$ and
\begin{align*}
  h^*(X,T^*) &= E(Y|X,T^*)\\
  T^* &= \mbox{unobserved binary regressor}\\
  X &= \mbox{vector of covariates}\\
  Y &= \mbox{outcome}
\end{align*}
Since $T^*$ is binary, without loss of generality,
\begin{align*}
  h^*(X,T^*) &= h_0^*(X) + \tau^*(x)T^*\\
  h^*_0(X) &= h^*(X,0)\\
  \tau^*(x) &= h^*(x,1) - h^*(x,0)
\end{align*}
Goal is to estimate $\tau^*(x)$.
Below we will partition $X$ according to $X = (V,Z)$ where $V$ is an ``instrument-like variable'' and $Z$ is the set of remaining covariates.

\paragraph{Assumption A1} 
There exists $E(Y|X,T^*,T) = E(Y|X,T^*)$.
Equivalently, $Y$ is mean-independent of $T-T^*$, conditional on $X,T^*$ -- mis-classification does not affect true expected outcome.
This rules out placebo effects, etc.

\paragraph{More Notation}
\begin{align*}
  r^*(x) &= E(T^*|X=x) = P(T^*=1|X=x)\\
  b_0(x) &= P(T=1|T^*=0,X=x)\\
  b_1(x) &= P(T=0|T^*=1,X=x)\\
  r(x) &= E(T|X=x)\\
  \tau(x) &= h(x,1) - h(x,0)\\
  h(x,t) &= E(Y|X=x,T=t)
\end{align*}

\paragraph{Assumption A2} There exist $b_0(x) + b_1(x) <1$ and $0 < r^*(x) <1$ for all $x$ in the support of $X$.
\paragraph{Theorem 1} Under Assumption A1 there exists a function $m(x)$ such that $|m(x)|\leq 1$ and $\tau(x) = \tau^*(x)m(x)$.
If we add Assumption A2 then $m(x)>0$ as well.

\begin{itemize}
  \item Analogous to attenuation bias under classical measurement error: A1 implies that $\tau(x)$ is a lower bound for the magnitude of $\tau^*(x)$.
    Adding A2, sign of $\tau(x)$ agrees with that of $\tau^*(x)$.
  \item Assumptions A1 and A2 imply that $\tau^*(x)=0$ iff $\tau(x)=0$. Thus, if we only want to test $\tau^*(x) = 0$ we can simply ignore mis-classification.
  \item As shown below:
    \begin{align*}
      m(x) &= P(T^*=1|T=1,X=x) - P(T^*=1|T=0,X=x) \implies |m(x)|< 1\\
      m(x) &= M[b_0(x),b_1(x),r(x)] = \frac{1}{1 - b_1(x) - b_0(x)}\left\{ 1 -  \frac{\left[ 1 - b_1(x) \right]b_0(x)}{r(x)} - \frac{\left[ 1 - b_0(x) \right]b_1(x)}{1 - r(x)} \right\} \\
      m(x) &= \frac{[1 - r^*(x)]r^*(x)}{\left[ 1 - r(x) \right]r(x)}
      \left[ 1 - b_0(x) - b_1(x) \right] \implies m(x) >0 \mbox{ if } 1 - b_0(x) + b_1(x) > 0 
    \end{align*}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proof}[Proof of Theorem 1]
Let $p_t(X) = P(T^*=1|X,T=t)$. 
By A1 we have 
\[
  E(Y|X,T^*,T) = E(Y|X,T^*) = h^*_0(X) + \tau^*(X) T^*
\]
combining this with iterated expectations, 
\begin{align*}
  E(Y|X,T=t) &= E_{T^*|X,T=t}\left[ E(Y|X,T=t,T^*) \right] = E_{T^*|X,T=t}\left[ h^*_0(X) + \tau^*(X) T^* \right]\\
  &= p_t(X) \left[ h^*_0(X) + \tau^*(X) \right] + \left[ 1 - p_t(X) \right] h^*_0(X)\\
  &= h^*_0(X) + p_t(X) \tau^*(X)
\end{align*}
Taking the difference of the preceding expression evaluated at $T=1$ and $T=0$
\[
  E(Y|X,T=1) - E(Y|X,T=0) = \left[p_1(X) - p_0(X)\right] \tau^*(X)
\]
whereas we defined 
\[
  \tau(X) = h(X,1) - h(X,0) = E(Y|X,T=1) - E(Y|X,T=0)
\]
Combining these two equations, we see that
\[
  \tau(X) = \left[ p_1(X) - p_0(X) \right] \tau^*(X) 
\]
so that the function $m(x)$ defined in Theorem 1 equals $p_1(x) - p_0(x)$.
Since $m$ is a difference of probabilities, it follows immediately that $-1 \leq m(x) \leq 1$.
Now, by Bayes' Rule, 
\begin{align*}
  p_0(x) &= P(T^*=1|X=x,T=0) = \frac{P(T=0|X=x,T^*=1)P(T^*=1|X=x)}{P(T=0|X=x)} = \frac{b_1(x) r^*(x)}{1-r(x)} \\
  p_1(x) &= P(T^*=1|X=x,T=1) = \frac{P(T=1|X=x,T^*=1)P(T^*=1|X=x)}{P(T=1|X=x)} = \frac{[1-b_1(x)] r^*(x)}{r(x)} 
\end{align*}
and by iterated expectations,
\begin{align*}
  r(x) &= E(T|X=x) = E(T|X=x,T^*=1)P(T^*=1|X=x) + E(T|X=x,T^*=0)P(T^*=0|X=x)\\ 
    &= [1 - b_1(x)]r^*(x) + b_0(x) [1 - r^*(x)] = b_0(x) + r^*(x)[1 - b_0(x) - b_1(x)] 
\end{align*}
Using this expression, if $b_0(x) + b_1(x) = 1$ then $r(x) = b_0$.
If instead $b_0(x) + b_1(x) \neq 1$, then we can divide through by $1 - b_0(x) - b_1(x)$ to solve for $r^*(x)$ and $1 - r^*(x)$ as follows:
\begin{align*}
  r^*(x) &= \frac{r(x) - b_0(x)}{1 - b_0(x) - b_1(x)}\\
  1 - r^*(x) &= \frac{[1 - b_0(x) - b_1(x)] - [r(x) - b_0(x)]}{1 - b_0(x) - b_1(x)} = \frac{1 - b_1(x) - r(x)}{1 - b_0(x) - b_1(x)}
\end{align*}
Using the expressions we have just derived for $p_0, p_1, r^*$ and $1 - r^*$ and suppressing the dependence on $x$ for simplicity, it follows that
\begin{align*}
  m &= p_1 - p_0 = \frac{(1 - b_1)r^*}{r} - \frac{b_1r^*}{1- r} = \frac{(1 - r)(1 - b_1) r^* - r b_1 r^*}{r(1 - r)}\\
  &= \frac{r^*( 1 - r - b_1 + r b_1 - r b_1 )}{r (1 - r)} = \frac{r^*(1 - r - b_1)}{r(1-r)}
\end{align*}
Rearranging,
\[
  (1-r)rm = r^*(1 - r - b_1)
\]
and combining this with $1 - r^* = (1 - b_1 - r) / (1 - b_0 - b_1)$, we have
\[
  (1-r)rm = (1 - r^*)r^*(1 - b_0 - b_1)
\]
Both $r$ and $r^*$ are strictly between zero and one.
Thus, if $b_0 + b_1 = 1$ we have $m = 0$.
If instead $b_0 + b_1 <1$ we have $m>0$ and if $b_0 + b_1 > 1$ then $m <0$.
Finally,
\[
  m = \frac{r^*(1 - r - b_1)}{r(1-r)} = \frac{r - b_0}{1 - b_0 - b_1}\left[ \frac{1 - r - b_1}{r(1 - r)} \right] 
\]
and since 
\begin{align*}
 (r - b_0)(1 - r - b_1) &= r(1-r) - b_0(1-r) - b_1r + b_1 b_0  \\
  &= r(1-r) - b_0(1-r) - b_1r + b_1 b_0  + (b_1 b_0 r - b_1 b_0 r)\\
  &= r(1-r) - b_0(1-r) - b_1r + b_1 b_0(1 - r)  + b_1 b_0 r \\
  &= r(1-r) - b_0(1-r) - b_1r(1 - b_0) + b_1 b_0(1 - r) \\
  &= r(1-r) + b_0(1-r)(b_1 - 1) - b_1r(1 - b_0) \\
  &= r(1-r) - b_0(1-r)(1 - b_1) - b_1r(1 - b_0) 
\end{align*}
we find that
\begin{align*}
  m &= M(b_0, b_1, r) = \frac{1}{1 - b_0 - b_1}\left[ \frac{r(1-r) - (1 - b_1)b_0(1-r) - (1-b_0)b_1r}{r(1-r)} \right]\\
  &= \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r} - \frac{(1-b_0)b_1}{1-r} \right].
\end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Assumption A3} Assume that $r(x)$ and $\tau(x)$ are identified.
Note that this only requires that we can consistently estimate conditional expectations of observable quantities.

\paragraph{Assumption A4} Suppose that we have partitioned $X$ into two subvectors: $X = (V,Z)$. 
We assume that for each $z$ in the support of $Z$ there exists a subset $\Omega_z$ of the support of $V$ such that:
\begin{enumerate}[(i)]
  \item for all $v, v'\in \Omega_z$, $b_0(v,z)=b_0(v',z)$, $b_1(v,z)=b_1(v',z)$, and $\tau^*(v,z) = \tau^*(v',z)$
  \item for all $v,v' \in \Omega_z$ such that $v\neq v'$, $r^*(v,z) \neq r^*(v',z)$.
\end{enumerate}
The basic idea here is that $V$ affects the probability of being treated $r^*$ but not the treatment effect $\tau^*$ after we have conditioned on $Z$.
As sufficient condition for the restriction on $\tau^*$ to hold is $E(Y|Z=z,V=v,T^*=t) = s_1(z,t) + s_2(z,v)$ for some functions $s_1$ and $s_2$.

\paragraph{Some More Notation} Let $b_0(z), b_1(z), \tau^*(z)$ denote $b_0(v,z), b_1(v,z), \tau^*(v_z)$ for $v \in \Omega_z$ since, under A4, these do not vary with $v$.

\paragraph{Assumption A5} Each set $\Omega_z$ from A4 contains three elements $v_k$, $k=0,1,2$ such that
\[
  \left[ \frac{\tau(v_0,z)}{r(v_1,z)} - \frac{\tau(v_1,z)}{r(v_0,z)} \right]\left[ \frac{\tau(v_0,z)}{1 - r(v_2,z)} - \frac{\tau(v_2,z)}{1 - r(v_0,z)} \right] \neq 
  \left[ \frac{\tau(v_0,z)}{r(v_2,z)} - \frac{\tau(v_2,z)}{r(v_0,z)} \right]\left[ \frac{\tau(v_0,z)}{1 - r(v_1,z)} - \frac{\tau(v_1,z)}{1 - r(v_0,z)} \right] 
\]
\begin{itemize}
  \item The key requirement is that $V$ takes on at least three values.
  \item Assumption A5 depends only on observables, so we can test it.
  \item An equivalent way to state A5 is: $\tau^*(z)\neq 0$, $b_0(z)+b_1(z) \neq 1$ and an inequality involving $r$ and $r^*$ which is explained below.
  \item The triplets $(v_0, v_1, v_2)$ are allowed to depend on $z$.
\end{itemize}

\paragraph{Theorem 2} Under A1--A5, the mis-classification probabilities $b_0(x),b_1(x)$ are identified as is the probability of treatment $r^*(x)$ and treatment effect $\tau^*(x)$.
If we replace $b_0(x)+b_1(x)<1$ from A2 with $b_0(x)+b_1(x) \neq 1$, then $\tau^*(x)$ is identified up to sign. 
\begin{itemize}
  \item For simplicity, suppress dependence on $z$. By Theorem 1 and A4,
    \[
      \tau(v_k) M\left[ b_0, b_1, r(v_0) \right] = \tau(v_0)M\left[ b_0, b_1, r(v_k) \right]
    \]
  \item Again suppressing dependence on $z$, recall from above that \[M[b_0, b_1, r(v_k)] = \frac{1}{1 - b_1 - b_0}\left\{ 1 - \frac{(1 - b_1)b_0}{r(v_k)} - \frac{(1 - b_0)b_1}{1 - r(v_k)} \right\}\]
  \item Evaluating the two preceding expressions at $k=1$ and $k=2$ gives two equations relating the identified functions $r$ and $\tau$ to the unknown mis-classification probabilities $b_0$ and $b_1$.
  \item Since each equation involves $v_0$, we need $V$ to take on at least three values to get as many equations as unknowns (two).
  \item The proof of Theorem 2 shows that the two equations admit a unique solution, so that $b_0$ and $b_1$ are identified.
\item Given knowledge of $b_0$ and $b_1$, Theorem 1 allows us to solve for $r^*$ and $\tau^*$.
\item If $V$ only took on two values, we could not identify $b_0$ and $b_1$ without further restrictions. However if either $b_0$ or $b_1$ were known, e.g.\ known to be zero as in a one-sided mis-classification setting, then we could identify the model using only a binary $V$.
\end{itemize}

\begin{proof}[Proof of Theorem 2]
For a fixed value of the covariates $z$, Assumption A4 ensures that there is a subset  $\Omega_z$ of the support of $V$ -- a subset that may depend on $z$ -- such that for all $v, v' \in \Omega_z$ we have $b_0(v,z) = b_0(v',z)$, $b_1(v,z) = b_1(v',z)$ and $\tau^*(v,z) = \tau^*(v',z)$.
Assumption A5 ensures that $\Omega_z$ contains at least three values: $v_0, v_1$ and $v_2$.
Suppress dependence on the covariates $z$: let $r_k = r(v_k)$ and $\tau_k = \tau(v_k)$ for $k=0, 1, 2$.
Since $b_0$ and $b_1$ do not depend on $v$ for $v \in \Omega_z$, there is no subscript on these quantities.
By Theorem 1, $\tau(v,z) = \tau^*(v,z)m(v,z)$.
Again suppressing dependence on $z$ and using the expression for $m$ derived in the proof of Theorem 1,
  $\tau_k = M(b_0, b_1, r_k) \tau^*_k$
for all $k = 0, 1, 2$.
But by Assumption A4, $\tau^*_0 = \tau^*_1 = \tau^*_2$, which yields
\[
  \frac{\tau_k}{M(b_0, b_1, r_k)} = \frac{\tau_\ell}{M(b_0, b_1, r_\ell)}
\]
for any $k,\ell$.
Rearranging, 
\[
  M(b_0, b_1, r_k)\tau_\ell = M(b_0, b_1, r_\ell)\tau_k
\]
For $k \neq \ell$ this yields a nontrivial equation relating $b_0$ and $b_1$ to observables: $\tau_k, r_k$ and $\tau_\ell, r_\ell$.
In particular, take $\ell = 0$ and $k = 1, 2$.
We obtain,
\begin{align*}
  0 &= M(b_0, b_1, r_k)\tau_0 - M(b_0, b_1, r_0)\tau_k \\
  0 &= \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r_k} - \frac{(1-b_0)b_1}{1-r_k} \right]\tau_0 - \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r_0} - \frac{(1-b_0)b_1}{1-r_0} \right]\tau_k\\
  0 &= \left[ 1 - \frac{(1 - b_1)b_0}{r_k} - \frac{(1 - b_0)b_1}{1-r_k} \right]\tau_0 - \left[ 1 - \frac{(1 - b_1)b_0}{r_0} - \frac{(1 - b_0)b_1}{1-r_0} \right]\tau_k\\
  0 &= (1 - b_1)b_0 \left( \frac{\tau_k}{r_0} - \frac{\tau_0}{r_k} \right) + (1 - b_0)b_1 \left( \frac{\tau_k}{1 - r_0} - \frac{\tau_0}{1 - r_k} \right) + (\tau_0 - \tau_k)\\
  0 &= (1 - b_1)b_0 \left( \frac{\tau_0}{r_k} - \frac{\tau_k}{r_0} \right) + (1 - b_0)b_1 \left( \frac{\tau_0}{1 - r_k} - \frac{\tau_k}{1 - r_0} \right) + (\tau_k - \tau_0)
\end{align*}
This is a \emph{linear} equation of the form
\[
  0 = B_0 w_{0k} + B_1 w_{1k} + w_{2k}
\]
where the unknowns $B_0, B_1$ are defined as
\begin{align*}
  B_0 &= b_0 (1- b_1)\\
  B_1 &= b_1(1- b_0)
\end{align*}
and the observable constants $w_{0k}, w_{1k}, w_{2k}$ are
\begin{align*}
  w_{0k} &= \frac{\tau_0}{r_k} - \frac{\tau_k}{r_0}\\
  w_{1k} &= \frac{\tau_0}{1 - r_k} - \frac{\tau_k}{1 - r_0}\\
  w_{2k} &= \tau_k - \tau_0
\end{align*}
Since we have an equation for $k=1$ and $k=2$, we have a linear system of $k$ equations in two unknowns.
In matrix form, 
\[
\left[\begin{array}{cc}
w_{01} & w_{11}\\
w_{02} & w_{12}
\end{array}\right]\left[\begin{array}{c}
B_{0}\\
B_{1}
\end{array}\right]=\left[\begin{array}{c}
-w_{21}\\
-w_{22}
\end{array}\right]
\]
In this notation, assumption A5 is simply $w_{01}w_{12}-w_{02}w_{11}\neq0$ which ensures the system has a unique solution.
Now, given that $B_{0}=(1-b_{1})b_{0}$ and $B_{1}=(1-b_{0})b_{1}$, we can solve for the mis-classification rates as follows.
First, rearranging the definition of $B_1$ gives $b_0 = 1 - B_1/b_1$.
Substituting this into the definition of $B_0$, we see that $B_0 = (1 - B_1/b_1)(1-b_1)$, yielding the following quadratic equation
\begin{equation*}
  b_1^2 - (1 - B_0 + B_1) b_1 + B_1 = 0
\end{equation*}
The discriminant of this quadratic equation is
\begin{align*}
  (1 - B_0 + B_1)^2 - 4 B_1 &=\left[ 1 - b_0\left( 1 - b_1 \right) + \left( 1 - b_0 \right)b_1 \right]^2 - 4b_1(1 - b_0) \\
  &= \left( 1 - b_0 + b_1 \right)^2 - 4b_1(1 - b_0) \\
  &= \left[ (1 - b_0 - b_1) + 2b_1 \right]^2 - 4b_1(1 - b_0)\\
  &= \left[(1 - b_0 - b_1)^2 - 4b_1(1- b_0)\right] - 
  4(b_1 - b_0 b_1) \\
  &= (1 -b_0 - b_1)^2
\end{align*}
Since the discriminant is necessarily positive, the solutions are always real.
Moreover, we have established that
\begin{equation*}
  1 - b_0 - b_1 = \pm \sqrt{(1 - B_0 + B_1)^2 - 4 B_1 }
\end{equation*}

Solving, we find that
\[
b_{1}=\frac{1}{2}\left[1-B_{0}+B_{1}\pm\sqrt{(1-B_{0}+B_{1})^{2}-4B_{1}}\right]
\]
and since $b_0 = B_0/(1- b_1)$
\[
b_{0}=\frac{B_{0}}{1-\frac{1}{2}\left[1-B_{0}+B_{1}\pm\sqrt{(1-B_{0}+B_{1})^{2}-4B_{1}}\right]}
\]
To determine the nature of these solutions, we re-express the discriminant: \begin{align*}
  (1 - B_0 + B_1)^2 - 4 B_1 &=\left[ 1 - b_0\left( 1 - b_1 \right) + \left( 1 - b_0 \right)b_1 \right]^2 - 4b_1(1 - b_0) \\
  &= \left( 1 - b_0 + b_1 \right)^2 - 4b_1(1 - b_0) \\
  &= \left[ (1 - b_0 - b_1) + 2b_1 \right]^2 - 4b_1(1 - b_0)\\
  &= \left[(1 - b_0 - b_1)^2 - 4b_1(1- b_0)\right] - 
  4(b_1 - b_0 b_1) \\
  &= (1 -b_0 - b_1)^2
\end{align*}
Since the discriminant is necessarily positive, the solutions are always real.
But more importantly, we have established that
\begin{equation*}
  1 - \alpha_0 - \alpha_1 = \pm \sqrt{(1 - B_0 + B_1)^2 - 4 B_1 }
\end{equation*}


\end{proof}


\section{Notes on Mahajan (2006)}

\todo[inline]{It should be easy to adapt the proof for Lewbel (2007) above to get the result from Mahajan (2006). 
We just need to replace Lewbel's assumption that $\tau^*(Z,V)$ does not depend on $V$ with the stronger assumption that $\tau^*(Z,V)$ and $h_0^*(Z,V)$ do not depend on $V$. Then we only need a binary $V$. Does Mahajan's assumption imply that $\tau_k = \tau_\ell$? (Rather than simply that $\tau_k^* = \tau_\ell$?)}


Mahajan (2006) considers regression models of the form
\begin{equation}
  E\left[ y - g(x^*,z) \right] = 0
\end{equation}
where $x^*$ is an unobserved binary regressor and $z$ is a $d_z \times 1$ vector of control regressors.
Rather than $x^*$ we observe a noisy measure $x$ called the ``surrogate'' and an additional variable $v$ that acts, in essence, as an instrumental variable.
Since $v$ does not, strictly speaking, meet the traditional requirements for an instrument, Mahajan refers to it as an ``instrument-like variable'' or ILV for short.
Throughout the paper, Mahajan assumes that $v$ is binary although he claims that the same idea applies to arbitrary discrete variables.
The paper considers two main cases: one in which $x^*$ is assumed to be exogenous, and another in which it is not.

\subsection{The Case of Exogenous $x^*$}
The first is based on the restriction
\begin{equation}
  E\left[ y - g(x^*,z)\left. \right| x^*,x, z, v \right] = 0
  \label{eq:mahajan1}
\end{equation}

\subsection{The Case of Endogenous $x^*$}
While the preceding case required $x^*$ to be exogenous, Mahajan claims (page 640) that his identification results can be extended to account for endogeneity provided that one is willing to restrict attending to additively separable models of the form
\begin{equation}
  y = g^*(x^*,z) + \varepsilon
  \label{eq:mahajan10}
\end{equation}
In this case, the ILV is assumed to satisfy the usual instrumental variables mean independence assumption
\begin{equation}
E\left[ \varepsilon|z,v \right]=0 
\label{eq:ivassump}
\end{equation}
and Equation \ref{eq:mahajan1} is replaced by
\begin{equation}
  E\left[ y|x^*,x,z,v \right] = E\left[ y|x^*,z \right]
  \label{eq:mahajan11}
\end{equation}

Unfortunately, Mahajan's proof is incorrect and the model in Equation \ref{eq:mahajan10} is unidentified.
The mistake stems from a false analogy with the identification proof in the case of exogenous $x^*$.
In A.2 Mahajan argues, correctly, that under \ref{eq:mahajan10}--\ref{eq:mahajan11} knowledge of the mis-classification rates is sufficient to identify the model even when $x^*$ is endogenous.
He then appeals to Theorem 1 to argue that the mis-classification rates are indeed identified.
The proof of Theorem 1, however, depends crucially on the assumption that $x^*$ is exogenous.
Without this assumption, the mis-classification rates are unidentified, as we now show
For ease of exposition we consider the case without covariates.
Equivalently, one can interpret all of the expressions that follow as implicitly conditioned on $z=z_a$ where $z_a$ is a value in the support of $z$.\footnote{Because the covariates are held fixed throughout the proof of Mahajan's Theorem 1, there is no loss of generality.}

Without covariates we can write 
\begin{equation}
  y = \alpha + \beta x^* + \varepsilon
  \label{eq:nocovariates}
\end{equation}
where $\alpha=g^*(0)$ and $\beta = g^*(1) - g^*(0)$ and the mis-classification rates become $\eta_0 =  P(x=1|x^*=0)$ and $\eta_1 =  P(x=0|x^*=1)$.
Now define
\begin{equation}
m_{jk} = E\left[ \varepsilon|x^*=j, v = k \right]
  \label{eq:mdef}
\end{equation}

\section{Notation and Results for New Draft -- May 8, 2017}


Additively separable model
\[
  y = m(T^*,\mathbf{x})+\varepsilon
\]
where $\varepsilon$ is a mean-zero error term, $T^*$ is an endogenous binary regressor of interest and $\mathbf{x}$ is a vector of exogenous controls.
Since $T^*$ is binary, we can re-write this as linear in $T^*$ conditional on $\mathbf{x}$
\begin{align*}
  y &= c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon\\
  \beta(\mathbf{x}) &= m(1,\mathbf{x}) - m(0,\mathbf{x})\\
  c(\mathbf{x}) &= m(0,\mathbf{x})
\end{align*}
Goal is to use an instrumental variable $z$ to identify $\beta(\mathbf{x})$ when we observe not $T^*$ but a mis-measured binary surrogate $T$. 
Define
\begin{align*}
  \alpha_0(\mathbf{x},z) &= \mathbb{P}\left(T=1|T^*=0,\mathbf{x},z  \right)\\
  \alpha_1(\mathbf{x},z) &= \mathbb{P}\left(T=0|T^*=1,\mathbf{x},z  \right)
\end{align*}
Identification will only rely on two values for $z$ so throughout the remainder of the paper we assume that $z$ is binary and takes on values $0$ and $1$.

\begin{assump} \mbox{}
  \begin{enumerate}[(i)] 
    \item $y = c(\mathbf{x}) + \beta(\mathbf{x})T^* + \varepsilon$ where $\mathbb{E}[\varepsilon]=0$.
    \item $\alpha_0(\mathbf{x},z) = \alpha_1(\mathbf{x})$,   $\alpha_1(\mathbf{x},z) = \alpha_1(\mathbf{x})$
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon|\mathbf{x},z, T^*]$
    \item $\mathbb{E}\left[ T^*|\mathbf{x},z=0 \right] \neq \mathbb{E}\left[ T^*|\mathbf{x},z=1 \right]$
    \item $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) \neq 1$ ($T$ is relevant for $T^*$)
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z] = 0$
  \end{enumerate}
\end{assump}

\begin{assump} \mbox{}
  \label{assump:2ndMoment}
  \begin{enumerate}[(i)] 
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^2|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z] = \mathbb{E}[\varepsilon^2|\mathbf{x}]$
  \end{enumerate}
\end{assump}

\begin{assump} \mbox{}
  \begin{enumerate}[(i)] 
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^2|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z] = \mathbb{E}[\varepsilon^3|\mathbf{x}]$
  \end{enumerate}
\end{assump}

\begin{assump}
  $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) <1$ ($T$ is positively correlated with $T^*$)
\end{assump}


\paragraph{Notation}

\begin{align}
  \label{eq:theta1_def}
  \theta_1(\mathbf{x}) &= \beta(\mathbf{x})\left[ 1 - \left\{ \alpha_0(\mathbf{x}) + \mathbf{\alpha}_1(\mathbf{x}) \right\} \right]^{-1}\\
  \label{eq:theta2_def}
  \theta_2(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^2 \left[ 1 + \left\{\alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})\right\}\right] \\
  \label{eq:theta3_def}
  \theta_3(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^3\left[ \left( 1 - \left\{\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x})\right\} \right)^2 + 6\alpha_0(\mathbf{x})\left\{ 1 - \alpha_1(\mathbf{x}) \right\} \right]
\end{align}

\begin{align*}
  \mbox{Cov}(y,z|\mathbf{x}) - \mbox{Cov}(T,z|\mathbf{x}) \theta_1(\mathbf{x}) &= 0\\
  \mbox{Cov}(y^2,z|\mathbf{x}) - 2\mbox{Cov}(yT,z|\mathbf{x}) \theta_1(\mathbf{x}) + \mbox{Cov}(T,z|\mathbf{x})\theta_2(\mathbf{x}) &= 0\\
  \mbox{Cov}(y^3,z|\mathbf{x}) - 3 \mbox{Cov}(y^2T,z|\mathbf{x}) \theta_1(\mathbf{x}) + 3\mbox{Cov}(yT,z|\mathbf{x}) \theta_2(\mathbf{x}) - \mbox{Cov}(T,z|\mathbf{x}) \theta_3(\mathbf{x})&= 0
\end{align*}

\begin{align*}
  q(\mathbf{x}) &= \mathbb{P}(z=1|\mathbf{x})\\
  \pi(\mathbf{x}) &= \mbox{Cov}(T,z|\mathbf{x})\\
  \eta_j(\mathbf{x}) &= \mbox{Cov}(y^j,z|\mathbf{x})\\
  \tau_j(\mathbf{x}) &= \mbox{Cov}(Ty^j,z|\mathbf{x})
\end{align*}

\begin{align*}
 \eta_1(\mathbf{x}) &= \pi(\mathbf{x})\theta_1(\mathbf{x}) \\
  \eta_2(\mathbf{x}) &=  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x}) \\
  \eta_3(\mathbf{x}) &=  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})
\end{align*}



\begin{lem}[Various bits and pieces, e.g.\ $\eta_1$ equation]
  
\end{lem}


\begin{lem}[Lemma for Appendix only with Bayes' Rule]
For mis-classification probabilities
\begin{align*}
  P(T^*=1|T=1, Z=k) &= P(T=1 | T^*=1) \left(\frac{p_k^*}{p_k}\right) = (1 - \alpha_1)\left( \frac{p_k^*}{p_k} \right)\\
  P(T^*=1|T=0, Z=k) &= P(T=0 | T^*=1) \left(\frac{p_k^*}{1 - p_k}\right) = \alpha_1 \left( \frac{p_k^*}{1 - p_k} \right)\\
  P(T^*=0|T=1, Z=k) &= P(T=1 | T^*=0) \left(\frac{1 - p_k^*}{p_k}\right) = \alpha_0 \left( \frac{1 - p_k^*}{p_k} \right)\\
  P(T^*=0|T=0, Z=k) &= P(T=0 | T^*=0) \left(\frac{1 - p_k^*}{1 - p_k}\right) = (1 - \alpha_0)\left( \frac{1 - p_k^*}{1 - p_k} \right)
\end{align*}
\end{lem}

\begin{thm}[Non-identification Result]
  \label{thm:nonident}
\end{thm}
\begin{proof}[Proof of Theorem \ref{thm:nonident}]
\end{proof}

\begin{pro}
  Under Assumptions ???, $\eta_2(\mathbf{x}) =  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x})$.
  \label{pro:eta2}
\end{pro}
\begin{proof}[Proof of Proposition \ref{pro:eta2}]
  Since $z$ is binary, $\mbox{Cov}(w,z|\mathbf{x}) = q(\mathbf{x})\left[ 1 - q(\mathbf{x}) \right] \Delta_z\mathbb{E}(w|\mathbf{x},z)$ for any $w$, where we define $\Delta_z \mathbb{E}(w|\mathbf{x},z) \equiv \mathbb{E}(w|\mathbf{x},z=1) - \mathbb{E}(w|\mathbf{x},z=0)$.
Hence it suffices to show that 
\[
  \Delta_z\mathbb{E}(y^2|\mathbf{x},z) = 2 \Delta_z \mathbb{E}(Ty|\mathbf{x},z) \theta_1(\mathbf{x}) - \Delta_z\mathbb{E}(T|\mathbf{x},z)\theta_2(\mathbf{x}).
\]
By iterated expectations 
  \begin{align*} 
      \mathbb{E}\left( y^2|\mathbf{x},z \right) &= \mathbb{E}(y^2|\mathbf{x},T^*=0,z)\mathbb{P}(T^*=0|\mathbf{x},z) + \mathbb{E}(y^2|\mathbf{x},T^*=1,z)\mathbb{P}(T^*=1|\mathbf{x},z)\\
      \mathbb{E}\left( Ty|\mathbf{x},z \right) &= \mathbb{E}(Ty|\mathbf{x},T^*=0,z)\mathbb{P}(T^*=0|\mathbf{x},z) + \mathbb{E}(Ty|\mathbf{x},T^*=1,z)\mathbb{P}(T^*=1|\mathbf{x},z)
    \end{align*}
    and since $y = c(\mathbf{x}) + \beta(\mathbf{x})T^* + \varepsilon$,
  \begin{align*}
    \mathbb{E}\left( y^2|\mathbf{x},T^*=1,z \right) &= \left[c(\mathbf{x}) + \beta(\mathbf{x})\right]^2 + 2\left[ c(\mathbf{x}) + \beta(\mathbf{x}) \right]\mathbb{E}\left( \varepsilon|\mathbf{x},T^*=1,z \right) + \mathbb{E}\left( \varepsilon^2|\mathbf{x},T^*=1,z \right)\\
  \mathbb{E}\left( Ty|\mathbf{x},T^*=1,z \right)&= \left[c(\mathbf{x}) + \beta(\mathbf{x})\right]\mathbb{E}(T|\mathbf{x},T^*=1,z) + \mathbb{E}(T\varepsilon|\mathbf{x},T^*=1,z)\\
    \mathbb{E}\left( y^2|\mathbf{x},T^*=0,z \right) &= c(\mathbf{x})^2 + 2c(\mathbf{x})\mathbb{E}\left( \varepsilon|\mathbf{x},T^*=0,z \right) + \mathbb{E}\left( \varepsilon^2|\mathbf{x},T^*=0,z \right)\\
    \mathbb{E}\left( Ty|\mathbf{x},T^*=0,z \right)&= c(\mathbf{x})\,\mathbb{E}(T|\mathbf{x},T^*=0,z) + \mathbb{E}(T\varepsilon|\mathbf{x},T^*=0,z).
  \end{align*}


  using Assumptions ?? and \ref{assump:2ndMoment}.
\end{proof}
  

\begin{pro}
  Under Assumptions ???, $\eta_3(\mathbf{x}) =  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})$
  \label{pro:eta3}
\end{pro}
\begin{proof}[Proof of Proposition \ref{pro:eta3}]
\end{proof}

\begin{cor}
  \label{cor:theta_ident}
  Suppose that $\eta_1(\mathbf{x}), \eta_2(\mathbf{x}), \eta_3(\mathbf{x}), \tau_1(\mathbf{x}), \tau_2(\mathbf{x})$ and $\pi(\mathbf{x})$ are identified and that $\pi(\mathbf{x})\neq 0$.
  Then, under the conditions of Lemmas ???, $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$, and $\theta_3(\mathbf{x})$ are identified.
\end{cor}
\begin{proof}[Proof of Corollary \ref{cor:theta_ident}]
  For ease of notation we suppress dependence on $\mathbf{x}$ throughout this argument.
  Propositions ??? yield a triangular linear system of equations in $\theta_1, \theta_2, \theta_3$, namely
\begin{align*}
 \pi\theta_1 &= \eta_1 \\
 2\tau_1 \theta_1 - \pi \theta_2 &= \eta_2 \\
 3\tau_2 \theta_1 - 3\tau_1 \theta_2 + \pi\theta_3 &= \eta_3
\end{align*}
By inspection, the determinant of the system is $-\pi^3$ so a unique solution exists if $z$ is correlated with $T^*$.
In particular,
\begin{align*}
  \theta_1 &= \eta_1 / \pi\\
  \theta_2 &= 2\tau_1\eta_1/\pi^2 - \eta_2/\pi\\
  \theta_3 &= \eta_3/\pi - 3(\tau_2\eta_1 + \tau_1\eta_2)/\pi^2 + 6\tau_1\eta_1/\pi^3.
\end{align*}
\end{proof}


\begin{thm}[Identification of $\beta$, $\alpha_0$, $\alpha_1$]
  \label{thm:main_ident}
\end{thm}
\begin{proof}[Proof of Theorem \ref{thm:main_ident}]
  For ease of notation we suppress dependence on $\mathbf{x}$ throughout this argument.
  So long as $\beta \neq 0$, we can rearrange Equations \ref{eq:theta2_def} and \ref{eq:theta3_def} to obtain 
  \begin{align}
    \label{eq:quadraticA}
  A &= \theta_2/\theta_1^2 = 1 + (\alpha_0 - \alpha_1)  \\
  \label{eq:quadraticB}
  B &= \theta_3/\theta_1^3 = (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1)
  \end{align}
  Equation \ref{eq:quadraticA} gives $(1 - \alpha_1)= A - \alpha_0$.
  Hence $(1 - \alpha_0 - \alpha_1) = A - 2\alpha_0$ and $\alpha_0(1 - \alpha_1) = \alpha_0(A - \alpha_0)$.
  Substituting into Equation \ref{eq:quadraticB} and simplifying, $(A^2 - B) + 2A \alpha_0 - 2\alpha_0^2=0$.
  Substituting for $\alpha_0$ analogously yields a quadratic in $(1 - \alpha_1)$ with \emph{identical} coefficients.
It follows that one root of $(A^2-B) + 2Ar - 2r^2=0$ is $\alpha_0$ and the other is $1 - \alpha_1$.
Solving,
  \begin{equation}
    r = \frac{A}{2} \pm \sqrt{3 A^2 - 2B} = \frac{1}{\theta_1^2}\left(\frac{\theta_2}{2} \pm  \sqrt{3\theta_2^2  - 2\theta_1 \theta_3}\right).
  \end{equation}
By Equations \ref{eq:theta2_def} and \ref{eq:theta3_def}, 
  \begin{align*}
    3\theta^2_2 - 2\theta_1 \theta_3 &= 3 \left[ \theta_1^2 \left( 1 + \alpha_0 - \alpha_1 \right) \right]^2 - 2 \theta_1 \left\{ \theta_1^3 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0 (1 - \alpha_1) \right] \right\} \\
    &= \theta_1^4 \left\{ 3(1 + \alpha_0 - \alpha_1)^2 - 2 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1) \right] \right\}.
  \end{align*}
Expanding the first term we find that 
  \begin{align*}
    3(1 + \alpha_0 - \alpha_1)^2 
    %&= 3\left[ 1 + (\alpha_0 - \alpha_1) \right]^2 
    &= 3\left[ 1 + 2(\alpha_0 - \alpha_1) + (\alpha_0 - \alpha_1)^2 \right]\\
    &= 3 + 6\alpha_0 - 6\alpha_1 + 3 \alpha_0^2 + 3 \alpha_1^2 - 6\alpha_0\alpha_1 
  \end{align*}
and expanding the second 
  \begin{align*}
    2\left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0(1 - \alpha_1) \right]
    %&= 2\left\{ \left[ 1 - (\alpha_0 + \alpha_1) \right]^2 + 6\alpha_0(1 - \alpha_1) \right\}\\
  &=2\left[ 1 - 2(\alpha_0 + \alpha_1) + (\alpha_0 + \alpha_1)^2 + 6\alpha_0 - 6 \alpha_0 \alpha_1 \right]\\
    %&= 2 - 4(\alpha_0 + \alpha_1) + 2(\alpha_0 + \alpha_1)^2 + 12\alpha_0 - 12 \alpha_0 \alpha_1 \\
    &= 2 + 8\alpha_0 - 4\alpha_1 + 2\alpha_0^2 +  2\alpha_1^2 - 8 \alpha_0 \alpha_1.
  \end{align*}
Therefore
  \begin{align*}
    3\theta^2_2 - 2\theta_1 \theta_3 
    %&= \theta_1^4 \left\{ 3(1 + \alpha_0 - \alpha_1)^2 - 2 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1) \right] \right\}\\
    &= \theta_1^4 \left\{ 1 - 2 \alpha_0 - 2 \alpha_1 + \alpha_0^2 - \alpha_1^2 + 2\alpha_0 \alpha_1 \right\}\\
    &= \theta_1^4 \left[ (1 - \alpha_0 - \alpha_1)^2 \right]
  \end{align*}
which is strictly greater than zero since $\theta_1 \neq 0$ and $\alpha_0 + \alpha_1 \neq 0$.
It follows that both roots of the quadratic are real.
Moreover, $3\theta_2^2/\theta_1^4 - 2\theta_3/\theta_1^3$ identifies $(1 - \alpha_0 - \alpha_1)^2$.
Substituting into Equation \ref{eq:theta1_def}, it follows that $\beta$ is identified up to sign.
If $\alpha_0 + \alpha_1 < 1$ then $\mbox{sign}(\beta) = \mbox{sign}(\theta_1)$ so that both the sign and magnitude of $\beta$ are identified.
If $\alpha_0 + \alpha_1 < 1$ then $1 - \alpha_1 > \alpha_0$ so $(1 - \alpha_1)$ is the larger root of $(A^2 - B) + 2Ar - 2r^2=0$ and $\alpha_0$ is the smaller root.
\end{proof}




\end{document}
