\documentclass[12pt]{article}
\usepackage{../frankstyle}

\title{Notes for Paper on Mis-measured, Binary, Endogenous Regressors}
\author{Francis J.\ DiTraglia \& Camilo Garc\'{i}a-Jimeno}

\begin{document}

\maketitle

\section{Model and Notation}

\paragraph{Probabilities}
\begin{eqnarray*}
p^*_{tk} &=& P(T^*=t, Z=k)\\
p_{tk} &=& P(T=t, Z=k)\\
p^*_k &=& P(T^* = 1|Z = k)\\
p_k &=& P(T = 1|Z = k)\\
q &=& P(Z = 1)
\end{eqnarray*}

\begin{eqnarray*}
  p^*_{00} &=& P(T^* = 0|Z=0)P(Z=0) = (1 - p_0^*)(1 - q) =  \left( \frac{1 - p_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{10} &=& P(T^* = 1|Z=0)P(Z=0) = p_0^*(1 - q) =  \left( \frac{p_0 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{01} &=& P(T^* = 0|Z=1)P(Z=1) = (1 - p_1^*)q =  \left( \frac{1 - p_1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) q\\
  p^*_{11} &=& P(T^* = 1|Z=1)P(Z=1) = p_1^*(1 - q)  =  \left( \frac{p_1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)q
\end{eqnarray*}

\paragraph{CDFs}
For $t, Z \in \left\{ 0,1 \right\}$ define
\begin{eqnarray*}
F_{tk}^*(\tau) &=&  P(Y \leq \tau|T^* = t, Z = k) \\
F_{tk}(\tau) &=&  P(Y \leq \tau|T = t, Z = k)\\
F_k(\tau) &=& P(Y \leq \tau | Z=k) 
\end{eqnarray*}
Note that the second two are observed for all $t,k$ while the first is never observed since it depends on the unobserved RV $T^*$.


\section{Weakest Bounds on $\alpha_0, \alpha_1$}
Assume that $\alpha_0 + \alpha_1 < 1$ that $T$ is independent of $Z$ conditional on $T^*$.
These standard assumptions turn out to yield informative bounds on $\alpha_0$ and $\alpha_1$ without \emph{any further restrictions of any kind}.
In particular, we assume nothing about the validity of the instrument $Z$ and nothing about the relationship between the mis-classification error and the outcome $Y$: we impose only that the mis-classification error rates do not depend on $z$ and that the mis-classification is not so bad that $1 - T$ is a better measure of $T^*$ than $T$. 

By the Law of Total Probability and the assumption that $T$ is conditionally independent of $Z$ given $T^*$,
\begin{eqnarray*}
  p_k &=& P(T=1|Z=k,T^*=0) (1 - p_k^*) + P(T=1|Z=k,T^*=1)p_k^*\\
  &=& P(T=1|T^*=0)(1 - p_k^*) + P(T=1|T^*=1)p_k^*\\
  &=& \alpha_0 (1 - p_k^*) + (1 - \alpha_1) p_k^*\\
  &=& \alpha_0 +(1 - \alpha_0 - \alpha_1) p_k^* 
\end{eqnarray*}
and similarly 
\begin{eqnarray*}
  1 - p_k &=& P(T=0|Z=k,T^*=0) (1 - p_k^*) + P(T=0|Z=k,T^*=1)p_k^*\\
  &=& P(T=0|T^*=0)(1 - p_k^*) + P(T=0|T^*=1)p_k^*\\
  &=& (1 - \alpha_0)(1 - p_k^*) + \alpha_1 p_k^*\\
  &=& \alpha_1 + (1 - p_k^*)(1 - \alpha_0 - \alpha_1)
\end{eqnarray*}
and hence
\begin{eqnarray*}
  p_k - \alpha_0 &=& (1 - \alpha_0 - \alpha_1)p_k^*\\
  (1 - p_k) - \alpha_1 &=& (1 - \alpha_0 - \alpha_1)(1 - p_k^*)
\end{eqnarray*}
Now, since $p_k^*$ and $(1 - p_k^*)$ are probabilities they are between zero and one which means that the sign of $p_k - \alpha_0$ as well as that of $(1 - p_k) - \alpha_1$ are both determined by that of $1 - \alpha_0 - \alpha_1$.
Accordingly, provided that $1 - \alpha_0 - \alpha_1 < 1$, we have
\begin{eqnarray*}
  \alpha_0 &<& p_k\\
  \alpha_1 &<& (1 - p_k)
\end{eqnarray*}
so long as $p_k^*$ does not equal zero or one, which is not a realistic case for any example that we consider.
Since these bounds hold for all $k$, we can take the tightest bound over all values of $Z$.

\todo[inline]{Important: using these to bound $\beta$ gives $\beta \in [\mbox{ITT}, \mbox{Wald}]$.}

\section{Stronger Bounds for $\alpha_0, \alpha_1$}
Now suppose we add the assumption that $T$ is conditionally independent of $Y$ given $T^*$. 
This is essentially the non-differential measurement error assumption although it is slightly stronger than the version used by Mahajan (2006) who assumes only conditional mean independence.
This assumption allows us to considerably strengthen the bounds from the preceding section by exploiting information contained in the conditional distribution of $Y$ given $T$ and $Z$.
The key ingredient is a relationship that we can derive between the unobservable distributions $F_{tk}^*$ and the observable distributions $F_{tk}$ using this new conditional independence assumption.
To begin, note that by Bayes' rule we have
\begin{eqnarray*}
  P(T^*=1|T=1, Z=k) &=& P(T=1 | T^*=1) \left(\frac{p_k^*}{p_k}\right) = (1 - \alpha_1)\left( \frac{p_k^*}{p_k} \right)\\
  P(T^*=1|T=0, Z=k) &=& P(T=0 | T^*=1) \left(\frac{p_k^*}{1 - p_k}\right) = \alpha_1 \left( \frac{p_k^*}{1 - p_k} \right)\\
  P(T^*=0|T=1, Z=k) &=& P(T=1 | T^*=0) \left(\frac{1 - p_k^*}{p_k}\right) = \alpha_0 \left( \frac{1 - p_k^*}{p_k} \right)\\
  P(T^*=0|T=0, Z=k) &=& P(T=0 | T^*=0) \left(\frac{1 - p_k^*}{1 - p_k}\right) = (1 - \alpha_0)\left( \frac{1 - p_k^*}{1 - p_k} \right)
\end{eqnarray*}
Now, by the conditional independence assumption
\begin{eqnarray*}
  P(Y\leq \tau|T^* = 0, T=t , Z = k) = P(Y \leq \tau|T^*=0, Z =k) = F_{0k}^*(\tau)\\
  P(Y\leq \tau|T^* = 1, T=t , Z = k) = P(Y \leq \tau|T^*=1, Z =k) = F_{1k}^*(\tau)
\end{eqnarray*}
Finally, putting everything together using the Law of Total Probability, we find that
\begin{eqnarray*}
  (1 - p_k) F_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  p_k F_{1k}(\tau) = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$.
Defining the shorthand 
\begin{eqnarray*}
  \widetilde{F}_{0k}(\tau)&\equiv& (1 - p_k) F_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\equiv& p_k F_{1k}(\tau) 
\end{eqnarray*}
this becomes
\begin{eqnarray}
  \label{eq:F0kTilde}
  \widetilde{F}_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  \label{eq:F1kTilde}
  \widetilde{F}_{1k}(\tau)  = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray}
Now, solving Equation \ref{eq:F0kTilde} for $p_k^* F_{1k}^*(\tau)$ we have
\[
  p_{k}^* F_{1k}^*(\tau) = \frac{1}{\alpha_1}\left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) (1 - p_k^*) F_{0k}^*(\tau)\right]
\]
Substituting this into Equation \ref{eq:F1kTilde},
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{\alpha_1} \left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) ( 1 - p_k^*)F_{0k}^*(\tau) \right]\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \alpha_0 - \frac{(1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \frac{\alpha_0 \alpha_1 - (1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ (1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ 1 - \alpha_1 -  \alpha_0 }{\alpha_1} \right]\left( \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) F_{0k}^*(\tau)\\
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) -  \frac{1 - p_k - \alpha_1}{\alpha_1}  F_{0k}^*(\tau)
  \label{eq:F1kTildeAlpha1}
\end{equation}
Equation \ref{eq:F1kTildeAlpha1} relates the observable $\widetilde{F}_{1k}(\tau)$ to the mis-classification error rate $\alpha_1$ and the unobservable CDF $F_{0k}^*\left( \tau \right)$.
Since $F_{0k}^*(\tau)$ is a CDF, however, it lies in the interval $\left[ 0,1 \right]$.
Accordingly, substituting $0$ in place of $F^*_{0k}(\tau)$ gives 
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_leq_a1}
\end{equation}
while substituting $1$ gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau) - \frac{1 - p_k - \alpha_1}{\alpha_1}
  \label{eq:F1ktilde_F0kTilde_geq_a1}
\end{equation}
Rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a1}
\begin{eqnarray*}
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau)\\
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& \widetilde{F}_{0k}(\tau) - \alpha_1 \widetilde{F}_{0k}(\tau)\\
 \alpha_1 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right]&\leq& \widetilde{F}_{0k}(\tau) 
\end{eqnarray*}
since $\alpha_1 \in [0,1]$ and therefore
\begin{equation}
  \alpha_1  \leq \frac{\widetilde{F}_{0k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = (1 - p_k) \left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
  \label{eq:Alpha1_Bound1}
\end{equation}
since $\widetilde{F}_{1k}(\tau) + \widetilde{F}_{1k}(\tau) \geq 0$.
Proceeding similarly for Equation \ref{eq:F1ktilde_F0kTilde_geq_a1},
\begin{eqnarray*}
  \alpha_1 \widetilde{F}_{1k}(\tau) &\geq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau) - (1 - p_k - \alpha_1)\\
  \alpha_1 \left[\widetilde{F}_{1k}(\tau) + \widetilde{F}_{0k}(\tau) - 1\right] &\geq& \widetilde{F}_{0k}(\tau) - (1 - p_k)\\
  -\alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\geq& -\left[1 - \widetilde{F}_{0k}(\tau) - p_k \right]\\
  \alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\leq& 1 - \widetilde{F}_{0k}(\tau) - p_k 
\end{eqnarray*}
Now since $\widetilde{F}_{1k}(\tau) = p_k F_{1k}(\tau) \leq p_k$ and $\widetilde{F}_{0k}(\tau) = (1 - p_k) F_{0k}(\tau) \leq (1 - p_k)$ it follows that $1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \geq 0$ and hence
\begin{equation}
  \alpha_1 \leq \frac{1 - \widetilde{F}_{0k}(\tau) - p_k}{1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau)} = (1 - p_k) \left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha1_Bound2}
\end{equation}
The bounds given in Equations \ref{eq:Alpha1_Bound1} and \ref{eq:Alpha1_Bound2} relate $\alpha_1$ to observable quantities \emph{only} and hold for all values of $\tau$ for which their respective denominators are non-zero.
Moreover, these bounds hold for any value $k$ that the instrument takes on.

We can proceed similarly for $\alpha_0$.
First solve Equation \ref{eq:F0kTilde} for $(1 - p_k^*)F^*_{0k}(\tau)$:
\[
  (1 - p_k^*)F^*_{0k}(\tau) = \frac{1}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right]
\]
and then substitute into Equation \ref{eq:F1kTilde}:
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \frac{\alpha_0}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right] + (1 - \alpha_1) p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ (1 - \alpha_1) - \frac{\alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{(1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_0}   \right] \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} F_{1k}^*(\tau) 
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) +  \frac{p_k - \alpha_0}{1 - \alpha_0} F_{1k}^*(\tau) 
\end{equation}
Now we can again obtain two bounds by substituting the smallest and largest possible values of $F_{1k}^*(\tau)$.
Substituting zero gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_geq_a0}
\end{equation}
while substituting one gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \frac{p_k - \alpha_0}{1 - \alpha_0}
  \label{eq:F1ktilde_F0kTilde_leq_a0}
\end{equation}
Now, rearranging Equation \ref{eq:F1ktilde_F0kTilde_geq_a0}, 
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \widetilde{F}_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] 
\end{eqnarray*}
since $1 - \alpha_0 \geq 0$.
Therefore,
\begin{equation}
  \alpha_0 \leq \frac{\widetilde{F}_{1k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{F_{1k}(\tau)}{F_{k}(\tau)}\right]
  \label{eq:Alpha0_Bound1}
\end{equation}
since $\left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] \geq 0$.
Similarly, rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a0}
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\leq& \alpha_0\widetilde{F}_{0k}(\tau) + p_k - \alpha_0\\
  \widetilde{F}_{1k}(\tau) - p_k &\leq& \alpha_0\left[\widetilde{F}_{0k}(\tau)  + \widetilde{F}_{1k}(\tau) - 1 \right] \\
  -\left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\leq& -\alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] \\
  \left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\geq& \alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] 
\end{eqnarray*}
Therefore
\begin{equation}
\alpha_0 \leq \frac{1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)}{1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha0_Bound2}
\end{equation}

\paragraph{Putting Everything Together} 
For all $k$ we have
\begin{equation}
  \alpha_0 \leq p_k \min_\tau\left\{\left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{1k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq p_k 
\end{equation}
\begin{equation}
  \alpha_1 \leq (1 - p_k) \min_\tau \left\{\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{0k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq (1 - p_k) 
\end{equation}
Note that these bounds can only improve upon those derived in the previous section since the ratio of CDFs tends to one as $\tau \rightarrow \infty$.
To derive these tighter bounds we have made no assumption regarding the relationship between $Z$ and the error term $\varepsilon$.
These bounds use only the assumption that $\alpha_0 + \alpha_1 < 1$, and the assumption that $T$ is conditionally independent of $Z,Y$ given $T^*$.
Notice that that the bounds are related.
In particular,
\[
  p_k \left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
\]
and 
\[
p_k \left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
\]


\section{Even Stronger Bounds on $\alpha_0, \alpha_1$}
Try applying the stochastic dominance conditions from our simulation study.

\section{Independent Instrument}
Assume that $Z \perp U$.  
The model is $Y = \beta T^* + U$ and
\[ F_{U}(\tau) = P(U \leq\tau) = P(Y - \beta T^* \leq \tau)\]
but if $Z$ is independent of $U$ then it follows that
\begin{eqnarray*}
F_U(\tau) &=&  F_{U|Z=k}(\tau) = P(U\leq \tau |Z=k) = P(Y  - \beta T^* \leq \tau |Z=k)\\
&=&  P(Y \leq \tau |T^* = 0, Z = k)(1 - p_k^*) + P(Y\leq \tau + \beta| T^* = 1, Z = k)p_k^* \\
&=& (1 - p_k^*) F^*_{0k}(\tau) + p_k^* F^*_{1k}(\tau + \beta)
\end{eqnarray*} 
for all $k$ by the Law of Total Probability.
Similarly, 
\[ F_k(\tau) = (1 - p_k^*) F_{0k}^*(\tau)  + p_k^* F_{1k}^*(\tau)\]
and rearranging
\[  (1 - p_k^*) F_{0k}^*(\tau)  = F_k(\tau) - p_k^* F_{1k}^*(\tau)\]
Substituting this expression into the equation for $F_U(\tau)$ from above, we have
\[F_U(\tau) = F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]\]
for all $k$ and all $\tau$.
Evaluating at two values $k$ and $\ell$ in the support of $Z$ and equating 
\[ F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right] =  F_\ell(\tau) + p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right]\]
or equivalently
\begin{equation}
 F_k(\tau) - F_\ell(\tau) =  p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right] - p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]  
 \label{eq:CDFs1}
\end{equation}
for all $\tau$.
Now we simply need to re-express all of the ``star'' quantities, namely $p_k^*, p_\ell^*$ and $F_{1k}^*, F_{1\ell}^*$ in terms of $\alpha_0, \alpha_1$ and the \emph{observable} probability distributions $F_{1k}$ and $F_{1\ell}$ and observable probabilities $p_k, p_\ell$.
To do this, we use the fact that
\begin{eqnarray*}
  F_{0k}(\tau) &=& \frac{1 - \alpha_0}{1 - p_k} (1 - p^*_k)F_{0k}^*(\tau) + \frac{\alpha_1}{1 - p_k}p_k^* F_{1k}^*(\tau)\\ \\
  F_{1k}(\tau) &=& \frac{ \alpha_0}{p_k}(1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{p_k}p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$ by Bayes' rule.
Solving these equations,
\begin{equation*}
  p_k^* F_{1k}^*(\tau) = \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} p_k F_{1k}(\tau) - \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} (1 - p_k) F_{0k}(\tau) 
\end{equation*}
for all $k$.
Combining this with Equation \ref{eq:CDFs1}, we find that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Now, define
\[
  \Delta^\tau_{tk}(\beta) = F_{tk}(\tau + \beta) - F_{tk}(\tau) = E\left[ \frac{\mathbf{1}\left\{ T = t, Z = k \right\}}{p_{tk}}\left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right) \right]
\]
and note that we can express $F_k(\tau) - F_\ell(\tau)$ similarly as 
\[
  F_k(\tau)  - F_{\ell}(\tau) = E\left[ \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right) \right]
\]
Using this notation, we can write the preceding as
\begin{equation*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_{\ell}(\tau) \right] = \alpha_0\left[ (1 - p_k) \Delta^\tau_{0k}(\beta) - (1 - p_\ell) \Delta^\tau_{0\ell}(\beta) \right] - (1 - \alpha_0)\left[ p_k \Delta^\tau_{1k}(\beta) - p_\ell \Delta^\tau_{1\ell}(\beta) \right]
\end{equation*}
or in moment-condition form
\begin{align*}
   E\Bigg[ &(1 - \alpha_0 - \alpha_1) \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right)  - 
   \left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right)\Bigg\{ \\
   &\alpha_0 \bigg((1 - p_k)\frac{\mathbf{1}\left\{ T = 0, Z = k \right\}}{p_{0k}} - 
    (1 - p_\ell)\frac{\mathbf{1}\left\{ T = 0, Z = \ell \right\}}{p_{0\ell}}\bigg)\\
   &-(1 - \alpha_0) \bigg( p_k\frac{\mathbf{1}\left\{ T = 1, Z = k \right\}}{p_{1k}} - 
 p_\ell \frac{\mathbf{1}\left\{ T = 1, Z = \ell \right\}}{p_{1\ell}}\bigg) \Bigg\}\Bigg] = 0
\end{align*}
Each value of $\tau$ yields a moment condition.

\section{Special Case: $\alpha_0 = 0$}
In this case the expressions from above simplify to
\begin{align}
  (1 - \alpha_1)\left[ F_k(\tau) - F_\ell(\tau)\right] = \left[ p_\ell F_{1\ell}(\tau + \beta) 
 - p_k  F_{1k}(\tau+ \beta) 
 - p_\ell F_{1\ell}(\tau) 
 + p_k F_{1k}(\tau) \right]
 \label{eq:specialCDF}
\end{align}
for all $\tau$.
Now, provided that all of the CDFs are differentiable we have\footnote{There must be a way to generalize this using Lebesgue.}
\begin{align*}
  e^{i\omega \tau}(1 - \alpha_1)\left[f_k(\tau) - f_\ell(\tau)\right] = e^{i\omega \tau}\left[ p_\ell f_{1\ell}(\tau + \beta) - p_k  f_{1k}(\tau+ \beta) - p_\ell f_{1\ell}(\tau) + p_k f_{1k}(\tau) \right]
\end{align*}
where we have pre-multiplied both sides by $e^{i\omega \tau}$.
Finally, integrating both sides with respect to $\tau$ over $(-\infty, \infty)$, we have
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] = \left\{  \int_{-\infty}^{\infty} e^{i\omega \tau} \left[p_\ell f_{1\ell}(\tau + \beta) - p_k f_{1k}(\tau+ \beta)\right] \; d\tau - p_\ell \varphi_{1\ell}(\omega) + p_k \varphi_{1k}(\omega) \right\}
\end{align*}
where $\varphi_k$ is the conditional characteristic function of $Y$ given $Z=k$ and $\varphi_{1k}$ is the conditional characteristic function of $Y$ given $T=1, Z=k$.
Finally, 
\begin{align*}
  \int_{-\infty}^{\infty} e^{i\omega \tau} p_\ell f_{1\ell}(\tau + \beta) \; d\tau &=  e^{ i\omega \beta } p_\ell \int_{u = -\infty + \beta}^{u = \infty + \beta} e^{ i\omega u }f_{1\ell}(u)\; du \\
  &= e^{-i\omega \beta } p_\ell \varphi_{1\ell}(\omega)
\end{align*}
using the substitution $u = \tau + \beta$.
Changing subscripts, the same holds for $k$ and thus
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  e^{-i\omega \beta}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] +  \left[p_k \varphi_{1k}(\omega) -  p_\ell \varphi_{1\ell}(\omega)\right]
\end{align*}
which, after collecting terms, simplifies to
\begin{align}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  \left(e^{-i\omega \beta} - 1\right)\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] 
  \label{eq:CharacteristicSpecial}
\end{align}
for all $\omega$.  
Equation \ref{eq:CharacteristicSpecial} contains exactly the same information as Equation \ref{eq:specialCDF} but gives us a more convenient way to prove identification since $\beta$ enters in a simpler way.
Leibniz's formula for the $r$th derivative of a product of two functions $f$ and $g$ is:
\begin{align*}
  (fg)^{(r)} = \sum_{s=0}^r {r \choose s} f^{(s)}g^{(r-s)}
\end{align*}
where $f^{(r)}$ denotes the $r$th derivative of the function $f$ and $g^{(r-s)}$ denotes the $(r-s)$th derivative of the function $g$.
Applying this to the RHS, $R(\omega)$ of Equation \ref{eq:CharacteristicSpecial} gives
\begin{align*}
  \frac{d}{d\omega^r}R(\omega)
  &=  \sum_{s=0}^r {r \choose s} \frac{d}{d\omega^s}\left( e^{-i\omega\beta} - 1\right)\frac{d}{d\omega^{r - s}}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] \\
  &= \left( e^{-i\omega \beta} - 1 \right) \left[ p_\ell \varphi_{1\ell}^{(r)}(\omega) - p_k \varphi_{1k}^{r}(\varphi) \right] + e^{-i\omega\beta} \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(\omega) - p_k \varphi^{(r-s)}_{1k}(\omega) \right] 
\end{align*}
where we split off the $s=0$ term because our generic expression for the $s$th derivative of $(e^{-i\omega\beta} - 1)$ only applies for $s\geq 1$.
Evaluating at zero:
\begin{align*}
  \frac{d}{d\omega^r}R(0)
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Combining this with the LHS of Equation \ref{eq:CharacteristicSpecial}, also differentiated $r$ times and evaluated at zero, we have
\begin{align*}
  (1 - \alpha_1) \left[ \varphi_{k}^{(r)}(0) - \varphi_{\ell}^{(r)}(0) \right] 
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Now, recall that if $\varphi(\omega)$ is the characteristic function of $Y$ then $\varphi^{(r)}(0) = i^r E[Y^r]$ provided that the expectation exists where $\varphi^{(r)}$ denotes the $r$th derivative of $\varphi$.
The same applies for the conditional characteristic functions we consider here.
Hence, provided that the $r$th moments exist, 
\footnotesize
\begin{align*}
  i^r(1 - \alpha_1)\left\{ E[Y^r|Z=k] - E[Y^r|Z=\ell]\right\} = \sum_{s=1}^r {r \choose s} (-i\beta)^s i^{r-s}\left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
After simplifying the terms involving $i$ and cancelling them from both sides, 
\small
\begin{align*}
  (1 - \alpha_1)\left(E[Y^r|Z=k] - E[Y^r|Z=\ell]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
again provided that the moments exist.
Abbreviating the conditional expectations according to $E[Y^r|Z=k] = E_k[Y^r]$ and $E[Y^r|T=t,Z=k] = E_{tk}[Y^r]$, this becomes
\begin{equation}
  (1 - \alpha_1)\left(E_k[Y^r] - E_\ell[Y^r]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{r-s}\right] - p_k E_{1k}\left[ Y^{r-s}\right] \right)
  \label{eq:MomentsSpecial}
\end{equation}
Equation \ref{eq:MomentsSpecial} can be used to generate moment equations that are implied by the Equation \ref{eq:CharacteristicSpecial} and the equivalent representation in terms of CDFs: Equation \ref{eq:specialCDF}.
Assuming that the conditional first moments exist, we can evaluate Equation \ref{eq:MomentsSpecial} at $r=1$, yielding
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y] - E_\ell[Y]\right) &= \sum_{s=1}^1 {1 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{1-s}\right] - p_k E_{1k}\left[ Y^{1-s}\right] \right)\\
  &=  - \beta\left( p_\ell - p_k \right) 
\end{align*}
Rearranging, this gives us the expression for the probability limit of the Wald estimator
\begin{equation}
  \mathcal{W} \equiv \frac{E_{k}[Y]- E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_1} 
  \label{eq:WaldSpecial}
\end{equation}
Evaluating Equation \ref{eq:MomentsSpecial} at $r = 2$, we have
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y^2] - E_\ell[Y^2]\right) &= \sum_{s=1}^2 {2 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{2-s}\right] - p_k E_{1k}\left[ Y^{2-s}\right] \right)\\
  &= 2\beta\left( p_k E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta^2\left( p_k - p_{\ell} \right)
\end{align*}
Rearranging, we have
\begin{equation}
  E_k[Y^2] - E_\ell[Y^2] 
  =  \frac{\beta}{1 - \alpha_1}\left[2\left( p_k  E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta(p_k - p_\ell)\right]
  \label{eq:SpecialSquared}
\end{equation}
Substituting Equation \ref{eq:WaldSpecial}, we can replace $\beta/(1-\alpha_1)$ with a function of observables only, namely $\mathcal{W}$.
Solving, we find that 
\begin{align}
  \beta &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} 
  \label{eq:BetaSpecial}
\end{align}
This allows us to state low-level sufficient conditions for identification:
\begin{enumerate}[(a)]
  \item $\alpha_1 < 1$
  \item $p_k \neq p_\ell$ 
  \item $E_k[Y] \neq E_\ell[Y]$ 
  \item $E_{1k}[|Y|], E_{1\ell}[|Y|], E_k[|Y^2|], E_\ell[|Y^2|] < \infty$.
\end{enumerate}
Note that, although $\beta = 0$ is always a solution of Equation \ref{eq:specialCDF} this solution is ruled out by the assumption that $E_k[Y] \neq E_\ell[Y]$ via Equation \ref{eq:WaldSpecial}.
The mis-classification error rate $\alpha_1$ is likewise uniquely identified under these assumptions.
Substituting $\beta/\mathcal{W} = 1-\alpha_1$ into Equation \ref{eq:BetaSpecial}
\begin{align*}
  (1 - \alpha_1) &= \left\{ \frac{p_k - p_\ell}{E_k[Y] - E_\ell[Y]} \right\}\left\{\frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} \right\}\\
  &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} - (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\}
\end{align*}
and thus
\begin{align*}
  \alpha_1
  &= 1 + (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\} - \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} 
\end{align*}

\section{Identification in the General Case}

\section{Characteristic Functions}
Recall from above that in the general case an independent instrument combined with non-differential measurement error implies that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Using the same steps as in the preceding section, we can convert this expression into characteristic function form by differentiating each side, multiplying by $e^{i\omega\tau}$ and then integrating with respect to $\tau$, yielding
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left[ \varphi_k(\omega) - \varphi_{\ell}(\omega) \right] &= \alpha_0 \left\{ (1 - p_k)\left(e^{-i\omega\beta} - 1\right)\varphi_{0k}(\omega) - (1 - p_\ell)\left( e^{-i\omega\beta} - 1\right) \varphi_{0\ell}(\omega)  \right\}\\
  &\quad - (1 - \alpha_0) \left\{ p_k\left(e^{-i\omega\beta} - 1 \right)\varphi_{1k}(\omega) - p_\ell \left( e^{-i\omega\beta} - 1\right) \varphi_{1\ell}(\omega) \right\}
\end{align*}
which simplifies to
\begin{align*}
  \varphi_k(\omega) - \varphi_{\ell}(\omega) &= \left( e^{-i\omega\beta} - 1 \right)\left(\frac{\alpha_0\left[ (1 - p_k)\varphi_{0k}(\omega) - (1-p_\ell)\varphi_{0\ell}(\omega) \right]  - (1 - \alpha_0)\left[ p_k \varphi_{1k}(\omega) - p_\ell \varphi_{1\ell}(\omega) \right]}{1 - \alpha_0 - \alpha_1}\right).
\end{align*}
As above, we will differentiate both sides of this expression $r$ times and evaluate at $\omega = 0$.
Steps nearly identical to those given above yield
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left(  E_k[Y^r] - E_\ell[Y^r]\right) 
  &= \alpha_0 \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{ (1 - p_k) E_{0k}[Y^{r-s}] - (1 - p_\ell) E_{0\ell}[Y^{r-s}] \right\}\\
  &\quad - (1 - \alpha_0) \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{p_k E_{1k}[Y^{r-s}] - p_\ell E_{1\ell}[Y^{r-s}] \right\}
\end{align*}

\paragraph{First Moments}
Taking $r = 1$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left( E_k[Y] - E_{\ell}[Y] \right) = \beta (p_k - p_\ell)
\end{align*}
Simplifying,
\begin{equation}
  \mathcal{W} \equiv \frac{E_k[Y] - E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_0 - \alpha_1}
  \label{eq:Wald}
\end{equation}

\paragraph{Second Moments}
Now, taking $r = 2$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left( E_{k}[Y^2] - E_{\ell}[Y^2] \right) &=
  \alpha_0\left\{ \left[ (1 - p_k) E_{0k}[Y] - (1 - p_\ell) E_{0\ell} \right] - \beta^2\left( p_k - p_\ell \right) \right\}\\
  &\quad  -(1 - \alpha_0)\left\{ -2\beta\left( p_k E_{1k}[Y] - p_{\ell}E_{1\ell}[Y] \right) + \beta^2\left( p_k - p_\ell \right) \right\}\\
  &= -2\beta \alpha_0\left\{ (1 - p_k)E_{0k}[Y] - (1 - p_\ell) E_{0\ell}[Y] p_k E_{1k}[Y] + p_{\ell}E_{1\ell}[Y]\right\} \\ 
  &\quad +2\beta \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) 
  - (p_k - p_\ell)\beta^2\left( \alpha_0 + 1 - \alpha_0 \right)\\
  &= -2\beta\left\{ \alpha_0 \left( E_k[Y] - E_\ell[Y] \right) - \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) \right\} - \beta^2(p_k - p_\ell)
\end{align*}
Now, simplifying
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\beta \alpha_0 \left(\frac{E_k[Y]-E_k[Y]}{p_k - p_\ell}\right) + 2\beta \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) - \beta^2
\end{align*}
and substituting Equation \ref{eq:Wald} to eliminate $\beta$, this becomes
\small
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 (1 - \alpha_0 - \alpha_1)\mathcal{W}^2 + 2\mathcal{W}(1 - \alpha_0 - \alpha_1) \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) \\
  &\quad \quad - (1 - \alpha_0 - \alpha_1)^2 \mathcal{W}^2\\
  \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 \mathcal{W}^2 + 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2
\end{align*}
\normalsize
And thus, simplifying
\begin{align*}
  -2\alpha_0 \mathcal{W}^2 - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2 &= \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)- 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  \\
  \alpha_1  - \alpha_0   &= 1 +  \left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\mathcal{W}^2(p_k - p_\ell)} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{\mathcal{W}(p_k - p_\ell)} \right] 
\end{align*}
and therefore
\begin{equation}
  \alpha_1  - \alpha_0  = 1 +  (p_k - p_\ell)\left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\left( E_k[Y] - E_\ell[Y] \right)^2} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{E_k[Y] - E_\ell[Y]} \right] 
\end{equation}

\paragraph{``Product'' Moments}
Recall that in our initial draft of the paper we worked with moments such as $E[TY|Z=k], E[TY|Z=\ell]$ and $E[TY^2|Z=k], E[TY^2|Z=\ell]$.
In the notation of this document, we can express these quantities as follows:
\begin{align*}
  E[TY^r|z=k] &= E[TY^r|T=1,z=k]p_k + E[TY^r|T=0,z=k](1 - p_k)\\
  &= p_k E[Y^r|T=1,z=k] + 0\\
  &= p_k E_{1k}[Y^r]
\end{align*}
for any $r$. 
We will use this relationship to motivate some shorthand notation below.

\paragraph{Some Shorthand}
The notation above is becoming very cumbersome and we haven't even looked at the third moments yet! 
To make life easier, define the following: 
\begin{align*}
  \widetilde{y^r_{1k}} &= p_k E_{1k}[Y^r] \\
  \widetilde{y^r_{0k}} &= (1 - p_k) E_{1k}[Y^r] \\
  \Delta \overline{y^r} &= E_k[Y^r] - E_\ell[Y^r]\\
  \Delta \overline{Ty^r} &= p_k E_{1k}[Y^r] - p_\ell E_{1\ell}[Y^r] = \widetilde{y^r_{1k}} - \widetilde{y^r_{1k}}\\
  \mathcal{W} &= (E_k[Y] - E_\ell[Y]) / (p_k - p_\ell)
\end{align*}
for all $r$.
When no $r$ superscript is given this means $r=1$.
Note, moreover, that when $r =0$ we have $\widetilde{y_{1k}^0} = p_k$ and $\widetilde{y_{0k}^0} = (1 - p_k)$.
Thus $\Delta \overline{Ty^0} = p_k - p_\ell$.
In contrast, $\Delta y^0 = 0$.

Among other things, this notation will make it easier for us to link the derivations here to our earlier derivations from the first draft of the paper that used slightly different notation and did not work explicitly with the independence of the instrument.

\paragraph{Simplifying the Moment Equalities}
Using the final two pieces of notation defined in the preceding section, we can re-rewrite the collection of moment equalities arising from the characteristic function equations as
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left[\alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right) \right]
\end{align*}
Now, simplifying the terms in the square brackets,
\begin{align*}
  \alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)
  &= \alpha_0\left[ \left( \widetilde{y_{0k}^{r-s}} + \widetilde{y_{1k}^{r-s}} \right) - \left( \widetilde{y_{0\ell}^{r-s}} + \widetilde{y_{1\ell}^{r-s}} \right)  \right] - \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)\\
  &= \alpha_0\left( E_k[Y^{r-s}] - E_\ell[Y^{r-s}] \right) - \Delta \overline{Ty^{r-s}}\\
  &= \alpha_0 \Delta \overline{y^{r-s}} - \Delta\overline{Ty^{r-s}}
\end{align*}
and hence
\begin{align}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{r-s}} - \Delta\overline{Ty^{r-s}} \right) 
  \label{eq:MomentEqualitiesSimplified}
\end{align}

\paragraph{Third Moments}
Evaluating Equation \ref{eq:MomentEqualitiesSimplified} at $r=3$ 
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^3} 
  &= \sum_{s=1}^3 {3 \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{3-s}} - \Delta\overline{Ty^{3-s}} \right) \\
  &= -3\beta\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta^2\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^3 (p_k - p_\ell)
\end{align*}

\paragraph{Solving the System}
Using $\mathcal{W} = \beta/(1 - \alpha_0 - \alpha_1)$ we can re-write the third moment expression as follows
\begin{align*}
  \Delta \overline{y^3} &= -3\mathcal{W}\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta \mathcal{W}\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^2 \mathcal{W} (p_k - p_\ell)\\
  \frac{\Delta \overline{y^3}}{\mathcal{W} (p_k - p_\ell)} 
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} }{p_k - p_\ell}\right) \\
  \frac{\Delta \overline{y^3} - 3\mathcal{W}\Delta\overline{y^2T}}{\mathcal{W}(p_k - p_\ell)}
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2}  }{p_k - p_\ell}\right) 
\end{align*}
Now, translating the second moment equation into the shorthand notation defined above, we have


\paragraph{Simplifying the Characteristic Function Equation}
From above, we have
\begin{align*}
  \varphi_k(\omega) - \varphi_{\ell}(\omega) &= \left( e^{-i\omega\beta} - 1 \right)\left(\frac{\alpha_0\left[ (1 - p_k)\varphi_{0k}(\omega) - (1-p_\ell)\varphi_{0\ell}(\omega) \right]  - (1 - \alpha_0)\left[ p_k \varphi_{1k}(\omega) - p_\ell \varphi_{1\ell}(\omega) \right]}{1 - \alpha_0 - \alpha_1}\right).
\end{align*}
Using the fact that $\varphi_{k} = p_k \varphi_{1k} + (1 - p_k) \varphi_{0k}$, we can simplify this further, yielding
\[
(1 - \alpha_0 - \alpha_1) = \left( e^{-i\omega \beta} - 1 \right)\left[ \alpha_0 - \xi(\omega)\right] 
\]
where we define
\[
  \xi(\omega) \equiv \frac{\varphi_k(\omega) - \varphi_\ell(\omega)}{p_k \varphi_{1k}(\omega) - p_\ell\varphi_{1\ell}(\omega)}
\]
Now, re-arranging
\[
  (1 - \alpha_1) - \xi(\omega) = e^{-i\omega\beta}\left[ \alpha_0 - \xi(\omega) \right]  
\]
or equivalently
\[
  e^{i\omega\beta}\left[(1 - \alpha_1) - \xi(\omega)\right] =  \alpha_0 - \xi(\omega) 
\]
or
\[
  e^{i\omega \beta} = \frac{\alpha_0 - \xi(\omega)}{(1 - \alpha_1) - \xi(\omega)}
\]
provided the denominator does not vanish.
By taking differences or ratios evaluated at $\omega_1$ and $\omega_2$ we can eliminate $\beta$, $\alpha_0$ or $\alpha_1$ but it's not clear how or if we can prove identification in terms of a restriction on the characteristic functions.

Suppose we consider three values $\omega_1, \omega_2$ and $\omega_3$ for which that yield to distinct, non-zero values $\xi_1, \xi_2$ and $\xi_3$ of $\xi(\omega)$.
\begin{align*}
  e^{i\omega_1\beta}\left[ (1 - \alpha_1) - \xi_1 \right] - e^{i\omega_2\beta}\left[ (1 - \alpha_1) - \xi_2 \right] = \xi_2 - \xi_1
\end{align*}

\subsection{Simplifying the Characteristic CDF Equation}
Recall from above that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
We can simplify the RHS as follows
\begin{align*}
  \mbox{RHS} &= \alpha_0 \left\{ \left[ F_k(\tau + \beta) - F_\ell(\tau + \beta) \right] - \left[ F_k(\tau) - F_\ell(\tau) \right] \right\}\\
  &- \left\{ \left[ p_k F_{1k}(\tau + \beta) - p_\ell F_{1\ell}(\tau + \beta) \right]  - \left[ p_k F_{1k}(\tau) - p_\ell F_{1\ell}(\tau) \right]\right\}
\end{align*}
Now, define
\begin{align*}
  \Delta(\tau) &= F_k(\tau) - F_\ell(\tau)\\
  \widetilde{\Delta}_1(\tau) &= p_k F_{1k}(\tau) - p_\ell F_{1\ell}(\tau)
\end{align*}
Using this notation, our equation becomes
\[
  (1 - \alpha_0 - \alpha_1) \Delta(\tau) = \alpha_0 \left[ \Delta(\tau + \beta) - \Delta(\tau)\right] - \left[ \widetilde{\Delta}_1(\tau+\beta) - \widetilde{\Delta}_1(\tau)\right]
\]
which simplifies to
\[
    \widetilde{\Delta}_1(\tau+\beta) - \widetilde{\Delta}_1(\tau) = \alpha_0 \Delta(\tau + \beta) - (1 - \alpha_1) \Delta(\tau)
\]

\paragraph{Suppose $\alpha_0 = 0$:}
In this case we obtain
\[
  (1 - \alpha_1)  = \frac{ \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}
\]
Now, evaluating at two values of $\tau$ and taking differences, we find
\[
  \frac{ \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)} - 
  \frac{ \widetilde{\Delta}_1(\tau') - \widetilde{\Delta}_1(\tau' + \beta)}{\Delta(\tau')} = 0
\]

\paragraph{Suppose $\alpha_1 = 0$:}
In this case we obtain
\[
  \alpha_0 = \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + \Delta(\tau)}{\Delta(\tau + \beta)}
\]
Again, taking differences evaluated at two values of $\tau$,
\[
  \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + \Delta(\tau)}{\Delta(\tau + \beta)} - 
  \frac{\widetilde{\Delta}_1(\tau' + \beta) - \widetilde{\Delta}_1(\tau') + \Delta(\tau')}{\Delta(\tau' + \beta)} = 0
\]

\paragraph{Some Equations to Check Numerically}
We can use the same basic idea when either $\alpha_0$ or $\alpha_1$ is known but nonzero.
This isn't realistic in practice, but can be used to check our equations:
\begin{align*}
  \alpha_0 &= \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + (1 - \alpha_1) \Delta(\tau)}{\Delta(\tau + \beta)}\\ \\
  (1 - \alpha_1) &= \frac{\alpha_0 \Delta(\tau + \beta) + \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}
\end{align*}
As above, after substituting the true value of either $\alpha_1$ or $\alpha_0$, we can eliminate the remaining mis-classification probability by evaluating at two quantiles $\tau$, $\tau'$ and taking differences.
\todo[inline]{These appear to work just fine!}

\paragraph{What if $\alpha_0$ and $\alpha_1$ are both unknown?}
Suppose we take differences at two quantiles $\tau$ and $\nu$ to eliminate $\alpha_1$:
\begin{align*}
  \left[\frac{\alpha_0 \Delta(\tau + \beta) + \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}\right]
  - \left[\frac{\alpha_0 \Delta(\nu + \beta) + \widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0 \\ 
  \alpha_0 \left[ \frac{\Delta(\tau+ \beta)}{\Delta(\tau)} - \frac{\Delta(\nu + \beta)}{\Delta(\nu)} \right] - \left[ \frac{\widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau+ \beta)}{\Delta(\tau)} - \frac{\widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0
 \end{align*}

 \paragraph{The Equation that Didn't Work\ldots}
\[
  \frac{[\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau)] - [\widetilde{\Delta}_1(\tau' + \beta) - \widetilde{\Delta}_1(\tau')]}{\Delta(\tau + \beta) - \Delta(\tau' + \beta)}
- \frac{[\widetilde{\Delta}_1(\nu + \beta) - \widetilde{\Delta}_1(\nu)] - [\widetilde{\Delta}_1(\nu' + \beta) - \widetilde{\Delta}_1(\nu')]}{\Delta(\nu + \beta) - \Delta(\nu' + \beta)} = 0
 \]
 where $\Delta(\nu) = \Delta(\nu')$ and $\Delta(\tau) = \Delta(\tau')$.
%Now, recall that $\Delta(\tau)$ is a difference of CDFs. 
%This means that it its limits as $\tau \rightarrow +\infty$ and as $\tau \rightarrow -\infty$ both equal zero.
%If $Y$ is continuous, then it follows that for any $\tau$ we can always find a $\tau' \neq \tau$ such that $\Delta(\tau) = \Delta(\tau')$.
%Now if we take \emph{another} difference, between pairs  $(\tau, \nu)$ and $(\tau', \nu')$ such that $\Delta(\tau) = \Delta(\tau')$ and $\Delta(\nu) = \Delta(\nu')$, the $\alpha_0$ term disappears:
%\begin{align*}
%  \left[ \frac{\widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau+ \beta)}{\Delta(\tau)} - \frac{\widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0
% \end{align*}
%But so long as $\widetilde{\Delta}_1(\tau) \neq \widetilde{\Delta}_1(\tau')$ and  $\widetilde{\Delta}_1(\nu) \neq \widetilde{\Delta}_1(\nu')$ the equation itself does not vanish and can be used to solve for $\beta$.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{New Results from September 2016}

\subsubsection{Relationship between observed and unobserved CDFs}
Let
\begin{align*}
F^*_{tk}(\tau) &= P(Y \leq \tau|T^*=t, z_k)\\
F_{tk}(\tau) &= P(Y \leq \tau|T=t, z_k)
\end{align*}
Now, by the assumption of non-differential measurement error,
\begin{align*}
  p_k F_{1k}(\tau) &= (1 - \alpha_1) p_k^* F_{1k}^*(\tau) + \alpha_0 (1 - p_k^*)F_{0k}^*(\tau)\\
  (1 - p_k) F_{0k}(\tau) &= \alpha_1 p_k^* F_{1k}^*(\tau) + (1 - \alpha_0) (1 - p_k^*)F_{0k}^*(\tau)
\end{align*}
Solving the linear system as above, we find that
\begin{align*}
  F_{0k}^*(\tau) &= F_{0k}(\tau) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ F_{0k}(\tau) - F_{1k}(\tau) \right]\\
  F_{1k}^*(\tau) &= F_{1k}(\tau) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ F_{1k}(\tau) - F_{0k}(\tau) \right]\\
\end{align*}

\subsection{Can we relax the measurement error assumptions?}
Suppose that we continue to assume that $P(Y|T^*,T,z) = P(Y|T^*,z)$ but relax the assumption that $P(T|T^*,z) = P(T|T^*)$.
Define:
\begin{align*}
  \alpha_{0k} &= P\left( T=1|T^*=1, z_k \right)\\
  \alpha_{1k} &= P\left( T=1|T^*=0, z_k \right)
\end{align*}
As before, the Wald estimator converges in probability to
\[
  \mathcal{W} = \frac{E[Y|z_k]-E[Y|z_\ell]}{p_k - p_\ell}
\]
but the relationship between $p_1 - p_0$ and the unobserved $p^*_1 - p^*_0$ changes.
By the law of total probability
\begin{align*}
  p_k &= P(T=1|z_k) = P(T=1|T^*=1,z_k)P(T^*=1|z_k) + P(T=1|T^*=0,z_k)P(T^*=0|z_k)\\
  &= (1 - \alpha_{1k})p_k^* + \alpha_{0k}(1 - p^*_k) = p_k^*(1 - \alpha_{0k} - \alpha_{1k}) + \alpha_{0k}
\end{align*}
and thus
\[
  p_k^* = \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}},
  \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_{1k}}{1 - \alpha_{0k} - \alpha_{1k}}.
\]
Thus, we have
\begin{align*}
  p^*_k - p^*_\ell &= \left( \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}} \right) - \left( \frac{p_0 - \alpha_{0\ell}}{1 - \alpha_{0\ell} - \alpha_{1\ell}} \right)\\
  &= \frac{\left( p_k - \alpha_{0k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right) - \left( p_0 - \alpha_{0\ell} \right)\left( 1 - \alpha_{0k} - \alpha_{1k} \right)}{\left( 1 - \alpha_{0k} - \alpha_{1k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right)}
\end{align*}



\subsection{Is there a LATE interpretation of our results?}
Let $J \in \left\{ a, c, d, n \right\}$ index an individual's \emph{type}: always-taker, complier, defier, or never-taker.
Let $\pi_a, \pi_c, \pi_d, \pi_n$ denote the population proportions of always-takers, compliers, defiers, and never-takers.
The unconfounded type assumption is $P(J=j|z=1) = P(J=j|z=0)$.
Combined with the law of total probability, this gives
\begin{align*}
  p^*_1 &= P(T^*=1|z=1) = \pi_a + \pi_c \\
  1 - p^*_1 &= P(T^*=0|z=1) = \pi_d + \pi_n \\
  p^*_0 &= P(T^*=1|z=0) = \pi_d + \pi_a \\
  1-p^*_0 &= P(T^*=0|z=0) = \pi_n + \pi_c 
\end{align*}
Imposing no-defiers, $\pi_d = 0$, these expressions simplify to
\begin{align*}
  p^*_1 &=  \pi_a + \pi_c \\
  1 - p^*_1 &=  \pi_n \\
  p^*_0 &=  \pi_a \\
  1-p^*_0 &=  \pi_n + \pi_c 
\end{align*}
Solving for $\pi_c$, we see that
\begin{align*}
  \pi_c &= p_1^* - p_0^*\\
  \pi_a &= p_0^*\\
  \pi_n &= 1 - p_1^*
\end{align*}

Now, let $Y(1)$ indicate the potential outcome when $T^*=1$ and $Y(0)$ indicate the potential outcome when $T^*=0$.
The standard LATE assumptions (no defiers, mean exclusion, unconfounded type) imply
\begin{align*}
  \mathbb{E}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{E}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{E}\left[ Y(1)|J=c \right] \\
  \mathbb{E}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=n \right]\\
  \mathbb{E}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{E}\left[ Y(1)|J=a \right]\\
  \mathbb{E}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{E}\left[ Y(0)|J=n \right]
\end{align*}



\subsubsection{LATE Version of Theorem 2 from the Draft}
\begin{align*}
  \Delta\overline{yT} &= \mathbb{E}\left( yT|z=1 \right) - \mathbb{E}\left( yT|z=0 \right) \\
  &= (1 - \alpha_1) \left[ p_1^* \mathbb{E}\left( y|T^*=1, z=1 \right) - p_0^* \mathbb{E}\left(y|T^*=1, z=0\right) \right] \\
  & \; \; \quad \quad + \alpha_0 \left[ (1 - p_1^*)\mathbb{E}\left( y|T^*=0, z=1\right) - (1 - p_0^*)\mathbb{E}\left(y|T^*,z=0 \right) \right]
\end{align*}
So we find that
\begin{align*}
  \Delta\overline{yT} &= (p_1^* - p_0^*)\left\{ (1 - \alpha_1) \mathbb{E}\left[ Y(1)|J=c \right] - \alpha_0\mathbb{E}\left[ Y(0)|J=c \right] \right\}\\
  &= (1 - \alpha_1) \left\{ \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1} (p_1 - p_0) \right\} + (p_1  - p_0) \mathbb{E}\left[ Y(0)|J=c \right]
\end{align*}
Recall that the analogous expression in the homogeneous treatment effect case is
\begin{align*}
  \Delta\overline{yT} &= (1 - \alpha_1) \mathcal{W} (p_1 - p_0) + \mu_{10}^*\\
  &= (1 - \alpha_1) \left(\frac{\beta}{1 - \alpha_0 - \alpha_1}\right) (p_1 - p_0) + (p_1 - \alpha_0)m_{11}^* - (p_0 - \alpha_0)m_{10}^*
\end{align*}
while the expression for the difference of variances is 
\begin{align*}
  \Delta\overline{y^2} &= \beta \mathcal{W}(p_1 - p_0) + 2\mathcal{W} \mu_{10}^*
\end{align*}
From above we see that the analogue of $\mu_{10}^*$ in the heterogeneous treatment effects setting is $(p_1 - p_0)E\left[ Y(0)|J=c \right]$ and since the LATE is $\mathbb{E}\left[ Y(1) - Y(0) |J=c\right]$, the analogue of $\mathcal{W}$ is
\[
  \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1}
\]
so \emph{if} we could establish that 
\[
  \Delta\overline{y^2} =  \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0) |J=c \right]
\]
in the heterogeneous treatment effects case, the proof of Theorem 2 would go through immediately.
Now, if we assume an exclusion restriction on the \emph{second} moment of $y$ an argument almost identical to the standard LATE derivation gives
\[
  \Delta\overline{y^2} = \frac{\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right]}{p_1^* - p_0^*} = \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right] 
\]
so we see that the necessary and sufficient condition for our proof to go through is 
\[
  \mathbb{E}\left[ Y^2(1) - Y^2(0)|J=c \right] = \mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0)|J=c \right]
\]
Rearranging, this in turn is equivalent to
\[
  \mbox{Var}\left[ Y(1)|J=c \right] = \mbox{Var}\left[ Y(0)|J=c \right]
\]


\subsection{Partial Identification Under Independence Assumption}
Suppose we only make the LATE independence assumption $Y(T^*,z) = Y(T^*)$ rather than the conditional independence assumption $P(Y<\tau|T^*,z_k) = P(Y<\tau|T^*,z_\ell)$.
Then we still obtain
\begin{align*}
  \mathbb{P}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{P}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{P}\left[ Y(1)|J=c \right] \\
  \mathbb{P}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=n \right]\\
  \mathbb{P}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{P}\left[ Y(1)|J=a \right]\\
  \mathbb{P}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{P}\left[ Y(0)|J=n \right]
\end{align*}
From above, we also know that
\begin{align*}
  P(Y|T^*=0,z_k) &= P(Y|T=0, z_k) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ P(Y|T=0,z_k) - P(Y|T=1,z_k) \right]\\
  P(Y|T^*=1,z_k) &= P(Y|T=1, z_k) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ P(Y|T=1,z_k) - P(Y|T=0,z_k) \right]
\end{align*}
The notation is getting a bit unwieldy so let $\pi^*_{tk}(y)= P(Y=y|T^*=t,z_k)$ and similarly define $\pi_{tk}(y) = P(Y=y|T=t,z_k)$.
Using this new notation, we have
\begin{align*}
  (1 - p_k - \alpha_1) \pi^*_{0k}(y) &= (1 - p_k - \alpha_1) \pi_{0k}(y) + \alpha_1 p_k \left[ \pi_{0k}(y) - \pi_{1k}(y) \right]\\
  (p_k - \alpha_0) \pi_{1k}^*(y) &= (p_k - \alpha_0) \pi_{1k}(y) + \alpha_0 (1 - p_k)\left[ \pi_{1k}(y) - \pi_{0k}(y) \right]
\end{align*}
Writing these out for all values of $k$,
\begin{align*}
  (p_1 - \alpha_0) \pi_{11}^*(y) &= (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  (1 - p_0 - \alpha_1) \pi^*_{00}(y) &= (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0) \pi_{10}^*(y) &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) \pi^*_{01}(y) &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
Similarly, using the fact that $p_k^* = (p_k - \alpha_0)/(1 - \alpha_0 - \alpha_1)$,  
\begin{align*}
  \pi^*_{11}(y) &= \left( \frac{p_0 - \alpha_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=a \right] + \left( \frac{p_1 - p_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=c \right]\\
  \pi^*_{00}(y) &= \left( \frac{p_1 - p_0}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1 - \alpha_1}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=n \right]\\
  \pi^*_{10}(y) &= P\left[ Y(1)|J=a \right]\\
  \pi^*_{01}(y) &= P\left[ Y(0)|J=n \right] 
\end{align*}
or equivalently,
\begin{align*}
  (p_1 - \alpha_0)\pi^*_{11}(y) &= \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right]\\
  (1 - p_0 - \alpha_1)\pi^*_{00}(y) &=\left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right]\\
  (p_0 - \alpha_0)\pi^*_{10}(y) &= (p_0 - \alpha_0)P\left[ Y(1)|J=a \right]\\
  (1 - p_1 - \alpha_1)\pi^*_{01}(y) &= (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] 
\end{align*}
Equating,
\begin{align*}
  \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
and substituting the third and fourth equalities into the first and second we obtain
\footnotesize
\begin{align*}
   (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Simplifying and re-arranging,
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y) + (1 - p_1)\pi_{01}(y) - (1 - p_0) \pi_{00}(y) }{p_1 - p_0} \right] \\ 
  P\left[Y(0) =y|J=c \right] &= \left[ \frac{(1 - p_0)\pi_{00}(y) - (1 - p_1)\pi_{01}(y)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{(1 - p_0) \pi_{00}(y) - (1 - p_1)\pi_{01}(y) + p_0 \pi_{10}(y) - p_1 \pi_{11}(y)}{p_1 - p_0} \right] \\
  P\left[Y(1) = y|J=a \right] &=  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  P\left[ Y(0) = y|J=n \right] &=  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Notice that the first two equations can be simplified as follows
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{P(Y = y,T=1|z=1) - P(Y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y =y|z=0)}{p_1 - p_0} \right] \\ 
  P\left[Y(0) = y|J=c \right] &= \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] 
\end{align*}
\normalsize
Now, since probabilities must be between zero and one, we obtain the bounds
\begin{align*}
  0 &\leq \left[ \frac{P(Y = y,T=1|z=1) - P(Y = y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y = y|z=0)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] \leq 1 
\end{align*}
\normalsize
which we abbreviate 
\begin{align*}
  0 &\leq \left[ \frac{\Delta P(Y=y,T=1)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \alpha_1 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] - \left[\frac{ \Delta P(Y=y,T=0)}{p_1 - p_0} \right] \leq 1 
\end{align*}
where
\begin{align*}
  \Delta P(Y=y) &= P(Y=y|z=1) - P(Y=y|z=0)\\
  \Delta P(Y=y, T=t) &= P(Y=y, T=t|z=1) - P(Y=y,T=t|z=0).
\end{align*}
To manipulate these bounds, we need to know the sign of $R = \Delta P(Y=y)/(p_1 - p_0)
$. 
Presumably this will be positive for most values of $y$, but it could be negative.
\paragraph{Case I: $R$ is positive.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)}\\
  \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)}
\end{align*}

\paragraph{Case II: $R$ is negative.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} \\
  \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)}
\end{align*}
Note that we \emph{two-sided} bounds for the misclassification probabilities.
These may be trivial in some cases, but I don't think it's obvious that they always will be.
\todo[inline]{Do these bounds have anything to do with the testability of the LATE assumptions? That is, do we get a lower bound for measurement error \emph{precisely when} we would otherwise violate a testable LATE assumption?}

Note that we also obtain bounds from the potential outcome distributions of always-takers and never-takers, namely 
\begin{align*}
  0 &\leq  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right] \leq 1\\
  0&\leq  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right] \leq 1
\end{align*}
but these are redundant.
From the assumption of non-differential measurement error, we already have 
\begin{align*}
  \pi_{0k}^* &= \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \\
  \pi_{1k}^* &= \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) 
\end{align*}
for all $k$ as given at the beginning of this section.
These expressions imply
\begin{align*}
 0 &\leq \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \leq 1 \\
  0 &\leq \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) \leq 1
\end{align*}
Re-arranging, we have
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \pi_{0k} + \alpha_1 p_k \left( \pi_{0k} - \pi_{1k} \right) \leq 1 - p_k - \alpha_1 \\
 0 &\leq p_k \pi_{1k}- \alpha_0\pi_{1k} + \alpha_0 (1 - p_k) \left( \pi_{1k} - \pi_{0k} \right) \leq p_k - \alpha_0
\end{align*}
and thus
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \left[(1 - p_k)\pi_{0k} + p_k \pi_{1k} \right] \leq 1 - p_k - \alpha_1 \\
  0 &\leq p_k \pi_{1k} - \alpha_0\left[p_k\pi_{1k} + (1 - p_k)\pi_{0k}\right] \leq p_k - \alpha_0
\end{align*}
Now consider the first inequality.
Re-arranging the right-hand side we obtain
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)(1 - \pi_{0k})}{1 - \left[ (1 - p_k)\pi_{0k} + p_k \pi_{1k} \right]} = (1 - p_k) \left[ \frac{P(Y=0|T=0, z=k)}{P(Y=0|z=k)} \right]
\end{align*}
and re-arranging the left-hand side we find
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)\pi_{0k}}{(1 - p_k)\pi_{0k} + p_k \pi_{1k}} = (1 - p_k) \left[ \frac{P(Y=1|T=0,z=k)}{P(Y=1|z=k)} \right]
\end{align*}
For the second inequality, the left-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k \pi_{1k}}{p_k \pi_{1k} + (1 - p_k)\pi_{0k}} = p_k \left[ \frac{P(Y=1|T=1,z=k)}{P(Y=1|z_k)} \right] 
\end{align*}
while the right-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k (1 - \pi_{1k})}{1 - \left[ p_k \pi_{1k} + (1 - p_k) \pi_{0k} \right]} = p_k \left[ \frac{P(Y=0|T=1,z=k)}{P(Y=0|z=k)} \right]
\end{align*}
These are analogous to our CDF bounds from above although they may not be tighter than the bounds 
\[
  \alpha_0 \leq p_k, \quad \alpha_1 \leq (1 - p_k)
\]
because we cannot argue, as we did above, about a limit in which the ratio of CDFs approaches one.
As before, however, we can take the tightest bound over $k = 0, 1$.

\subsection{Bounding the LATE}
Even if we didn't know anything about $\alpha_0$ and $\alpha_1$ beyond the fact that they are probabilities, it looks like we could still bound the LATE.
I think we can do this without using the independence of the instrument, that is only using the mean exclusion restriction.
Write out the LATE expressions with the $\alpha_0$ and $\alpha_1$ in them and them just plug in zero and one.
Could then tighten the bounds by imposing additional assumptions to get bounds for $\alpha_0$ and $\alpha_1$, from weakest to strongest.
If you have an independent instrument, you also get bounds for the outcome distributions.
Need to think some more about this\ldots


\subsection{Stochastic Dominance Conditions}
What if we imposed a stochastic ordering, e.g.\ $Y(1) > Y(0)$ for compliers?
Presumably this would give joint bounds for $\alpha_0$ and $\alpha_1$ from the LATE expressions from above.
Alternatively, perhaps one would choose to impose an ordering on the $Y(0)$ distributions for compliers versus never-takers or the $Y(1)$ distributions for the compliers versus always-takers.
This might be interesting in situations where one is concerned that the assumption we need for identification does not in fact hold and should give additional bounds.


\section{Outline For New Draft}
\begin{enumerate}
  \item Introduction / Literature Review
    \begin{enumerate}
      \item Why is this an important question?
        \begin{itemize}
          \item Treatments of interest in economics usually endogenous and often binary.
          \item Randomized encouragement designs are common in applied work.
          \item Treatment status is often self-reported.
          \item This problem is much more challenging that people realize.
        \end{itemize}
      \item Why are we different from Ura?
        \begin{itemize}
          \item Main difference is that we, in line with the existing literature, study the case of non-differential measurement error. This allows us to obtain point identification under certain assumptions.
          \item In contrast, Ura considers arbitray forms of mis-classification but as a consequence presents only partial identification results.
          \item Second, while we do provide results for LATE in Section blah, we mainly focus on additively separable model in which heterogeneity is captured by observed covariates while Ura considers only a LATE model. (And also doesn't allow for covariates.) 
        \end{itemize}
    \end{enumerate}
  \item Mahajan/Lewbell-style Assumptions 
    \begin{enumerate}
      \item Setup and Assumptions:
        \begin{itemize}
          \item Homogenous treatment effect model (additively separable)
          \item Conditional mean version of non-differential measurement error assumption.
          \item Conditional mean independence for IV.
        \end{itemize}
      \item Show that the model is not identified, regardless of (discrete) support of IV.
      \item Derive sharp bounds for $\alpha_0, \alpha_1$ and treatment effect.
      \item Show that second and third-moment independence for IV identifies this model? Maybe this isn't interesting in and of itself?
    \end{enumerate}
  \item Independence Assumption
    \begin{enumerate}
      \item Motivation
    \begin{itemize}
      \item Showed above that stronger assumptions are needed for identification, but the additional moment restrictions seem a bit artificial.
      \item When instruments are derived from economic theory that yields conditional mean independence only, we wouldn't want to use them.
      \item They would make sense, however, in an an RCT or natural experiment.
      \item The whole point in these settings is \emph{not} to rely on functional form assumptions. It would be strange to say that $z$ is an instrument for $y$ but not $\log y$.
      \item This points towards an \emph{independence} assumption for the instrument.
      \item Can make a similar argument for measurement error: seems strange to assume that $T$ is non-differential for $y$ but not $\log y$.
    \end{itemize}
      \item Sharp Bounds for $\alpha_0$ and $\alpha_1$ without valid instrument  
        \begin{itemize}
          \item Assume ``independence'' version of non-differential measurement error.
          \item Derive CDF bounds.
        \end{itemize}
      \item Conditional Independence for Instrument
        \begin{itemize}
          \item Exactly what assumptions do we need here? 
          \item Characteristic functions.
          \item Identification conditions?
          \item Overidentifying restrictions? Test model?
        \end{itemize}
    \end{enumerate}
  \item LATE Model
    \begin{enumerate}
      \item Introduction
        \begin{itemize}
          \item Most of the existing mis-classification literature focuses on a homogeneous treatment effects model.
          \item What if we don't have an additively separable model?
          \item These results complement Ura because we work under the assumption of non-differential measurement error while he asks what can be learned when one is unwilling to make any assumptions about the form of the mis-classification.
        \end{itemize}
      \item Mahajan/Lewbel Setup
        \begin{itemize}
          \item Presumably the partial identification results go through for a LATE.
          \item The second and third moment conditions would require restrictions on form of heterogeneity.
            These would seem to be satisfied by a generalized Roy model.
        \end{itemize}
      \item Independence Assumptions
        \begin{itemize}
          \item Presumably the CDF bounds go through as before but need to state exact form of independence assumption in terms of potential outcomes.
          \item Kitagawa-style independence assumption for IV: $Y(T^*,z) = Y(T^*)$. This gives bounds for all quantile treatment effects.
          \item Stochastic dominance conditions?
        \end{itemize}
    \end{enumerate}
  \item Estimation / Inference
  \item Simulation Study
  \item Empirical Examples
    \begin{itemize}
      \item Try to look at a number of examples under different assumptions to illustrate both point and partial identification results. Don't forget about Oreopoulous: the sample size is so huge that inference isn't a major concern.
    \end{itemize}
\end{enumerate}

\section{Weak Identification}

\subsection{Moment Equations}
First we write the moment equations in a more familiar GMM-style form.
\paragraph{First Moment Condition:}
This is simply the IV moment condition: 
\[
  \mbox{Cov}(y,z)/ \mbox{Cov}(T,z) = \beta/(1 - \alpha_0 - \alpha_1)
\]
Rearranging gives a more ``canonical'' GMM form:
\[
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right) \mbox{Cov}(T,z) = 0
\]

\paragraph{Second Moment Condition:}
The equations used to identify $(\alpha_0 - \alpha_1)$ in the paper are 
\begin{align*}
  \mu_{k\ell}^* &= (p_k - \alpha_0) m_{1k}^* - (p_\ell - \alpha_0) m_{1\ell}^* \\
  \Delta \overline{y^2} &= \beta \mathcal{W} (p_k - p_\ell) + 2 \mathcal{W} \mu_{k\ell}^*\\
  \Delta \overline{yT} &= (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) + \mu_{k\ell}^*
\end{align*}
Re-arranging the third equation,
$\mu_{k\ell}^* = \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell)$.
Substituting into the second equation, 
\begin{align*}
  \Delta\overline{y^2} &= \mathcal{W}\left[ \beta(p_k - p_\ell) + 2 \mu_{k\ell}^* \right]\\
  &= \mathcal{W}\left\{ \beta(p_k - p_\ell) + 2\left[ \Delta\overline{yT} - (1 - \alpha_1)\mathcal{W}(p_k - p_\ell) \right]  \right\}\\
  &= \mathcal{W} \left\{ (p_k - p_\ell)\left[ \beta - 2(1 - \alpha_1)\mathcal{W} \right] + 2\Delta\overline{yT} \right\}
\end{align*}
Now, substituting $\mathcal{W} = \beta/(1 - \alpha_0 - \alpha_1)$
\begin{align*}
  \Delta\overline{y^2}&= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{\beta (p_k - p_\ell)\left[ 1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right] + 2 \Delta\overline{yT} \right\}\\
  &= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\Delta\overline{yT} - \beta(p_k - p_\ell)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\}
\end{align*}
We now write this in a more standard form.
Let $w$ be any random variable. 
Then,
\begin{align*}
  \mbox{Cov}(w,z) &= E(wz) - E(w)E(z) = \left[ 1 \times E(w|z=1)q + 0 \times E(w|z=0)(1 - q) \right] - E(w)q\\
  &= q E(w|z=1) - qE(w) = q E(w|z=1) - q\left[ E(w|z=1)q + E(w|z=0)(1 - q) \right]\\
  &= q\left[ E(w|z=1)(1 - q) + E(w|z=0)(1 - q)\right]\\
  &= q(1-q)\left[ E(w|z=1) - E(w|z=0) \right]
\end{align*}
Using this fact, we can express the quantities that appear in the second moment equality in terms of covariances as follows
\[
  \Delta\overline{y^2} = \frac{\mbox{Cov}(y^2,z)}{q(1 - q)}, \quad
  \Delta\overline{yT} = \frac{\mbox{Cov}(yT,z)}{q(1 - q)}, \quad
  (p_k - p_\ell) = \frac{\mbox{Cov}(T,z)}{q(1 - q)}
\]
leading to
\[\frac{\mbox{Cov}(y^2,z)}{q(1-q)}= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{\frac{2\mbox{Cov}(yT,z)}{q(1-q)} - \beta \frac{\mbox{Cov}(T,z)}{q(1-q)}\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\}
\]
Or, multiplying through by $q(1-q)$ and re-arranging,
\[\mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\} = 0
\]

\paragraph{Third Moment Condition:}
  \begin{align*}
    \Delta\overline{y^3} &= \beta^2 [\mathcal{W} (p_k - p_\ell)]  + 3 \beta [\mathcal{W} \mu_{k\ell}^*] + 3 \mathcal{W} \lambda^*_{k\ell}\\
    \Delta\overline{y^2T} &=  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) + 2(1-\alpha_1)\mathcal{W}\mu_{k\ell}^* + \lambda_{k\ell}^*
  \end{align*}
Combine with $\mu_{k\ell}^* = \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell)$ from the derivation of the second moment equation from above.

\subsection{Simple Special Case: $\alpha_0 = 0$}
Suppose that $\alpha_0$.
Then the model is identified using the first and second moment equalities, which simplify to
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 -\alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \left(\frac{\beta}{1 - \alpha_1}\right)\left[2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z) \right] &= 0
\end{align*}
In this simple special case, it is easy to solve for $\beta$ by substituting the first moment condition into the second:
\[
  \beta = \frac{2 \mbox{Cov}(yT,z)}{\mbox{Cov}(T,z)} - \frac{\mbox{Cov}(y^2,z)}{\mbox{Cov}(y,z)}
\]
\todo[inline]{I checked this equation in our simulation experiment and it is indeed correct} 
Notice that if $\beta \approx 0$ then both $\mbox{Cov}(y^2,z)$ and $\mbox{Cov}(y,z)$ are close to zero so their ratio becomes extremely noisy.

\paragraph{Standard GMM form:}
To express this system in the standard GMM form, we need to agument these moment equalities with expressions for the means of $z, y, y^2, T,$ and $yT$ as follows.
Let $\mathbf{w}_i = (y_i, z_i, T_i)'$, $\theta = (\beta, \alpha_1)'$ and $\gamma = (q, p, \mu, s, r)'$ where
\begin{align*}
  q &= \mathbb{E}\left[z \right] \\
  p &= \mathbb{E}\left[T \right] \\
  \mu &= \mathbb{E}\left[y \right] \\
  s &= \mathbb{E}\left[y^2 \right] \\
  r &= \mathbb{E}\left[yT \right].
\end{align*}
We can express our problem in terms of two blocks of moment conditions, namely
\[
  f(\mathbf{w}; \theta, \gamma) = \left[
  \begin{array}{c}
    g(\mathbf{w}; \theta, \gamma)\\
    h(\mathbf{w}; \gamma)
  \end{array}
\right]
\]
where
\[
  g(\mathbf{w}; \theta, \gamma) = \left[
  \begin{array}{c}
    (zy - q\mu) - \displaystyle\left( \frac{\beta}{1 - \alpha_1} \right)(zT - qp) \\
    (zy^2 - qs) - 2\displaystyle\left( \frac{\beta}{1 -\alpha_1}\right) (zyT - qr) + \left(\frac{\beta^2}{1 - \alpha_1}\right)(zT - qp)  
  \end{array}
\right]
\]
and
\[
  h(\mathbf{w}; \gamma) = 
  \left[
  \begin{array}{c}
    z - q \\ T - p \\ y - \mu \\ y^2 - s \\ yT - r
  \end{array}
\right]
\]
We can view this as a two-step or ``plug-in'' GMM estimation problem where $\widehat{\gamma}$ solves the sample moment condition
\[
  \frac{1}{n}\sum_{i=1}^n h(\mathbf{w}_i; \gamma) = 0
\]
and $\widehat{\theta}$ solves
\[
  \frac{1}{n}\sum_{i=1}^n g(\mathbf{w}_i; \theta, \widehat{\gamma}) = 0.
\]
Unfortunately, in our example the first-step estimation affects the asymptotic variance of the second since an inconsistent estimator of $\gamma$ yields an inconsistent estimator of $\theta$.\footnote{See Newey \& McFadden (1994), Section 6.}
This means that we will have to proceed ``the hard way.''

Under standard regularity conditions, a GMM estimator based on the sample analogue $f_n(\theta, \gamma)$ of $\mathbb{E}[f(\mathbf{w};\theta,\gamma)]=0$ using a weighting matrix $\widehat{W}\rightarrow_p W$ converges in distribution to
\[
  -(F'WF)^{-1}F'W M, \quad M\sim N(0, \Omega) 
\]
where $\sqrt{n}f_n(\theta_0, \gamma_0) \rightarrow_d M$ and $F = \mathbb{E}[\nabla_\theta' f(\mathbf{w};\theta_0, \gamma_0), \nabla_\gamma' f(\mathbf{w}; \theta_0, \gamma_0)]$.
The present example, however, is just-identified which means that $F$ is square and hence
\[
  -(F'WF)^{-1}F'W = F^{-1}W^{-1}(F')^{-1}F'W = -F^{-1}
\]
Now, given the special structure of our example,
\begin{align*}
  F &= \left[
    \begin{array}{cc}
      \mathbb{E}\left\{\nabla'_\theta g(\mathbf{w};\theta_0, \gamma_0)\right\} & 
      \mathbb{E}\left\{\nabla'_\gamma g(\mathbf{w};\theta_0, \gamma_0)\right\}\\
\mathbf{0}& -\mathbf{I}
    \end{array}
  \right] \equiv
  \left[
  \begin{array}{cc}
    G_\theta & G_\gamma\\
    \mathbf{0} & - \mathbf{I}
  \end{array}
\right]
\end{align*}
becuase $h$ does not involve $\theta$ and $\nabla_\gamma' h(\mathbf{w},\gamma) = -\mathbf{I}$.
Inverting, we have
\[
  -F^{-1} = 
  \left[
  \begin{array}{cc}
    -G_\theta & -G_\gamma \\
    \mathbf{0} & \mathbf{I}
  \end{array}
\right]^{-1} =
\left[
\begin{array}{cc}
  -G_{\theta}^{-1} & -G_{\theta}^{-1}G_{\gamma}\\
  \mathbf{0} & \mathbf{I}
\end{array}
\right]
\]
We see from this expression that if $G_\gamma$ were zero, the first step-estimation would not affect the limit distribution of $\widehat{\theta}$.
Differentiating,
\[ 
  \left[
  \begin{array}{cc}
    \nabla g_\beta & \nabla g_{\alpha_1}
  \end{array}
\right] = 
  \left[
  \begin{array}{cc}
    \displaystyle -\left(\frac{zT - qp}{1 - \alpha_1}\right) & \displaystyle -\left\{\frac{\beta(zT - qp)}{(1 - \alpha_1)^2}\right\} \\ \\
    \displaystyle 2\left\{ \frac{\beta(zT - qp) - (zyT - qr)}{1 - \alpha_1}\right\} & \displaystyle \frac{\beta^2(zT - qp) - 2\beta (zyT - qr)}{(1 - \alpha_1)^2}
  \end{array}
\right]
\]
and thus, taking expectations,
\[
  G_{\theta} = 
  \left[
  \begin{array}{cc}
    \displaystyle \frac{-\mbox{Cov}(z,T)}{1 - \alpha_1} & \displaystyle  \frac{-\beta \mbox{Cov}(z,T)}{(1 - \alpha_1)^2} \\ \\
    \displaystyle 2\left\{ \frac{\beta\mbox{Cov}(z,T) - \mbox{Cov}(yT,z) }{1 - \alpha_1} \right\} & \displaystyle \frac{ \beta^2\mbox{Cov}(z,T) - 2\beta \mbox{Cov}(yT,z)}{(1 - \alpha_1)^2} 
  \end{array}
\right]
\]
Now, for $G_\gamma$ we have
\begin{align*}
  G_\gamma &= \mathbb{E}
  \left[
  \begin{array}{ccccc}
    \nabla_q g & \nabla_p \mu & \nabla_\mu g & \nabla_s g & \nabla_r g
  \end{array}
\right] \\
&=
\left[
\begin{array}{ccccc}
  \displaystyle \left( \frac{p\beta}{1 - \alpha_1}  - \mu \right)  & \displaystyle\left(\frac{q\beta}{1 - \alpha_1}\right)& -q & 0 & 0 \\ \\
  \displaystyle\left( \frac{\beta}{1 - \alpha_1} \right)(2r - \beta p) - s &\displaystyle \frac{-q\beta^2}{1 - \alpha_1} & 0 & -q & \displaystyle \frac{2\beta q}{1 - \alpha_1}
\end{array}
\right]
\end{align*}
The next step is to invert $G_\theta$.
First we calculate the determinant.
For the purposes of this calculation, use the shorthand $C = \mbox{Cov}(z,T)$ and $D \mbox{Cov}(yT,z)$.
We have:
\begin{align*}
  |G_\theta| &= \left[ \frac{-C}{1 - \alpha_1} \right]\left[ \frac{\beta^2 C - 2\beta D}{(1 - \alpha_1)^2} \right] - \left[ \frac{-\beta C}{(1 - \alpha_1)^2} \right]\left[ \frac{2\beta C - 2D}{1 - \alpha_1} \right]
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Auxiliary Moment Inequalities}
Notice that if $\beta=0$, then the preceding moment equalities do \emph{not} identify $\alpha_1$.
However, we do have auxiliary moment \emph{inequalities} that partially identify $\alpha_1$ regardless of the value of $\beta$.
The simplest of these comes from the relationship  
\[
  p_k^* = \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1}, \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1}
\]
where $p_k = P(T=1|z_k)$ and $p_k^* = P(T^*=1|z_k)$.
(This follows from the Law of Total Probability and our assumption that the mis-classification probabilities rates depend only on $T^*$, not $z$.)
Under our assumption that $\alpha_0 + \alpha_1 < 1$, we obtain $\alpha_0 < \min_k p_k$ and $\alpha_1 < \min_k (1 - p_k)$.
If $\alpha_0 = 0$, as we assume in the present special case, then without any assumption on the true value of $\alpha_1$ we have 
\[0 \leq \alpha_1 < \min_k (1 - p_k) = 1 - \max_k p_k.\] 




\section{Under Normality}
  In our simulation for the CDF bounds on $\alpha_0$ and $\alpha_1$, we found that the upper bounds were in fact equal to the true parameter values.
  This is very surprising and is very likely comes from the specific parametric model from which we simulated.
  This happens to have been a model with normally distributed errors.
  Can we say anything about such a model theoretically?
  Perhaps try to write down the likelihood function?
  This seems important to figure out\ldots

\end{document}

