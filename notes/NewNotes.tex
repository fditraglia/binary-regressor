\documentclass[12pt]{article}
\usepackage{../frankstyle}

\title{Notes for Paper on Mis-measured, Binary, Endogenous Regressors}
\author{Francis J.\ DiTraglia \& Camilo Garc\'{i}a-Jimeno}

\begin{document}

\maketitle

\section{Model and Notation}

\paragraph{Probabilities}
\begin{eqnarray*}
p^*_{tk} &=& P(T^*=t, Z=k)\\
p_{tk} &=& P(T=t, Z=k)\\
p^*_k &=& P(T^* = 1|Z = k)\\
p_k &=& P(T = 1|Z = k)\\
q &=& P(Z = 1)
\end{eqnarray*}

\begin{eqnarray*}
  p^*_{00} &=& P(T^* = 0|Z=0)P(Z=0) = (1 - p_0^*)(1 - q) =  \left( \frac{1 - p_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{10} &=& P(T^* = 1|Z=0)P(Z=0) = p_0^*(1 - q) =  \left( \frac{p_0 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{01} &=& P(T^* = 0|Z=1)P(Z=1) = (1 - p_1^*)q =  \left( \frac{1 - p_1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) q\\
  p^*_{11} &=& P(T^* = 1|Z=1)P(Z=1) = p_1^*(1 - q)  =  \left( \frac{p_1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)q
\end{eqnarray*}

\paragraph{CDFs}
For $t, Z \in \left\{ 0,1 \right\}$ define
\begin{eqnarray*}
F_{tk}^*(\tau) &=&  P(Y \leq \tau|T^* = t, Z = k) \\
F_{tk}(\tau) &=&  P(Y \leq \tau|T = t, Z = k)\\
F_k(\tau) &=& P(Y \leq \tau | Z=k) 
\end{eqnarray*}
Note that the second two are observed for all $t,k$ while the first is never observed since it depends on the unobserved RV $T^*$.


\section{Weakest Bounds on $\alpha_0, \alpha_1$}
Assume that $\alpha_0 + \alpha_1 < 1$ that $T$ is independent of $Z$ conditional on $T^*$.
These standard assumptions turn out to yield informative bounds on $\alpha_0$ and $\alpha_1$ without \emph{any further restrictions of any kind}.
In particular, we assume nothing about the validity of the instrument $Z$ and nothing about the relationship between the mis-classification error and the outcome $Y$: we impose only that the mis-classification error rates do not depend on $z$ and that the mis-classification is not so bad that $1 - T$ is a better measure of $T^*$ than $T$. 

By the Law of Total Probability and the assumption that $T$ is conditionally independent of $Z$ given $T^*$,
\begin{eqnarray*}
  p_k &=& P(T=1|Z=k,T^*=0) (1 - p_k^*) + P(T=1|Z=k,T^*=1)p_k^*\\
  &=& P(T=1|T^*=0)(1 - p_k^*) + P(T=1|T^*=1)p_k^*\\
  &=& \alpha_0 (1 - p_k^*) + (1 - \alpha_1) p_k^*\\
  &=& \alpha_0 +(1 - \alpha_0 - \alpha_1) p_k^* 
\end{eqnarray*}
and similarly 
\begin{eqnarray*}
  1 - p_k &=& P(T=0|Z=k,T^*=0) (1 - p_k^*) + P(T=0|Z=k,T^*=1)p_k^*\\
  &=& P(T=0|T^*=0)(1 - p_k^*) + P(T=0|T^*=1)p_k^*\\
  &=& (1 - \alpha_0)(1 - p_k^*) + \alpha_1 p_k^*\\
  &=& \alpha_1 + (1 - p_k^*)(1 - \alpha_0 - \alpha_1)
\end{eqnarray*}
and hence
\begin{eqnarray*}
  p_k - \alpha_0 &=& (1 - \alpha_0 - \alpha_1)p_k^*\\
  (1 - p_k) - \alpha_1 &=& (1 - \alpha_0 - \alpha_1)(1 - p_k^*)
\end{eqnarray*}
Now, since $p_k^*$ and $(1 - p_k^*)$ are probabilities they are between zero and one which means that the sign of $p_k - \alpha_0$ as well as that of $(1 - p_k) - \alpha_1$ are both determined by that of $1 - \alpha_0 - \alpha_1$.
Accordingly, provided that $1 - \alpha_0 - \alpha_1 < 1$, we have
\begin{eqnarray*}
  \alpha_0 &<& p_k\\
  \alpha_1 &<& (1 - p_k)
\end{eqnarray*}
so long as $p_k^*$ does not equal zero or one, which is not a realistic case for any example that we consider.
Since these bounds hold for all $k$, we can take the tightest bound over all values of $Z$.

\todo[inline]{Important: using these to bound $\beta$ gives $\beta \in [\mbox{ITT}, \mbox{Wald}]$.}

\section{Stronger Bounds for $\alpha_0, \alpha_1$}
Now suppose we add the assumption that $T$ is conditionally independent of $Y$ given $T^*$. 
This is essentially the non-differential measurement error assumption although it is slightly stronger than the version used by Mahajan (2006) who assumes only conditional mean independence.
This assumption allows us to considerably strengthen the bounds from the preceding section by exploiting information contained in the conditional distribution of $Y$ given $T$ and $Z$.
The key ingredient is a relationship that we can derive between the unobservable distributions $F_{tk}^*$ and the observable distributions $F_{tk}$ using this new conditional independence assumption.
To begin, note that by Bayes' rule we have
\begin{eqnarray*}
  P(T^*=1|T=1, Z=k) &=& P(T=1 | T^*=1) \left(\frac{p_k^*}{p_k}\right) = (1 - \alpha_1)\left( \frac{p_k^*}{p_k} \right)\\
  P(T^*=1|T=0, Z=k) &=& P(T=0 | T^*=1) \left(\frac{p_k^*}{1 - p_k}\right) = \alpha_1 \left( \frac{p_k^*}{1 - p_k} \right)\\
  P(T^*=0|T=1, Z=k) &=& P(T=1 | T^*=0) \left(\frac{1 - p_k^*}{p_k}\right) = \alpha_0 \left( \frac{1 - p_k^*}{p_k} \right)\\
  P(T^*=0|T=0, Z=k) &=& P(T=0 | T^*=0) \left(\frac{1 - p_k^*}{1 - p_k}\right) = (1 - \alpha_0)\left( \frac{1 - p_k^*}{1 - p_k} \right)
\end{eqnarray*}
Now, by the conditional independence assumption
\begin{eqnarray*}
  P(Y\leq \tau|T^* = 0, T=t , Z = k) = P(Y \leq \tau|T^*=0, Z =k) = F_{0k}^*(\tau)\\
  P(Y\leq \tau|T^* = 1, T=t , Z = k) = P(Y \leq \tau|T^*=1, Z =k) = F_{1k}^*(\tau)
\end{eqnarray*}
Finally, putting everything together using the Law of Total Probability, we find that
\begin{eqnarray*}
  (1 - p_k) F_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  p_k F_{1k}(\tau) = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$.
Defining the shorthand 
\begin{eqnarray*}
  \widetilde{F}_{0k}(\tau)&\equiv& (1 - p_k) F_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\equiv& p_k F_{1k}(\tau) 
\end{eqnarray*}
this becomes
\begin{eqnarray}
  \label{eq:F0kTilde}
  \widetilde{F}_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  \label{eq:F1kTilde}
  \widetilde{F}_{1k}(\tau)  = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray}
Now, solving Equation \ref{eq:F0kTilde} for $p_k^* F_{1k}^*(\tau)$ we have
\[
  p_{k}^* F_{1k}^*(\tau) = \frac{1}{\alpha_1}\left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) (1 - p_k^*) F_{0k}^*(\tau)\right]
\]
Substituting this into Equation \ref{eq:F1kTilde},
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{\alpha_1} \left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) ( 1 - p_k^*)F_{0k}^*(\tau) \right]\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \alpha_0 - \frac{(1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \frac{\alpha_0 \alpha_1 - (1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ (1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ 1 - \alpha_1 -  \alpha_0 }{\alpha_1} \right]\left( \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) F_{0k}^*(\tau)\\
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) -  \frac{1 - p_k - \alpha_1}{\alpha_1}  F_{0k}^*(\tau)
  \label{eq:F1kTildeAlpha1}
\end{equation}
Equation \ref{eq:F1kTildeAlpha1} relates the observable $\widetilde{F}_{1k}(\tau)$ to the mis-classification error rate $\alpha_1$ and the unobservable CDF $F_{0k}^*\left( \tau \right)$.
Since $F_{0k}^*(\tau)$ is a CDF, however, it lies in the interval $\left[ 0,1 \right]$.
Accordingly, substituting $0$ in place of $F^*_{0k}(\tau)$ gives 
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_leq_a1}
\end{equation}
while substituting $1$ gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau) - \frac{1 - p_k - \alpha_1}{\alpha_1}
  \label{eq:F1ktilde_F0kTilde_geq_a1}
\end{equation}
Rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a1}
\begin{eqnarray*}
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau)\\
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& \widetilde{F}_{0k}(\tau) - \alpha_1 \widetilde{F}_{0k}(\tau)\\
 \alpha_1 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right]&\leq& \widetilde{F}_{0k}(\tau) 
\end{eqnarray*}
since $\alpha_1 \in [0,1]$ and therefore
\begin{equation}
  \alpha_1  \leq \frac{\widetilde{F}_{0k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = (1 - p_k) \left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
  \label{eq:Alpha1_Bound1}
\end{equation}
since $\widetilde{F}_{1k}(\tau) + \widetilde{F}_{1k}(\tau) \geq 0$.
Proceeding similarly for Equation \ref{eq:F1ktilde_F0kTilde_geq_a1},
\begin{eqnarray*}
  \alpha_1 \widetilde{F}_{1k}(\tau) &\geq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau) - (1 - p_k - \alpha_1)\\
  \alpha_1 \left[\widetilde{F}_{1k}(\tau) + \widetilde{F}_{0k}(\tau) - 1\right] &\geq& \widetilde{F}_{0k}(\tau) - (1 - p_k)\\
  -\alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\geq& -\left[1 - \widetilde{F}_{0k}(\tau) - p_k \right]\\
  \alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\leq& 1 - \widetilde{F}_{0k}(\tau) - p_k 
\end{eqnarray*}
Now since $\widetilde{F}_{1k}(\tau) = p_k F_{1k}(\tau) \leq p_k$ and $\widetilde{F}_{0k}(\tau) = (1 - p_k) F_{0k}(\tau) \leq (1 - p_k)$ it follows that $1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \geq 0$ and hence
\begin{equation}
  \alpha_1 \leq \frac{1 - \widetilde{F}_{0k}(\tau) - p_k}{1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau)} = (1 - p_k) \left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha1_Bound2}
\end{equation}
The bounds given in Equations \ref{eq:Alpha1_Bound1} and \ref{eq:Alpha1_Bound2} relate $\alpha_1$ to observable quantities \emph{only} and hold for all values of $\tau$ for which their respective denominators are non-zero.
Moreover, these bounds hold for any value $k$ that the instrument takes on.

We can proceed similarly for $\alpha_0$.
First solve Equation \ref{eq:F0kTilde} for $(1 - p_k^*)F^*_{0k}(\tau)$:
\[
  (1 - p_k^*)F^*_{0k}(\tau) = \frac{1}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right]
\]
and then substitute into Equation \ref{eq:F1kTilde}:
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \frac{\alpha_0}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right] + (1 - \alpha_1) p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ (1 - \alpha_1) - \frac{\alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{(1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_0}   \right] \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} F_{1k}^*(\tau) 
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) +  \frac{p_k - \alpha_0}{1 - \alpha_0} F_{1k}^*(\tau) 
\end{equation}
Now we can again obtain two bounds by substituting the smallest and largest possible values of $F_{1k}^*(\tau)$.
Substituting zero gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_geq_a0}
\end{equation}
while substituting one gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \frac{p_k - \alpha_0}{1 - \alpha_0}
  \label{eq:F1ktilde_F0kTilde_leq_a0}
\end{equation}
Now, rearranging Equation \ref{eq:F1ktilde_F0kTilde_geq_a0}, 
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \widetilde{F}_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] 
\end{eqnarray*}
since $1 - \alpha_0 \geq 0$.
Therefore,
\begin{equation}
  \alpha_0 \leq \frac{\widetilde{F}_{1k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{F_{1k}(\tau)}{F_{k}(\tau)}\right]
  \label{eq:Alpha0_Bound1}
\end{equation}
since $\left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] \geq 0$.
Similarly, rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a0}
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\leq& \alpha_0\widetilde{F}_{0k}(\tau) + p_k - \alpha_0\\
  \widetilde{F}_{1k}(\tau) - p_k &\leq& \alpha_0\left[\widetilde{F}_{0k}(\tau)  + \widetilde{F}_{1k}(\tau) - 1 \right] \\
  -\left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\leq& -\alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] \\
  \left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\geq& \alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] 
\end{eqnarray*}
Therefore
\begin{equation}
\alpha_0 \leq \frac{1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)}{1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha0_Bound2}
\end{equation}

\paragraph{Putting Everything Together} 
For all $k$ we have
\begin{equation}
  \alpha_0 \leq p_k \min_\tau\left\{\left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{1k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq p_k 
\end{equation}
\begin{equation}
  \alpha_1 \leq (1 - p_k) \min_\tau \left\{\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{0k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq (1 - p_k) 
\end{equation}
Note that these bounds can only improve upon those derived in the previous section since the ratio of CDFs tends to one as $\tau \rightarrow \infty$.
To derive these tighter bounds we have made no assumption regarding the relationship between $Z$ and the error term $\varepsilon$.
These bounds use only the assumption that $\alpha_0 + \alpha_1 < 1$, and the assumption that $T$ is conditionally independent of $Z,Y$ given $T^*$.
Notice that that the bounds are related.
In particular,
\[
  p_k \left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
\]
and 
\[
p_k \left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
\]


\section{Even Stronger Bounds on $\alpha_0, \alpha_1$}
Try applying the stochastic dominance conditions from our simulation study.

\section{Independent Instrument}
Assume that $Z \perp U$.  
The model is $Y = \beta T^* + U$ and
\[ F_{U}(\tau) = P(U \leq\tau) = P(Y - \beta T^* \leq \tau)\]
but if $Z$ is independent of $U$ then it follows that
\begin{eqnarray*}
F_U(\tau) &=&  F_{U|Z=k}(\tau) = P(U\leq \tau |Z=k) = P(Y  - \beta T^* \leq \tau |Z=k)\\
&=&  P(Y \leq \tau |T^* = 0, Z = k)(1 - p_k^*) + P(Y\leq \tau + \beta| T^* = 1, Z = k)p_k^* \\
&=& (1 - p_k^*) F^*_{0k}(\tau) + p_k^* F^*_{1k}(\tau + \beta)
\end{eqnarray*} 
for all $k$ by the Law of Total Probability.
Similarly, 
\[ F_k(\tau) = (1 - p_k^*) F_{0k}^*(\tau)  + p_k^* F_{1k}^*(\tau)\]
and rearranging
\[  (1 - p_k^*) F_{0k}^*(\tau)  = F_k(\tau) - p_k^* F_{1k}^*(\tau)\]
Substituting this expression into the equation for $F_U(\tau)$ from above, we have
\[F_U(\tau) = F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]\]
for all $k$ and all $\tau$.
Evaluating at two values $k$ and $\ell$ in the support of $Z$ and equating 
\[ F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right] =  F_\ell(\tau) + p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right]\]
or equivalently
\begin{equation}
 F_k(\tau) - F_\ell(\tau) =  p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right] - p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]  
 \label{eq:CDFs1}
\end{equation}
for all $\tau$.
Now we simply need to re-express all of the ``star'' quantities, namely $p_k^*, p_\ell^*$ and $F_{1k}^*, F_{1\ell}^*$ in terms of $\alpha_0, \alpha_1$ and the \emph{observable} probability distributions $F_{1k}$ and $F_{1\ell}$ and observable probabilities $p_k, p_\ell$.
To do this, we use the fact that
\begin{eqnarray*}
  F_{0k}(\tau) &=& \frac{1 - \alpha_0}{1 - p_k} (1 - p^*_k)F_{0k}^*(\tau) + \frac{\alpha_1}{1 - p_k}p_k^* F_{1k}^*(\tau)\\ \\
  F_{1k}(\tau) &=& \frac{ \alpha_0}{p_k}(1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{p_k}p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$ by Bayes' rule.
Solving these equations,
\begin{equation*}
  p_k^* F_{1k}^*(\tau) = \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} p_k F_{1k}(\tau) - \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} (1 - p_k) F_{0k}(\tau) 
\end{equation*}
for all $k$.
Combining this with Equation \ref{eq:CDFs1}, we find that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Now, define
\[
  \Delta^\tau_{tk}(\beta) = F_{tk}(\tau + \beta) - F_{tk}(\tau) = E\left[ \frac{\mathbf{1}\left\{ T = t, Z = k \right\}}{p_{tk}}\left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right) \right]
\]
and note that we can express $F_k(\tau) - F_\ell(\tau)$ similarly as 
\[
  F_k(\tau)  - F_{\ell}(\tau) = E\left[ \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right) \right]
\]
Using this notation, we can write the preceding as
\begin{equation*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_{\ell}(\tau) \right] = \alpha_0\left[ (1 - p_k) \Delta^\tau_{0k}(\beta) - (1 - p_\ell) \Delta^\tau_{0\ell}(\beta) \right] - (1 - \alpha_0)\left[ p_k \Delta^\tau_{1k}(\beta) - p_\ell \Delta^\tau_{1\ell}(\beta) \right]
\end{equation*}
or in moment-condition form
\begin{align*}
   E\Bigg[ &(1 - \alpha_0 - \alpha_1) \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right)  - 
   \left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right)\Bigg\{ \\
   &\alpha_0 \bigg((1 - p_k)\frac{\mathbf{1}\left\{ T = 0, Z = k \right\}}{p_{0k}} - 
    (1 - p_\ell)\frac{\mathbf{1}\left\{ T = 0, Z = \ell \right\}}{p_{0\ell}}\bigg)\\
   &-(1 - \alpha_0) \bigg( p_k\frac{\mathbf{1}\left\{ T = 1, Z = k \right\}}{p_{1k}} - 
 p_\ell \frac{\mathbf{1}\left\{ T = 1, Z = \ell \right\}}{p_{1\ell}}\bigg) \Bigg\}\Bigg] = 0
\end{align*}
Each value of $\tau$ yields a moment condition.

\section{Special Case: $\alpha_0 = 0$}
In this case the expressions from above simplify to
\begin{align}
  (1 - \alpha_1)\left[ F_k(\tau) - F_\ell(\tau)\right] = \left[ p_\ell F_{1\ell}(\tau + \beta) 
 - p_k  F_{1k}(\tau+ \beta) 
 - p_\ell F_{1\ell}(\tau) 
 + p_k F_{1k}(\tau) \right]
 \label{eq:specialCDF}
\end{align}
for all $\tau$.
Now, provided that all of the CDFs are differentiable we have\footnote{There must be a way to generalize this using Lebesgue.}
\begin{align*}
  e^{i\omega \tau}(1 - \alpha_1)\left[f_k(\tau) - f_\ell(\tau)\right] = e^{i\omega \tau}\left[ p_\ell f_{1\ell}(\tau + \beta) - p_k  f_{1k}(\tau+ \beta) - p_\ell f_{1\ell}(\tau) + p_k f_{1k}(\tau) \right]
\end{align*}
where we have pre-multiplied both sides by $e^{i\omega \tau}$.
Finally, integrating both sides with respect to $\tau$ over $(-\infty, \infty)$, we have
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] = \left\{  \int_{-\infty}^{\infty} e^{i\omega \tau} \left[p_\ell f_{1\ell}(\tau + \beta) - p_k f_{1k}(\tau+ \beta)\right] \; d\tau - p_\ell \varphi_{1\ell}(\omega) + p_k \varphi_{1k}(\omega) \right\}
\end{align*}
where $\varphi_k$ is the conditional characteristic function of $Y$ given $Z=k$ and $\varphi_{1k}$ is the conditional characteristic function of $Y$ given $T=1, Z=k$.
Finally, 
\begin{align*}
  \int_{-\infty}^{\infty} e^{i\omega \tau} p_\ell f_{1\ell}(\tau + \beta) \; d\tau &=  e^{ i\omega \beta } p_\ell \int_{u = -\infty + \beta}^{u = \infty + \beta} e^{ i\omega u }f_{1\ell}(u)\; du \\
  &= e^{-i\omega \beta } p_\ell \varphi_{1\ell}(\omega)
\end{align*}
using the substitution $u = \tau + \beta$.
Changing subscripts, the same holds for $k$ and thus
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  e^{-i\omega \beta}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] +  \left[p_k \varphi_{1k}(\omega) -  p_\ell \varphi_{1\ell}(\omega)\right]
\end{align*}
which, after collecting terms, simplifies to
\begin{align}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  \left(e^{-i\omega \beta} - 1\right)\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] 
  \label{eq:CharacteristicSpecial}
\end{align}
for all $\omega$.  
Equation \ref{eq:CharacteristicSpecial} contains exactly the same information as Equation \ref{eq:specialCDF} but gives us a more convenient way to prove identification since $\beta$ enters in a simpler way.
Leibniz's formula for the $r$th derivative of a product of two functions $f$ and $g$ is:
\begin{align*}
  (fg)^{(r)} = \sum_{s=0}^r {r \choose s} f^{(s)}g^{(r-s)}
\end{align*}
where $f^{(r)}$ denotes the $r$th derivative of the function $f$ and $g^{(r-s)}$ denotes the $(r-s)$th derivative of the function $g$.
Applying this to the RHS, $R(\omega)$ of Equation \ref{eq:CharacteristicSpecial} gives
\begin{align*}
  \frac{d}{d\omega^r}R(\omega)
  &=  \sum_{s=0}^r {r \choose s} \frac{d}{d\omega^s}\left( e^{-i\omega\beta} - 1\right)\frac{d}{d\omega^{r - s}}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] \\
  &= \left( e^{-i\omega \beta} - 1 \right) \left[ p_\ell \varphi_{1\ell}^{(r)}(\omega) - p_k \varphi_{1k}^{r}(\varphi) \right] + e^{-i\omega\beta} \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(\omega) - p_k \varphi^{(r-s)}_{1k}(\omega) \right] 
\end{align*}
where we split off the $s=0$ term because our generic expression for the $s$th derivative of $(e^{-i\omega\beta} - 1)$ only applies for $s\geq 1$.
Evaluating at zero:
\begin{align*}
  \frac{d}{d\omega^r}R(0)
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Combining this with the LHS of Equation \ref{eq:CharacteristicSpecial}, also differentiated $r$ times and evaluated at zero, we have
\begin{align*}
  (1 - \alpha_1) \left[ \varphi_{k}^{(r)}(0) - \varphi_{\ell}^{(r)}(0) \right] 
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Now, recall that if $\varphi(\omega)$ is the characteristic function of $Y$ then $\varphi^{(r)}(0) = i^r E[Y^r]$ provided that the expectation exists where $\varphi^{(r)}$ denotes the $r$th derivative of $\varphi$.
The same applies for the conditional characteristic functions we consider here.
Hence, provided that the $r$th moments exist, 
\footnotesize
\begin{align*}
  i^r(1 - \alpha_1)\left\{ E[Y^r|Z=k] - E[Y^r|Z=\ell]\right\} = \sum_{s=1}^r {r \choose s} (-i\beta)^s i^{r-s}\left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
After simplifying the terms involving $i$ and cancelling them from both sides, 
\small
\begin{align*}
  (1 - \alpha_1)\left(E[Y^r|Z=k] - E[Y^r|Z=\ell]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
again provided that the moments exist.
Abbreviating the conditional expectations according to $E[Y^r|Z=k] = E_k[Y^r]$ and $E[Y^r|T=t,Z=k] = E_{tk}[Y^r]$, this becomes
\begin{equation}
  (1 - \alpha_1)\left(E_k[Y^r] - E_\ell[Y^r]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{r-s}\right] - p_k E_{1k}\left[ Y^{r-s}\right] \right)
  \label{eq:MomentsSpecial}
\end{equation}
Equation \ref{eq:MomentsSpecial} can be used to generate moment equations that are implied by the Equation \ref{eq:CharacteristicSpecial} and the equivalent representation in terms of CDFs: Equation \ref{eq:specialCDF}.
Assuming that the conditional first moments exist, we can evaluate Equation \ref{eq:MomentsSpecial} at $r=1$, yielding
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y] - E_\ell[Y]\right) &= \sum_{s=1}^1 {1 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{1-s}\right] - p_k E_{1k}\left[ Y^{1-s}\right] \right)\\
  &=  - \beta\left( p_\ell - p_k \right) 
\end{align*}
Rearranging, this gives us the expression for the probability limit of the Wald estimator
\begin{equation}
  \mathcal{W} \equiv \frac{E_{k}[Y]- E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_1} 
  \label{eq:WaldSpecial}
\end{equation}
Evaluating Equation \ref{eq:MomentsSpecial} at $r = 2$, we have
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y^2] - E_\ell[Y^2]\right) &= \sum_{s=1}^2 {2 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{2-s}\right] - p_k E_{1k}\left[ Y^{2-s}\right] \right)\\
  &= 2\beta\left( p_k E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta^2\left( p_k - p_{\ell} \right)
\end{align*}
Rearranging, we have
\begin{equation}
  E_k[Y^2] - E_\ell[Y^2] 
  =  \frac{\beta}{1 - \alpha_1}\left[2\left( p_k  E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta(p_k - p_\ell)\right]
  \label{eq:SpecialSquared}
\end{equation}
Substituting Equation \ref{eq:WaldSpecial}, we can replace $\beta/(1-\alpha_1)$ with a function of observables only, namely $\mathcal{W}$.
Solving, we find that 
\begin{align}
  \beta &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} 
  \label{eq:BetaSpecial}
\end{align}
This allows us to state low-level sufficient conditions for identification:
\begin{enumerate}[(a)]
  \item $\alpha_1 < 1$
  \item $p_k \neq p_\ell$ 
  \item $E_k[Y] \neq E_\ell[Y]$ 
  \item $E_{1k}[|Y|], E_{1\ell}[|Y|], E_k[|Y^2|], E_\ell[|Y^2|] < \infty$.
\end{enumerate}
Note that, although $\beta = 0$ is always a solution of Equation \ref{eq:specialCDF} this solution is ruled out by the assumption that $E_k[Y] \neq E_\ell[Y]$ via Equation \ref{eq:WaldSpecial}.
The mis-classification error rate $\alpha_1$ is likewise uniquely identified under these assumptions.
Substituting $\beta/\mathcal{W} = 1-\alpha_1$ into Equation \ref{eq:BetaSpecial}
\begin{align*}
  (1 - \alpha_1) &= \left\{ \frac{p_k - p_\ell}{E_k[Y] - E_\ell[Y]} \right\}\left\{\frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} \right\}\\
  &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} - (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\}
\end{align*}
and thus
\begin{align*}
  \alpha_1
  &= 1 + (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\} - \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} 
\end{align*}

\section{Identification in the General Case}

\section{Characteristic Functions}
Recall from above that in the general case an independent instrument combined with non-differential measurement error implies that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Using the same steps as in the preceding section, we can convert this expression into characteristic function form by differentiating each side, multiplying by $e^{i\omega\tau}$ and then integrating with respect to $\tau$, yielding
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left[ \varphi_k(\omega) - \varphi_{\ell}(\omega) \right] &= \alpha_0 \left\{ (1 - p_k)\left(e^{-i\omega\beta} - 1\right)\varphi_{0k}(\omega) - (1 - p_\ell)\left( e^{-i\omega\beta} - 1\right) \varphi_{0\ell}(\omega)  \right\}\\
  &\quad - (1 - \alpha_0) \left\{ p_k\left(e^{-i\omega\beta} - 1 \right)\varphi_{1k}(\omega) - p_\ell \left( e^{-i\omega\beta} - 1\right) \varphi_{1\ell}(\omega) \right\}
\end{align*}
which simplifies to
\begin{align*}
  \varphi_k(\omega) - \varphi_{\ell}(\omega) &= \left( e^{-i\omega\beta} - 1 \right)\left(\frac{\alpha_0\left[ (1 - p_k)\varphi_{0k}(\omega) - (1-p_\ell)\varphi_{0\ell}(\omega) \right]  - (1 - \alpha_0)\left[ p_k \varphi_{1k}(\omega) - p_\ell \varphi_{1\ell}(\omega) \right]}{1 - \alpha_0 - \alpha_1}\right).
\end{align*}
As above, we will differentiate both sides of this expression $r$ times and evaluate at $\omega = 0$.
Steps nearly identical to those given above yield
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left(  E_k[Y^r] - E_\ell[Y^r]\right) 
  &= \alpha_0 \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{ (1 - p_k) E_{0k}[Y^{r-s}] - (1 - p_\ell) E_{0\ell}[Y^{r-s}] \right\}\\
  &\quad - (1 - \alpha_0) \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{p_k E_{1k}[Y^{r-s}] - p_\ell E_{1\ell}[Y^{r-s}] \right\}
\end{align*}

\paragraph{First Moments}
Taking $r = 1$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left( E_k[Y] - E_{\ell}[Y] \right) = \beta (p_k - p_\ell)
\end{align*}
Simplifying,
\begin{equation}
  \mathcal{W} \equiv \frac{E_k[Y] - E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_0 - \alpha_1}
  \label{eq:Wald}
\end{equation}

\paragraph{Second Moments}
Now, taking $r = 2$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left( E_{k}[Y^2] - E_{\ell}[Y^2] \right) &=
  \alpha_0\left\{ \left[ (1 - p_k) E_{0k}[Y] - (1 - p_\ell) E_{0\ell} \right] - \beta^2\left( p_k - p_\ell \right) \right\}\\
  &\quad  -(1 - \alpha_0)\left\{ -2\beta\left( p_k E_{1k}[Y] - p_{\ell}E_{1\ell}[Y] \right) + \beta^2\left( p_k - p_\ell \right) \right\}\\
  &= -2\beta \alpha_0\left\{ (1 - p_k)E_{0k}[Y] - (1 - p_\ell) E_{0\ell}[Y] p_k E_{1k}[Y] + p_{\ell}E_{1\ell}[Y]\right\} \\ 
  &\quad +2\beta \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) 
  - (p_k - p_\ell)\beta^2\left( \alpha_0 + 1 - \alpha_0 \right)\\
  &= -2\beta\left\{ \alpha_0 \left( E_k[Y] - E_\ell[Y] \right) - \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) \right\} - \beta^2(p_k - p_\ell)
\end{align*}
Now, simplifying
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\beta \alpha_0 \left(\frac{E_k[Y]-E_k[Y]}{p_k - p_\ell}\right) + 2\beta \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) - \beta^2
\end{align*}
and substituting Equation \ref{eq:Wald} to eliminate $\beta$, this becomes
\small
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 (1 - \alpha_0 - \alpha_1)\mathcal{W}^2 + 2\mathcal{W}(1 - \alpha_0 - \alpha_1) \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) \\
  &\quad \quad - (1 - \alpha_0 - \alpha_1)^2 \mathcal{W}^2\\
  \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 \mathcal{W}^2 + 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2
\end{align*}
\normalsize
And thus, simplifying
\begin{align*}
  -2\alpha_0 \mathcal{W}^2 - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2 &= \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)- 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  \\
  \alpha_1  - \alpha_0   &= 1 +  \left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\mathcal{W}^2(p_k - p_\ell)} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{\mathcal{W}(p_k - p_\ell)} \right] 
\end{align*}
and therefore
\begin{equation}
  \alpha_1  - \alpha_0  = 1 +  (p_k - p_\ell)\left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\left( E_k[Y] - E_\ell[Y] \right)^2} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{E_k[Y] - E_\ell[Y]} \right] 
\end{equation}

\paragraph{``Product'' Moments}
Recall that in our initial draft of the paper we worked with moments such as $E[TY|Z=k], E[TY|Z=\ell]$ and $E[TY^2|Z=k], E[TY^2|Z=\ell]$.
In the notation of this document, we can express these quantities as follows:
\begin{align*}
  E[TY^r|z=k] &= E[TY^r|T=1,z=k]p_k + E[TY^r|T=0,z=k](1 - p_k)\\
  &= p_k E[Y^r|T=1,z=k] + 0\\
  &= p_k E_{1k}[Y^r]
\end{align*}
for any $r$. 
We will use this relationship to motivate some shorthand notation below.

\paragraph{Some Shorthand}
The notation above is becoming very cumbersome and we haven't even looked at the third moments yet! 
To make life easier, define the following: 
\begin{align*}
  \widetilde{y^r_{1k}} &= p_k E_{1k}[Y^r] \\
  \widetilde{y^r_{0k}} &= (1 - p_k) E_{1k}[Y^r] \\
  \Delta \overline{y^r} &= E_k[Y^r] - E_\ell[Y^r]\\
  \Delta \overline{Ty^r} &= p_k E_{1k}[Y^r] - p_\ell E_{1\ell}[Y^r] = \widetilde{y^r_{1k}} - \widetilde{y^r_{1k}}\\
  \mathcal{W} &= (E_k[Y] - E_\ell[Y]) / (p_k - p_\ell)
\end{align*}
for all $r$.
When no $r$ superscript is given this means $r=1$.
Note, moreover, that when $r =0$ we have $\widetilde{y_{1k}^0} = p_k$ and $\widetilde{y_{0k}^0} = (1 - p_k)$.
Thus $\Delta \overline{Ty^0} = p_k - p_\ell$.
In contrast, $\Delta y^0 = 0$.

Among other things, this notation will make it easier for us to link the derivations here to our earlier derivations from the first draft of the paper that used slightly different notation and did not work explicitly with the independence of the instrument.

\paragraph{Simplifying the Moment Equalities}
Using the final two pieces of notation defined in the preceding section, we can re-rewrite the collection of moment equalities arising from the characteristic function equations as
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left[\alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right) \right]
\end{align*}
Now, simplifying the terms in the square brackets,
\begin{align*}
  \alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)
  &= \alpha_0\left[ \left( \widetilde{y_{0k}^{r-s}} + \widetilde{y_{1k}^{r-s}} \right) - \left( \widetilde{y_{0\ell}^{r-s}} + \widetilde{y_{1\ell}^{r-s}} \right)  \right] - \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)\\
  &= \alpha_0\left( E_k[Y^{r-s}] - E_\ell[Y^{r-s}] \right) - \Delta \overline{Ty^{r-s}}\\
  &= \alpha_0 \Delta \overline{y^{r-s}} - \Delta\overline{Ty^{r-s}}
\end{align*}
and hence
\begin{align}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{r-s}} - \Delta\overline{Ty^{r-s}} \right) 
  \label{eq:MomentEqualitiesSimplified}
\end{align}

\paragraph{Third Moments}
Evaluating Equation \ref{eq:MomentEqualitiesSimplified} at $r=3$ 
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^3} 
  &= \sum_{s=1}^3 {3 \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{3-s}} - \Delta\overline{Ty^{3-s}} \right) \\
  &= -3\beta\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta^2\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^3 (p_k - p_\ell)
\end{align*}

\paragraph{Solving the System}
Using $\mathcal{W} = \beta/(1 - \alpha_0 - \alpha_1)$ we can re-write the third moment expression as follows
\begin{align*}
  \Delta \overline{y^3} &= -3\mathcal{W}\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta \mathcal{W}\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^2 \mathcal{W} (p_k - p_\ell)\\
  \frac{\Delta \overline{y^3}}{\mathcal{W} (p_k - p_\ell)} 
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} }{p_k - p_\ell}\right) \\
  \frac{\Delta \overline{y^3} - 3\mathcal{W}\Delta\overline{y^2T}}{\mathcal{W}(p_k - p_\ell)}
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2}  }{p_k - p_\ell}\right) 
\end{align*}
Now, translating the second moment equation into the shorthand notation defined above, we have


\paragraph{Simplifying the Characteristic Function Equation}
From above, we have
\begin{align*}
  \varphi_k(\omega) - \varphi_{\ell}(\omega) &= \left( e^{-i\omega\beta} - 1 \right)\left(\frac{\alpha_0\left[ (1 - p_k)\varphi_{0k}(\omega) - (1-p_\ell)\varphi_{0\ell}(\omega) \right]  - (1 - \alpha_0)\left[ p_k \varphi_{1k}(\omega) - p_\ell \varphi_{1\ell}(\omega) \right]}{1 - \alpha_0 - \alpha_1}\right).
\end{align*}
Using the fact that $\varphi_{k} = p_k \varphi_{1k} + (1 - p_k) \varphi_{0k}$, we can simplify this further, yielding
\[
(1 - \alpha_0 - \alpha_1) = \left( e^{-i\omega \beta} - 1 \right)\left[ \alpha_0 - \xi(\omega)\right] 
\]
where we define
\[
  \xi(\omega) \equiv \frac{\varphi_k(\omega) - \varphi_\ell(\omega)}{p_k \varphi_{1k}(\omega) - p_\ell\varphi_{1\ell}(\omega)}
\]
Now, re-arranging
\[
  (1 - \alpha_1) - \xi(\omega) = e^{-i\omega\beta}\left[ \alpha_0 - \xi(\omega) \right]  
\]
or equivalently
\[
  e^{i\omega\beta}\left[(1 - \alpha_1) - \xi(\omega)\right] =  \alpha_0 - \xi(\omega) 
\]
or
\[
  e^{i\omega \beta} = \frac{\alpha_0 - \xi(\omega)}{(1 - \alpha_1) - \xi(\omega)}
\]
provided the denominator does not vanish.
By taking differences or ratios evaluated at $\omega_1$ and $\omega_2$ we can eliminate $\beta$, $\alpha_0$ or $\alpha_1$ but it's not clear how or if we can prove identification in terms of a restriction on the characteristic functions.

Suppose we consider three values $\omega_1, \omega_2$ and $\omega_3$ for which that yield to distinct, non-zero values $\xi_1, \xi_2$ and $\xi_3$ of $\xi(\omega)$.
\begin{align*}
  e^{i\omega_1\beta}\left[ (1 - \alpha_1) - \xi_1 \right] - e^{i\omega_2\beta}\left[ (1 - \alpha_1) - \xi_2 \right] = \xi_2 - \xi_1
\end{align*}

\subsection{Simplifying the Characteristic CDF Equation}
Recall from above that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
We can simplify the RHS as follows
\begin{align*}
  \mbox{RHS} &= \alpha_0 \left\{ \left[ F_k(\tau + \beta) - F_\ell(\tau + \beta) \right] - \left[ F_k(\tau) - F_\ell(\tau) \right] \right\}\\
  &- \left\{ \left[ p_k F_{1k}(\tau + \beta) - p_\ell F_{1\ell}(\tau + \beta) \right]  - \left[ p_k F_{1k}(\tau) - p_\ell F_{1\ell}(\tau) \right]\right\}
\end{align*}
Now, define
\begin{align*}
  \Delta(\tau) &= F_k(\tau) - F_\ell(\tau)\\
  \widetilde{\Delta}_1(\tau) &= p_k F_{1k}(\tau) - p_\ell F_{1\ell}(\tau)
\end{align*}
Using this notation, our equation becomes
\[
  (1 - \alpha_0 - \alpha_1) \Delta(\tau) = \alpha_0 \left[ \Delta(\tau + \beta) - \Delta(\tau)\right] - \left[ \widetilde{\Delta}_1(\tau+\beta) - \widetilde{\Delta}_1(\tau)\right]
\]
which simplifies to
\[
    \widetilde{\Delta}_1(\tau+\beta) - \widetilde{\Delta}_1(\tau) = \alpha_0 \Delta(\tau + \beta) - (1 - \alpha_1) \Delta(\tau)
\]

\paragraph{Suppose $\alpha_0 = 0$:}
In this case we obtain
\[
  (1 - \alpha_1)  = \frac{ \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}
\]
Now, evaluating at two values of $\tau$ and taking differences, we find
\[
  \frac{ \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)} - 
  \frac{ \widetilde{\Delta}_1(\tau') - \widetilde{\Delta}_1(\tau' + \beta)}{\Delta(\tau')} = 0
\]

\paragraph{Suppose $\alpha_1 = 0$:}
In this case we obtain
\[
  \alpha_0 = \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + \Delta(\tau)}{\Delta(\tau + \beta)}
\]
Again, taking differences evaluated at two values of $\tau$,
\[
  \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + \Delta(\tau)}{\Delta(\tau + \beta)} - 
  \frac{\widetilde{\Delta}_1(\tau' + \beta) - \widetilde{\Delta}_1(\tau') + \Delta(\tau')}{\Delta(\tau' + \beta)} = 0
\]

\paragraph{Some Equations to Check Numerically}
We can use the same basic idea when either $\alpha_0$ or $\alpha_1$ is known but nonzero.
This isn't realistic in practice, but can be used to check our equations:
\begin{align*}
  \alpha_0 &= \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + (1 - \alpha_1) \Delta(\tau)}{\Delta(\tau + \beta)}\\ \\
  (1 - \alpha_1) &= \frac{\alpha_0 \Delta(\tau + \beta) + \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}
\end{align*}
As above, after substituting the true value of either $\alpha_1$ or $\alpha_0$, we can eliminate the remaining mis-classification probability by evaluating at two quantiles $\tau$, $\tau'$ and taking differences.
\todo[inline]{These appear to work just fine!}

\paragraph{What if $\alpha_0$ and $\alpha_1$ are both unknown?}
Suppose we take differences at two quantiles $\tau$ and $\nu$ to eliminate $\alpha_1$:
\begin{align*}
  \left[\frac{\alpha_0 \Delta(\tau + \beta) + \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}\right]
  - \left[\frac{\alpha_0 \Delta(\nu + \beta) + \widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0 \\ 
  \alpha_0 \left[ \frac{\Delta(\tau+ \beta)}{\Delta(\tau)} - \frac{\Delta(\nu + \beta)}{\Delta(\nu)} \right] - \left[ \frac{\widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau+ \beta)}{\Delta(\tau)} - \frac{\widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0
 \end{align*}

 \paragraph{The Equation that Didn't Work\ldots}
\[
  \frac{[\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau)] - [\widetilde{\Delta}_1(\tau' + \beta) - \widetilde{\Delta}_1(\tau')]}{\Delta(\tau + \beta) - \Delta(\tau' + \beta)}
- \frac{[\widetilde{\Delta}_1(\nu + \beta) - \widetilde{\Delta}_1(\nu)] - [\widetilde{\Delta}_1(\nu' + \beta) - \widetilde{\Delta}_1(\nu')]}{\Delta(\nu + \beta) - \Delta(\nu' + \beta)} = 0
 \]
 where $\Delta(\nu) = \Delta(\nu')$ and $\Delta(\tau) = \Delta(\tau')$.
%Now, recall that $\Delta(\tau)$ is a difference of CDFs. 
%This means that it its limits as $\tau \rightarrow +\infty$ and as $\tau \rightarrow -\infty$ both equal zero.
%If $Y$ is continuous, then it follows that for any $\tau$ we can always find a $\tau' \neq \tau$ such that $\Delta(\tau) = \Delta(\tau')$.
%Now if we take \emph{another} difference, between pairs  $(\tau, \nu)$ and $(\tau', \nu')$ such that $\Delta(\tau) = \Delta(\tau')$ and $\Delta(\nu) = \Delta(\nu')$, the $\alpha_0$ term disappears:
%\begin{align*}
%  \left[ \frac{\widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau+ \beta)}{\Delta(\tau)} - \frac{\widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0
% \end{align*}
%But so long as $\widetilde{\Delta}_1(\tau) \neq \widetilde{\Delta}_1(\tau')$ and  $\widetilde{\Delta}_1(\nu) \neq \widetilde{\Delta}_1(\nu')$ the equation itself does not vanish and can be used to solve for $\beta$.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{New Results from September 2016}

\subsubsection{Relationship between observed and unobserved CDFs}
Let
\begin{align*}
F^*_{tk}(\tau) &= P(Y \leq \tau|T^*=t, z_k)\\
F_{tk}(\tau) &= P(Y \leq \tau|T=t, z_k)
\end{align*}
Now, by the assumption of non-differential measurement error,
\begin{align*}
  p_k F_{1k}(\tau) &= (1 - \alpha_1) p_k^* F_{1k}^*(\tau) + \alpha_0 (1 - p_k^*)F_{0k}^*(\tau)\\
  (1 - p_k) F_{0k}(\tau) &= \alpha_1 p_k^* F_{1k}^*(\tau) + (1 - \alpha_0) (1 - p_k^*)F_{0k}^*(\tau)
\end{align*}
Solving the linear system as above, we find that
\begin{align*}
  F_{0k}^*(\tau) &= F_{0k}(\tau) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ F_{0k}(\tau) - F_{1k}(\tau) \right]\\
  F_{1k}^*(\tau) &= F_{1k}(\tau) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ F_{1k}(\tau) - F_{0k}(\tau) \right]\\
\end{align*}

\subsection{Can we relax the measurement error assumptions?}
Suppose that we continue to assume that $P(Y|T^*,T,z) = P(Y|T^*,z)$ but relax the assumption that $P(T|T^*,z) = P(T|T^*)$.
Define:
\begin{align*}
  \alpha_{0k} &= P\left( T=1|T^*=1, z_k \right)\\
  \alpha_{1k} &= P\left( T=1|T^*=0, z_k \right)
\end{align*}
As before, the Wald estimator converges in probability to
\[
  \mathcal{W} = \frac{E[Y|z_k]-E[Y|z_\ell]}{p_k - p_\ell}
\]
but the relationship between $p_1 - p_0$ and the unobserved $p^*_1 - p^*_0$ changes.
By the law of total probability
\begin{align*}
  p_k &= P(T=1|z_k) = P(T=1|T^*=1,z_k)P(T^*=1|z_k) + P(T=1|T^*=0,z_k)P(T^*=0|z_k)\\
  &= (1 - \alpha_{1k})p_k^* + \alpha_{0k}(1 - p^*_k) = p_k^*(1 - \alpha_{0k} - \alpha_{1k}) + \alpha_{0k}
\end{align*}
and thus
\[
  p_k^* = \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}},
  \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_{1k}}{1 - \alpha_{0k} - \alpha_{1k}}.
\]
Thus, we have
\begin{align*}
  p^*_k - p^*_\ell &= \left( \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}} \right) - \left( \frac{p_0 - \alpha_{0\ell}}{1 - \alpha_{0\ell} - \alpha_{1\ell}} \right)\\
  &= \frac{\left( p_k - \alpha_{0k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right) - \left( p_0 - \alpha_{0\ell} \right)\left( 1 - \alpha_{0k} - \alpha_{1k} \right)}{\left( 1 - \alpha_{0k} - \alpha_{1k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right)}
\end{align*}



\subsection{Is there a LATE interpretation of our results?}
Let $J \in \left\{ a, c, d, n \right\}$ index an individual's \emph{type}: always-taker, complier, defier, or never-taker.
Let $\pi_a, \pi_c, \pi_d, \pi_n$ denote the population proportions of always-takers, compliers, defiers, and never-takers.
The unconfounded type assumption is $P(J=j|z=1) = P(J=j|z=0)$.
Combined with the law of total probability, this gives
\begin{align*}
  p^*_1 &= P(T^*=1|z=1) = \pi_a + \pi_c \\
  1 - p^*_1 &= P(T^*=0|z=1) = \pi_d + \pi_n \\
  p^*_0 &= P(T^*=1|z=0) = \pi_d + \pi_a \\
  1-p^*_0 &= P(T^*=0|z=0) = \pi_n + \pi_c 
\end{align*}
Imposing no-defiers, $\pi_d = 0$, these expressions simplify to
\begin{align*}
  p^*_1 &=  \pi_a + \pi_c \\
  1 - p^*_1 &=  \pi_n \\
  p^*_0 &=  \pi_a \\
  1-p^*_0 &=  \pi_n + \pi_c 
\end{align*}
Solving for $\pi_c$, we see that
\begin{align*}
  \pi_c &= p_1^* - p_0^*\\
  \pi_a &= p_0^*\\
  \pi_n &= 1 - p_1^*
\end{align*}

Now, let $Y(1)$ indicate the potential outcome when $T^*=1$ and $Y(0)$ indicate the potential outcome when $T^*=0$.
The standard LATE assumptions (no defiers, mean exclusion, unconfounded type) imply
\begin{align*}
  \mathbb{E}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{E}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{E}\left[ Y(1)|J=c \right] \\
  \mathbb{E}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=n \right]\\
  \mathbb{E}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{E}\left[ Y(1)|J=a \right]\\
  \mathbb{E}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{E}\left[ Y(0)|J=n \right]
\end{align*}



\subsubsection{LATE Version of Theorem 2 from the Draft}
\begin{align*}
  \Delta\overline{yT} &= \mathbb{E}\left( yT|z=1 \right) - \mathbb{E}\left( yT|z=0 \right) \\
  &= (1 - \alpha_1) \left[ p_1^* \mathbb{E}\left( y|T^*=1, z=1 \right) - p_0^* \mathbb{E}\left(y|T^*=1, z=0\right) \right] \\
  & \; \; \quad \quad + \alpha_0 \left[ (1 - p_1^*)\mathbb{E}\left( y|T^*=0, z=1\right) - (1 - p_0^*)\mathbb{E}\left(y|T^*,z=0 \right) \right]
\end{align*}
So we find that
\begin{align*}
  \Delta\overline{yT} &= (p_1^* - p_0^*)\left\{ (1 - \alpha_1) \mathbb{E}\left[ Y(1)|J=c \right] - \alpha_0\mathbb{E}\left[ Y(0)|J=c \right] \right\}\\
  &= (1 - \alpha_1) \left\{ \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1} (p_1 - p_0) \right\} + (p_1  - p_0) \mathbb{E}\left[ Y(0)|J=c \right]
\end{align*}
Recall that the analogous expression in the homogeneous treatment effect case is
\begin{align*}
  \Delta\overline{yT} &= (1 - \alpha_1) \mathcal{W} (p_1 - p_0) + \mu_{10}^*\\
  &= (1 - \alpha_1) \left(\frac{\beta}{1 - \alpha_0 - \alpha_1}\right) (p_1 - p_0) + (p_1 - \alpha_0)m_{11}^* - (p_0 - \alpha_0)m_{10}^*
\end{align*}
while the expression for the difference of variances is 
\begin{align*}
  \Delta\overline{y^2} &= \beta \mathcal{W}(p_1 - p_0) + 2\mathcal{W} \mu_{10}^*
\end{align*}
From above we see that the analogue of $\mu_{10}^*$ in the heterogeneous treatment effects setting is $(p_1 - p_0)E\left[ Y(0)|J=c \right]$ and since the LATE is $\mathbb{E}\left[ Y(1) - Y(0) |J=c\right]$, the analogue of $\mathcal{W}$ is
\[
  \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1}
\]
so \emph{if} we could establish that 
\[
  \Delta\overline{y^2} =  \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0) |J=c \right]
\]
in the heterogeneous treatment effects case, the proof of Theorem 2 would go through immediately.
Now, if we assume an exclusion restriction on the \emph{second} moment of $y$ an argument almost identical to the standard LATE derivation gives
\[
  \Delta\overline{y^2} = \frac{\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right]}{p_1^* - p_0^*} = \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right] 
\]
so we see that the necessary and sufficient condition for our proof to go through is 
\[
  \mathbb{E}\left[ Y^2(1) - Y^2(0)|J=c \right] = \mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0)|J=c \right]
\]
Rearranging, this in turn is equivalent to
\[
  \mbox{Var}\left[ Y(1)|J=c \right] = \mbox{Var}\left[ Y(0)|J=c \right]
\]


\subsection{Partial Identification Under Independence Assumption}
Suppose we only make the LATE independence assumption $Y(T^*,z) = Y(T^*)$ rather than the conditional independence assumption $P(Y<\tau|T^*,z_k) = P(Y<\tau|T^*,z_\ell)$.
Then we still obtain
\begin{align*}
  \mathbb{P}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{P}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{P}\left[ Y(1)|J=c \right] \\
  \mathbb{P}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=n \right]\\
  \mathbb{P}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{P}\left[ Y(1)|J=a \right]\\
  \mathbb{P}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{P}\left[ Y(0)|J=n \right]
\end{align*}
From above, we also know that
\begin{align*}
  P(Y|T^*=0,z_k) &= P(Y|T=0, z_k) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ P(Y|T=0,z_k) - P(Y|T=1,z_k) \right]\\
  P(Y|T^*=1,z_k) &= P(Y|T=1, z_k) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ P(Y|T=1,z_k) - P(Y|T=0,z_k) \right]
\end{align*}
The notation is getting a bit unwieldy so let $\pi^*_{tk}(y)= P(Y=y|T^*=t,z_k)$ and similarly define $\pi_{tk}(y) = P(Y=y|T=t,z_k)$.
Using this new notation, we have
\begin{align*}
  (1 - p_k - \alpha_1) \pi^*_{0k}(y) &= (1 - p_k - \alpha_1) \pi_{0k}(y) + \alpha_1 p_k \left[ \pi_{0k}(y) - \pi_{1k}(y) \right]\\
  (p_k - \alpha_0) \pi_{1k}^*(y) &= (p_k - \alpha_0) \pi_{1k}(y) + \alpha_0 (1 - p_k)\left[ \pi_{1k}(y) - \pi_{0k}(y) \right]
\end{align*}
Writing these out for all values of $k$,
\begin{align*}
  (p_1 - \alpha_0) \pi_{11}^*(y) &= (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  (1 - p_0 - \alpha_1) \pi^*_{00}(y) &= (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0) \pi_{10}^*(y) &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) \pi^*_{01}(y) &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
Similarly, using the fact that $p_k^* = (p_k - \alpha_0)/(1 - \alpha_0 - \alpha_1)$,  
\begin{align*}
  \pi^*_{11}(y) &= \left( \frac{p_0 - \alpha_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=a \right] + \left( \frac{p_1 - p_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=c \right]\\
  \pi^*_{00}(y) &= \left( \frac{p_1 - p_0}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1 - \alpha_1}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=n \right]\\
  \pi^*_{10}(y) &= P\left[ Y(1)|J=a \right]\\
  \pi^*_{01}(y) &= P\left[ Y(0)|J=n \right] 
\end{align*}
or equivalently,
\begin{align*}
  (p_1 - \alpha_0)\pi^*_{11}(y) &= \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right]\\
  (1 - p_0 - \alpha_1)\pi^*_{00}(y) &=\left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right]\\
  (p_0 - \alpha_0)\pi^*_{10}(y) &= (p_0 - \alpha_0)P\left[ Y(1)|J=a \right]\\
  (1 - p_1 - \alpha_1)\pi^*_{01}(y) &= (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] 
\end{align*}
Equating,
\begin{align*}
  \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
and substituting the third and fourth equalities into the first and second we obtain
\footnotesize
\begin{align*}
   (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Simplifying and re-arranging,
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y) + (1 - p_1)\pi_{01}(y) - (1 - p_0) \pi_{00}(y) }{p_1 - p_0} \right] \\ 
  P\left[Y(0) =y|J=c \right] &= \left[ \frac{(1 - p_0)\pi_{00}(y) - (1 - p_1)\pi_{01}(y)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{(1 - p_0) \pi_{00}(y) - (1 - p_1)\pi_{01}(y) + p_0 \pi_{10}(y) - p_1 \pi_{11}(y)}{p_1 - p_0} \right] \\
  P\left[Y(1) = y|J=a \right] &=  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  P\left[ Y(0) = y|J=n \right] &=  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Notice that the first two equations can be simplified as follows
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{P(Y = y,T=1|z=1) - P(Y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y =y|z=0)}{p_1 - p_0} \right] \\ 
  P\left[Y(0) = y|J=c \right] &= \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] 
\end{align*}
\normalsize
Now, since probabilities must be between zero and one, we obtain the bounds
\begin{align*}
  0 &\leq \left[ \frac{P(Y = y,T=1|z=1) - P(Y = y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y = y|z=0)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] \leq 1 
\end{align*}
\normalsize
which we abbreviate 
\begin{align*}
  0 &\leq \left[ \frac{\Delta P(Y=y,T=1)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \alpha_1 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] - \left[\frac{ \Delta P(Y=y,T=0)}{p_1 - p_0} \right] \leq 1 
\end{align*}
where
\begin{align*}
  \Delta P(Y=y) &= P(Y=y|z=1) - P(Y=y|z=0)\\
  \Delta P(Y=y, T=t) &= P(Y=y, T=t|z=1) - P(Y=y,T=t|z=0).
\end{align*}
To manipulate these bounds, we need to know the sign of $R = \Delta P(Y=y)/(p_1 - p_0)
$. 
Presumably this will be positive for most values of $y$, but it could be negative.
\paragraph{Case I: $R$ is positive.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)}\\
  \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)}
\end{align*}

\paragraph{Case II: $R$ is negative.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} \\
  \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)}
\end{align*}
Note that we \emph{two-sided} bounds for the misclassification probabilities.
These may be trivial in some cases, but I don't think it's obvious that they always will be.
\todo[inline]{Do these bounds have anything to do with the testability of the LATE assumptions? That is, do we get a lower bound for measurement error \emph{precisely when} we would otherwise violate a testable LATE assumption?}

Note that we also obtain bounds from the potential outcome distributions of always-takers and never-takers, namely 
\begin{align*}
  0 &\leq  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right] \leq 1\\
  0&\leq  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right] \leq 1
\end{align*}
but these are redundant.
From the assumption of non-differential measurement error, we already have 
\begin{align*}
  \pi_{0k}^* &= \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \\
  \pi_{1k}^* &= \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) 
\end{align*}
for all $k$ as given at the beginning of this section.
These expressions imply
\begin{align*}
 0 &\leq \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \leq 1 \\
  0 &\leq \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) \leq 1
\end{align*}
Re-arranging, we have
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \pi_{0k} + \alpha_1 p_k \left( \pi_{0k} - \pi_{1k} \right) \leq 1 - p_k - \alpha_1 \\
 0 &\leq p_k \pi_{1k}- \alpha_0\pi_{1k} + \alpha_0 (1 - p_k) \left( \pi_{1k} - \pi_{0k} \right) \leq p_k - \alpha_0
\end{align*}
and thus
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \left[(1 - p_k)\pi_{0k} + p_k \pi_{1k} \right] \leq 1 - p_k - \alpha_1 \\
  0 &\leq p_k \pi_{1k} - \alpha_0\left[p_k\pi_{1k} + (1 - p_k)\pi_{0k}\right] \leq p_k - \alpha_0
\end{align*}
Now consider the first inequality.
Re-arranging the right-hand side we obtain
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)(1 - \pi_{0k})}{1 - \left[ (1 - p_k)\pi_{0k} + p_k \pi_{1k} \right]} = (1 - p_k) \left[ \frac{P(Y=0|T=0, z=k)}{P(Y=0|z=k)} \right]
\end{align*}
and re-arranging the left-hand side we find
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)\pi_{0k}}{(1 - p_k)\pi_{0k} + p_k \pi_{1k}} = (1 - p_k) \left[ \frac{P(Y=1|T=0,z=k)}{P(Y=1|z=k)} \right]
\end{align*}
For the second inequality, the left-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k \pi_{1k}}{p_k \pi_{1k} + (1 - p_k)\pi_{0k}} = p_k \left[ \frac{P(Y=1|T=1,z=k)}{P(Y=1|z_k)} \right] 
\end{align*}
while the right-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k (1 - \pi_{1k})}{1 - \left[ p_k \pi_{1k} + (1 - p_k) \pi_{0k} \right]} = p_k \left[ \frac{P(Y=0|T=1,z=k)}{P(Y=0|z=k)} \right]
\end{align*}
These are analogous to our CDF bounds from above although they may not be tighter than the bounds 
\[
  \alpha_0 \leq p_k, \quad \alpha_1 \leq (1 - p_k)
\]
because we cannot argue, as we did above, about a limit in which the ratio of CDFs approaches one.
As before, however, we can take the tightest bound over $k = 0, 1$.

\subsection{Bounding the LATE}
Even if we didn't know anything about $\alpha_0$ and $\alpha_1$ beyond the fact that they are probabilities, it looks like we could still bound the LATE.
I think we can do this without using the independence of the instrument, that is only using the mean exclusion restriction.
Write out the LATE expressions with the $\alpha_0$ and $\alpha_1$ in them and them just plug in zero and one.
Could then tighten the bounds by imposing additional assumptions to get bounds for $\alpha_0$ and $\alpha_1$, from weakest to strongest.
If you have an independent instrument, you also get bounds for the outcome distributions.
Need to think some more about this\ldots


\subsection{Stochastic Dominance Conditions}
What if we imposed a stochastic ordering, e.g.\ $Y(1) > Y(0)$ for compliers?
Presumably this would give joint bounds for $\alpha_0$ and $\alpha_1$ from the LATE expressions from above.
Alternatively, perhaps one would choose to impose an ordering on the $Y(0)$ distributions for compliers versus never-takers or the $Y(1)$ distributions for the compliers versus always-takers.
This might be interesting in situations where one is concerned that the assumption we need for identification does not in fact hold and should give additional bounds.


\section{Outline For New Draft}
\begin{enumerate}
  \item Introduction / Literature Review
    \begin{enumerate}
      \item Why is this an important question?
        \begin{itemize}
          \item Treatments of interest in economics usually endogenous and often binary.
          \item Randomized encouragement designs are common in applied work.
          \item Treatment status is often self-reported.
          \item This problem is much more challenging that people realize.
        \end{itemize}
      \item Why are we different from Ura?
        \begin{itemize}
          \item Main difference is that we, in line with the existing literature, study the case of non-differential measurement error. This allows us to obtain point identification under certain assumptions.
          \item In contrast, Ura considers arbitray forms of mis-classification but as a consequence presents only partial identification results.
          \item Second, while we do provide results for LATE in Section blah, we mainly focus on additively separable model in which heterogeneity is captured by observed covariates while Ura considers only a LATE model. (And also doesn't allow for covariates.) 
        \end{itemize}
    \end{enumerate}
  \item Mahajan/Lewbell-style Assumptions 
    \begin{enumerate}
      \item Setup and Assumptions:
        \begin{itemize}
          \item Homogenous treatment effect model (additively separable)
          \item Conditional mean version of non-differential measurement error assumption.
          \item Conditional mean independence for IV.
        \end{itemize}
      \item Show that the model is not identified, regardless of (discrete) support of IV.
      \item Derive sharp bounds for $\alpha_0, \alpha_1$ and treatment effect.
      \item Show that second and third-moment independence for IV identifies this model? Maybe this isn't interesting in and of itself?
    \end{enumerate}
  \item Independence Assumption
    \begin{enumerate}
      \item Motivation
    \begin{itemize}
      \item Showed above that stronger assumptions are needed for identification, but the additional moment restrictions seem a bit artificial.
      \item When instruments are derived from economic theory that yields conditional mean independence only, we wouldn't want to use them.
      \item They would make sense, however, in an an RCT or natural experiment.
      \item The whole point in these settings is \emph{not} to rely on functional form assumptions. It would be strange to say that $z$ is an instrument for $y$ but not $\log y$.
      \item This points towards an \emph{independence} assumption for the instrument.
      \item Can make a similar argument for measurement error: seems strange to assume that $T$ is non-differential for $y$ but not $\log y$.
    \end{itemize}
      \item Sharp Bounds for $\alpha_0$ and $\alpha_1$ without valid instrument  
        \begin{itemize}
          \item Assume ``independence'' version of non-differential measurement error.
          \item Derive CDF bounds.
        \end{itemize}
      \item Conditional Independence for Instrument
        \begin{itemize}
          \item Exactly what assumptions do we need here? 
          \item Characteristic functions.
          \item Identification conditions?
          \item Overidentifying restrictions? Test model?
        \end{itemize}
    \end{enumerate}
  \item LATE Model
    \begin{enumerate}
      \item Introduction
        \begin{itemize}
          \item Most of the existing mis-classification literature focuses on a homogeneous treatment effects model.
          \item What if we don't have an additively separable model?
          \item These results complement Ura because we work under the assumption of non-differential measurement error while he asks what can be learned when one is unwilling to make any assumptions about the form of the mis-classification.
        \end{itemize}
      \item Mahajan/Lewbel Setup
        \begin{itemize}
          \item Presumably the partial identification results go through for a LATE.
          \item The second and third moment conditions would require restrictions on form of heterogeneity.
            These would seem to be satisfied by a generalized Roy model.
        \end{itemize}
      \item Independence Assumptions
        \begin{itemize}
          \item Presumably the CDF bounds go through as before but need to state exact form of independence assumption in terms of potential outcomes.
          \item Kitagawa-style independence assumption for IV: $Y(T^*,z) = Y(T^*)$. This gives bounds for all quantile treatment effects.
          \item Stochastic dominance conditions?
        \end{itemize}
    \end{enumerate}
  \item Estimation / Inference
  \item Simulation Study
  \item Empirical Examples
    \begin{itemize}
      \item Try to look at a number of examples under different assumptions to illustrate both point and partial identification results. Don't forget about Oreopoulous: the sample size is so huge that inference isn't a major concern.
    \end{itemize}
\end{enumerate}

\section{Weak Identification}

\subsection{Moment Equations}
First we write the moment equations in a more familiar GMM-style form.
\paragraph{First Moment Condition:}
This is simply the IV moment condition: 
\[
  \mbox{Cov}(y,z)/ \mbox{Cov}(T,z) = \beta/(1 - \alpha_0 - \alpha_1)
\]
Rearranging gives a more ``canonical'' GMM form:
\[
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right) \mbox{Cov}(T,z) = 0
\]

\paragraph{Second Moment Condition:}
The equations used to identify $(\alpha_0 - \alpha_1)$ in the paper are 
\begin{align*}
  \mu_{k\ell}^* &= (p_k - \alpha_0) m_{1k}^* - (p_\ell - \alpha_0) m_{1\ell}^* \\
  \Delta \overline{y^2} &= \beta \mathcal{W} (p_k - p_\ell) + 2 \mathcal{W} \mu_{k\ell}^*\\
  \Delta \overline{yT} &= (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) + \mu_{k\ell}^*
\end{align*}
Re-arranging the third equation,
$\mu_{k\ell}^* = \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell)$.
Substituting into the second equation, 
\begin{align*}
  \Delta\overline{y^2} &= \mathcal{W}\left[ \beta(p_k - p_\ell) + 2 \mu_{k\ell}^* \right]\\
  &= \mathcal{W}\left\{ \beta(p_k - p_\ell) + 2\left[ \Delta\overline{yT} - (1 - \alpha_1)\mathcal{W}(p_k - p_\ell) \right]  \right\}\\
  &= \mathcal{W} \left\{ (p_k - p_\ell)\left[ \beta - 2(1 - \alpha_1)\mathcal{W} \right] + 2\Delta\overline{yT} \right\}
\end{align*}
Now, substituting $\mathcal{W} = \beta/(1 - \alpha_0 - \alpha_1)$
\begin{align*}
  \Delta\overline{y^2}&= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{\beta (p_k - p_\ell)\left[ 1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right] + 2 \Delta\overline{yT} \right\}\\
  &= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\Delta\overline{yT} - \beta(p_k - p_\ell)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\}
\end{align*}
We now write this in a more standard form.
Let $w$ be any random variable. 
Then,
\begin{align*}
  \mbox{Cov}(w,z) &= E(wz) - E(w)E(z) = \left[ 1 \times E(w|z=1)q + 0 \times E(w|z=0)(1 - q) \right] - E(w)q\\
  &= q E(w|z=1) - qE(w) = q E(w|z=1) - q\left[ E(w|z=1)q + E(w|z=0)(1 - q) \right]\\
  &= q\left[ E(w|z=1)(1 - q) + E(w|z=0)(1 - q)\right]\\
  &= q(1-q)\left[ E(w|z=1) - E(w|z=0) \right]
\end{align*}
Using this fact, we can express the quantities that appear in the second moment equality in terms of covariances as follows
\[
  \Delta\overline{y^2} = \frac{\mbox{Cov}(y^2,z)}{q(1 - q)}, \quad
  \Delta\overline{yT} = \frac{\mbox{Cov}(yT,z)}{q(1 - q)}, \quad
  (p_k - p_\ell) = \frac{\mbox{Cov}(T,z)}{q(1 - q)}
\]
leading to
\[\frac{\mbox{Cov}(y^2,z)}{q(1-q)}= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{\frac{2\mbox{Cov}(yT,z)}{q(1-q)} - \beta \frac{\mbox{Cov}(T,z)}{q(1-q)}\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\}
\]
Or, multiplying through by $q(1-q)$ and re-arranging,
\[
  \mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\} = 0
\]

\paragraph{Third Moment Condition:}
The third and final set of moment conditions is
\begin{align*}
    \Delta\overline{y^3} &= \beta^2 [\mathcal{W} (p_k - p_\ell)]  + 3 \beta [\mathcal{W} \mu_{k\ell}^*] + 3 \mathcal{W} \lambda^*_{k\ell}\\
    \Delta\overline{y^2T} &=  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) + 2(1-\alpha_1)\mathcal{W}\mu_{k\ell}^* + \lambda_{k\ell}^*
\end{align*}
  To put this into a more familiar format, we first eliminate $\mu_{k\ell}^*$ using 
\[
  \mu_{k\ell}^* = \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell)
\]
from the derivation of the second moment equation from above, yielding
\begin{align*}
  \Delta\overline{y^3} &= \beta^2 [\mathcal{W} (p_k - p_\ell)]  + 3 \beta \mathcal{W}\left[ \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) \right]  + 3 \mathcal{W} \lambda^*_{k\ell}\\
    \Delta\overline{y^2T} &=  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) + 2(1-\alpha_1)\mathcal{W}\left[ \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) \right] + \lambda_{k\ell}^*
\end{align*}
Re-arranging and factoring the first equation gives
\[
  \Delta\overline{y^3} = \mathcal{W}\left( p_k - p_\ell \right)
  \left\{ \beta^2 + \frac{3\beta \Delta\overline{yT}}{p_k - p_\ell} - 3\beta \mathcal{W} (1 - \alpha_1) + \frac{3\lambda^*_{k\ell}}{p_k - p_\ell} \right\}
\]
Now, by re-arranging the second equation we find that
\begin{align*}
  \lambda_{k\ell}^* &= \Delta\overline{y^2T} -  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) - 2(1-\alpha_1)\mathcal{W}\left[ \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) \right] \\
  &= \Delta\overline{y^2T} -  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) - 2(1-\alpha_1)\mathcal{W} \Delta \overline{yT} + 2(1-\alpha_1)^2\mathcal{W}^2(p_k - p_\ell)
\end{align*}
and thus
\begin{align*}
  \frac{3 \lambda^*_{k\ell}}{p_k - p_\ell} &= 
  3\left(\frac{\Delta\overline{y^2T}}{p_k - p_\ell}\right) -  3\beta(1-\alpha_1)\mathcal{W} - 6(1-\alpha_1)\mathcal{W} \left(\frac{\Delta \overline{yT}}{p_k - p_\ell}\right) + 6(1-\alpha_1)^2\mathcal{W}^2
\end{align*}
so that 
\small
\begin{align*}
  \frac{\Delta\overline{y^3}}{\mathcal{W}(p_k - p_\ell)} &=
  \left\{ \beta^2 - 6\beta\mathcal{W}(1 - \alpha_1) + 6\mathcal{W}^2(1 - \alpha_1)^2 + \left( \frac{\Delta\overline{yT}}{p_k - p_\ell} \right)\left[ 3\beta - 6\mathcal{W}(1 - \alpha_1) \right] + 3\left( \frac{\Delta\overline{y^2T}}{p_k - p_\ell} \right)  \right\}\\
  &= \left\{ \beta^2\left[1 - \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}  + \frac{6(1 - \alpha_1)^2}{(1 - \alpha_0 - \alpha_1)^2}\right]   + 3\beta\left[ 1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right] \left( \frac{\Delta\overline{yT}}{p_k - p_\ell} \right)+ 3\left( \frac{\Delta\overline{y^2T}}{p_k - p_\ell} \right) \right\}
\end{align*}
\normalsize
Simplifying, we find that
\[
  \left[1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}\right] 
  = \frac{(1 - \alpha_0 - \alpha_1) - 2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}
  =\frac{1 - \alpha_0 - \alpha_1 - 2 + 2\alpha_1}{1 - \alpha_0 - \alpha_1}
  = \frac{\alpha_1 - \alpha_0 - 1}{1 - \alpha_0 - \alpha_1}
\]
and
\begin{align*}
\left[1 - \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}  + \frac{6(1 - \alpha_1)^2}{(1 - \alpha_0 - \alpha_1)^2}\right] 
&= 1 - \left[ \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right]\left[ 1 - \frac{1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right]\\
&= 1 - \left[ \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right]\left[  \frac{(1 - \alpha_0 - \alpha_1) - (1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right]\\
&= 1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2}
\end{align*}
so that
\begin{align*}
  \frac{\Delta\overline{y^3}}{\mathcal{W}(p_k - p_\ell)} 
  &= \left\{ \beta^2\left[1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2} \right] - 3\beta\left[ \frac{1 - (\alpha_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] \left( \frac{\Delta\overline{yT}}{p_k - p_\ell} \right)+ 3\left( \frac{\Delta\overline{y^2T}}{p_k - p_\ell} \right) \right\}
\end{align*}
Therefore, re-arranging and multiplying through by $q(1 - q)$,
\footnotesize
\begin{align*}
  \mbox{Cov}(z,y^3) 
  &= \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right)\left\{ \beta^2\left[1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2} \right] \mbox{Cov}(z,T) - 3\beta\left[ \frac{1 - (\alpha_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] \mbox{Cov}(z,yT) + 3\mbox{Cov}(z,y^2T) \right\}
\end{align*}
\normalsize
\paragraph{Full Set of Moment Conditions}
\footnotesize
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\} &= 0\\
  \mbox{Cov}(y^3,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right)\left\{ \beta^2\left[1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2} \right] \mbox{Cov}(T,z) - 3\beta\left[ \frac{1 - (\alpha_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] \mbox{Cov}(yT,z) + 3\mbox{Cov}(y^2T,z) \right\} &= 0
\end{align*}
\normalsize

\subsection{Simple Special Case: $\alpha_0 = 0$}
Suppose that $\alpha_0$.
Then the model is identified using the first and second moment equalities, which simplify to
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 -\alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \left(\frac{\beta}{1 - \alpha_1}\right)\left[2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z) \right] &= 0
\end{align*}
In this simple special case, it is easy to solve for $\beta$ by substituting the first moment condition into the second:
\[
  \beta = \frac{2 \mbox{Cov}(yT,z)}{\mbox{Cov}(T,z)} - \frac{\mbox{Cov}(y^2,z)}{\mbox{Cov}(y,z)}
\]
\todo[inline]{I checked this equation in our simulation experiment and it is indeed correct} 
Notice that if $\beta \approx 0$ then both $\mbox{Cov}(y^2,z)$ and $\mbox{Cov}(y,z)$ are close to zero so their ratio becomes extremely noisy.

\paragraph{Standard GMM form:}
To express this system in the standard GMM form, we need to agument these moment equalities with expressions for the means of $z, y, y^2, T,$ and $yT$ as follows.
Let $\mathbf{w}_i = (y_i, z_i, T_i)'$, $\theta = (\beta, \alpha_1)'$ and $\gamma = (q, p, \mu, s, r)'$ where
\begin{align*}
  q &= \mathbb{E}\left[z \right] \\
  p &= \mathbb{E}\left[T \right] \\
  \mu &= \mathbb{E}\left[y \right] \\
  s &= \mathbb{E}\left[y^2 \right] \\
  r &= \mathbb{E}\left[yT \right].
\end{align*}
We can express our problem in terms of two blocks of moment conditions, namely
\[
  f(\mathbf{w}; \theta, \gamma) = \left[
  \begin{array}{c}
    g(\mathbf{w}; \theta, \gamma)\\
    h(\mathbf{w}; \gamma)
  \end{array}
\right]
\]
where
\[
  g(\mathbf{w}; \theta, \gamma) = \left[
  \begin{array}{c}
    (zy - q\mu) - \displaystyle\left( \frac{\beta}{1 - \alpha_1} \right)(zT - qp) \\
    (zy^2 - qs) - 2\displaystyle\left( \frac{\beta}{1 -\alpha_1}\right) (zyT - qr) + \left(\frac{\beta^2}{1 - \alpha_1}\right)(zT - qp)  
  \end{array}
\right]
\]
and
\[
  h(\mathbf{w}; \gamma) = 
  \left[
  \begin{array}{c}
    z - q \\ T - p \\ y - \mu \\ y^2 - s \\ yT - r
  \end{array}
\right]
\]
We can view this as a two-step or ``plug-in'' GMM estimation problem where $\widehat{\gamma}$ solves the sample moment condition
\[
  \frac{1}{n}\sum_{i=1}^n h(\mathbf{w}_i; \gamma) = 0
\]
and $\widehat{\theta}$ solves
\[
  \frac{1}{n}\sum_{i=1}^n g(\mathbf{w}_i; \theta, \widehat{\gamma}) = 0.
\]
Unfortunately, in our example the first-step estimation affects the asymptotic variance of the second since an inconsistent estimator of $\gamma$ yields an inconsistent estimator of $\theta$.\footnote{See Newey \& McFadden (1994), Section 6.}
This means that we will have to proceed ``the hard way.''

Under standard regularity conditions, a GMM estimator based on the sample analogue $f_n(\theta, \gamma)$ of $\mathbb{E}[f(\mathbf{w};\theta,\gamma)]=0$ using a weighting matrix $\widehat{W}\rightarrow_p W$ converges in distribution to
\[
  -(F'WF)^{-1}F'W M, \quad M\sim N(0, \Omega) 
\]
where $\sqrt{n}f_n(\theta_0, \gamma_0) \rightarrow_d M$ and $F = \mathbb{E}[\nabla_\theta' f(\mathbf{w};\theta_0, \gamma_0), \nabla_\gamma' f(\mathbf{w}; \theta_0, \gamma_0)]$.
The present example, however, is just-identified which means that $F$ is square and hence
\[
  -(F'WF)^{-1}F'W = F^{-1}W^{-1}(F')^{-1}F'W = -F^{-1}
\]
Now, given the special structure of our example,
\begin{align*}
  F &= \left[
    \begin{array}{cc}
      \mathbb{E}\left\{\nabla'_\theta g(\mathbf{w};\theta_0, \gamma_0)\right\} & 
      \mathbb{E}\left\{\nabla'_\gamma g(\mathbf{w};\theta_0, \gamma_0)\right\}\\
\mathbf{0}& -\mathbf{I}
    \end{array}
  \right] \equiv
  \left[
  \begin{array}{cc}
    G_\theta & G_\gamma\\
    \mathbf{0} & - \mathbf{I}
  \end{array}
\right]
\end{align*}
becuase $h$ does not involve $\theta$ and $\nabla_\gamma' h(\mathbf{w},\gamma) = -\mathbf{I}$.
Inverting, we have
\[
  -F^{-1} = 
  \left[
  \begin{array}{cc}
    -G_\theta & -G_\gamma \\
    \mathbf{0} & \mathbf{I}
  \end{array}
\right]^{-1} =
\left[
\begin{array}{cc}
  -G_{\theta}^{-1} & -G_{\theta}^{-1}G_{\gamma}\\
  \mathbf{0} & \mathbf{I}
\end{array}
\right]
\]
We see from this expression that if $G_\gamma$ were zero, the first step-estimation would not affect the limit distribution of $\widehat{\theta}$.
Differentiating,
\[ 
  \left[
  \begin{array}{cc}
    \nabla g_\beta & \nabla g_{\alpha_1}
  \end{array}
\right] = 
  \left[
  \begin{array}{cc}
    \displaystyle -\left(\frac{zT - qp}{1 - \alpha_1}\right) & \displaystyle -\left\{\frac{\beta(zT - qp)}{(1 - \alpha_1)^2}\right\} \\ \\
    \displaystyle 2\left\{ \frac{\beta(zT - qp) - (zyT - qr)}{1 - \alpha_1}\right\} & \displaystyle \frac{\beta^2(zT - qp) - 2\beta (zyT - qr)}{(1 - \alpha_1)^2}
  \end{array}
\right]
\]
and thus, taking expectations,
\[
  G_{\theta} = 
  \left[
  \begin{array}{cc}
    \displaystyle \frac{-\mbox{Cov}(z,T)}{1 - \alpha_1} & \displaystyle  \frac{-\beta \mbox{Cov}(z,T)}{(1 - \alpha_1)^2} \\ \\
    \displaystyle 2\left\{ \frac{\beta\mbox{Cov}(z,T) - \mbox{Cov}(yT,z) }{1 - \alpha_1} \right\} & \displaystyle \frac{ \beta^2\mbox{Cov}(z,T) - 2\beta \mbox{Cov}(yT,z)}{(1 - \alpha_1)^2} 
  \end{array}
\right]
\]
Now, for $G_\gamma$ we have
\begin{align*}
  G_\gamma &= \mathbb{E}
  \left[
  \begin{array}{ccccc}
    \nabla_q g & \nabla_p \mu & \nabla_\mu g & \nabla_s g & \nabla_r g
  \end{array}
\right] \\
&=
\left[
\begin{array}{ccccc}
  \displaystyle \left( \frac{p\beta}{1 - \alpha_1}  - \mu \right)  & \displaystyle\left(\frac{q\beta}{1 - \alpha_1}\right)& -q & 0 & 0 \\ \\
  \displaystyle\left( \frac{\beta}{1 - \alpha_1} \right)(2r - \beta p) - s &\displaystyle \frac{-q\beta^2}{1 - \alpha_1} & 0 & -q & \displaystyle \frac{2\beta q}{1 - \alpha_1}
\end{array}
\right]
\end{align*}
The next step is to invert $G_\theta$.
First we calculate the determinant.
For the purposes of this calculation, use the shorthand $C = \mbox{Cov}(z,T)$ and $D = \mbox{Cov}(yT,z)$.
We have:
\begin{align*}
  |G_\theta| &= \left[ \frac{-C}{1 - \alpha_1} \right]\left[ \frac{\beta^2 C - 2\beta D}{(1 - \alpha_1)^2} \right] - \left[ \frac{-\beta C}{(1 - \alpha_1)^2} \right]\left[ \frac{2\beta C - 2D}{1 - \alpha_1} \right]\\
  &= \left( \frac{1}{1 - \alpha_1} \right)^3 \left[ 2\beta C D - \beta^2 C^2 + 2\beta^2 C^2 - 2\beta CD  \right]\\
  &= \frac{\beta^2 \mbox{Cov}(z,T)^2}{(1 - \alpha_1)^3} 
\end{align*}
Thus,
\begin{align*}
  G_\theta^{-1} &= \frac{(1 - \alpha_1)^3}{\beta^2 \mbox{Cov}(z,T)^2} 
  \left[
  \begin{array}{cc}
    \displaystyle \frac{ \beta^2\mbox{Cov}(z,T) - 2\beta \mbox{Cov}(yT,z)}{(1 - \alpha_1)^2}  & \displaystyle  \frac{\beta \mbox{Cov}(z,T)}{(1 - \alpha_1)^2} \\ \\
    \displaystyle -2\left\{ \frac{\beta\mbox{Cov}(z,T) - \mbox{Cov}(yT,z) }{1 - \alpha_1} \right\} & \displaystyle\frac{-\mbox{Cov}(z,T)}{1 - \alpha_1}
  \end{array}
\right] \\ \\
  &=\left[
  \begin{array}{cc}
    \displaystyle \left\{\frac{1 - \alpha_1}{\mbox{Cov}(z,T)}\right\}\left\{ 1 - \frac{2\mbox{Cov}(yT,z)}{\beta\mbox{Cov}(z,T)} \right\}& \displaystyle  \frac{1 - \alpha_1}{\beta \mbox{Cov}(z,T)} \\ \\
    \displaystyle \frac{2(1 - \alpha_1)^2}{\beta \mbox{Cov}(z,T)}\left\{ \frac{\mbox{Cov}(yT,z)}{\beta\mbox{Cov}(z,T)} - 1 \right\} & \displaystyle \frac{-(1 - \alpha_1)^2}{\beta^2 \mbox{Cov}(z,T)} 
  \end{array}
\right] \\
\end{align*}
The next step is to calculate $\Omega$:
\begin{align*}
  \Omega = \lim_{n\rightarrow \infty}\mbox{Var}\left[\sqrt{n}f_n(\theta_0, \gamma_0)\right] 
  = \lim_{n\rightarrow \infty}\mbox{Var}\left[\frac{1}{\sqrt{n}} \sum_{i=1}^n f(\mathbf{w}_i; \theta_0, \gamma_0)  \right]
\end{align*}
If $\mathbf{w}_i$ is an iid sequence of RVs, then
\begin{align*}
  \Omega = \lim_{n\rightarrow \infty}\frac{1}{n}\mbox{Var}\left[ \sum_{i=1}^n f(\mathbf{w}_i; \theta_0, \gamma_0)\right] = \lim_{n\rightarrow \infty} \frac{1}{n}\sum_{i=1}^n \mbox{Var}\left[ f(\mathbf{w}_i;\theta_0, \gamma_0) \right] = \mbox{Var}\left[ f(\mathbf{w}_i;\theta_0, \gamma_0) \right]  
\end{align*}
And assuming that our model is correctly specified, so that $\mathbb{E}\left[ f(\mathbf{w}_i;\theta_0, \gamma_0) \right]=0$,
\begin{align*}
  \mbox{Var}\left[ f(\mathbf{w}_i; \theta_0, \gamma_0) \right] 
  &=\mathbb{E}
  \left[
  \begin{array}{cc}
    g(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    g(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)' \\
    h(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    h(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)'
  \end{array}
\right]\\ &\equiv
\left[
\begin{array}{cc}
  \Omega_{gg} & \Omega_{gh}\\
  \Omega_{gh} & \Omega_{hh}
\end{array}
\right]
\end{align*}
We now calculate each block.
\todo[inline]{I don't think this is actually going to give us anything interpretable. The expressions are quite involved and it seems unlikely that they'll cancel in a useful way. This doesn't matter for implementation, of course, since it's easy to calculate the estimate of $\widehat{\Omega}$ by plugging the GMM estimates into the sample moment conditions, taking the outer product, and averaging.}

We are only interested in the asymptotic variance matrix $V_\theta$ of our parameters of interest $\theta = (\beta, \alpha_1)$.
We calculate this as follows:
\begin{align*}
  V_\theta &= \left[
  \begin{array}{cc}
    G_\theta^{-1} & G_{\theta}^{-1}G_\gamma
  \end{array}
\right]
\left[
\begin{array}{cc}
  \Omega_{gg} & \Omega_{gh}\\
  \Omega_{hg} & \Omega_{hh}\\
\end{array}
\right]
\left[
\begin{array}{c}
  \left( G_\theta^{-1} \right)'\\
  \left( G_\theta^{-1}G_{\gamma} \right)'\\
\end{array}
\right] \\
&= 
  \left[\begin{array}{cc}
      G_\theta^{-1}\left(\Omega_{gg} + G_\gamma \Omega_{gh}  \right) & 
      G_{\theta}^{-1}\left( \Omega_{gh} + G_\gamma \Omega_{hh} \right)
  \end{array}
\right]
\left[
\begin{array}{c}
  \left( G_\theta^{-1} \right)'\\
  G_{\gamma}' \left( G_\theta^{-1}\right)'\\
\end{array}
\right]\\
&= G_\theta^{-1}\left(\Omega_{gg} + G_\gamma \Omega_{hg}  \right)\left( G_{\theta}^{-1} \right)' + G_\theta^{-1}\left( \Omega_{gh} + G_\gamma \Omega_{hh}\right)G_\gamma' \left( G_\theta^{-1} \right)'\\
&= G_\theta^{-1}\left(\Omega_{gg} + G_\gamma \Omega_{hg} + \Omega_{gh}G_\gamma' + G_\gamma \Omega_{hh} G_\gamma'\right)\left( G_\theta^{-1} \right)'
\end{align*}

\subsection{Easier(?) Derivation of Simple Special Case: $\alpha_0 = 0$}
Recall that we could eliminate $\alpha_1$ from the moment conditions, yielding,
\[
  \beta = \frac{2 \mbox{Cov}(yT,z)}{\mbox{Cov}(T,z)} - \frac{\mbox{Cov}(y^2,z)}{\mbox{Cov}(y,z)}
\]
We can treat this as our $g$ block of moment conditions with a parameter vector $\theta$ that is simply $\beta$.
This gives
\[
  g(\mathbf{w}; \beta, \gamma) = \left[ 2\left(\frac{zTy - qr}{zT - qp}\right) - \frac{zy^2 - qs}{zy - q\mu} - \beta \right]
\]
The $h$ block of moment conditions is unchanged.
Now, we have
\begin{align*}
  F &= \left[
    \begin{array}{cc}
      \mathbb{E}\left\{\nabla'_\beta g(\mathbf{w};\beta_0, \gamma_0)\right\} & 
      \mathbb{E}\left\{\nabla'_\gamma g(\mathbf{w};\beta_0, \gamma_0)\right\}\\
\mathbf{0}& -\mathbf{I}
    \end{array}
  \right] \equiv
  \left[
  \begin{array}{cc}
    G_\beta & G_\gamma\\
    \mathbf{0} & - \mathbf{I}
  \end{array}
\right] = \left[
\begin{array}{cc}
  -1 & G_\gamma \\
    \mathbf{0} & - \mathbf{I}
\end{array}
\right]
\end{align*}
since the derivative of $g$ with respect to $\beta$ is -1 and that of $h$ with respect to $\gamma$ is $-\mathbf{I}$.
Inverting, 
\[
  -F^{-1} = 
  \left[
  \begin{array}{cc}
    -G_\beta & -G_\gamma \\
    \mathbf{0} & \mathbf{I}
  \end{array}
\right]^{-1} =
\left[
\begin{array}{cc}
  -G_{\beta}^{-1} & -G_{\beta}^{-1}G_{\gamma}\\
  \mathbf{0} & \mathbf{I}
\end{array}
\right] = \left[
\begin{array}{cc}
  1 & G_\gamma \\
  \mathbf{0} & \mathbf{I}
\end{array}
\right]
\]
We calculate $G_\gamma$ as follows: 
\begin{align*}
  G_\gamma &= \mathbb{E}
  \left[
  \begin{array}{ccccc}
    \nabla_q g & \nabla_p g & \nabla_\mu g & \nabla_s g & \nabla_r g
  \end{array}
\right] \\ \\
  \nabla_q g(\mathbf{w};\beta_0, \gamma_0) &= \nabla_q \left[ 2\left(\frac{zTy - qr}{zT - qp}\right) - \frac{zy^2 - qs}{zy - q\mu} - \beta \right]\\
  &= 2\left[ \frac{-r(zT - qp) + p(zTy - qr)}{(zT - qp)^2} \right] - \frac{-s(zy - q\mu) + \mu (zy^2 - qs)}{(zy - q\mu)^2}\\
  &=  \frac{2zT(py - r)}{(zT - qp)^2}  - \frac{zy (\mu y - s)  }{(zy - q\mu)^2}\\ \\
  \nabla_p g(\mathbf{w};\beta_0, \gamma_0) &= \frac{2q(zTy - qr)}{(zT - qp)^2} \\ \\
  \nabla_\mu g(\mathbf{w};\beta_0, \gamma_0) &= \frac{-q(zy^2 - qs)}{(zy - q\mu)^2}\\ \\
  \nabla_s g(\mathbf{w};\beta_0, \gamma_0) &= \frac{q}{zy - q\mu} \\ \\
  \nabla_r g(\mathbf{w};\beta_0, \gamma_0) &= \frac{-2q}{zT - qp}
\end{align*}

The next step is to calculate $\Omega$:
\begin{align*}
  \Omega &= \mbox{Var}\left[ f(\mathbf{w}_i; \theta_0, \gamma_0) \right] 
  =\mathbb{E}
  \left[
  \begin{array}{cc}
    g(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    g(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)' \\
    h(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    h(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)'
  \end{array}
\right]\\ &\equiv
\left[
\begin{array}{cc}
  \Omega_{gg} & \Omega_{gh}\\
  \Omega_{gh} & \Omega_{hh}
\end{array}
\right]
\end{align*}
We now calculate each block.
\todo[inline]{Still need to do this!}


\subsection{Two-Step Inference Idea}
If $\beta$ is small, then confidence intervals based on the GMM limit distribution from above will perform badly.
But even in this case, inference for the \emph{identified set} $\left[ \beta_{RF}, \beta_{IV} \right]$ should still be well-behaved, so long as the instrument is strong.
The idea of this section is to explore a two-step procedure that chooses between reporting the GMM confidence interval or inference for the identified set based on a pre-test of $\beta_{RF}$.
Presumably conducting valid inference based on such a procedure will require a Bonferroni correction.
The first step, however, is to determine the joint limiting behavior of the reduced form, IV, and GMM estimators.

\paragraph{Reduced Form Estimator}
The reduced form is given by
\begin{align*}
  y &= \gamma_0 + \gamma_1 z + \eta \\
  \gamma_0 &= \mathbb{E}[y|z=0]\\
  \gamma_1 &= \mathbb{E}[y|z=0] - \mathbb{E}[y|z=1]
\end{align*}
Now, we need to write $\eta$ in terms of the ``primitives'' of our model.
The first stage and main equation are
\begin{align*}
  y &= c + \beta T^* + \varepsilon\\
  T^* &= \pi_0 + \pi_1 z + v\\
  \pi_1 &= p_1^* - p_0^*
\end{align*}
which implies
\[
  y = (c + \beta \pi_0) + (\beta \pi_1) z + (\varepsilon + \beta v)
\]
so that
\begin{align*}
\gamma_1 &= \beta(p_1^* - p_0^*)\\
\eta &= \varepsilon + \beta v
\end{align*}
Now, define $W = (\mathbf{1}, \mathbf{z})$ and $\boldsymbol{\gamma} = (\gamma_0, \gamma_1)'$.
Then the reduced form estimator is 
\[
  \widehat{\boldsymbol{\gamma}} = \left(W'W\right)^{-1}  W'\mathbf{y} 
  = \left( W'W \right)^{-1}W'\left( W\boldsymbol{\gamma} + \boldsymbol{\eta} \right) = \boldsymbol{\gamma} + \left( W'W \right)^{-1}W'\boldsymbol{\eta}
\]
and hence
\[
  \sqrt{n}\left( \widehat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}\right) = \left( \frac{W'W}{n} \right)^{-1}\frac{W'\boldsymbol{\eta}}{\sqrt{n}}
\]
Now,
\[
  \left(\frac{W'W}{n}\right)^{-1} = \left[
  \begin{array}{cc}
    1 & \bar{\mathbf{z}} \\
    \bar{\mathbf{z}} & \mathbf{z}'\mathbf{z}/n
  \end{array}
\right]^{-1} \rightarrow_p 
\left[
\begin{array}{cc}
  1 & q \\
  q & q
\end{array}
\right]^{-1}
= \frac{1}{q(1-q)}\left[
\begin{array}{cc}
  q & -q \\
  -q & 1
\end{array}
\right]
\]
and by the Central Limit Theorem,
\[
  \frac{W'\boldsymbol{\eta}}{\sqrt{n}} = \frac{1}{\sqrt{n}}\sum_{i=1}^n \eta_i\left[
  \begin{array}{c}
    1 \\ z_i 
  \end{array}
\right] 
= \frac{1}{\sqrt{n}}\sum_{i=1}^n (\varepsilon_i + \beta v_i)\left[
  \begin{array}{c}
    1 \\ z_i 
  \end{array}
\right] \rightarrow_d N\left(\mathbf{0}, \Sigma \right)
\]
where
\begin{align*}
  \Sigma &= 
  \mathbb{E}\left[
  \begin{array}{cc}
   \eta_i^2 & z_i \eta_i^2\\
   z_i \eta_i^2 & z_i^2 \eta_i^2
  \end{array}
\right]
=
  \mathbb{E}\left[
  \begin{array}{cc}
   \left( \varepsilon_i + \beta v_i  \right)^2 & 
   z_i \left( \varepsilon_i + \beta v_i  \right)^2\\ 
   z_i \left( \varepsilon_i + \beta v_i  \right)^2 &
   z_i^2\left( \varepsilon_i + \beta v_i  \right)^2  
  \end{array}
\right]\\
&=
  \mathbb{E}\left[
  \begin{array}{cc}
   \varepsilon_i^2 + 2\beta \varepsilon_i v_i + \beta^2 v_i^2  & 
 z_i^2\varepsilon_i + 2\beta z_i \varepsilon_i v_i + \beta^2 z_i v_i^2 \\
   z_i^2\varepsilon_i + 2\beta z_i \varepsilon_i v_i + \beta^2 z_i v_i^2 &
   z_i^2\varepsilon_i^2 + 2\beta z_i^2 \varepsilon_i v_i + \beta^2 z_i^2 v_i^2  
  \end{array}
\right]
\end{align*}
The next step is to work out the joint distribution of $v$ and the other primitives of our model:
\begin{align*}
  T^*=0, z = 0 &\implies 0 = \pi_0 + v \implies v = -\pi_0 = -p_0^*\\
  T^*=1, z = 0 &\implies 1 = \pi_0 + v \implies v = 1-\pi_0 = 1-p_0^*\\
  T^*=0, z = 1 &\implies 0 = \pi_0 + \pi_1 + v \implies v = -(\pi_0 + \pi_1) = -p_1^*\\
  T^*=1, z = 1 &\implies 1 = \pi_0 + \pi_1 + v \implies v = 1-(\pi_0 + \pi_1) = 1-p_1^*
\end{align*}
Thus,
\begin{align*}
  \mathbb{P}(T^*=0, z = 0) &= \mathbb{P}(v = -p_0^*) = (1-p_0^*)(1-q)\\
  \mathbb{P}(T^*=1, z = 0) &= \mathbb{P}(v = 1-p_0^*) = p_0^*(1-q)\\
  \mathbb{P}(T^*=0, z = 1) &= \mathbb{P}(v = -p_1^*) = (1-p_1^*)q\\
  \mathbb{P}(T^*=1, z = 1) &= \mathbb{P}(v = 1-p_1^*) = p_1^* q 
\end{align*}
Notice that, as must be true \emph{by construction}, $v$ is mean zero:
\begin{align*}
  \mathbb{E}[v] &= -p_0^*(1-p_0^*)(1 - q) + (1 - p_0^*)p_0^*(1-q) - p_1^*(1-p_1^*)q + (1-p_1^*)p_1^*q  \\
  &= (1-q)(1 - p_0^*) (p_0^* - p_0^*) + q(1-p_1^*)(p_1^* - p_1^*) = 0
\end{align*}
and uncorrelated with $v$:
\begin{align*}
  \mathbb{E}[zv] &= q\mathbb{E}[v|z=1] = q\left\{ p_1^*  \mathbb{E}[v|T^*=1,z=1] + (1 - p_1^*)\mathbb{E}[v|T^*=0,z=1]\right\} \\
  &= q \left[ p_1^* (1 - p_1^*) - p_1^* (1 - p_1^*)  \right] = 0
\end{align*}
Now, to calculate $\Sigma$, we need $\mathbb{E}[v^2], \mathbb{E}[\varepsilon v], \mathbb{E}[z\varepsilon v], \mathbb{E}[z^2\varepsilon v], \mathbb{E}[zv^2]$, and $\mathbb{E}[z^2v^2]$:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[v^2] &=p_0^{*2}(1-p_0^*)(1 - q) + (1 - p_0^*)^2 p_0^*(1-q) + p_1^{*2}(1-p_1^*)q + (1-p_1^*)^2p_1^*q  \\ 
  &= (1-q)p_0^*(1 - p_0^*)\left[p_0^* + (1 - p_0^*)  \right] + qp_1^*(1 - p_1^*)\left[ p_1^* + (1 - p_1^*) \right]\\
  &= (1-q)p_0^*(1 - p_0^*) + qp_1^*(1 - p_1^*)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[\varepsilon v] &= \mathbb{E}\left[ v \mathbb{E}\left( \varepsilon|v \right) \right]\\
  &= -p_0^*\mathbb{P}(v = -p_0^*)\mathbb{E}(\varepsilon|v = -p_0^* ) - p_1^* \mathbb{P}(v = -p_1^* )\mathbb{E}(\varepsilon|v = -p_1^*) \\
  & \qquad {} + (1 - p_0^*)\mathbb{P}(v = 1-p_0^*)\mathbb{E}(\varepsilon|v = 1-p_0^*) + (1 -p_1^*)\mathbb{P}(v = 1- p_1^*)\mathbb{E}(\varepsilon|v = 1-p_1^*) \\
   &= -p_0^*(1-p_0^*)(1-q)\mathbb{E}[\varepsilon|T^*=0,z=0] - p_1^* (1-p_1^*)q\mathbb{E}[\varepsilon|T^*=0,z=1]\\
   &\qquad {} + (1 - p_0^*) p_0^*(1-q)\mathbb{E}[\varepsilon|T^*=1,z=0] + (1 - p_1^*)p_1^*q \mathbb{E}[\varepsilon|T^*=1,z=1]\\
   &= -p_0^*(1-p_0^*)(1-q)(m^*_{00}-c) -p_1^* (1-p_1^*)q(m^*_{01} - c)\\
   &\qquad {} + (1-p_0^*)p_0^*(1-q)(m^*_{10}-c) + (1 - p_1^*)p_1^*q (m^*_{11}-c)\\
   &= (1-q)p_0^*(1-p_0^*)(m_{10}^* - m_{00}^*) + qp_1^*(1-p_1^*)(m_{11}^* - m_{01}^*)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[z\varepsilon v] &= \mathbb{E}[z^2\varepsilon v] = \mathbb{E}\left[ z^2 \mathbb{E}\left[ \varepsilon v|z \right] \right] = q \mathbb{E}\left[ \varepsilon v|z=1 \right] = q \mathbb{E}_{T^*|z=1}\left[ \mathbb{E}\left[ \varepsilon v |T^*=1,z=1 \right] \right]\\
  &= q\left\{p_1^* \mathbb{E}\left[ \varepsilon v \right|T^*=1,z=1] + (1 - p_1^*)\mathbb{E}[\varepsilon v|T^*=0,z=1]\right\}\\
  &= q\left\{p_1^*(1 - p_1^*) \mathbb{E}\left[ \varepsilon  \right|T^*=1,z=1] - p_1^* (1 - p_1^*)\mathbb{E}[\varepsilon |T^*=0,z=1]\right\}\\
  &= q p_1^*(1-p_1^*)\left[m^*_{11} - m^*_{01} \right]
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[z v^2] &= \mathbb{E}[z^2v^2] =  \mathbb{E}\left[ z^2 \mathbb{E}\left[ v^2|z \right]  \right] = q\mathbb{E}[v^2|z=1] = q \mathbb{E}_{T^*|z=1}\left[ \mathbb{E}\left[ v^2|T^*,z=1 \right] \right]\\
  &= q\left\{ p_1^* \mathbb{E}[v^2|T^*=1,z=1] + (1 - p_1^*) \mathbb{E}\left[ v^2|T^*=0,z=1 \right] \right\}\\
  &= q\left\{ p_1^*(1-p_1^*)^2 + p_1^{*2} (1 - p_1^*) \right\}\\
  &= qp_1^*(1 - p_1^*)\left[ (1-p_1^*)+ p_1^* \right]\\
  &= qp_1^*(1 - p_1^*)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Using these calculations, we find the elements of $\Sigma$ as follows:
\begin{align*}
  \mathbb{E}[\eta^2] &= \mathbb{E}[\varepsilon^2 + 2\beta \varepsilon v + \beta^2 v^2]  \\
  &= \sigma_\varepsilon^2 + 2 \beta \left[ (1-q)p_0^*(1-p_0^*)(m_{10}^* - m_{00}^*) + qp_1^*(1-p_1^*)(m_{11}^* - m_{01}^*)\right]\\
  &\qquad {} + \beta^2\left[(1-q)p_0^*(1 - p_0^*) + qp_1^*(1 - p_1^*)\right]\\
  &= \sigma_\varepsilon^2 + (1-q)p_0^*(1-p_0^*)\beta\left[ \beta + 2 \left( m_{10}^* - m_{00}^* \right) \right] + q p_1^*(1 - p_1^*)\beta\left[ \beta + 2\left( m_{11}^* - m_{01}^* \right) \right]\\
  \mathbb{E}[z^2 \eta^2] =  \mathbb{E}[z \eta^2] &= \mathbb{E}[ z \varepsilon^2 + 2\beta z \varepsilon v + \beta^2 z v^2 ] = \sigma_\varepsilon^2 + 2\beta \mathbb{E}[z \varepsilon v] + \beta^2 \mathbb{E}[zv^2]\\
  &= q\sigma_\varepsilon^2 + 2\beta q p_1^*(1-p_1^*)\left( m_{11}^* - m_{01}^* \right) + \beta^2 q p_1^*(1-p_1^*) \\
  &= q\left\{\sigma_\varepsilon^2 + p_1^*(1-p_1^*)\beta\left[ \beta + 2 \left( m_{11}^* - m_{01}^* \right) \right]\right\}
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Now, using the fact that $\mathbb{E}[z^2\eta] = \mathbb{E}[z\eta]$, the asymptotic variance of the reduced form estimator can be written as:
\begin{align*}
  \mbox{AVAR}\left[ \sqrt{n}\left( \widehat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}\right) \right] &= \frac{1}{q^2(1-q)^2}
  \left[
  \begin{array}{cc}
    q & -q \\ -q & 1
  \end{array}
\right]
\left[
\begin{array}{cc}
  \mathbb{E}(\eta^2) & \mathbb{E}(z\eta^2)\\
  \mathbb{E}(z\eta^2) & \mathbb{E}(z\eta^2)
\end{array}
\right]
\left[
\begin{array}{cc}
  q & -q \\ -q & 1
\end{array}
\right]\\
&= 
\frac{1}{q^2(1-q)^2}
\left[
\begin{array}{cc}
  q & -q \\ -q & 1
\end{array}
\right]
\left[
\begin{array}{cc}
  q\left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} &
  \mathbb{E}(z\eta^2) - q \mathbb{E}(\eta^2)\\
  0 & (1-q)\mathbb{E}(z\eta^2)
\end{array}
\right] \\ 
&= 
\frac{1}{q^2(1-q)^2}
\left[
\begin{array}{cc}
  q^2 \left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} & 
  -q^2 \left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} \\
  -q^2 \left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} &
  q^2 \mathbb{E}(\eta^2) + (1-2q) \mathbb{E}(z\eta^2)
\end{array}
\right]
\end{align*}
Now, we are only interested in the reduced form slope coefficient $\gamma_1$.
The asymptotic variance of its OLS estimator is:
\begin{align*}
  \mbox{AVAR}\left[ \sqrt{n}(\widehat{\gamma}_1 - \gamma_1) \right] &= 
  \frac{1}{q^2(1-q)^2}\left[q^2 \mathbb{E}(\eta^2) + (1-2q) \mathbb{E}(z\eta^2)\right]\\
  &= \frac{\mathbb{E}(\eta^2)}{(1-q)^2} + \left[ \frac{1-2q}{q^2(1-q)^2} \right]\mathbb{E}(z\eta^2)
\end{align*}
In general, this will lead to quite a complicated expression.
In our simulation design, however, 
\begin{align*}
  q &= 1/2\\ 
  p^*_0(1-p^*_0) &= \delta(1-\delta)\\
  p^*_1(1-p^*_1) &= (1 -\delta) \delta\\
  (m^*_{10} - m^*_{00}) &= (m^*_{11} - m_{01}^*) > 0 
\end{align*}
leading to the following simplifications:
\small
\begin{align*}
  \mbox{AVAR}\left[ \sqrt{n}(\widehat{\gamma}_1 - \gamma_1) \right] &= 
  4 \mathbb{E}(\eta^2)\\
  &= 4\left\{ \sigma_\varepsilon^2 + 1/2 \times \delta(1 - \delta)\beta\left[ \beta + 2 \left( m_{10}^* - m_{00}^* \right) \right] + 1/2 \times \delta(1 - \delta)\beta\left[ \beta + 2\left( m_{11}^* - m_{01}^* \right) \right]\right\}\\
  &= 4\left\{ \sigma_\varepsilon^2 + \delta(1 - \delta)\beta\left[ \beta + 2\left( m_{11}^* - m_{01}^* \right) \right]\right\}
\end{align*}
\normalsize
Notice that, since $(m_{11}^* - m_{01}^*)$ is positive, as in the simulation design, the asymptotic variance is \emph{smallest} when $\beta = 0$ so that there is no treatment effect.

\paragraph{Robust Standard Errors} 
When implementing the reduced form estimator in practice we base our inference on sample residuals:
\[
  \widehat{\eta}_i = y_i - \widehat{\gamma}_0 - \widehat{\gamma}_1 z_i
\]
As we saw above, however, the errors $\eta$ are heteroskedastic: $\mathbb{E}(z\eta^2) \neq \mathbb{E}(z)\mathbb{E}(\eta^2)$ etc.
For this reason, we must use robust standard errors:
\[
  \widehat{\Sigma} = \frac{1}{n} \sum_{i=1}^n \left[
  \begin{array}{cc}
    \widehat{\eta}_i^2 & z_i \widehat{\eta}_i^2\\
    z_i \widehat{\eta}_i^2 & z_i^2 \widehat{\eta}_i^2\\
  \end{array}
\right]
\]
leading to
\[
  \widehat{\mbox{AVAR}}\left[ \sqrt{n}(\widehat{\gamma}_1 - \gamma_1) \right] = \frac{\widehat{\mathbb{E}}(\eta^2)}{(1-\widehat{q})^2} + \left[ \frac{1-2\widehat{q}}{\widehat{q}^2(1-\widehat{q})^2} \right]\widehat{\mathbb{E}}(z\eta^2)
\]
Note that $\widehat{q}$ is fixed and equal to $1/2$ in our simulation design, so this becomes $4 \widehat{\sigma}^2_{\eta}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Values of $m^*_{tk}$ in the Simulation}
To calculate $\Sigma$ in our simulation design, we'll need to know the values of $m_{tk}^*$ in the threshold-crossing model with bivariate normal errors:
\begin{align*}
  \left[
  \begin{array}{c}
    \varepsilon \\ \xi
  \end{array}
  \right] &\sim N\left( \left[
  \begin{array}{c}
    0 \\ 0
  \end{array}
\right], \left[
\begin{array}{cc}
  1 & \rho \\
  \rho & 1
\end{array}
\right]\right)\\ \\
  T^* &= \mathbf{1}\left\{ \kappa_0 + \kappa_1 z + \xi > 0 \right\}\\
  \kappa_0 &= \Phi^{-1}(\delta)\\
  \kappa_1 &= \Phi^{-1}(1-\delta) - \Phi^{-1}(\delta)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  m_{00}^* - c &= \mathbb{E}[\varepsilon|T^*=0,z=0] = \mathbb{E}[\varepsilon|\xi \leq -\kappa_0]\\
  m_{01}^* - c &= \mathbb{E}[\varepsilon|T^*=0,z=1] = \mathbb{E}[\varepsilon|\xi\leq -(\kappa_0 + \kappa_1)]\\
  m_{10}^* - c &= \mathbb{E}[\varepsilon|T^*=1,z=0] = \mathbb[\varepsilon|\xi > -\kappa_0]\\
  m_{11}^* - c &= \mathbb{E}[\varepsilon|T^*=1,z=1] = \mathbb{E}[\varepsilon|\xi > -(\kappa_0 + \kappa_1)]
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
  \xi|\varepsilon \sim N(\rho \varepsilon, 1 - \rho^2)
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
\mathbb{P}(\varepsilon \leq x|\xi \leq a) &= \frac{\mathbb{P}(\varepsilon \leq x,\xi \leq a)}{\mathbb{P}(\xi \leq a)} = \frac{\int_{-\infty}^x \int_{-\infty}^a f(\xi|\varepsilon)f(\varepsilon) d \xi \, d \varepsilon }{\Phi(a)}\\
&=\frac{\int_{-\infty}^x F_{\xi|\varepsilon}(a)f(\varepsilon) d\varepsilon}{\Phi(a)} = \frac{\displaystyle\int_{-\infty}^x \varphi(\varepsilon)\Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)d\varepsilon }{\Phi(a)} \\ \\
  f(\varepsilon|\xi\leq a) &= \frac{\varphi(\varepsilon)}{\Phi(a)}\Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right) \\ \\
  \mathbb{E}\left[ \varepsilon|\xi \leq a \right] & = \frac{1}{\Phi(a)} \int_{-\infty}^\infty \varepsilon \varphi(\varepsilon) \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right) d\varepsilon
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
\mathbb{P}(\varepsilon \leq x|\xi > a) &= \frac{\mathbb{P}(\varepsilon \leq x,\xi > a)}{\mathbb{P}(\xi > a)} = \frac{\int_{-\infty}^x \int_a^\infty f(\xi|\varepsilon)f(\varepsilon) d \xi \, d \varepsilon }{1 - \Phi(a)}\\
&=\frac{\int_{-\infty}^x [1 - F_{\xi|\varepsilon}(a)]f(\varepsilon) d\varepsilon}{1- \Phi(a)} = \frac{\displaystyle\int_{-\infty}^x \varphi(\varepsilon)\left[1 - \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)\right]d\varepsilon }{1 - \Phi(a)} \\ \\
f(\varepsilon|\xi > a) &= \frac{\varphi(\varepsilon)}{1-\Phi(a)}\left[1 - \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)\right] \\ \\
\mathbb{E}\left[ \varepsilon|\xi > a \right] & = \frac{1}{1-\Phi(a)} \int_{-\infty}^\infty \varepsilon \varphi(\varepsilon)\left[1 -  \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)\right] d\varepsilon
\end{align*}

\todo[inline]{I've checked all of these integrals in R and they're definitely correct.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{IV Estimator}
First we calculate the probability limits of the IV estimators of $\beta$ and $c$.
Notice that \emph{both} are inconsistent:
\begin{align*}
  \widehat{\beta}_{IV} &= \left( \widetilde{Z}\widetilde{T}/n \right)^{-1}\left(\widetilde{Z}'\mathbf{y}/n\right) =  \left( \widetilde{Z}\widetilde{T}/n \right)^{-1}\widetilde{Z}' \left( \widetilde{T}^* \widetilde{\beta} + \boldsymbol{\varepsilon} \right)/n\\
&= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \left[\widetilde{Z}'\widetilde{T}^*/n \right] \widetilde{\beta} + \widetilde{Z}'\boldsymbol{\varepsilon}/n \right) \\ \\
  \widetilde{Z}'\boldsymbol{\varepsilon}/n &\rightarrow_p \mathbb{E}\left[
  \begin{array}{c}
    \varepsilon \\ z \varepsilon
  \end{array}
\right] = \mathbf{0}\\ \\ 
\left(\widetilde{Z}'\widetilde{T}/n\right)^{-1} &= \left[
\begin{array}{cc}
  1 & \bar{T} \\ \bar{z} & \mathbf{z}'\mathbf{T}/n
\end{array}
\right] \rightarrow_p \left[
\begin{array}{cc}
  1 & p \\ q & p_1 q
\end{array}
\right] = \frac{1}{q(p_1 - p)}\left[
\begin{array}{cc}
  p_1 q & -p \\ -q & 1
\end{array}
\right] \\ \\ 
\widetilde{Z}'\widetilde{T}^*/n &= \left[
\begin{array}{cc}
  1 & \bar{T^*} \\ \bar{z} & \mathbf{z}'\mathbf{T}^*/n
\end{array}
\right] \rightarrow_p \left[
\begin{array}{cc}
  1 & p^* \\ q & p_1^* q
\end{array}
\right] \\ \\ 
\left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\widetilde{T}^*/n \right) &\rightarrow_p \frac{1}{q(p_1 -p)}\left[
\begin{array}{cc}
  qp_1 & - p\\
  -q & 1 
\end{array}
\right]
\left[
\begin{array}{cc}
  1 & p^*\\
  q & qp_1^*
\end{array}
\right]= 
\left[
\begin{array}{cc}
  1 & (p_1 p^* - p_1^* p)/(p_1 - p)\\
  0 & (p_1^* - p^*) / (p_1 - p)
\end{array}
\right] \\ \\ 
\frac{p_1^* - p^*}{p_1 - p} &= \frac{1}{p_1 - p}\left( \frac{p_1 - \alpha_0}{1 - \alpha_0 - \alpha_1} - \frac{p - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) = \frac{1}{1 - \alpha_0 - \alpha_1}\\ \\
\frac{p_1 p^* - p_1^*p}{p_1 - p} &= \frac{1}{p_1 - p}\left[ \frac{p_1(p - \alpha_0)}{1 - \alpha_0 - \alpha_1} -  \frac{p(p_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] = \frac{-\alpha_0}{1 - \alpha_0 - \alpha_1}\\ \\
\boldsymbol{\beta}_{IV} &\rightarrow_p 
\left[
\begin{array}{cc}
  1 & (p_1 p^* - p_1^* p)/(p_1 - p)\\
  0 & (p_1^* - p^*) / (p_1 - p)
\end{array}
\right]\left[
\begin{array}{c}
  c \\ \beta
\end{array}
\right] = \frac{1}{1 - \alpha_0 - \alpha_1}\left[
\begin{array}{c} 
  c - \alpha_0 \beta \\
  \beta
\end{array}
\right] \equiv
\left[
\begin{array}{c}
  c_{IV}\\ \beta_{IV}
\end{array}
\right]
\end{align*}
Now that we have the probability limit of the IV estimator, we can work out the error term $\omega$ that corresponds to it as a function of the ``primitives'' of our model.
We have:
\begin{align*}
  \zeta &= y - (c_{IV} + \beta_{IV} T) = (c + \beta T^* + \varepsilon) - \frac{1}{1 - \alpha_0 - \alpha_1}\left[ \left(c - \alpha_0 \beta\right) + \beta T \right] \\
  &= \varepsilon + c - \left(\frac{c - \alpha_0 \beta}{1 - \alpha_0 - \alpha_1} \right) + \beta\left( T^* - \frac{T}{1 - \alpha_0 - \alpha_1} \right)\\
  &= \varepsilon + \left[\frac{\alpha_0 (\beta - c) - \alpha_1 c}{1 - \alpha_0 - \alpha_1} \right] + \beta\left[ \frac{(T^* - T) - (\alpha_0 + \alpha_1)T^*}{1 - \alpha_0 - \alpha_1} \right]\\
  &= \varepsilon + \left[\frac{\alpha_0 (\beta - c) - \alpha_1 c}{1 - \alpha_0 - \alpha_1} \right] + \beta\left[ \frac{w - (\alpha_0 + \alpha_1)(\pi_0 + \pi_1 z + v)}{1 - \alpha_0 - \alpha_1} \right]
\end{align*}
\todo[inline]{This looks pretty complicated. I'm also not sure we need to work this out analytically. All we really need is to write out the joint limit distribution of the IV and RF\ldots}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Joint Distribution of IV and RF}
To carry out inference for the identified set, we need to work out the joint distribution of the reduced form and IV estimators:
\begin{align*}
  \widehat{\boldsymbol{\beta}}_{IV} &= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\mathbf{y}/n \right)\\
  \widehat{\boldsymbol{\gamma}} &= \left( \widetilde{Z}'\widetilde{Z} \right)^{-1}\left( \widetilde{Z}'\boldsymbol{y}/n \right)
\end{align*}
The probability limits of each estimator define an associated error term, each of which depends in a complicated way on our ``primitive'' model parameters:
\begin{align*}
  y &= c_{IV} + \beta_{IV} T + \zeta \\
  y &= \gamma_0 + \gamma_1 z + \eta 
\end{align*}
But because these two estimators depend only on observable quantities, we can use their residuals to work out the covariance matrix of $(\zeta, \eta)$. 
Accordingly, we proceed as follows:
\begin{align*}
  \widehat{\boldsymbol{\beta}}_{IV} &= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\left[ \widetilde{T}\boldsymbol{\beta}_{IV} + \boldsymbol{\zeta} \right]/n \right) = \boldsymbol{\beta}_{IV} +\left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\boldsymbol{\zeta}/n\right) \\
  \widehat{\boldsymbol{\gamma}} &= \left( \widetilde{Z}'\widetilde{Z} \right)^{-1}\left( \widetilde{Z}'\left[ \widetilde{Z}\boldsymbol{\gamma} + \boldsymbol{\eta} \right]/n \right) = \boldsymbol{\gamma} + \left( \widetilde{Z}'\widetilde{Z}/n \right)^{-1}\left( \widetilde{Z}\boldsymbol{\eta}/n \right)
\end{align*}
yielding
\begin{align*}
  \sqrt{n}\left(\widehat{\boldsymbol{\beta}}_{IV} - \boldsymbol{\beta}_{IV}\right) &= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\boldsymbol{\zeta}/\sqrt{n}\right) \\
  \sqrt{n}\left(\widehat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}\right) &=  \left( \widetilde{Z}'\widetilde{Z}/n \right)^{-1}\left( \widetilde{Z}'\boldsymbol{\eta}/\sqrt{n} \right)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other Stuff\ldots}

\begin{itemize}
  \item Manski was interested in a heterogenous treatment effect model and whether we could bound \emph{ATE} rather than \emph{LATE}.
  \item Would it help in the performance of the GMM estimator if we enforced the bounds on $\alpha_0$ and $\alpha_1$?
\end{itemize}



\paragraph{Exogenous Covariates in a Linear Model:} These should be very easy to handle because we can just stack the GMM moment conditions to include an IV estimator for the parameter on the exogenous covariates in the main equation.
Recall that the usual IV estimator for these parameters is unaffected by measurement error.
We should write this out since it's the case that many people will use in practice given the extreme sample size demands of fully non-parametric estimation!

\paragraph{More About Weak Identification:}
Sophocles pointed out in an email exchange that I had been assuming (incorrectly) that $\mbox{Cov}(y,z)$ is always well-behaved.
This is not the case if $z$ is a weak instrument.
I don't think we can simply assume we have a strong instrument and consider the weak identification that arises from $\beta \approx 0$ in isolation.
I think the two problems of $\beta \approx 0$ and weak $z$ interact in an important way since, as we saw from above, the determinant $|G_\theta|$ that measures the strength of identification depends on the \emph{product} of $\beta$ and $Cov(z,T)$.
I think it the correct interpretation of this is that the magnitude of $\beta$ that gives strong identification should is always relative to the strength of $z$.
If $z$ is very strong, then $\beta$ can be smaller without causing problems.
But if $z$ is weak then I think $\beta$ needs to be really large to get strong identification.
If I recall correctly, we uncovered something in our simulations that appears to agree with this intuition but I need to go back and check.

To see why $\mbox{Cov}(y,z)$ is badly behaved when $z$ is weak, write out an explicit first-stage equation for our model as follows:
\[
T^* = \pi_0 + \pi_1 z + v
\]
where
\begin{align*}
\pi_0 &= \mathbb{E}[T^*|z=0] = p^*_0\\
\pi_1 &= \mathbb{E}[T^*=1|z=1] - \mathbb{E}[T^*|z=0] = p^*_1 - p^*_0
\end{align*}
and $\mathbb{E}[zv]=0$ by construction.
Now,
\begin{align*}
  \mbox{Cov}(z,y) &= \mathbb{E}(zy) - \mathbb{E}(z)\mathbb{E}(y)\\
  &=\mathbb{E}\left[z\left( c + \beta T^* + \varepsilon \right) \right] - q \mathbb{E}\left( c + \beta T^* + \varepsilon \right)\\
  &=\mathbb{E}\left[z\left\{ c + \beta \left( \pi_0 + \pi_1 z + v \right) + \varepsilon \right\} \right] - q \mathbb{E}\left[ c + \beta \left( \pi_0 + \pi_1 z + v \right) + \varepsilon \right]\\
  &= q (c + \beta \pi_0) + \beta \pi_1 \mathbb{E}(z^2) - q\left( c + \beta \pi_0 + \beta \pi_1 \mathbb{E}[z] \right) \\
  &= q (c + \beta \pi_0 + \beta \pi_1)  - q\left( c + \beta \pi_0 + \beta \pi_1 q \right) \\
  &= q(\beta \pi_1  - \beta \pi_1 q) = \beta \pi_1 q(1 - q)\\
  &= \beta (p^*_1 - p^*_0) q(1-q)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Auxiliary Moment Inequalities}
Notice that if $\beta=0$, then the preceding moment equalities do \emph{not} identify $\alpha_1$.
However, we do have auxiliary moment \emph{inequalities} that partially identify $\alpha_1$ regardless of the value of $\beta$.
The simplest of these comes from the relationship  
\[
  p_k^* = \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1}, \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1}
\]
where $p_k = P(T=1|z_k)$ and $p_k^* = P(T^*=1|z_k)$.
(This follows from the Law of Total Probability and our assumption that the mis-classification probabilities rates depend only on $T^*$, not $z$.)
Under our assumption that $\alpha_0 + \alpha_1 < 1$, we obtain $\alpha_0 < \min_k p_k$ and $\alpha_1 < \min_k (1 - p_k)$.
If $\alpha_0 = 0$, as we assume in the present special case, then without any assumption on the true value of $\alpha_1$ we have 
\[0 \leq \alpha_1 < \min_k (1 - p_k) = 1 - \max_k p_k.\] 
Is there some way to use these moment inequalities in estimation?



\paragraph{Under Normality}
  In our simulation for the CDF bounds on $\alpha_0$ and $\alpha_1$, we found that the upper bounds were in fact equal to the true parameter values.
  This is very surprising and is very likely comes from the specific parametric model from which we simulated.
  This happens to have been a model with normally distributed errors.
  Can we say anything about such a model theoretically?
  Perhaps try to write down the likelihood function?
  This could also be a useful way to look at the weak identification problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{April 2017 -- New GMM Formulation}
  Consider the special case in which $\alpha_0 = 0$ so the model is identified from
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 -\alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \left(\frac{\beta}{1 - \alpha_1}\right)\left[2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z) \right] &= 0
\end{align*}
Above we wrote this in a standard GMM form by adding auxiliary moment equations to identify $\mathbb{E}[y^2]$, $\mathbb{E}[yT]$, etc.\
But there's a simpler and more transparent way to do this.
Under our assumptions and $\alpha_0 = 0$, some algebra shows that 
\todo[inline]{Add the algebra from the whiteboard notes later}
\[
  \mathbb{E}\left[ y - \frac{\beta}{1 - \alpha_1}T \right] = c
\]
and that
\[
  \mathbb{E}\left[ y^2 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha} T \right] = \sigma_{\varepsilon\varepsilon} + c^2
\]
where $c$ is the intercept from the regression model and $\sigma_{\varepsilon\varepsilon} = \mbox{Var}(\varepsilon)$.
Now, re-writing the first covariance equation using the linearity of expectation,
\begin{align*}
  \mathbb{E}\left[ yz - \frac{\beta}{1 - \alpha_1} Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ y - \frac{\beta}{1 - \alpha} T\right] &=  0\\
  \mathbb{E}\left[ yz - \frac{\beta}{1 - \alpha_1} Tz \right] - \mathbb{E}[z] c&=  0\\
  \mathbb{E}\left[ \left\{y - c -  \frac{\beta}{1 - \alpha_1} T\right\}z \right] &=  0
\end{align*}
and proceeding similarly for the second covariance equation
\begin{align*}
  \mathbb{E}\left[ y^2z - \frac{\beta}{1 - \alpha_1} 2yTz + \frac{\beta^2}{1 - \alpha_1} Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ y - \frac{\beta}{1 - \alpha_1}yT + \frac{\beta^2}{1 - \alpha_1} T \right] &= 0\\
  \mathbb{E}\left[ y^2z - \frac{\beta}{1 - \alpha_1} 2yTz + \frac{\beta^2}{1 - \alpha_1} Tz \right] - \mathbb{E}[z]\left( \sigma_{\varepsilon\varepsilon} + c^2 \right) &= 0\\
  \mathbb{E}\left[ \left\{y^2 - \sigma_{\varepsilon\varepsilon} - c^2 -\frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right\}z \right] = 0 
\end{align*}
Thus, we can express our estimator in terms of the following four moment equations:
\begin{align*}
  \mathbb{E}\left[ y - c -  \frac{\beta}{1 - \alpha_1} T \right] &= 0\\
  \mathbb{E}\left[ y^2 - \sigma_{\varepsilon\varepsilon} - c^2 -\frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right] &= 0 \\
  \mathbb{E}\left[ \left\{y - c -  \frac{\beta}{1 - \alpha_1} T\right\}z \right] &= 0\\
  \mathbb{E}\left[ \left\{y^2 - \sigma_{\varepsilon\varepsilon} - c^2 -\frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right\}z \right] &= 0 
\end{align*}
To simplify the notation, let $\boldsymbol{\theta} = (\alpha_1, \beta, c, \sigma_{\varepsilon\varepsilon})'$ and define
\begin{align*}
  u(\boldsymbol{\theta}) &= y - c - \frac{\beta}{1 - \alpha_1} T\\
  v(\boldsymbol{\theta}) &= y^2 - \sigma_{\varepsilon\varepsilon} - c^2 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1}
\end{align*}
Then we can express the four moment equalities as 
\[
  \mathbb{E}\left[ g_1(\mathbf{x}, \boldsymbol{\theta}) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    u(\boldsymbol{\theta})\\ v(\boldsymbol{\theta})
  \end{array}
\right] = \mathbf{0}, \quad
\mathbb{E}\left[ g_2(\mathbf{x}, \boldsymbol{\theta}) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    u(\boldsymbol{\theta}) z\\ v(\boldsymbol{\theta}) z
  \end{array}
\right] = \mathbf{0}
\]
We also have two moment inequalities, namely $\alpha_1 \leq 1 - p_1$ and $\alpha_1 \leq 1 - p_0$.
After some algebra (see the whiteboard), we can show that these are equivalent to
\[
  \mathbb{E}\left[ h(\mathbf{x},\theta) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    (1 - \alpha_1) - T(1 - z)/(1 - q)\\
    (1 - \alpha_1) - Tz/q
  \end{array}
\right] \geq \mathbf{0}
\]
where $q = \mathbb{E}[z]$.
We will \emph{condition} on $z$, i.e.\ hold it fixed in repeated samples, so we will not add an extra moment condition for $q$.
Instead we will simply substitute the sample analogue.

To formulate the GMM estimator with moment inequalities as in Moon and Schorfheide (2009), we introduce some further notation.
Let $\boldsymbol{\lambda}$ denote the slack in $h(\mathbf{x}, \theta)$, so that
\[
  \mathbb{E}[h(\mathbf{x},\theta)] = \boldsymbol{\lambda} \geq \mathbf{0} \iff \mathbb{E}[h(\mathbf{x}, \theta) - \boldsymbol{\lambda}] = \mathbf{0}
\]
Further define
\[
  f(\mathbf{x}) = \left[
  \begin{array}{c}
    g(\mathbf{x},\theta) \\ h(\mathbf{x},\boldsymbol{\theta})
  \end{array}
\right], \quad
\psi(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda}) = 
\left[
\begin{array}{c}
  g(\mathbf{x}, \boldsymbol{\theta})\\
  h(\mathbf{x}, \boldsymbol{\theta}) - \boldsymbol{\lambda}
\end{array}
\right]
\]
and let $\Theta = \left\{ \boldsymbol{\theta}\colon \alpha_1 \geq 0, \sigma_{\varepsilon\varepsilon}\geq 0 \right\}$.
Then, the GMM estimator based on our moment equalities and inequalities is
\[
  (\widehat{\boldsymbol{\theta}}, \widehat{\boldsymbol{\lambda}}) = \underset{\boldsymbol{\theta} \in \Theta, \boldsymbol{\lambda} \geq \mathbf{0}}{\arg\min} \quad Q_n(\boldsymbol{\theta}, \boldsymbol{\lambda})
\]
where
\[ 
  Q_n(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \frac{1}{2}\bar{\psi}_n(\mathbf{x},\boldsymbol{\theta}, \boldsymbol{\lambda})' \mathbf{W}_n \bar{\psi}_n(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda})
\]
\[
  \bar{\psi}_n(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \frac{1}{n}\sum_{i=1}^n \psi(\mathbf{x}_i, \boldsymbol{\theta}, \boldsymbol{\lambda}) 
\]
and $\mathbf{W}_n$ is a weighting matrix that should be irrelevant in our case because we're just identified.
We scale the criterion by $1/2$ so that the derivative looks nice.
\todo[inline]{Is this still the case with the moment inequalities? Mechanically we have introduced two new parameters to match the two extra conditions.}

At various points we will need the derivatives of the moment equations.
Let
\[
  \mathbf{G}(\boldsymbol{\theta}) = \mathbb{E}[\nabla_{\boldsymbol{\theta}'} g(\mathbf{x},\boldsymbol{\theta})], \quad \mathbf{H}(\boldsymbol{\theta}) = \mathbb{E}[\nabla_{\boldsymbol{\theta}'} h(\mathbf{x},\boldsymbol{\theta})]
\]
and define 
\begin{align*}
  \boldsymbol{\Psi}(\boldsymbol{\theta}, \boldsymbol{\lambda}) &= \mathbb{E}
  \left[
  \begin{array}{cc}
    \nabla_{\boldsymbol{\theta}'} \psi(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda}) &
    \nabla_{\boldsymbol{\lambda}'} \psi(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda}) 
  \end{array}
\right]\\
&= \mathbb{E}\left[
\begin{array}{cc}
 fill & this \\
 in & later
\end{array}
\right]
\end{align*}

\section{April 1--15, 2017 -- Andrews \& Soares (2010)}
Our moment equalities from above do not identify $\alpha$ when $\beta=0$.
More generally, the estimator based on them performs poorly when $\beta$ is relatively small compared to the error variance.
Continue to assume that $\alpha_0 = 0$ so the moment conditions simplify.

We now consider an inference procedure following Andrews \& Soares.
The basic idea is to ``isolate'' the problematic parameters, in our case $\alpha$ and $\beta$, and carry out joint inference for these using the Anderson-Rubin test statistic.
This is constructed by substituting a null hypothesis $H_0\colon \theta = \theta_0$ into the sample analogue of the GMM moment conditions and relying on the fact that this sample analogue remains ``well-behaved'' even in situations where inference for the GMM \emph{parameter estimator} breaks down.
Examples include parameters on the boundary, and parameters that may not be identified, e.g.\ $\alpha_1$ if $\beta = 0$

\subsection{Simple Example: $\alpha_0 = 0$, $c=0$, and $\sigma_{\varepsilon\varepsilon}=1$}
In our problem $c$ and $\sigma_{\varepsilon\varepsilon}$ are essentially nuisance parameters.
Fortunately, they are \emph{always identified} from our moment conditions regardless of the values of $\beta$ and $\alpha$, as we will discuss further below.
For the moment, we will suppose that $c$ is known to equal zero and $\varepsilon_{\varepsilon\varepsilon}$ is known to equal one as is the case in our baseline simulation.
Later we will estimate them which requires only a small modification of the procedure we know outline.
With the simplifications $\alpha_0, c=0, \sigma_{\varepsilon\varepsilon}=1$ the equality moment conditions become
\begin{align*}
  \mathbb{E}\left[ \left\{y - \frac{\beta}{1 - \alpha_1} T\right\}z \right] &= 0\\
  \mathbb{E}\left[ \left\{y^2 - 1 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right\}z \right] &= 0 
\end{align*}
since we no longer need the $g_1$ block of moment conditions to identify the ``intercepts'' $c$ and $\sigma_{\varepsilon\varepsilon}$.
For this simplified set of moment conditions, our parameter vector is $\boldsymbol{\theta} = (\alpha_1, \beta)'$ and the residuals are given by
\begin{align*}
  u(\boldsymbol{\theta}) &= y - \frac{\beta}{1 - \alpha_1} T\\
  v(\boldsymbol{\theta}) &= y^2 - 1 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1}
\end{align*}
and we can write the equality moment conditions as
\[
\mathbb{E}\left[ g(\mathbf{x}, \boldsymbol{\theta}) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    u(\boldsymbol{\theta}) z\\ v(\boldsymbol{\theta}) z
  \end{array}
\right] = \mathbf{0}
\]
The inequality moment conditions are unchanged from above, namely
\[
  \mathbb{E}\left[ h(\mathbf{x},\theta) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    (1 - \alpha_1) - T(1 - z)/(1 - q)\\
    (1 - \alpha_1) - Tz/q
  \end{array}
\right] \geq \mathbf{0}
\]
where $q = \mathbb{E}[z]$.
As above we will \emph{condition} on $z$, i.e.\ hold it fixed in repeated samples, so we will not add an extra moment condition for $q$.
Instead we will simply substitute the sample analogue.
Note that this means we should hold $q$ \emph{fixed} when bootstrapping below.
We now introduce some notation from Andrews and Soares (2010).

\paragraph{Population Moment Conditions}
\[
  \mathbb{E} \left[ m_j(\mathbf{w}_i,\theta_0) \right]
  \left\{
  \begin{array}{cc}
    \geq 0 & \mbox{for } j = 1, \cdots, p\\
    = 0 & \mbox{for } j = p + 1, \cdots,k \mbox{ where } k = p + v
  \end{array}
  \right.
\]
where $p$ is the number of inequality moment conditions (in our case $p = 2$), $v$ is the number of equality moment conditions (in our case $v = 2$), $\theta_0$ is the true parameter vector, and $\mathbf{w}_i$ is the vector of observations for individual $i$ (in our case $\mathbf{w}_i = (T_i, z_i, y_i)$).

\paragraph{Sample Moment Functions, etc.}
\[
  \bar{m}_n(\theta) = \left[
  \begin{array}{c}
    \bar{m}_{n,1}(\theta)\\
    \vdots \\
    \bar{m}_{n,k}(\theta)\\
  \end{array}
\right], \quad
\bar{m}_{n,j} = \frac{1}{n} \sum_{i=1}^{n} m_j(\mathbf{w}_i, \theta) \mbox{ for } j = 1, \cdots, k
\]
Now, let $\Sigma(\theta_0)$ denote the asymptotic variance of $\sqrt{n}\; \bar{m}_n(\theta)$.
We estimate this quantity using $\widehat{\Sigma}_n(\theta)$.
For iid observations, as in our example, the estimator is
\[
  \widehat{\Sigma}(\theta) = \frac{1}{n} \sum_{i=1}^n \left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]\left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]', \quad 
  m(\mathbf{w}_i, \theta) = \left[
  \begin{array}{c}
    m_1(\mathbf{w}_i, \theta)\\
    \vdots \\
    m_k(\mathbf{w}_i, \theta)\\
  \end{array}
\right]
\]

\paragraph{Test Statistic}
The test statistic takes the form $T_n(\theta) = S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right)$ for some real-valued function $S$.
The example we will use is $S_1$, defined by
\[
  S_1(m, \Sigma) = \sum_{j=1}^p [m_j/\sigma_j]^2_- + \sum_{j=p+1}^{p+v} (m_j/\sigma_j)^2
\]
where $m = (m_1, \cdots, m_k)'$,
\[
  [x]_- = \left\{
  \begin{array}{cc}
    x, & \mbox{if } x <0\\
    0, & \mbox{if } x \geq 0
  \end{array}
\right.
\]
and $\sigma_j^2$ is the $j$th diagonal element of $\Sigma$.
Notice that $S_1$ only gives weight to inequality moment conditions that are \emph{violated}.

\paragraph{Basic Idea of the Test}
Let's return to our specific example for a moment.
The idea is essentially to plug a null hypothesis $\theta^* = (\alpha_1^*, \beta^*)'$ into the sample analogue:
\[
  \sqrt{n}\; \bar{m}_n(\alpha_1^*, \beta^*) = \frac{1}{\sqrt{n}}\sum_{i=1}^n
  \left[
  \begin{array}{r}
    (1 - \alpha_1^*) - T_i (1 - z_i)/(1-q)\\ \\
    (1 - \alpha_1^*) - T_i z_i/q\\ \\
    \left(y_i - \displaystyle\frac{\beta^*}{1 - \alpha_1^*}T_i\right) z_i \\ \\
    \left( y_i^2 - 1 - \displaystyle \frac{\beta^*}{1 - \alpha_1^*} 2y_i T_i + \displaystyle\frac{\beta^{*2}}{1 - \alpha_1^*}T_i\right)z_i 
  \end{array}
\right]
\]
and see if the result is ``large'' after standardizing and squaring the individual elements.
We only give weight to an inequality if it is violated.
The variance matrix of the sample analogue is calculated \emph{under the null}, i.e.\ assuming that $\theta = \theta^*$.
Note that we also use the \emph{centered variance matrix estimator}.

We reject the null if the test statistic is too large.
This gives us \emph{joint inference} for $\alpha$ and $\beta$ \emph{simultaneously}.
To construct a joint confidence region, we need to test pairs $(\alpha_1, \beta)$.
Of course, we restrict $\alpha_1$ to lie in $[0, 1)$.
The resulting confidence region \emph{need not be convex}.
In fact it could even be disconnected!
However, in our particular example, it might be possible to prove that one gets a connected or even convex region.
This is something we should think about since it would reduce the computational burden substantially.
To get \emph{marginal} inference, say for $\beta$ only, one projects the joint confidence set.
This is necessarily conservative, but may not be too bad in practice. 
We'll have to see\ldots

A particularly salient null hypothesis is $\beta = 0$.
Imposing this yields
\[
  \sqrt{n}\; \bar{m}_n(\alpha_1^*, 0) = \frac{1}{\sqrt{n}}\sum_{i=1}^n
  \left[
  \begin{array}{r}
    (1 - \alpha_1^*) - T_i (1 - z_i)/(1-q)\\ 
    (1 - \alpha_1^*) - T_i z_i/q\\ 
    y_i z_i \\ 
    \left( y_i^2 - 1 \right)z_i 
  \end{array}
\right]
\]
We see that this function depends on $\alpha_1^*$ \emph{only via the moment inequalities}.
What is more, the test statistic based on $S_1$ does \emph{not} depend on $\alpha_1^*$ unless the inequality constraints are violated.

\paragraph{Calculating the Critical Value}
The test statistic we will use to test $\theta = \theta^*$ is fairly simple to compute: we simply substitute into the GMM sample analogue.
The critical value for the test, however, is much more complicated.
Following Andrews \& Soares (2010), we use the following bootstrap procedure.
First we define some additional notation.
All of the test statistics considered in Andrews \& Soares (2010) satisfy
\[
  T_n = S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right) = S\left( \widehat{D}^{-1/2}(\theta) \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Omega}_n(\theta) \right)
\]
where
\[
  \widehat{D}_n(\theta) = \mbox{diag}\left( \widehat{\Sigma}\left( \theta \right) \right), \quad \widehat{\Omega}_n(\theta) = \widehat{D}_n^{-1/2}(\theta)\, \widehat{\Sigma}(\theta)\, \widehat{D}_n^{-1/2}(\theta)
\]
Now, let $\left\{ \mathbf{w}_i^* \right\}_{i=1}^n$ denote a bootstrap sample and define the associated bootstrap quantities
\begin{align*}
  M_n^*(\theta) &=  \sqrt{n} \left( \widehat{D}^*\left(\theta\right) \right)^{-1/2} \left( \bar{m}^*_n\left( \theta \right) - \bar{m}_n\left( \theta) \right) \right)\\
  \widehat{\Omega}^*(\theta) &= \left( \widehat{D}^*\left( \theta \right) \right)^{-1/2} \widehat{\Sigma}_n^*(\theta)\left( \widehat{D}^*\left( \theta \right) \right)^{-1/2} \\
  \widehat{D}^*\left( \theta \right) &= \mbox{diag}\left( \widehat{\Sigma}_n^*\left( \theta \right) \right)\\
  m_n^*(\theta) &= \frac{1}{n} \sum_{i=1}^n m(\mathbf{w}_i^*,\theta)\\
  \widehat{\Sigma}(\theta)^* &= \frac{1}{n} \sum_{i=1}^n \left[ m(\mathbf{w}_i^*, \theta) - \bar{m}_n^*(\theta) \right]\left[ m(\mathbf{w}_i^*, \theta) - \bar{m}_n^*(\theta) \right]'
\end{align*}
Note that $M_n^*(\theta)$ is centered around the \emph{non-bootstrap}  sample analogue $\bar{m}_n(\theta)$: this is \emph{very important!}
Now we describe the procedure for calculating the bootstrap critical value:
\begin{enumerate}
  \item Calculate $\sqrt{n}\; \bar{m}_n\left( \theta_0 \right)$ and $\widehat{\Sigma}(\theta_0)$ under the null hypothesis $H_0\colon \theta = \theta_0$.
  \item Determine which inequality moment conditions are ``far from binding'' as follows:
    \begin{itemize}
      \item Let $j \in J = \{ 1, \cdots, p\}$ index the inequality moment conditions.
      \item Let $\widehat{\sigma}_{n,j}(\theta_0)^2$ denote the $(j,j)$ element of $\widehat{\Sigma}(\theta_0)$
      \item For each $j\in J$ calculate the ``t-statistic'' $ t_{n,j} = \sqrt{n}\; \bar{m}_j(\theta_0)/\widehat{\sigma}_{n,j}(\theta_0)$
      \item Let $\mathcal{FB}$ denote the subset of $J$ for which $t_{n,j} >\sqrt{\log n}$.
        These are the inequality moment conditions that are ``far from binding'' under $H_0\colon\theta = \theta_0$. 
    \end{itemize}
  \item Calculate the test statistic $T_n = S_1\left(\sqrt{n}\; \bar{m}_n\left( \theta_0 \right), \widehat{\Sigma}\left( \theta_0 \right)  \right)$.
  \item Calculate the bootstrap critical value for the test as follows:
    \begin{itemize}
      \item Draw $R$ bootstrap samples -- each with sample size $n$. 
      \item For each bootstrap sample, $r$, calculate $M^{**}_{n,r}(\theta_0)$ and $\widehat{\Omega}^{**}_{n,r}(\theta_0)$ -- the bootstrap versions of $M_n(\theta_0)$ and $\widehat{\Omega}(\theta_0)$, defined above but with a slight change: \emph{drop} any moment inequality $j \in \mathcal{FB}$.
        That is, drop any inequality that we determined was far from binding \emph{on the basis of the real data} (i.e.\ \emph{not} this bootstrap sample!)
      \item For each bootstrap sample $r$ calculate $T_{n,r}^{**} = S_1\left(M^{**}_{n,r}\left( \theta_0 \right), \widehat{\Omega}^{**}_{n,r}\left( \theta_0 \right)  \right)$.
      \item Set $\widehat{c}_n(\theta_0, 1-\delta)$ equal to the $1-\delta$ sample quantile of the $\left\{ T_{n,r}^{**} \right\}_{r=1}^R$
    \end{itemize}
  \item Reject $H_0\colon \theta = \theta_0$ if $T_n > \widehat{c}_n(\theta_0, 1-\delta)$
  \item To construct a $(1 - \delta)\times 100\%$ confidence set, invert the test of $H_0\colon \theta = \theta_0$ for $\theta_0 \in \Theta$.
\end{enumerate}

\section{April 15--18, 2017}
 
\subsection*{Second Moment Inequalities}

\paragraph{Step 1: Second Moment Bounds}
Our model has $\varepsilon = y - c - bT^*$ where $\mathbb{E}[\varepsilon]=0$.
Conditional on $T^*=0$, $\varepsilon = y-c$ and conditional on $T^*=1$, $\varepsilon = y - c -\beta$.
Hence,
\begin{align*}
  \mathbb{E}\left[ \varepsilon^2| T^*=0, z=k\right] &= \mathbb{E}\left[ y^2 - 2cy|T^*=0, z=k \right] + c^2\\
  \mathbb{E}\left[ \varepsilon^2| T^*=1, z=k\right] &= \mathbb{E}\left[ y^2 - 2(\beta + c)y|T^*=1, z=k \right] + (\beta + c)^2
\end{align*}
The bounds will impose that $\mbox{Var}(\varepsilon|T^*,z)>0$ which is equivalent to $\mathbb{E}\left[ \varepsilon^2|T^*,z \right]>0$ since $\varepsilon$ is mean zero:
\begin{align*}
  \mathbb{E}\left[ y^2|T^*=0, z=k \right]&> 2c \mathbb{E}\left[ y|T^*=0, z=k \right] - c^2\\
  \mathbb{E}\left[ y^2|T^*=1, z=k \right]&> 2(\beta + c) \mathbb{E}\left[ y|T^*=1, z=k \right] - (\beta + c)^2
\end{align*}

\todo[inline]{These are not in fact the bounds we want! I messed up since $\varepsilon$ is \emph{not} mean zero \emph{conditional} on $T^*$ and $z$. These bounds are still correct, but they're not the best we can do and may not in fact be better than the simple first-moment bounds\ldots}

\paragraph{Step 2: Relate $\mathbb{E}[y^r|T^*,z]$ to $\mathbb{E}[y^r|T,z]$}
By iterated expectations and the assumption that $\mathbb{E}\left[ y^r|T,T^*,z \right]= \mathbb{E}\left[ y^r|T^*,z \right]$,
\begin{align*}
  \mathbb{E}\left[ y^r|T=0, z_k \right] &= \mathbb{E}\left[ y^r|T^*=0, z_k \right]\mathbb{P}(T^*=0|T=0, z_k) + \mathbb{E}\left[ y^r|T^*=1, z_k \right]\mathbb{P}(T^*=1|T=0, z_k)\\
  \mathbb{E}\left[ y^r|T=1, z_k \right] &= \mathbb{E}\left[ y^r|T^*=0, z_k \right]\mathbb{P}(T^*=0|T=1, z_k) + \mathbb{E}\left[ y^r|T^*=1, z_k \right]\mathbb{P}(T^*=1|T=1, z_k)
\end{align*}
The preceding is a linear system of the form
\begin{align*}
  a &= \pi x + (1 - \pi)y \\
  b &= (1 - \delta) x + \delta y
\end{align*}
and hence its solution is
\begin{align*}
  x &= \left[ \frac{\delta}{\pi\delta - (1 - \pi)(1 - \delta)} \right] a + \left[ \frac{-(1 - \pi)}{\pi \delta - (1 - \pi)(1 - \delta)} \right] b\\
  y &= \left[ \frac{-(1 - \delta)}{\pi\delta - (1 - \pi)(1 - \delta)} \right] a + \left[ \frac{\pi}{\pi \delta - (1 - \pi)(1 - \delta)} \right] b\\
\end{align*}
by Bayes' rule, as we show in the appendix to sick-instruments,
\begin{align*}
  \pi &= \mathbb{P}(T^*=0|T=0, z_k) =(1 - \alpha_0)(1 - p_k^*)/(1 - p_k)\\
  1 - \pi &= \mathbb{P}(T^*=1|T=0, z_k) =\alpha_1p_k^*/(1 - p_k)\\
  \delta &= \mathbb{P}(T^*=1|T=1, z_k) =(1 - \alpha_1)p_k^*/p_k\\
  1-\delta &= \mathbb{P}(T^*=0|T=1, z_k) = \alpha_0 (1 - p_k^*)/p_k
\end{align*}
Some algebra shows that 
\[
  \pi\delta - (1 - \pi)(1 - \delta) = \frac{(p_k - \alpha_0)(1 - p_k - \alpha_1)}{1 - \alpha_0 - \alpha_1}
\]
Hence, after simplifying and rearranging, it follows that 
\begin{align*}
  p_k(1 - p_k)(1 - p_k - \alpha_1)\mathbb{E}\left[ y^r|T^*=0, z_k \right] &= (1 - \alpha_1)(1 - p_k) \mathbb{E}[y^r|T=0, z_k] - \alpha_1 p_k \mathbb{E}[y^r|T=1,z_k]\\
  p_k(1 - p_k)(p_k - \alpha_0)\mathbb{E}\left[ y^r|T^*=1, z_k \right] &= (1 - \alpha_0)p_k\mathbb{E}[y^r|T=1,z_k] - \alpha_0(1 - p_k) \mathbb{E}[y^r|T=0,z_k]
\end{align*}


\paragraph{Step 3: Convert Conditional to Unconditional Moments}
By iterated expectations and the assumption that $\mathbb{E}\left[ y^r|T,T^*,z \right]= \mathbb{E}\left[ y^r|T^*,z \right]$
\begin{align*}
  p_k &= \mathbb{E}[T|z=k] = \mathbb{E}\left[ T \mathbf{1}(z=k) \right]/\mathbb{P}(z=k)\\
  \mathbb{E}\left[ y^r |T=0, z=k\right] &= \mathbb{E}\left[ y^r(1 - T) \mathbf{1}(z=k) \right]/\left[ (1 - p_k) \mathbb{P}(z=k)\right]\\
  \mathbb{E}\left[ y^r |T=1, z=k\right] &= \mathbb{E}\left[ y^r T \mathbf{1}(z=k) \right]/\left[p_k \mathbb{P}(z=k)\right]
\end{align*}

\paragraph{Step 4: Substitute Steps 2--3 into Step 1}
After some algebra, we find that
\begin{align*}
  \mathbb{E}\left[ \mathbf{1}(z=k)\left\{ T - (1 - \alpha_1) \right\}\left(y^2 - 2cy \right) \right] &< c^2 \mathbb{P}(z=k)p_k(1 - p_k)(1 - p_k - \alpha_1)\\
  \mathbb{E}\left[ \mathbf{1}(z=k)(\alpha_0 - T)\left\{ y^2 - 2(\beta + c)y \right\} \right] &< (\beta + c)^2 \mathbb{P}(z=k) p_k(1 - p_k)(p_k - \alpha_0)
\end{align*}

\paragraph{Step 5: Complete the Square}
\begin{align*}
  \mathbb{E}\left[ \mathbf{1}(z=k)\left\{ 1 - \alpha_1 - T \right\}\left(y - c\right)^2 \right] &> c^2 \mathbb{P}(z=k)(1 - p_k - \alpha_1)\left[ 1 - p_k(1 - p_k) \right]\\
  \mathbb{E}\left[ \mathbf{1}(z=k)(T - \alpha_0)\left\{ y - (\beta + c) \right\}^2 \right] &> (\beta + c)^2 \mathbb{P}(z=k) (p_k - \alpha_0)\left[ 1 - p_k(1 - p_k) \right]
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{April 21--23}
\subsection*{The Full Set of Moment Inequalities}
Above we considered the special case in which $\alpha_0= 0$ so that first and second moments were sufficient to identify the model.
We now derive the GMM-style moment conditions for the general case in which $\alpha_0$ may not equal zero and we need to use third moments for identification.

\todo[inline]{Note that we have checked all of these equalities numerically in our simulation study and they are indeed correct!}

\paragraph{Notation and Identification} Define the following re-parameterization
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right]
\end{align*}
Using this notation, the covariance equations from above become
\begin{align*}
  \mbox{Cov}(y,z) - \theta_1 \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \theta_1 2\mbox{Cov}(yT,z) + \theta_2 \mbox{Cov}(T,z)&= 0\\
  \mbox{Cov}(y^3,z) - \theta_1 3 \mbox{Cov}(y^2T,z) + \theta_2 3\mbox{Cov}(yT,z) - \theta_3\mbox{Cov}(T,z) &= 0
\end{align*}
Note that it is trivial to prove that $\theta_1, \theta_2$ and $\theta_3$ are identified.
To show that $\beta, \alpha_0$ and $\alpha_1$ are identified, write
\begin{align*}
  \theta_2/\theta_1^2 &= 1 + \left( \alpha_0 - \alpha_1 \right)\\
  \theta_3/\theta_1^3 &= (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0 (1 - \alpha_1) 
\end{align*}
which we can always do provided that $\theta_1\neq 0$ which is equivalent to $\beta \neq 0$.
Re-arranging the first equation allows us to solve for $\alpha_0$ as a function of $\alpha_1$.
Substituting this into the second gives us a quadratic in $\alpha_1$ only.
This should be identical to the quadratic we obtained in our original identification proof.

\paragraph{First Moment Equalities}
Expanding,
\begin{align*}
  \mbox{Cov}(y,z) - \theta_1 \mbox{Cov}(T,z) &= 0\\
  \mathbb{E}[yz - \theta_1 Tz] - \mathbb{E}[z]\mathbb{E}[y - \theta_1 T]&= 0
\end{align*}
and
\begin{align*}
  \mathbb{E}[y - \theta_1 T] &= c + \beta \mathbb{E}\left[ T^* - T/(1 - \alpha_0 - \alpha_1) \right]\\
  &= c + \beta\left\{ \frac{\mathbb{E}[T] - \alpha_0}{1 - \alpha_0 - \alpha_1} - \frac{\mathbb{E}[T]}{1 - \alpha_0 - \alpha_1} \right\}\\
&= c - \alpha_0 \theta_1 
\end{align*}
Thus,
\[
  \mathbb{E}\left[ \left\{ y - \left( c - \alpha_0 \theta_1 \right) - \theta_1 T\right\}z \right] = 0
\]
Now define 
\begin{align*}
\kappa_1 &= c - \alpha_0 \theta_1\\
u_1 &= y - \kappa_1 - \theta_1 T
\end{align*}
Using this notation, we obtain the unconditional moment equalities
\[
  \mathbb{E}\left[
  \begin{array}{l}
    u_1(\kappa_1, \theta_1)\\ u_1(\kappa_1, \theta_1) z
  \end{array}
\right] = \mathbf{0}
\]

\paragraph{Second Moment Equalities}

\begin{align*}
  \mbox{Cov}(y^2,z) - \theta_1 2\mbox{Cov}(yT,z) + \theta_2 \mbox{Cov}(T,z)&= 0\\
  \mathbb{E}\left[ y^2z - \theta_1 2yTz + \theta_2 Tz \right] -\mathbb{E}\left[ z \right]\mathbb{E}\left[ y^2 - \theta_1 2yT + \theta_2 T \right] &= 0
\end{align*}

Now, using some lemmas from the notes in our sick-instruments paper,
\begin{align*}
  \mathbb{E}[T\varepsilon] &= \mbox{Cov}(T,\varepsilon) \\
  &= \mbox{Cov}(T^*,\varepsilon) + \mbox{Cov}(w,\varepsilon)\\
  &= \mbox{Cov}(T^*,\varepsilon) - \mbox{Cov}(T^*,\varepsilon)(\alpha_0 + \alpha_1)\\
  &= \mathbb{E}(T^*\varepsilon)(1 - \alpha_0 - \alpha_1)
\end{align*}
hence,
\begin{align*}
  \mathbb{E}[yT] &= c \mathbb{E}[T] + \beta \mathbb{E}[TT^*] + \mathbb{E}\left[T,\varepsilon \right] \\
  &= cp + \beta \mathbb{P}(T=1,T^*=1) + \mathbb{E}[T^*\varepsilon](1 - \alpha_0 - \alpha_1)\\
  &= cp + \beta (1 - \alpha_1)p^* + \mathbb{E}[T^*\varepsilon](1 - \alpha_0 - \alpha_1)
\end{align*}
Combining this with
\[
  \mathbb{E}[y^2] = c^2 + \beta^2 p^* + \sigma_{\varepsilon\varepsilon} + 2 c\beta p^* + 2 \beta \mathbb{E}[T^*\varepsilon]
\]
we find that 
\[
  \mathbb{E}\left[ y^2 - \theta_1 2yT + \theta_2 T \right] = \cdots = c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0(\theta_2 - 2c\theta_1)
\]
\todo[inline]{For the steps, see our whiteboard notes from 2017-04-21 17.06.32}

Now, define
\begin{align*}
  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
  u_2 &= y^2 - \kappa_2 - \theta_1 2 yT + \theta_2 T
\end{align*}
Using this notation, we obtain the unconditional moment equalities
\[
  \mathbb{E}\left[
  \begin{array}{l}
    u_2(\kappa_2, \theta_1, \theta_2)\\ 
    u_2(\kappa_2, \theta_1, \theta_2) z
  \end{array}
\right] = \mathbf{0}
\]


\paragraph{Third Moment Equalities} 
\begin{align*}
  \mbox{Cov}(y^3,z) - \theta_1 3 \mbox{Cov}(y^2T,z) + \theta_2 3\mbox{Cov}(yT,z) - \theta_3\mbox{Cov}(T,z) &= 0\\
  \mathbb{E}\left[ y^3 z - \theta_1 3 y^2 Tz + \theta_2 3 yTz - \theta_3 Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ y^3 - \theta_1 3 y^2T + \theta_2 3 yT - \theta_3 T \right] &= 0
\end{align*}

\[
  \mathbb{E}[y^3] = \cdots = c^3 + \beta p^* \left( 3 c^2 + 3 c\beta + \beta^2 \right) + 3 \beta \mathbb{E}[\varepsilon T^*] (2c + \beta) + 3 c \sigma_{\varepsilon\varepsilon} + 3\beta \mathbb{E}[\varepsilon^2 T^*] + \mathbb{E}[\varepsilon^3]
\]

\todo[inline]{For derivations of the following, see our whiteboard notes from April 21st and 22nd, 2017}
\[
  \mathbb{E}\left[ y^2T \right] = \cdots = c^2 p + \beta(1 - \alpha_1)p^* + \mathbb{E}[T\varepsilon^2] + 2 c \beta(1 - \alpha_1) p^* + 2 c \mathbb{E}[T\varepsilon] + 2 \beta \mathbb{E}[TT^* \varepsilon] 
\]
and 
\begin{align*}
  \mathbb{E}[T\varepsilon^2] &= \cdots = \alpha_0 \sigma_{\varepsilon\varepsilon} + (1 - \alpha_0 - \alpha_1) \mathbb{E}[\varepsilon^2 T^*]\\
  \mathbb{E}[TT^*\varepsilon] &= \cdots = (1 - \alpha_1) \mathbb{E}[T^*\varepsilon] 
\end{align*}

We then need to calculate $\mathbb{E}[y^3 - \theta_1 3 y^2T + \theta_2 3yT - \theta_3 T]$.
Using the preceding expressions, we can show after some algebra that the $\mathbb{E}[T^*\varepsilon]$ and $\mathbb{E}[T^*\varepsilon^2]$ terms drop out, hence
\begin{align*}
\mathbb{E}[y^3 - \theta_1 3 y^2T + \theta_2 3yT - \theta_3 T]&=
\left\{ c^3 + \beta p^* \left( 3c^2 + 3 c\beta + \beta^2 \right) \right\} + 3c \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \theta_1 3 \alpha_0 \sigma_{\varepsilon\varepsilon} - \theta_3 p\\
&- \theta_1 3 \left\{ c^2p + \beta^2(1 - \alpha_1)p^* + 2 c \beta (1 - \alpha_1) p^* \right\} + \theta_2 3 \left\{ cp + \beta (1 - \alpha_1)p^* \right\}
\end{align*}
After \emph{even more algebra} we can show that this expression depends neither on $p$ nor on $p^*$:
\small
\begin{align*}
\mathbb{E}[y^3 - \theta_1 3 y^2T + \theta_2 3yT - \theta_3 T]&=
c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_2 \frac{1 - \alpha_1}{1 + \alpha_0 - \alpha_1} \right]\\
&= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]\\
\end{align*}
\normalsize
Now let 
\begin{align*}
  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]\\
  u_3 &= y^3 - \kappa_3 - \theta_1 3y^2 T + \theta_2 3yT - \theta_3 T
\end{align*}
we obtain the following moment equalities
\[
  \mathbb{E}\left[
  \begin{array}{l}
    u_3(\kappa_3, \theta_1, \theta_2, \theta_3)\\ 
    u_3(\kappa_3, \theta_1, \theta_2, \theta_3) z
  \end{array}
\right] = \mathbf{0}
\]

\paragraph{Putting Everything Together} Define
\begin{align*}
\boldsymbol{\kappa} &= (\kappa_1, \kappa_2, \kappa_3)' \\
\boldsymbol{\theta} &= (\theta_1, \theta_2, \theta_3)'\\
\mathbf{u}(\boldsymbol{\kappa}, \boldsymbol{\theta}) &= 
\left[
\begin{array}{ccc}
  u_1(\kappa_1, \theta_1) & 
  u_2(\kappa_2, \theta_2) &
  u_3(\kappa_3, \theta_3)
\end{array}
\right]'
\end{align*}
where
\begin{align*}
\kappa_1 &= c - \alpha_0 \theta_1\\
  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]
\end{align*}
and
\begin{align*}
u_1(\kappa_1, \theta_1) &= y - \kappa_1 - \theta_1 T\\
  u_2(\kappa_2, \theta_1, \theta_2) &= y^2 - \kappa_2 - \theta_1 2 yT + \theta_2 T\\
  u_3(\kappa_3, \theta_1, \theta_2, \theta_3) &= y^3 - \kappa_3 - \theta_1 3y^2 T + \theta_2 3yT - \theta_3 T
\end{align*}
Then the full system of moment equalities is given by
\[
  \mathbb{E}\left[
  \begin{array}{l}
    \mathbf{u}\left( \boldsymbol{\kappa}, \boldsymbol{\theta} \right)\\
    \mathbf{u}\left( \boldsymbol{\kappa}, \boldsymbol{\theta} \right)z
  \end{array}
\right] = \mathbf{0}
\]
where
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right]
\end{align*}


\paragraph{What about exogenous covariates?}
At one extreme, we could simply carry out the above \emph{conditional} on $\mathbf{x}$.
This would entail treating $\boldsymbol{\kappa}$ and $\boldsymbol{\theta}$ as functions of $\mathbf{x}$ such that $\alpha_0, \alpha_1$ and $\beta$ would be allowed to depend on $\mathbf{x}$.
To implement this, one would need to estimate a number of conditional mean functions: $\mathbb{E}[y|\mathbf{x}]$, $\mathbb{E}[T|\mathbf{x}]$, $\mathbb{E}[yT|\mathbf{x}]$, $\mathbb{E}[y^3|\mathbf{x}]$, and $\mathbb{E}[y^2T|\mathbf{x}]$.
It would appear that this leads to a standard two-step estimation problem although clearly you'd need a lot of data to have any chance!
Another idea would be to impose some restrictions on the way in which $\mathbf{x}$ affects $\alpha_0$ etc, leading to a semi-parametric estimator.
At the other extreme, we could impose the assumption that $\mathbf{x}$ affects $y$ linearly and $\alpha_0, \alpha_1$ do not vary with $\mathbf{x}$:
\[
  y = c + \beta T^* + \mathbf{x}'\boldsymbol{\gamma} + \varepsilon
\]
Under this assumption the result of Frazis \& Loewenstein (2003) holds -- the IV estimator would recover $\kappa_1, \theta_1, \boldsymbol{\gamma}$.
Since $(y - \mathbf{x}'\boldsymbol{\gamma} ) = c + \beta T^* + \varepsilon$, all of the preceding moment conditions should still hold only with $(y - \mathbf{x}'\boldsymbol{\gamma})$ in place of $y$.
To identify $\boldsymbol{\gamma}$ we add an extra moment condition in which $\mathbf{x}$ multiplies the $u_1$ error term, re-defined to subtract $\mathbf{x}'\boldsymbol{\gamma}$.

%\subsection*{Correcting the Second Moment Inequalities}
%In the derivation from above I mistakenly claimed that since $\varepsilon$ is mean zero, it is sufficient to work with the conditional means of $\varepsilon^2$ given $T^*$ and $z$. 
%This is not correct since the \emph{conditional} means of $\varepsilon$ are not zero.
%The bounds from above are not wrong, but they can be improved by looking at the conditional variance of $\varepsilon$ rather than simply $\mathbb{E}[\varepsilon^2]$.
%Steps 2--3 from above are still usable, but we need to change step 1.
%
%Our inequalities come from the fact that
%\[
%  \mbox{Var}(\varepsilon|T^*,z) > 0 \iff \mathbb{E}[\varepsilon^2 |T^*,z] > \left\{\mathbb{E}[\varepsilon|T^*,z]\right\}^2
%\]
%We've already done the calculations for $\varepsilon^2|T^*,z$.
%Now we simply need to do the same for $\varepsilon|T^*,z$, namely
%\begin{align*}
%  \mathbb{E}[\varepsilon|T^*=0,z=k] &= \mathbb{E}\left[ y |T^*=0, z=k \right] - c\\
%  \mathbb{E}[\varepsilon|T^*=1,z=k] &= \mathbb{E}\left[ y |T^*=1, z=k\right] - (c + \beta)
%\end{align*}
%Now we relate $\mathbb{E}[y|T^*,z]$ to $\mathbb{E}[y|T,z]$ using the calculations from above:
%\begin{align*}
%  \mathbb{E}\left[ y|T^*=0, z_k \right] &= \frac{(1 - \alpha_1)}{p_k (1 - p_k - \alpha_1)} \mathbb{E}[y|T=0, z_k] - \frac{\alpha_1 }{(1 - p_k)(1 - p_k - \alpha_1)}\mathbb{E}[y|T=1,z_k]\\
%  \mathbb{E}\left[ y|T^*=1, z_k \right] &= \frac{(1 - \alpha_0)}{( 1 - p_k)(p_k - \alpha_0)}\mathbb{E}[y|T=1,z_k] - \frac{\alpha_0}{p_k (p_k - \alpha_0)} \mathbb{E}[y|T=0,z_k]
%\end{align*}
%After lots of algebra (see our handwritten notes), we obtain:
%\small
%\begin{align*}
%  p_k (1 - p_k)(1 - p_k - \alpha_1) \left\{ (1  - \alpha_1)(1 - p_k)\mathbb{E}\left[ y^2|T=0,z_k \right] - \alpha_1 p_k \mathbb{E}[y^2|T=1,z_k \right\}  \\
%    > \left\{ (1 - \alpha_1)(1 - p_k)\mathbb{E}[y|T=0,z_k] - \alpha_1 p_k \mathbb{E}[y|T=1,z_k] \right\}^2
%\end{align*}
%and
%\begin{align*}
%  p_k (1 - p_k)(p_k - \alpha_0) \left\{ (1  - \alpha_0)p_k\mathbb{E}\left[ y^2|T=1,z_k \right] - \alpha_0 (1-p_k) \mathbb{E}[y^2|T=0,z_k \right\}  \\
%    > \left\{ (1 - \alpha_0)p_k\mathbb{E}[y|T=1,z_k] - \alpha_0 (1-p_k) \mathbb{E}[y|T=0,z_k] \right\}^2
%\end{align*}
%Finally, we need to convert these into expressions involving unconditional moments via
%\begin{align*}
%  p_k &= \mathbb{E}[T|z=k] = \mathbb{E}\left[ T \mathbf{1}(z=k) \right]/\mathbb{P}(z=k)\\
%  \mathbb{E}\left[ y^r |T=0, z=k\right] &= \mathbb{E}\left[ y^r(1 - T) \mathbf{1}(z=k) \right]/\left[ (1 - p_k) \mathbb{P}(z=k)\right]\\
%  \mathbb{E}\left[ y^r |T=1, z=k\right] &= \mathbb{E}\left[ y^r T \mathbf{1}(z=k) \right]/\left[p_k \mathbb{P}(z=k)\right]
%\end{align*}
%These appear to match our expressions from the sick-instruments paper.
%
%
%\subsection*{Estimated Parameters in Andrews \& Soares}
%In our simple example from above applying the Andrews and Soares (2010) generalized moment selection procedure to the case where $\alpha_0 = 0$, we assumed for simplicity that $c$ and $\sigma_{\varepsilon\varepsilon}$ were known.
%Without $\alpha_0 = 0$, the analogous assumption would be that $\boldsymbol{\kappa}$ is known.
%In practice, however, this parameter is estimated from the data. 
%Accounting for this fact requires a change in the procedure for calculating the variance matrix $\Sigma$ of the moment conditions under the null hypothesis. 
%We know explain how this works.

\section{May 5th, 2017 -- Todo for Revised Paper}

\begin{enumerate}
  \item New notation to accomodate covariates etc.\ as in Mahajan and Lewbel
  \item Move Mahajan stuff into appendix; possibly convert back to his notation later -- I have handwritten notes on this.
  \item Identification results: identification from higher moments, lack of identification from conditional means alone
  \item Explain briefly how to do fully non-parametric estimation, specialize to some simple cases, e.g.\ linear model
  \item Explain about inference versus estimation.
  \item Possibly show that Mahajan, Lewbel, etc.\ also suffer from a weak identification problem. This might be helpful for selling the paper.
    Would need to think about what inequalities to use in their case.
    Presumably we could still use the ``weak'' bounds, but there might be others we could exploit.
  \item Testing $\beta = 0$ should be very easy: don't need Andrews \& Soares at all. I think we can either just use the Stock \& Wright GMM-AR or even a plain old-fashioned GMM inference for testing a linear restriction.
  \item Canay et al.\ projection inference for $\beta$? Need to figure out if this actually gives a speed-up in our case.
    If so, could be very useful for simulations.
    Would be nice to show that we have power to reject the probability limit of the IV and reduced form.
    Is it easier to handle the strongly-identified parameters using their method?
  \item Could also talk briefly about the continuum of moment conditions idea and how one obtains identification from quantiles. There's also a question of using more inequalities by imposing the independence assumption for the measurement error.
    Could also proceed without our higher moment assumptions and just get inference for the identified set.
  \item Is there any way to handle treatment effect heterogeneity? Should be ok if it's modeled heterogeneity but maybe there are some other special cases we can handle? What about that quantile treatment effect idea?
  \item Simulations: want to show that we can actually learn something useful. Things we want to show are that we have more power than just the RF for testing $\beta=0$, that we have power to reject the RF and IV plims in certain cases.
  \item Need to show at least in simulations that our confidence regions aren't insane. Would be good if we could formally argue that they must be convex and connected, for example.
    Maybe the key is to show that our inequalities are always well-behaved and that adding the equalities cann't make the problem suddenly become badly behaved.
  \item Andrews \& Soares with estimates of the strongly identified parameters to get a joint region for $\alpha_0, \alpha_1$, and $\beta$. But this is slow, so we probably can't do simulations although it would be interesting in the empirical examples.
  \item Try to get the projection inference for $\beta$ working 
  \item Empirical example or examples: Oreopolous? Maybe the Heckman JPTA dataset? Petra suggested emailing someone about this but I forget who it was\ldots

\end{enumerate}

\section{Lewbel (2007)}

\paragraph{Notation}
Observe $Y,Z$ and $T$ where $T$ is a proxy for $T^*$ and
\begin{align*}
  h^*(X,T^*) &= E(Y|X,T^*)\\
  T^* &= \mbox{unobserved binary regressor}\\
  X &= \mbox{vector of covariates}\\
  Y &= \mbox{outcome}
\end{align*}
Since $T^*$ is binary, without loss of generality,
\begin{align*}
  h^*(X,T^*) &= h_0^*(X) + \tau^*(x)T^*\\
  h^*_0(X) &= h^*(X,0)\\
  \tau^*(x) &= h^*(x,1) - h^*(x,0)
\end{align*}
Goal is to estimate $\tau^*(x)$.
Below we will partition $X$ according to $X = (V,Z)$ where $V$ is an ``instrument-like variable'' and $Z$ is the set of remaining covariates.

\paragraph{Assumption A1} 
There exists $E(Y|X,T^*,T) = E(Y|X,T^*)$.
Equivalently, $Y$ is mean-independent of $T-T^*$, conditional on $X,T^*$ -- mis-classification does not affect true expected outcome.
This rules out placebo effects, etc.

\paragraph{More Notation}
\begin{align*}
  r^*(x) &= E(T^*|X=x) = P(T^*=1|X=x)\\
  b_0(x) &= P(T=1|T^*=0,X=x)\\
  b_1(x) &= P(T=0|T^*=1,X=x)\\
  r(x) &= E(T|X=x)\\
  \tau(x) &= h(x,1) - h(x,0)\\
  h(x,t) &= E(Y|X=x,T=t)
\end{align*}

\paragraph{Assumption A2} There exist $b_0(x) + b_1(x) <1$ and $0 < r^*(x) <1$ for all $x$ in the support of $X$.
\paragraph{Theorem 1} Under Assumption A1 there exists a function $m(x)$ such that $|m(x)|\leq 1$ and $\tau(x) = \tau^*(x)m(x)$.
If we add Assumption A2 then $m(x)>0$ as well.

\begin{itemize}
  \item Analogous to attenuation bias under classical measurement error: A1 implies that $\tau(x)$ is a lower bound for the magnitude of $\tau^*(x)$.
    Adding A2, sign of $\tau(x)$ agrees with that of $\tau^*(x)$.
  \item Assumptions A1 and A2 imply that $\tau^*(x)=0$ iff $\tau(x)=0$. Thus, if we only want to test $\tau^*(x) = 0$ we can simply ignore mis-classification.
  \item As shown below:
    \begin{align*}
      m(x) &= P(T^*=1|T=1,X=x) - P(T^*=1|T=0,X=x) \implies |m(x)|< 1\\
      m(x) &= M[b_0(x),b_1(x),r(x)] = \frac{1}{1 - b_1(x) - b_0(x)}\left\{ 1 -  \frac{\left[ 1 - b_1(x) \right]b_0(x)}{r(x)} - \frac{\left[ 1 - b_0(x) \right]b_1(x)}{1 - r(x)} \right\} \\
      m(x) &= \frac{[1 - r^*(x)]r^*(x)}{\left[ 1 - r(x) \right]r(x)}
      \left[ 1 - b_0(x) - b_1(x) \right] \implies m(x) >0 \mbox{ if } 1 - b_0(x) + b_1(x) > 0 
    \end{align*}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proof}[Proof of Theorem 1]
Let $p_t(X) = P(T^*=1|X,T=t)$. 
By A1 we have 
\[
  E(Y|X,T^*,T) = E(Y|X,T^*) = h^*_0(X) + \tau^*(X) T^*
\]
combining this with iterated expectations, 
\begin{align*}
  E(Y|X,T=t) &= E_{T^*|X,T=t}\left[ E(Y|X,T=t,T^*) \right] = E_{T^*|X,T=t}\left[ h^*_0(X) + \tau^*(X) T^* \right]\\
  &= p_t(X) \left[ h^*_0(X) + \tau^*(X) \right] + \left[ 1 - p_t(X) \right] h^*_0(X)\\
  &= h^*_0(X) + p_t(X) \tau^*(X)
\end{align*}
Taking the difference of the preceding expression evaluated at $T=1$ and $T=0$
\[
  E(Y|X,T=1) - E(Y|X,T=0) = \left[p_1(X) - p_0(X)\right] \tau^*(X)
\]
whereas we defined 
\[
  \tau(X) = h(X,1) - h(X,0) = E(Y|X,T=1) - E(Y|X,T=0)
\]
Combining these two equations, we see that
\[
  \tau(X) = \left[ p_1(X) - p_0(X) \right] \tau^*(X) 
\]
so that the function $m(x)$ defined in Theorem 1 equals $p_1(x) - p_0(x)$.
Since $m$ is a difference of probabilities, it follows immediately that $-1 \leq m(x) \leq 1$.
Now, by Bayes' Rule, 
\begin{align*}
  p_0(x) &= P(T^*=1|X=x,T=0) = \frac{P(T=0|X=x,T^*=1)P(T^*=1|X=x)}{P(T=0|X=x)} = \frac{b_1(x) r^*(x)}{1-r(x)} \\
  p_1(x) &= P(T^*=1|X=x,T=1) = \frac{P(T=1|X=x,T^*=1)P(T^*=1|X=x)}{P(T=1|X=x)} = \frac{[1-b_1(x)] r^*(x)}{r(x)} 
\end{align*}
and by iterated expectations,
\begin{align*}
  r(x) &= E(T|X=x) = E(T|X=x,T^*=1)P(T^*=1|X=x) + E(T|X=x,T^*=0)P(T^*=0|X=x)\\ 
    &= [1 - b_1(x)]r^*(x) + b_0(x) [1 - r^*(x)] = b_0(x) + r^*(x)[1 - b_0(x) - b_1(x)] 
\end{align*}
Using this expression, if $b_0(x) + b_1(x) = 1$ then $r(x) = b_0$.
If instead $b_0(x) + b_1(x) \neq 1$, then we can divide through by $1 - b_0(x) - b_1(x)$ to solve for $r^*(x)$ and $1 - r^*(x)$ as follows:
\begin{align*}
  r^*(x) &= \frac{r(x) - b_0(x)}{1 - b_0(x) - b_1(x)}\\
  1 - r^*(x) &= \frac{[1 - b_0(x) - b_1(x)] - [r(x) - b_0(x)]}{1 - b_0(x) - b_1(x)} = \frac{1 - b_1(x) - r(x)}{1 - b_0(x) - b_1(x)}
\end{align*}
Using the expressions we have just derived for $p_0, p_1, r^*$ and $1 - r^*$ and suppressing the dependence on $x$ for simplicity, it follows that
\begin{align*}
  m &= p_1 - p_0 = \frac{(1 - b_1)r^*}{r} - \frac{b_1r^*}{1- r} = \frac{(1 - r)(1 - b_1) r^* - r b_1 r^*}{r(1 - r)}\\
  &= \frac{r^*( 1 - r - b_1 + r b_1 - r b_1 )}{r (1 - r)} = \frac{r^*(1 - r - b_1)}{r(1-r)}
\end{align*}
Rearranging,
\[
  (1-r)rm = r^*(1 - r - b_1)
\]
and combining this with $1 - r^* = (1 - b_1 - r) / (1 - b_0 - b_1)$, we have
\[
  (1-r)rm = (1 - r^*)r^*(1 - b_0 - b_1)
\]
Both $r$ and $r^*$ are strictly between zero and one.
Thus, if $b_0 + b_1 = 1$ we have $m = 0$.
If instead $b_0 + b_1 <1$ we have $m>0$ and if $b_0 + b_1 > 1$ then $m <0$.
Finally,
\[
  m = \frac{r^*(1 - r - b_1)}{r(1-r)} = \frac{r - b_0}{1 - b_0 - b_1}\left[ \frac{1 - r - b_1}{r(1 - r)} \right] 
\]
and since 
\begin{align*}
 (r - b_0)(1 - r - b_1) &= r(1-r) - b_0(1-r) - b_1r + b_1 b_0  \\
  &= r(1-r) - b_0(1-r) - b_1r + b_1 b_0  + (b_1 b_0 r - b_1 b_0 r)\\
  &= r(1-r) - b_0(1-r) - b_1r + b_1 b_0(1 - r)  + b_1 b_0 r \\
  &= r(1-r) - b_0(1-r) - b_1r(1 - b_0) + b_1 b_0(1 - r) \\
  &= r(1-r) + b_0(1-r)(b_1 - 1) - b_1r(1 - b_0) \\
  &= r(1-r) - b_0(1-r)(1 - b_1) - b_1r(1 - b_0) 
\end{align*}
we find that
\begin{align*}
  m &= M(b_0, b_1, r) = \frac{1}{1 - b_0 - b_1}\left[ \frac{r(1-r) - (1 - b_1)b_0(1-r) - (1-b_0)b_1r}{r(1-r)} \right]\\
  &= \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r} - \frac{(1-b_0)b_1}{1-r} \right].
\end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Assumption A3} Assume that $r(x)$ and $\tau(x)$ are identified.
Note that this only requires that we can consistently estimate conditional expectations of observable quantities.

\paragraph{Assumption A4} Suppose that we have partitioned $X$ into two subvectors: $X = (V,Z)$. 
We assume that for each $z$ in the support of $Z$ there exists a subset $\Omega_z$ of the support of $V$ such that:
\begin{enumerate}[(i)]
  \item for all $v, v'\in \Omega_z$, $b_0(v,z)=b_0(v',z)$, $b_1(v,z)=b_1(v',z)$, and $\tau^*(v,z) = \tau^*(v',z)$
  \item for all $v,v' \in \Omega_z$ such that $v\neq v'$, $r^*(v,z) \neq r^*(v',z)$.
\end{enumerate}
The basic idea here is that $V$ affects the probability of being treated $r^*$ but not the treatment effect $\tau^*$ after we have conditioned on $Z$.
As sufficient condition for the restriction on $\tau^*$ to hold is $E(Y|Z=z,V=v,T^*=t) = s_1(z,t) + s_2(z,v)$ for some functions $s_1$ and $s_2$.

\paragraph{Some More Notation} Let $b_0(z), b_1(z), \tau^*(z)$ denote $b_0(v,z), b_1(v,z), \tau^*(v_z)$ for $v \in \Omega_z$ since, under A4, these do not vary with $v$.

\paragraph{Assumption A5} Each set $\Omega_z$ from A4 contains three elements $v_k$, $k=0,1,2$ such that
\[
  \left[ \frac{\tau(v_0,z)}{r(v_1,z)} - \frac{\tau(v_1,z)}{r(v_0,z)} \right]\left[ \frac{\tau(v_0,z)}{1 - r(v_2,z)} - \frac{\tau(v_2,z)}{1 - r(v_0,z)} \right] \neq 
  \left[ \frac{\tau(v_0,z)}{r(v_2,z)} - \frac{\tau(v_2,z)}{r(v_0,z)} \right]\left[ \frac{\tau(v_0,z)}{1 - r(v_1,z)} - \frac{\tau(v_1,z)}{1 - r(v_0,z)} \right] 
\]
\begin{itemize}
  \item The key requirement is that $V$ takes on at least three values.
  \item Assumption A5 depends only on observables, so we can test it.
  \item An equivalent way to state A5 is: $\tau^*(z)\neq 0$, $b_0(z)+b_1(z) \neq 1$ and an inequality involving $r$ and $r^*$ which is explained below.
  \item The triplets $(v_0, v_1, v_2)$ are allowed to depend on $z$.
\end{itemize}

\paragraph{Theorem 2} Under A1--A5, the mis-classification probabilities $b_0(x),b_1(x)$ are identified as is the probability of treatment $r^*(x)$ and treatment effect $\tau^*(x)$.
If we replace $b_0(x)+b_1(x)<1$ from A2 with $b_0(x)+b_1(x) \neq 1$, then $\tau^*(x)$ is identified up to sign. 
\begin{itemize}
  \item For simplicity, suppress dependence on $z$. By Theorem 1 and A4,
    \[
      \tau(v_k) M\left[ b_0, b_1, r(v_0) \right] = \tau(v_0)M\left[ b_0, b_1, r(v_k) \right]
    \]
  \item Again suppressing dependence on $z$, recall from above that \[M[b_0, b_1, r(v_k)] = \frac{1}{1 - b_1 - b_0}\left\{ 1 - \frac{(1 - b_1)b_0}{r(v_k)} - \frac{(1 - b_0)b_1}{1 - r(v_k)} \right\}\]
  \item Evaluating the two preceding expressions at $k=1$ and $k=2$ gives two equations relating the identified functions $r$ and $\tau$ to the unknown mis-classification probabilities $b_0$ and $b_1$.
  \item Since each equation involves $v_0$, we need $V$ to take on at least three values to get as many equations as unknowns (two).
  \item The proof of Theorem 2 shows that the two equations admit a unique solution, so that $b_0$ and $b_1$ are identified.
\item Given knowledge of $b_0$ and $b_1$, Theorem 1 allows us to solve for $r^*$ and $\tau^*$.
\item If $V$ only took on two values, we could not identify $b_0$ and $b_1$ without further restrictions. However if either $b_0$ or $b_1$ were known, e.g.\ known to be zero as in a one-sided mis-classification setting, then we could identify the model using only a binary $V$.
\end{itemize}

\begin{proof}[Proof of Theorem 2]
For a fixed value of the covariates $z$, Assumption A4 ensures that there is a subset  $\Omega_z$ of the support of $V$ -- a subset that may depend on $z$ -- such that for all $v, v' \in \Omega_z$ we have $b_0(v,z) = b_0(v',z)$, $b_1(v,z) = b_1(v',z)$ and $\tau^*(v,z) = \tau^*(v',z)$.
Assumption A5 ensures that $\Omega_z$ contains at least three values: $v_0, v_1$ and $v_2$.
Suppress dependence on the covariates $z$: let $r_k = r(v_k)$ and $\tau_k = \tau(v_k)$ for $k=0, 1, 2$.
Since $b_0$ and $b_1$ do not depend on $v$ for $v \in \Omega_z$, there is no subscript on these quantities.
By Theorem 1, $\tau(v,z) = \tau^*(v,z)m(v,z)$.
Again suppressing dependence on $z$ and using the expression for $m$ derived in the proof of Theorem 1,
  $\tau_k = M(b_0, b_1, r_k) \tau^*_k$
for all $k = 0, 1, 2$.
But by Assumption A4, $\tau^*_0 = \tau^*_1 = \tau^*_2$, which yields
\[
  \frac{\tau_k}{M(b_0, b_1, r_k)} = \frac{\tau_\ell}{M(b_0, b_1, r_\ell)}
\]
for any $k,\ell$.
Rearranging, 
\[
  M(b_0, b_1, r_k)\tau_\ell = M(b_0, b_1, r_\ell)\tau_k
\]
For $k \neq \ell$ this yields a nontrivial equation relating $b_0$ and $b_1$ to observables: $\tau_k, r_k$ and $\tau_\ell, r_\ell$.
In particular, take $\ell = 0$ and $k = 1, 2$.
We obtain,
\begin{align*}
  0 &= M(b_0, b_1, r_k)\tau_0 - M(b_0, b_1, r_0)\tau_k \\
  0 &= \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r_k} - \frac{(1-b_0)b_1}{1-r_k} \right]\tau_0 - \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r_0} - \frac{(1-b_0)b_1}{1-r_0} \right]\tau_k\\
  0 &= \left[ 1 - \frac{(1 - b_1)b_0}{r_k} - \frac{(1 - b_0)b_1}{1-r_k} \right]\tau_0 - \left[ 1 - \frac{(1 - b_1)b_0}{r_0} - \frac{(1 - b_0)b_1}{1-r_0} \right]\tau_k\\
  0 &= (1 - b_1)b_0 \left( \frac{\tau_k}{r_0} - \frac{\tau_0}{r_k} \right) + (1 - b_0)b_1 \left( \frac{\tau_k}{1 - r_0} - \frac{\tau_0}{1 - r_k} \right) + (\tau_0 - \tau_k)\\
  0 &= (1 - b_1)b_0 \left( \frac{\tau_0}{r_k} - \frac{\tau_k}{r_0} \right) + (1 - b_0)b_1 \left( \frac{\tau_0}{1 - r_k} - \frac{\tau_k}{1 - r_0} \right) + (\tau_k - \tau_0)
\end{align*}
This is a \emph{linear} equation of the form
\[
  0 = B_0 w_{0k} + B_1 w_{1k} + w_{2k}
\]
where the unknowns $B_0, B_1$ are defined as
\begin{align*}
  B_0 &= b_0 (1- b_1)\\
  B_1 &= b_1(1- b_0)
\end{align*}
and the observable constants $w_{0k}, w_{1k}, w_{2k}$ are
\begin{align*}
  w_{0k} &= \frac{\tau_0}{r_k} - \frac{\tau_k}{r_0}\\
  w_{1k} &= \frac{\tau_0}{1 - r_k} - \frac{\tau_k}{1 - r_0}\\
  w_{2k} &= \tau_k - \tau_0
\end{align*}
Since we have an equation for $k=1$ and $k=2$, we have a linear system of $k$ equations in two unknowns.
In matrix form, 
\[
\left[\begin{array}{cc}
w_{01} & w_{11}\\
w_{02} & w_{12}
\end{array}\right]\left[\begin{array}{c}
B_{0}\\
B_{1}
\end{array}\right]=\left[\begin{array}{c}
-w_{21}\\
-w_{22}
\end{array}\right]
\]
In this notation, Assumption A5 is simply $w_{01}w_{12}-w_{02}w_{11}\neq0$ which ensures the system has a unique solution.

Now, let $s = 1 - \alpha_0 - \alpha_1$.
Since $B_{0}=(1-b_{1})b_{0}$ and $B_{1}=(1-b_{0})b_{1}$, we have
\[
  (s + b_0)b_0 = (1 - b_0 - b_1 + b_0) b_0 =  (1 - b_1) b_0 = B_0\\
\]
and 
\begin{align*}
  B_0 - B_1 + 1 - s &= (1 - b_1)b_0 - (1 - b_0)b_1 + 1 - (1 - b_0 - b_1)\\
  &= b_0 - b_1 b_0 - b_1 + b_1 b_0 + b_0 + b_1 = 2 b_0
\end{align*}
Substituting $2b_0 = B_0 - B_1 + 1 - s$ into $B_0 = (s + b_0)b_0$, we obtain 
\begin{align*}
  4 B_0 &= (2s + 2 b_0) 2 b_0 = (2s + B_0 - B_1 + 1 - s)(B_0 - B_1 + 1 - s)\\
  &= (s + B_0 - B_1 + 1)(B_0 - B_1 + 1 - s)\\
  &= [(B_0 - B_1 + 1) + s][(B_0 - B_1 + 1) - s]\\
  &= (B_0 - B_1 + 1)^2 - s^2
\end{align*}
and rearranging to solve for $s$,
\[
  s = \pm \sqrt{(B_0 - B_1 + 1)^2 - 4 B_0}
\]
This identifies $s$ up to sign so long as $s\neq0$, and since
\begin{align*}
  M(b_0, b_1, r_k) &= \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r_k} - \frac{(1-b_0)b_1}{1-r_k} \right]  
= \frac{1}{s}\left[ 1 - \frac{B_0}{r_k} - \frac{B_1}{1 - r_k} \right]
\end{align*}
and $\tau_k = M(b_0,b_1,r_k)\tau^*$, it follows that $\tau^*$ is identified up to sign.
If $s > 0$, then $s$ is identified as are $b_0$ and $b_1$.
To solve for the misclassification probabilities, first use the fact that $B_0 - B_1 + 1 - s = 2 b_0$ as shown above to yield,
\[
  b_0 = (B_0 - B_1 + 1 - s) / 2
\]
and similarly, use the fact that
\begin{align*}
  -(B_0 - B_1 - 1 + s) &= -\left[ (1 - b_1)b_0 - (1 - b_0)b_1  - b_0 - b_1 \right] \\
  &= -(b_0 - b_1 b_0 - b_1 + b_1 b_0 - b_0 - b_1)\\ 
  &= 2 b_1
\end{align*}
we obtain
\[
  b_1 = -(B_0 - B_1 - 1 + s) / 2.
\]

\todo[inline]{Still need to verify the following:}
Finally, let 
\[
R_k = \displaystyle \frac{(1 - r_k^*)r_k^*}{(1 - r_k)r_k}
\]
Using the fact that $\tau_k = M(b_0, b_1, r_k) \tau^*$ along with 
\[
  m = \frac{(1 - b_1)r^*}{r} - \frac{b_1 r^*}{1-r}
\] 
the determinant condition can be expressed as  
\[
  \left[ \left( \frac{R_0}{r_1} - \frac{R_1}{r_0} \right)\left( \frac{R_0}{1 - r_2} - \frac{R_2}{1 - r_0} \right) - \left( \frac{R_0}{r_2} - \frac{R_2}{r_0} \right)\left( \frac{R_0}{1 - r_1} - \frac{R_1}{1 - r_0} \right) \right] \times (1 - b_0 - b_1) \tau^* \neq 0
\]

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notes on Mahajan (2006)}


\subsection{The Case of Exogenous $x^*$}
\paragraph{Model and Basic Notation:}
Consider a model of the form
\begin{equation}
  \mathbb{E}\left[ y - g(\widetilde{z})\left. \right| \widetilde{z} \right] = 0
  \label{eq:mahajanI}
\end{equation}
where $\widetilde{z} = (x^*, z)$, $x^*$ is an unobserved binary random variable, $z$ is  a $d_z\times 1$ vector of observed random variables (covariates), and the conditional expectation function $g(\widetilde{z}) = \mathbb{E}(y|\widetilde{z})$ is unknown.\footnote{The fact that $g$ is the conditional expectation of $y$ given $\widetilde{z}$ follows from the linearity of expectation and the fact that $g$ is a function of $\widetilde{z}$ only, since $0 = \mathbb{E}[y - g(\widetilde{z})|\widetilde{z}] = \mathbb{E}(y|\widetilde{z}) - \mathbb{E}[g(\widetilde{z})|\widetilde{z}] = \mathbb{E}(y|\widetilde{z}) - g(\widetilde{z})$.}
In addition to $z$ we also observe $x$, a binary, mis-classified surrogate for $x^*$ along with a ``special'' random variable $v$ -- in essence an instrumental variable.

\paragraph{Basic Model Assumption}
The basic assumption of the model is
\begin{equation}
  \mathbb{E}[y - g(\widetilde{z}) | \widetilde{z}, x, v] = \mathbb{E}[y - g(x^*,z) |x^*, x, v,z] = 0.
  \label{eq:mahajan1}
\end{equation}
Now, since
\[
  \mathbb{E}[y - g(\widetilde{z}) | \widetilde{z}, x, v] =
  \mathbb{E}[y| \widetilde{z}, x, v] - g(\widetilde{z}) = \mathbb{E}[y|\widetilde{z},z,v] - E[y|\widetilde{z}] = 0
\]
we see that this assumption is equivalent to 
\[
  \mathbb{E}[y|x^*,x,v,z] = \mathbb{E}[y|x^*,z] = g(x^*,z)
\]
which entails both non-differential measurement error and an exclusion restriction for $v$.

\paragraph{Relationship to an Additively Separable Model}
Our object of interest here is the conditional mean function $g(\widetilde{z})$.
Defining $\varepsilon = y - g(\widetilde{z})$, by construction we have $y = g(\widetilde{z}) + \varepsilon$ and $\mathbb{E}(\varepsilon|x^*,z)= 0$ which gives us an additively separable model.
Note that $g$ \emph{need not} be a structural function: unless $\widetilde{z}$ is exogenous, we cannot give it a causal interpretation.
But if we are only interested in learning the conditional mean function $g$, we do not have to worry about endogeneity since $x^*$ is by construction exogenous for the error term $\varepsilon$.
Note that by iterated expectations and Equation \ref{eq:mahajan1},
\begin{align*}
  \mathbb{E}[y|x^*,x,z]  &= 
  \mathbb{E}_{v|\widetilde{z},x}\left[\mathbb{E}\left( y|\widetilde{z},x,v \right)  \right]
  = \mathbb{E}_{v|\widetilde{z},x}\left[ g(\widetilde{z}) \right] = g(x^*,z)\\
  \mathbb{E}[y|x^*,v,z]  &= 
  \mathbb{E}_{x|\widetilde{z},v}\left[\mathbb{E}\left( y|\widetilde{z},x,v \right)  \right]
  = \mathbb{E}_{x|\widetilde{z},v}\left[ g(\widetilde{z}) \right] = g(x^*,z)
\end{align*}
or expressed in terms of $\varepsilon$,
\[
  \mathbb{E}[\varepsilon|x^*,x,z,v] = \mathbb{E}[\varepsilon|x^*,x,z] = \mathbb{E}[\varepsilon|x^*,v,z] = 0.
\]

\paragraph{A Note Concerning Covariates}
Throughout the following, the covariates $z$ are held fixed at $z = z_a$ so we will sometimes suppress the dependence of various functions on $z$.

\paragraph{Notation: Misclassification Rates}
(Note that these correspond to $\alpha_0$ and $\alpha_1$ in our notation.)
\begin{align*}
  \eta_0(z_a) = \mathbb{P}(x=1|x^*=0,z_a)\\
  \eta_1(z_a) = \mathbb{P}(x=0|x^*=1,z_a)\\
\end{align*}

\paragraph{Notation: Observed and Unobserved First-stage} 
(Note that these correspond to $p_k$ and $p_k^*$ in our notation.)
\begin{align*}
  \eta_2(z_a, v) &= \mathbb{P}(x=1|z_a, v) \\
  \eta_2^*(z_a,v) &= \mathbb{P}(x^*=1|z_a,v)
\end{align*}

In addition to Equations \ref{eq:mahajanI} and \ref{eq:mahajan1}, we maintain the following:
\begin{assump}[Identification of $g$ if $x^*$ were observed]\mbox{}
     \\ Knowledge of the distribution of $(y,x^*,z)$ suffices to identify $g(x^*,z_a)$.
  \label{assump:A1}
\end{assump}
\begin{assump}[Positive correlation between $x^*$ and $x$]\mbox{}
      \\ The sum of the mis-classification error rates is less than one: $\eta_0(z_a) + \eta_1(z_a) < 1$.
  \label{assump:A2}
\end{assump}
Assumption \ref{assump:A2} requires that the mis-classification is not so severe that $x$ is uncorrelated with or negatively correlated with $x^*$.

\begin{assump}
      The surrogate $x$ is conditionally independent of $v$ given $x^*$ and $z_a$.
  \label{assump:A3}
\end{assump}

In essence, Assumption \ref{assump:A3} requires that $v$ is excluded for the mis-classification error process.
In particular, it implies that
\begin{align*}
  \mathbb{P}(x=1|x^*=0,v, z_a) &= \mathbb{P}(x=1|x^*=0,z_a) = \eta_0(z_a)\\
  \mathbb{P}(x=0|x^*=1,v, z_a) &= \mathbb{P}(x=0|x^*=1,z_a) = \eta_1(z_a)
\end{align*}

\begin{assump}[First-stage]\mbox{}
  For some $v_1 \neq v_2$, $\eta^*_2(z_a,v_1) \neq \eta_2(z_a,v_2)$.
  \label{assump:A4}
\end{assump}

Assumption \ref{assump:A4} is just the standard IV relevance condition, conditional on $z = z_a$.


\begin{assump}[Relevance of $x^*$] The function $g$ depends on $x^*$:  $g(1,z_a) \neq g(0,z_a)$. 
  \label{assump:A5}
\end{assump}
The mis-classification error rates $\eta_0$ and $\eta_1$ are not identified if Assumption \ref{assump:A5} fails. 

\begin{thm}
  Under Assumptions \ref{assump:A1}--\ref{assump:A5} $g(x^*,z_a)$,  $\eta_0(z_a)$, and $\eta_1(z_a)$ are identified.
  \label{thm:mahajan}
\end{thm}
  
\paragraph{Steps in the Proof}
\begin{enumerate}
  \item Show that if $\eta_0(z_a)$ and $\eta_1(z_a)$ are identified, so is $g(x^*,z_a)$.
  \item Show that $\eta_0(z_a)$ and $\eta_1(z_a)$ are identified.
\end{enumerate}
In the proof we will suppose that $v$ is binary and takes on values $v_1\neq v_2$.
We will also suppress dependence on $(z,v)$ unless needed and employ the convention that $w = (x, y, xy)$.

\paragraph{Some Additional Notation}
  \begin{align*}
    r(z_a,v) \equiv \mathbb{E}(xy|z_a,v)\\
    \eta_2(z_a,v) \equiv \mathbb{E}(x|z_a,v)\\
    t(z_a,v) \equiv \mathbb{E}(y|z_a,v)
  \end{align*}
Note that we have already defined $\eta_2$ above. 
The definition given here is equivalent since $x$ is binary and hence $\mathbb{P}(x=1|\cdots) = \mathbb{E}(x|\cdots)$.

\begin{lem}
  $[1 - \eta_0(z_a) - \eta_1(z_a)]\, \eta^*_2(z_a,v) = \eta_2(z_a,v) - \eta_0(z_a)$.
  \label{lem:star_nonstar}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:star_nonstar}]
  By the Law of Total Probability and the fact that $\mathbb{P}(x=1|x^*,z_a, v)$ does not depend on $v$ by Assumption \ref{assump:A3}, we have
  \begin{align*}
    \eta_2(z_a,v) &= \mathbb{P}(x=1|x^*=0,z_a,v)\left[ 1 - \eta_2^*(z_a,v)]\right] + \mathbb{P}(x=1|x^*=1,z_a,v)\eta^*_2(z_a,v)\\
    &= \mathbb{P}(x=1|x^*=0,z_a)\left[ 1 - \eta_2^*(z_a,v)]\right] + \mathbb{P}(x=1|x^*=1,z_a)\eta^*_2(z_a,v)\\
    &= \eta_0(z_a)\left[ 1 - \eta_2^*(z_a,v)]\right] + \left[1 - \eta_1(z_a)\right]\eta^*_2(z_a,v)\\
    &= \eta_0(z_a) + \left[ 1 - \eta_0(z_a) - \eta_1(z_a) \right] \eta_2^*(z_a,v)
  \end{align*}
  The result follows by re-arranging.
\end{proof}

\paragraph{Note:} In our notation, the preceding corresponds to $(1 - \alpha_0 - \alpha_1) p_k^* = p_k - \alpha_0$.

\begin{lem}
  $t(z_a,v) = g(0, z_a)\left[ 1 - \eta_2^*(z_a,v) \right] + g(1,z_a)\eta_2^*(z_a,v)$.
  \label{lem:t}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:t}]
\end{proof}

\begin{lem}
  $r(z_a,v) = g(0,z_a)\eta_0(z_a) \left[ 1 - \eta_2^*(z_a,v) \right] + g(1, z_a) \left[ 1 - \eta_1(z_a) \right]\eta_2^*(z_a,v)$
  \label{lem:r} 
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:r}]
\end{proof}

\begin{lem}
  \[r(z_a,v_1) - r(z_a,v_2) = \left[ \eta_2^*(z_a,v_1) - \eta_2^*(z_a,v_2)\left\{ \left[ 1 - \eta_1(z_a) \right]g(1,z_a) - \eta_0(z_a)g(0,z_a) \right\} \right]\]
  \label{lem:r_diff}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:r_diff}]
\end{proof}

\begin{lem}[IV Estimator]
  \[\bar{t}(z_a) = k(a)\left[ g(1,z_a) - g(0,z_a) \right]\]
  where 
\[
\bar{t}(z_a) = \frac{t(z_a,v_1) - t(z_a,v_2)}{\eta_2(z_a,v_1) - \eta_2(z_a,v_2)}, \quad k(z_a) = \frac{1}{1 - \eta_0(z_a) - \eta_1(z_a)}
\]
  \label{lem:t_bar}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:t_bar}]
\end{proof}

\begin{lem}[``Modified'' IV Estimator]
  \[\bar{r}(z_a) = k(a)\left\{\left[1 - \eta_1(z_a)\right] g(1,z_a) - \eta_0(z_a) g(0,z_a) \right]\]
    where 
\[
\bar{r}(z_a) = \frac{r(z_a,v_1) - r(z_a,v_2)}{\eta_2(z_a,v_1) - \eta_2(z_a,v_2)}, \quad k(z_a) = \frac{1}{1 - \eta_0(z_a) - \eta_1(z_a)}
\]
  \label{lem:r_bar}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:r_bar}]
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:mahajan} -- Step 1]
  In this step, we assume that $\eta_0(z_a)$ and $\eta_1(z_a)$ are both identified.
  We then show that Assumptions \ref{assump:A1}--\ref{assump:A5} suffice to identify $g(x^*,z)$ given knowledge of $r(z_a,v), \eta_2(z_a,v)$, and $t(z_a,v)$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:mahajan} -- Step 2]
  In this step, we show that $\eta_0(z_a)$ and $\eta_1(z_a)$ are both identified under Assumptions \ref{assump:A1}--\ref{assump:A5}.
\end{proof}

\subsection{The Case of Endogenous $x^*$}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Frazis \& Loewenstein / Mahajan Moments}
\todo[inline]{I've checked everything out here by simulation and it's all correct as expected.}

Throughout this section I suppress dependence on covariates.
Consider a version of our model $y = c + \beta T^* + \epsilon$ with $\mathbb{E}[\varepsilon|T^*,z] = 0$, the setting from Mahajan (2006), Frazis \& Loewenstein, BBS, and KRS.
In this setting the Wald estimator continues to identify $\beta / (1 - \alpha_0 - \alpha_1)$, so one of the moment equations matches ours:
\begin{equation}
  \mbox{Cov}(y,z) - \theta_1\mbox{Cov}(T,z) = 0
  \label{eq:WaldExog}
\end{equation}
where $\theta_1 = \beta / (1 - \alpha_0 - \alpha_1)$.
The next equation appears as the first display on page 657 of Mahajan:
\[
  r(v_1) - r(v_2) = \left[ \eta^*_2(v_1) - \eta^*_2(v_2) \right]\left[ g(1) - (1 - \eta_1) - g(0) \eta_0 \right]
\]
Translating this into our notation, we have
\[
  \frac{\mbox{Cov}(YT,z)}{q(1-q)} = (p_1^* - p_0^*) \left[ (\beta+c)(1 - \alpha_1) - c\alpha_0 \right]  
\]
but since 
\[
  (p_1^* - p_0^*) = \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} = \frac{\mbox{Cov}(T,z)}{q (1 - q)(1 - \alpha_0 - \alpha_1)}
\]
after multiplying through by $q(1-q)$ we can re-write the expression from Mahajan as
\begin{equation}
  \mbox{Cov}(yT,z) - \theta_2 \mbox{Cov}(z,T) = 0 
  \label{eq:ModifiedWaldExog}
\end{equation}
where $\theta_2 = c + \theta_1 (1 -\alpha_1)$.
This expression is based on the same information as Equation (13d) from Frazis and Loewenstein, although their choice of parameterization is much more complicated.
Both are essentially the ``modified Wald'' in which we estimate the effect of $T$ on $yT$ using an instrument $z$.

The final expression we will need to close the model can be constructed from Equation (13c) of Frazis \& Loewenstein, namely 
\[
  \mbox{Cov}(T,y) = \beta \mbox{Cov}(T, T^*)
\]
where we set $\gamma$ to zero since covariates do not enter additively in our (and Mahajan's) formulation, but instead every expression tacitly conditions on $\mathbf{x}$.
Since,
\[
  \mbox{Cov}(T, T^*) = \frac{(p - \alpha_0)(1 - p - \alpha_1)}{1 - \alpha_0 - \alpha_1}
\]
we can re-write this expression as
\begin{equation}
  \mbox{Cov}(T,y) - \theta_1 (p - \alpha_0)(1 - p - \alpha_1) = 0.
  \label{eq:OLSExog}
\end{equation}
This expression incorporates the information from the OLS estimator.
Mahajan uses a \emph{conditional on $z$} version of the same expression, namely 
\[
  r(v) - \eta_2(v) t(v) = \bar{t} \left[ 1 - \eta_2(v) - \eta_1 \right]\left[ \eta_2(v) - \eta_0 \right]
\]
which can be written in our notation as
\[
  \mbox{Cov}(y,T|z) - \theta_1 (1 - p_k - \alpha_1)(p_k - \alpha_0) = 0.
\]


Both Mahajan and Frazis and Loewenstein prove identification in an extremely convoluted way.
A much simpler approach proveeds directly from Equations \ref{eq:WaldExog}--\ref{eq:OLSExog} by re-writing them as unconditional moment equations.
First,
\[
  \mbox{Cov}(y,z) - \theta_1 \mbox{Cov}(T,z) = \mathbb{E}\left[ yz - \theta_1 Tz \right] - \mathbb{E}[z]\mathbb{E}[y - \theta_1 T] = 0
\]
so that we can express Equation \ref{eq:WaldExog} using two unconditional moment equalities:
\begin{align}
  \label{eq:ExogUncond1a}
  \mathbb{E}\left[ y - \kappa_1 - \theta_1 T \right] &= 0 \\
  \label{eq:ExogUncond1b}
  \mathbb{E}\left[ (y - \kappa_1 - \theta_1 T) z \right] &= 0 
\end{align}
Proceeding similarly for Equation \ref{eq:ModifiedWaldExog}, we have:
\[
  \mbox{Cov}(yT,z) - \theta_2 \mbox{Cov}(T,z) = \mathbb{E}\left[ yTz - \theta_2 Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ yT - \theta_2 T \right] = 0
\]
which we can write as
\begin{align}
  \label{eq:ExogUncond2a}
  \mathbb{E}\left[ yT - \kappa_2 - \theta_2 T \right] &= 0 \\
  \label{eq:ExogUncond2b}
  \mathbb{E}\left[ (yT - \kappa_2 - \theta_2 T) z \right] &= 0 
\end{align}
It turns out that equations \ref{eq:ExogUncond1a} -- \ref{eq:ExogUncond2b} suffice to identify the model.
To see why, we need to re-write Equation \ref{eq:OLSExog}.
First, 
\begin{align*}
  \mbox{Cov}(T,y) &= \mathbb{E}[Ty] - \mathbb{E}[T]\mathbb{E}[c + \beta T^* + \varepsilon]\\
  &= \mathbb{E}[Ty] - p(c + \beta p^*)\\
  &= \mathbb{E}[Ty] - p\left[ c + \beta \left(\frac{p - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) \right]\\
  &= \mathbb{E}[Ty] - p\left[ c + \theta_1 \left( p - \alpha_0 \right) \right]
\end{align*}
so that Equation \ref{eq:OLSExog} becomes
\begin{align*}
  \mathbb{E}[Ty] - p\left[ c + \theta_1(p - \alpha_0) \right] - \theta_1(p-\alpha_0)(1 - p - \alpha_1) &= 0\\
  \mathbb{E}[Ty] - pc - \theta_1 (1 - \alpha_1)(p - \alpha_0) &= 0\\
  \mathbb{E}[Ty] - p \theta_2 + \theta_1 \alpha_0 (1 - \alpha_1) &= 0\\
  \mathbb{E}[(y - \theta_2)T] + \theta_1 \alpha_0 (1 - \alpha_1) &= 0
\end{align*}
But recall that we defined $\kappa_2$ such that $\mathbb{E}[yT - \kappa_2 - \theta_2 T] = 0$, in other words $\kappa_2 = \mathbb{E}[(y - \theta_2)T]$ which means that $\kappa_2 = -\theta_1 \alpha_0(1 - \alpha_1)$.
Thus, we can write the full system of unconditional moment equations as
\[
  \mathbb{E}\left\{ \left[
    \begin{array}{c}
    y - \kappa_1 - \theta_1 T \\ 
    yT - \kappa_2 - \theta_2 T
  \end{array}
\right]\otimes
\left[
\begin{array}{cc}
1 \\ z
\end{array}
\right]\right\} = \mathbf{0}
\]
where
\begin{align*}
  \theta_1 &= \beta / (1 - \alpha_0 - \alpha_1) \\
  \kappa_1 &= c - \alpha_0 \theta_1\\
  \theta_2 &= c + \theta_1 (1 - \alpha_1)\\
  \kappa_2 &= -\theta_1 \alpha_0 (1 - \alpha_1).
\end{align*}
Now, notice that
\[
  \theta_2 - \kappa_1 = \left[ c + \theta_1 (1 - \alpha_1) \right] - \left[ c - \alpha_0 \theta_1 \right] = \theta_1 \left[ (1 - \alpha_1) + \alpha_0 \right]
\]
Hence
\begin{align}
  \label{eq:IdentExog1}
  A &\equiv \left(\frac{\theta_2 - \kappa_1}{\theta_1}\right) = (1 - \alpha_1) + \alpha_0\\
  B &\equiv -\frac{\kappa_2}{\theta_1} = \alpha_0 (1 - \alpha_1)
\end{align}
Eliminating $(1 - \alpha_1)$ and $\alpha_0$, respectively, we obtain the following two quadratics:
\begin{align}
  \alpha_0^2 - A \alpha_0 + B &= 0\\
  (1 - \alpha_1)^2 - A (1 - \alpha_1) + B &= 0
\end{align}
It follows that one root of $x^2 - Ax + B = 0$ is $\alpha_0$ while the other is $(1 - \alpha_1)$.
The discriminant of this quadratic is
\begin{align*}
  D &= A^2 - 4 B  = \left( \frac{\theta_2 - \kappa_1}{\theta_1} \right)^2 - 4 \left( -\frac{\kappa_2}{\theta_1} \right) = [(1 - \alpha_1) + \alpha_0]^2 - 4 [\alpha_0 (1 - \alpha_1)]\\
  &= (1 - \alpha_1) + 2(1 - \alpha_1)\alpha_0 + \alpha_0^2 - 4\alpha_0 (1 - \alpha_1)\\
  &= (1 - \alpha_0 - \alpha_1)^2
\end{align*}
so we see that both roots are real provided that $\alpha_0 + \alpha_1 \neq 0$ and, moreover, that
\[
  (1 - \alpha_0 - \alpha_1)^2 = \left( \frac{\theta_2 - \kappa_1}{\theta_1} \right)^2 - 4 \left( -\frac{\kappa_2}{\theta_1} \right)
\]
Since $\beta = \theta_1 (1 - \alpha_0 - \alpha_1)$, we know that the sign of $\theta_1$ will match that of $\beta$ so long as $\alpha_0 + \alpha_1 < 1$, and in this case we find that
\[
  \beta = \mbox{sign}(\theta_1) \sqrt{\theta_1^2\left[ \left( \frac{\theta_2 - \kappa_1}{\theta_1} \right)^2 - 4 \left( -\frac{\kappa_2}{\theta_1} \right)
 \right]} = \sqrt{(\theta_2 - \kappa_1)^2 + 4 \theta_1 \kappa_2}
\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notation and Results for New Draft -- July 19th, 2017}


Additively separable model
\[
  y = h(T^*,\mathbf{x})+\varepsilon
\]
where $\varepsilon$ is a mean-zero error term, $T^*$ is an endogenous binary regressor of interest and $\mathbf{x}$ is a vector of exogenous controls.
Since $T^*$ is binary, we can re-write this as linear in $T^*$ conditional on $\mathbf{x}$
\begin{align*}
  y &= c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon\\
  \beta(\mathbf{x}) &= h(1,\mathbf{x}) - h(0,\mathbf{x})\\
  c(\mathbf{x}) &= h(0,\mathbf{x})
\end{align*}
Goal is to use an instrumental variable $z$ to identify $\beta(\mathbf{x})$ when we observe not $T^*$ but a mis-measured binary surrogate $T$. 
Define
\begin{align*}
  \alpha_0(\mathbf{x},z) &= \mathbb{P}\left(T=1|T^*=0,\mathbf{x},z  \right)\\
  \alpha_1(\mathbf{x},z) &= \mathbb{P}\left(T=0|T^*=1,\mathbf{x},z  \right)
\end{align*}
Identification will only rely on two values for $z$.
\todo[inline]{Be clear about the fact that, with the exception of the non-identification results, we assume throughout that $z$ is binary.}

\noindent Assumptions that would suffice to identify the model if $T^*$ were observed
\begin{assump} \mbox{}
  \label{assump:model}
  \begin{enumerate}[(i)] 
    \item $y = c(\mathbf{x}) + \beta(\mathbf{x})T^* + \varepsilon$ where $T^* \in \left\{ 0,1 \right\}$ and $\mathbb{E}[\varepsilon]=0$.
    \item  $z \in \left\{ 0,1 \right\}$, where $0 < \mathbb{P}(z=1|\mathbf{x}) < 1$, and $\mathbb{P}(T^*=1|\mathbf{x},z=1) \neq \mathbb{P}(T^*=1|\mathbf{x},z=0)$.
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z] = 0$
  \end{enumerate}
\end{assump}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Point out that even though Assumption \ref{assump:model} (ii) refers to the unobserved $T^*$, under Assumptions \ref{assump:misclassification} (i) and (ii) we have $(p_k^* - p_\ell^*)(1 - \alpha_0 - \alpha_1) = p_k - p_\ell$ so it suffices for an \emph{observed} first-stage to exist.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Stronger version of Assumption \ref{assump:model} (iii) used by Frazis \& Loewenstein, Mahajan, etc.
This assumption gives point identification without higher moment restrictions.
There should eventually be a result about this case in the paper.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{assump} \mbox{}
  \label{assump:jointExog}
    $\mathbb{E}[\varepsilon|\mathbf{x},z, T^*] = 0$
\end{assump}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent Assumptions on the mis-classification
\begin{assump} \mbox{}
  \label{assump:misclassification}
  \begin{enumerate}[(i)] 
    \item $\alpha_0(\mathbf{x},z) = \alpha_0(\mathbf{x})$,   $\alpha_1(\mathbf{x},z) = \alpha_1(\mathbf{x})$
    \item $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) <1$ ($T$ is positively correlated with $T^*$)
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon|\mathbf{x},z, T^*]$
  \end{enumerate}
\end{assump}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Explain what would happen if Assumption \ref{assump:misclassification} (ii) were weakened to $\alpha_0 + \alpha_1 \neq 1$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent Second moment assumption identifies the difference of mis-classification error rates.
\begin{assump} \mbox{}
  \label{assump:2ndMoment}
    $\mathbb{E}[\varepsilon^2|\mathbf{x},z] = \mathbb{E}[\varepsilon^2|\mathbf{x}]$ 
\end{assump}

\noindent Additional moment assumptions to get identification
\begin{assump} \mbox{}
  \label{assump:3rdMoment}
  \begin{enumerate}[(i)] 
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^2|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z] = \mathbb{E}[\varepsilon^3|\mathbf{x}]$
% The following assumption isn't actually needed! Not sure why we thought it was.
    %\item $\mathbb{E}[\varepsilon^3|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^3|\mathbf{x},z, T^*]$ 
  \end{enumerate}
\end{assump}



\paragraph{Notation}

\begin{align}
  \label{eq:theta1_def}
  \theta_1(\mathbf{x}) &= \beta(\mathbf{x})\left[ 1 - \left\{ \alpha_0(\mathbf{x}) + \mathbf{\alpha}_1(\mathbf{x}) \right\} \right]^{-1}\\
  \label{eq:theta2_def}
  \theta_2(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^2 \left[ 1 + \left\{\alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})\right\}\right] \\
  \label{eq:theta3_def}
  \theta_3(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^3\left[ \left( 1 - \left\{\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x})\right\} \right)^2 + 6\alpha_0(\mathbf{x})\left\{ 1 - \alpha_1(\mathbf{x}) \right\} \right]
\end{align}

\begin{align*}
  \mbox{Cov}(y,z|\mathbf{x}) - \mbox{Cov}(T,z|\mathbf{x}) \theta_1(\mathbf{x}) &= 0\\
  \mbox{Cov}(y^2,z|\mathbf{x}) - 2\mbox{Cov}(yT,z|\mathbf{x}) \theta_1(\mathbf{x}) + \mbox{Cov}(T,z|\mathbf{x})\theta_2(\mathbf{x}) &= 0\\
  \mbox{Cov}(y^3,z|\mathbf{x}) - 3 \mbox{Cov}(y^2T,z|\mathbf{x}) \theta_1(\mathbf{x}) + 3\mbox{Cov}(yT,z|\mathbf{x}) \theta_2(\mathbf{x}) - \mbox{Cov}(T,z|\mathbf{x}) \theta_3(\mathbf{x})&= 0
\end{align*}

\begin{align*}
  q(\mathbf{x}) &= \mathbb{P}(z=1|\mathbf{x})\\
  \pi(\mathbf{x}) &= \mbox{Cov}(T,z|\mathbf{x})\\
  \eta_j(\mathbf{x}) &= \mbox{Cov}(y^j,z|\mathbf{x})\\
  \tau_j(\mathbf{x}) &= \mbox{Cov}(Ty^j,z|\mathbf{x})
\end{align*}

\begin{align*}
 \eta_1(\mathbf{x}) &= \pi(\mathbf{x})\theta_1(\mathbf{x}) \\
  \eta_2(\mathbf{x}) &=  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x}) \\
  \eta_3(\mathbf{x}) &=  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})
\end{align*}


\begin{lem}[Lemma for Appendix only with Bayes' Rule]
For mis-classification probabilities
\begin{align*}
  P(T^*=1|T=1, Z=k) &= P(T=1 | T^*=1) \left(\frac{p_k^*}{p_k}\right) = (1 - \alpha_1)\left( \frac{p_k^*}{p_k} \right)\\
  P(T^*=1|T=0, Z=k) &= P(T=0 | T^*=1) \left(\frac{p_k^*}{1 - p_k}\right) = \alpha_1 \left( \frac{p_k^*}{1 - p_k} \right)\\
  P(T^*=0|T=1, Z=k) &= P(T=1 | T^*=0) \left(\frac{1 - p_k^*}{p_k}\right) = \alpha_0 \left( \frac{1 - p_k^*}{p_k} \right)\\
  P(T^*=0|T=0, Z=k) &= P(T=0 | T^*=0) \left(\frac{1 - p_k^*}{1 - p_k}\right) = (1 - \alpha_0)\left( \frac{1 - p_k^*}{1 - p_k} \right)
\end{align*}
\end{lem}


\noindent Both of the following two lemmas are known results from the literature.
The first gives the relationship between the observed first stage probabilities and their unobserved counterparts,
\begin{lem}
  Under Assumption \ref{assump:misclassification}, 
\begin{align*}
  \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right]\mathbb{P}(T^*=1|\mathbf{x}, z=k) &= \mathbb{P}(T=1|\mathbf{x},z=k) - \alpha_0(\mathbf{x})\\
  \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right]\mathbb{P}(T^*=0|\mathbf{x}, z=k) &= \mathbb{P}(T=0|\mathbf{x},z=k) - \alpha_1(\mathbf{x})
\end{align*}
\label{lem:p_pstar}
\end{lem} 

\begin{proof}
  Simple calculation using law of total probability.
\end{proof}

\noindent The second shows that the IV estimand does not equal $\beta$ in the presence of mis-classification
\begin{lem}
  Let $\eta_1(\mathbf{x}) = \mbox{Cov}(y,z|\mathbf{x})$ and $\pi(\mathbf{x}) = \mbox{Cov}(T,z|\mathbf{x})$.  
  Then, under Assumptions \ref{assump:model} and \ref{assump:misclassification}, $\eta_1(\mathbf{x}) = \pi(\mathbf{x}) \theta_1(\mathbf{x})$ where $\theta_1(\mathbf{x})$ is as defined in Equation \ref{eq:theta1_def}.
  \label{lem:eta1}
\end{lem}

\begin{proof}
  Immediate since $\mbox{Cov}(T,y|\mathbf{x}) = \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right] \mbox{Cov}(T^*,z|\mathbf{x})$ by Lemma \ref{lem:p_pstar}.
\end{proof}

\noindent If we strengthen Assumption \ref{assump:model} (iii) to Assumption \ref{assump:jointExog}, then a binary instrument suffices to identify $\beta$ under Assumptions ???.
(See KRS, BBS, FL, and Mahajan. Also possibly put a proof in the online-only appendix).
However, without this additional condition $\beta$ is unidentified regardless of the number of values that the instrument takes on, as we now show.\footnote{Footnote mentioning how this relates to Mahajan's mistake. He imposed an extra assumption that contradicts the existence of a first-stage. Direct the reader an appendix explaining the error.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thm}[Sharp Identified Set I]
  \label{thm:sharpI}
  Under Assumptions \ref{assump:model} and \ref{assump:misclassification} (i)--(ii), the sharp identified set is characterized by 
\[
    \mathbb{E}[y|\mathbf{x},z=k] = c(\mathbf{x}) + \beta(\mathbf{x}) \left[\frac{\mathbb{P}(T=1|\mathbf{x},z_k) - \alpha_0(\mathbf{x})}{1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})}\right]
  \] 
and
\[\alpha_0(\mathbf{x}) \leq \mathbb{P}(T=1|\mathbf{x}, z=k) \leq 1 -  \alpha_1(\mathbf{x})\]
for all $k = 0, 1$. 
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In our discussion of the results for the sharp identified set, we can discuss what would happen if there were more than 2 values of $k$.
Our method of proof never actually relies on there being only a binary instrument. 
You're not identified in any case, regardless of how many values $z$ takes on.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{proof}[Proof of Theorem \ref{thm:sharpI}]
  Throughout this argument we suppress dependence on $\mathbf{x}$ for simplicity. 
  Define $p_k = \mathbb{P}(T=1|z=k)$ and $p_k^* = \mathbb{P}(T^*=1|z_k)$.

We first show that so long as $\alpha_0 \leq p_k \leq 1 - \alpha_1$ then we can construct a valid joint probability distribution for $(T^*, T, z)$ that satisfies our assumptions.
First decompose the joint probability mass function as
\[
  p(T^*,T,z) = p(T|T^*,z)p(T^*|z)p(z).
\]
By Assumption \ref{assump:misclassification} (ii), $p(T|T^*,z) = p(T|T^*)$ and thus $\alpha_0$ and $\alpha_1$ fully determine $p(T|T^*,z)$.  
Under the proposed bounds, $\alpha_0$ and $\alpha_1$ are clearly valid probabilities.
Since $p(z)$ is observed, it thus suffices to ensure that $p(T^*|z)$ is a valid probability mass function.
By the law of total probability and Assumption \ref{assump:misclassification} (ii), 
\[
  p_k^* = \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1}.
\]
and $0 \leq p_k^* \leq 1$ if and only if $\alpha_0 \leq p_k \leq 1 - \alpha_1$.
Since $(p_k - p_\ell) = (p_k^* - p_\ell^*)(1 - \alpha_0 - \alpha_1)$, provided that $p_k - p_\ell \neq 0$ we have $p^*_k \neq p^*_\ell$. 

We now show how to construct a valid conditional distribution for $y$ given $(T^*,T,z)$ that satisfies our assumptions if $\beta (p_k - \alpha_0) = (1 - \alpha_0 - \alpha_1)[\mathbb{E}(y|z_k) - c]$ for all $k$.
Define
\begin{align*}
r_{tk} &\equiv \mathbb{P}(T^*=1|T=t,z=k) &
F_{t}(\tau) &\equiv \mathbb{P}(y \leq \tau|z=k) \\
F_{tk}(\tau) &\equiv \mathbb{P}(y \leq \tau|T=t, z=k) & 
F_{tk}^{t^*}(\tau) &\equiv \mathbb{P}(y \leq \tau|T^*=t^*,T=t, z=k)\\
G_k(\tau) &\equiv \mathbb{P}(\varepsilon \leq \tau|z=k) &
G^{t^*}_{tk}(\tau) &\equiv \mathbb{P}(\varepsilon \leq \tau|T^*=t^*, T=t,z=k).
\end{align*}
Assumption \ref{assump:model} (i) implies a relationship between $G^{t^*}_{tk}$ and $F^{t^*}_{tk}$ for each $t^*$, namely 
\begin{equation}
  G^0_{tk}(\tau) = F^0_{tk}(\tau + c), \quad
  G^1_{tk}(\tau) = F^1_{tk}(\tau + c + \beta)
  \label{eq:Gtstartk}
\end{equation}
and thus we see that
%\scriptsize
%\[
%  G_k(\tau) = p_k^*\left[\frac{r_{1k}p_k}{p_k^*} F^1_{1k}(\tau + c + \beta) + \frac{r_{0k}(1 - p_k)}{p_k^*} F^1_{0k}(\tau + c + \beta)  \right] + (1 - p_k^*) \left[ \frac{(1 - r_{1k})p_k}{1 - p_k^*}  F^0_{1k}(\tau + c) + \frac{(1 - r_{0k})(1 - p_k)}{1 - p_k^*} F^0_{0k}(\tau + c) \right] 
%\]
%\normalsize
\begin{align}
  G_k(\tau) &= r_{1k}p_k F^1_{1k}(\tau + c + \beta) + r_{0k}(1 - p_k) F^1_{0k}(\tau + c + \beta) \nonumber \\
  &\quad +  (1 - r_{1k})p_k F^0_{1k}(\tau + c) + (1 - r_{0k})(1 - p_k) F^0_{0k}(\tau + c)
  \label{eq:Gk}
\end{align}
applying the law of total probability and Bayes' rule.
Moreover, again applying the law of total probability, 
\begin{equation}
  F_{tk}(\tau) = r_{tk} F_{tk}^1(\tau) + (1 - r_{tk}) F_{tk}^0(\tau)
  \label{eq:Ftk}
\end{equation}
for all $t,k \in \left\{ 0,1 \right\}$, and by Bayes' rule,
\begin{equation}
  r_{1k} = \frac{(1 - \alpha_1)p_k^*}{p_k}, \quad
  r_{0k} = \frac{\alpha_1p_k^*}{1 - p_k}.
  \label{eq:rtk}
\end{equation}
There are four cases, corresponding to different possibilities for the  $r_{tk}$.

\paragraph{Case I: $r_{1k} = 0, r_{0k} \neq 0$}
By Equation \ref{eq:rtk}, this requires $\alpha_1 = 1$ which is ruled out by Assumption \ref{assump:misclassification} (ii).

\paragraph{Case II: $r_{0k} = r_{1k} = 0$}
By Equation \ref{eq:rtk}, this requires $p_k^* = 0$ which in turn requires $p_k = \alpha_0$.
Moreover, by Equation \ref{eq:Ftk} we have $F^0_{tk} = F_{tk}$, while $F^1_{tk}$ is undefined.
Substituting into Equation \ref{eq:Gk},
\[
  G_k(\tau) = p_k F_{1k}(\tau + c) + (1 - p_k) F_{0k}(\tau + c) = F_k(\tau + c)
\]
Now, since $F_k(\tau + c)$ is the conditional CDF of $y-c$ given that $z=k$, and $G_k$ is the conditional CDF of $\varepsilon$ given $z=k$,
we see that Assumption \ref{assump:model} (i) is satisfied if and only if $\mathbb{E}(y|z=k) = c$.
But since $p_k = \alpha_0$ in this case, $c = c + \beta (p_k - \alpha_0)/(1 - \alpha_0 - \alpha_1)$.

\paragraph{Case III: $r_{1k}\neq 0, r_{0k} = 0$}
By Equation \ref{eq:rtk} this requires $\alpha_1 = $ and $p_k^* \neq 0$.
By Equation \ref{eq:Ftk} we have $F^0_{0k} = F_{0k}$ and since $r_{1k} \neq 1$, we can solve to obtain
\[
  F^1_{1k}(\tau) = \frac{1}{r_{1k}}\left[F_{1k}(\tau) - (1 - r_{1k})F^0_{1k}(\tau)\right]
\]
Substituting into Equation \ref{eq:Gk}, we obtain
\begin{align*}
  G_k(\tau) &= \left[ (1 - p_k)F_{0k}(\tau + c) + p_k F_{1k}(\tau + c + \beta) \right] \\ 
  &\quad + p_k(1 - r_{1k})\left[ F^0_{1k}(\tau + c) - F^0_{1k}(\tau + c + \beta) \right]
\end{align*}
Now, $F_{0k}(\tau + c)$ is the conditional CDF of $(y-c)$ given $(T=0,z=k)$ while $F_{1k}(\tau + c + \beta)$ is the conditional CDF of $(y-c-\beta)$ given $(T=1,z=k)$.
Similarly, $F^0_{1k}(\tau + c)$ is the conditional CDF of $\varepsilon$ given $(T^*=0,T = 1, z=k)$ while $F^0_{1k}(\tau + c + \beta)$ is the conditional CDF of $(\varepsilon - \beta)$ given $(T^*=0, T=1, z=k)$.
Since $G_k(\tau)$ is the conditional CDF of $\varepsilon$ given $z=k$, we see that Assumption \ref{assump:model} (iii) is satisfied if and only if
\begin{align*}
  0 &= (1 - p_k) \mathbb{E}(y-c|T=0,z=k) + p_k \mathbb{E}(y - c - \beta|T=1,z=k)\\
  &\quad + p_k(1 - r_{1k})\left[ \mathbb{E}(\varepsilon|T^*=0,T=1,z=k) - \mathbb{E}(\varepsilon - \beta|T^*=0,T=1,z=k) \right]
\end{align*}
Rearranging, this is equivalent to 
\[
  \mathbb{E}(y|z=k) = c + (1 - \alpha_1) \beta\left( \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) = c + \beta\left( \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)
\]
since $\alpha_1 = 0$ in this case.
As explained above, $F^0_{0k} = F_{0k}$ in the present case while $F^1_{0k}$ is undefined. 
We are free to choose any distributions for $F^{0}_{1k}$ and $F^{1}_{1k}$ that satisfy Equation \ref{eq:Ftk}, for example $F^{0}_{1k} = F^{1}_{1k} = F_{1k}$.

\paragraph{Case IV: $r_{1k}\neq 0, r_{0k} \neq 0$}
In this case, we can solve Equation \ref{eq:Ftk} to obtain
\[
  F^1_{tk}(\tau) = \frac{1}{r_{tk}}\left[F_{tk}(\tau) - (1 - r_{tk})F^0_{tk}(\tau)\right]
\]
Substituting this into Equation \ref{eq:Gk}, we have
\begin{align*}
  G_k(\tau) = F_k(\tau + c + \beta) &+ p_k(1 - r_{1k})\left[F^0_{1k}(\tau + c) - F_{1k}^0(\tau + c + \beta)\right]\\
  &+ (1 - p_k)(1 - r_{0k}) \left[ F^0_{0k}(\tau + c) - F^0_{0k}(\tau + c + \beta) \right]
\end{align*}
using the fact that $F_k(\tau) = p_k F_{1k}(\tau) + (1 - p_k) F_{0k}(\tau)$.
Now, $F_k(\tau + c + \beta)$ is the conditional CDF of $(y - c - \beta)$ given $z=k$, while $F_{tk}^0(\tau + c)$ is the conditional CDF of $\varepsilon$ given $(T = t,z =k)$ and $F^0_{tk}(\tau + c + \beta)$ is the conditional CDF of $(\varepsilon - \beta)$ given $(T = t, z=k)$.
Since $G_k(\tau)$ is the conditional CDF of $\varepsilon$ given $z=k$, we see that Assumption \ref{assump:model} (iii) is satisfied if and only if
\begin{align*}
  0 &= \mathbb{E}[y - c - \beta|z=k] + p_k(1 - r_{1k})\left[ \mathbb{E}(\varepsilon|T^*=0,T=1,z=k) - \mathbb{E}(\varepsilon - \beta|T^*=0,T=1,z=k) \right] \\
   &\quad + (1 - p_k)(1 - r_{0k})\left[ \mathbb{E}(\varepsilon|T^*=0,T=0,z=k) - \mathbb{E}(\varepsilon - \beta|T^*=0,T=0,z=k) \right]\\
   0 &= \mathbb{E}[y - c - \beta|z=k] + \beta\left[p_k(1 - r_{1k}) + (1 - p_k)(1 - r_{0k})\right]
\end{align*}
But since $\left[p_k(1 - r_{1k}) + (1 - p_k)(1 - r_{0k})\right] = (1 - p_k^*)$ and $p_k^* = (p_k - \alpha_0) /(1 - \alpha_0 - \alpha_1)$, this simplifies to 
\[
\mathbb{E}[y|z=k] = c + \beta\left( \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} \right).
\]
Thus, in this case we are free to choose \emph{any} distributions for $F^{0}_{tk}$ and $F^1_{tk}$ that satisfy Equation \ref{eq:Ftk}.
For example we could take $F^0_{tk} = F^1_{tk} = F_{tk}$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Say how we get the following by plugging in the largest and smallest possible values for $\alpha_0 + \alpha_1$ and taking the difference of the expressions for $\mathbb{E}[y|z=k]$

\begin{cor}
  Under the conditions of Theorem \ref{thm:sharpI} the sharp identified set for $\beta(\mathbf{x})$ is the closed interval between $\Delta^y(\mathbf{x})$ and $\Delta^y(\mathbf{x})/\Delta^T(\mathbf{x})$ 
where $\Delta^y(\mathbf{x}) \equiv \mathbb{E}(y|\mathbf{x},z=1) - \mathbb{E}(y|\mathbf{x},z=0)$ and $\Delta^T(\mathbf{x})= \mathbb{E}(T|\mathbf{x},z=1) - \mathbb{E}(T|\mathbf{x},z=0)$.
\label{cor:sharpBeta1}
\end{cor}



Talk about applied settings in which there can be one-sided mis-classification.
This restricts the identified set.
Following corollary collects some common cases.

\begin{cor}
  Under the conditions of Theorem \ref{thm:sharpI}, restrictions on the misclassification probabilities shrink the sharp identified set for $\beta(\mathbf{x})$ to the closed interval between $B\Delta^y(\mathbf{x})/\Delta^T(\mathbf{x})$ and $\Delta^y(\mathbf{x})/\Delta^T(\mathbf{x})$ where
  \begin{enumerate}[(i)]
    \item $\alpha_0(\mathbf{x})=0$ implies $B = \max_k \mathbb{E}(T|\mathbf{x},z=k)$ 
    \item $\alpha_1(\mathbf{x})=0$ implies $B = 1 - \min_k \mathbb{E}(T|\mathbf{x},z=k)$ 
    \item $\alpha_0(\mathbf{x})=\alpha_1(\mathbf{x})$ implies $B =  1 - 2 \min\left\{ \min_k \mathbb{E}(T|\mathbf{x},z=k), 1 - \max_k \mathbb{E}(T|\mathbf{x},z=k) \right\}$ 
  \end{enumerate}
  for $k = 0,1$, where $\Delta^T(\mathbf{x})$ and $\Delta^y(\mathbf{x})$ are as defined in Corollary \ref{cor:sharpBeta1}.
\end{cor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The preceding results don't use the non-differential measurement error assumption 
Further imposing non-differential measurement error -- Assumption \ref{assump:misclassification} (iii) -- shrinks the identified set.
We impose some additional conditions that simplify the proof. 
These are discussed further below.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thm}[Sharp Identified Set II]
  \label{thm:sharpII}
  Suppose that the conditional distribution of $y$ given $(\mathbf{x}, T,z)$ is absolutely continuous with respect to the Lebesgue measure for any values of the conditioning variables and moreover that there is no $k$ such that $\mathbb{E}\left[ y|\mathbf{x},T=0,z=k \right] = \mathbb{E}\left[ y|\mathbf{x},T=1,z=k \right]$.
  Then, under Assumptions \ref{assump:model} and \ref{assump:misclassification}, the sharp identified set is characterized by
  \[
    BLAH BLAH BLAH
  \]
\end{thm}

\begin{proof}[Proof of Theorem \ref{thm:sharpII}]
Under Assumption \ref{assump:model} (i) and Assumption \ref{assump:misclassification} (iii), we obtain $\mathbb{E}(y|T^*,T,z) = \mathbb{E}(y|T^*,z)$.
Hence, by iterated expectations
  \begin{align*}
    \mathbb{E}(y|T=0,z=k) &= (1 - r_{0k}) \mathbb{E}(y|T^*=0,z=k) + r_{0k}\mathbb{E}(y|T^*=1,z=k)\\
    \mathbb{E}(y|T=1,z=k) &= (1 - r_{1k}) \mathbb{E}(y|T^*=0,z=k) + r_{1k}\mathbb{E}(y|T^*=1,z=k)
  \end{align*}
where $r_{tk}$ is defined as in the proof of Theorem \ref{thm:sharpI}.
This is system of two linear equations in two unknowns: $\mathbb{E}(y|T^*=0,z=k)$ and $\mathbb{E}(y|T^*=1,z=k)$.
After some algebra, we find that the determinant is
\[
  r_{1k} - r_{0k} = \left[ \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} \right]\left[ \frac{1 - p_k - \alpha_1}{p_k(1 - p_k)} \right] 
\]
and thus a unique solution exists provided that $\alpha_0 \neq p_k$ and $\alpha_1 \neq 1 - p_k$.
By our assumption that $\mathbb{E}[y|T=0,z=k] \neq \mathbb{E}[y|T=1,z=k]$, the system has no solution when the determinant condition fails.
Thus, Assumption \ref{assump:misclassification} (iii) rules out $\alpha_0 = p_k$ and $\alpha_1 = 1-p_k$.
Solving,
\begin{align*}
  \mu^0_{k} \equiv \mathbb{E}(y|T^*=0,z=k) &= \left(\frac{1}{1 - p_k - \alpha_1}\right)\left[ (1 - p_k)\mathbb{E}(y|T=0,z_k) - \alpha_1 \mathbb{E}(y|z=k) \right]\\
  \mu^1_{k} \equiv \mathbb{E}(y|T^*=1,z=k) &= \left(\frac{1}{p_k - \alpha_0}\right)\left[ p_k\mathbb{E}(y|T=1,z_k) - \alpha_0 \mathbb{E}(y|z=k) \right]
\end{align*}
Given $(\alpha_0, \alpha_1)$, we see that $r_{tk}, \mu^0_k$, and $\mu^1_{k}$ are fixed.
The question is whether, for a given pair $(\alpha_0, \alpha_1)$ and observed CDFs $F_{tk}$, we can construct valid CDFs $F_{tk}^0, F_{tk}^1$ such that 
\[
  \int_{\mathbb{R}} \tau F_{tk}^0(d\tau) = \mu_k^0, \quad
  \int_{\mathbb{R}} \tau F_{tk}^1(d\tau) = \mu_k^1, \quad 
  F_{tk}(\tau) = r_{tk} F^1_{tk}(\tau) + (1 - r_{tk}) F^0_{tk}(\tau)
\]
where $F_{tk}$ and $F^{t^*}_{tk}$ are as defined in the proof of Theorem \ref{thm:sharpII}.
For a given pair $(t,k)$, there are two cases: $0 < r_{tk} < 1$ and $r_{tk} \in \left\{ 0, 1 \right\}$.

%Note that the only joint restrictions enter via $\mu_{0k}$ and $\mu_{1k}$, so once we have fixed these, we can proceed separately for each pair $(t,k)$.

%Notice that $\mu_{0k}$ depends only on $\alpha_1$ and observables, while $\mu_{1k}$ depends only on $\alpha_0$ and observables.

\paragraph{Case I: $r_{tk}\in \left\{ 0,1 \right\}$}
Suppose that $r_{tk} = 1$.
Then, $\mu^1_k = \mathbb{E}[y|T=t,z=k]$ so we can simply set $F^1_{tk} = F_{tk}$.
In this case $F^0_{tk}$ is undefined.
If instead $r_{tk} = 0$, then $\mu^0_k = \mathbb{E}[y|T=t,z=k]$ so we can simply set $F^0_{tk} = F_{tk}$.
In this case $F^1_{tk}$ is undefined.

\paragraph{Case II: $0 < r_{tk} < 1$} 
Define
\begin{align*}
  m_{tk}(\xi) &= \mathbb{E}[y|y\in I_{tk}(\xi), T=t, z=k]\\
  I_{tk}(\xi) &= \left[ F^{-1}_{tk}(1 - \xi - r_{tk}), F^{-1}_{tk}(1 - \xi) \right]
\end{align*}
for $t,k = 0,1$ where $0 \leq \xi \leq 1 - r_{tk}$ and $F^{-1}_{tk}$ is the quantile function of $y$ given $(T=t, z=k)$.
We see that $m_{tk}$ is a decreasing function of $\xi$ that attains its maximum at $\xi = 0$ and minimum at $\xi = 1 - r_{tk}$.
Define these extrema as $\underline{m}_{tk} = m_{tk}(1 - r_{tk})$ and $\overline{m}_{tk} = m_{tk}(0)$.

Suppose first that $\mu^1_{k}$ does \emph{not} lie in the interval $[\underline{m}_{tk}, \overline{m}_{tk}]$.
We show that it is impossible to construct valid CDFs $F^0_{tk}$ and $F^{1}_{tk}$ that satisfy $F_{tk}(\tau) = r_{tk} F^1_{tk}(\tau) + (1 - r_{tk}) F^0_{tk}(\tau)$ where $F_{tk}$ and $F^{t^*}_{tk}$ are as defined in the proof of Theorem \ref{thm:sharpII}.
Since $r_{tk} \neq 1$, we can solve the expression for $F_{tk}$ to yield 
  $F^{0}_{tk}(\tau) = \left[ F_{tk}(\tau) - r_{tk} F^1_{tk}(\tau)\right] / (1 - r_{tk})$.
  Hence, since $r_{tk} \neq 0$, the requirement that $0 \leq F_{tk}^0(\tau) \leq 1$ implies
\begin{equation}
  \frac{F_{tk}(\tau) - (1 - r_{tk})}{r_{tk}} \leq F^{1}_{tk}(\tau) \leq \frac{F_{tk}(\tau)}{r_{tk}}
  \label{eq:F1tk_ineq}
\end{equation}
Now define 
\begin{align*}
  \underline{F}^{1}_{tk}(\tau) &=  \min\left\{ 1,\,  F_{tk}(\tau)/r_{tk} \right\}\\
\overline{F}^{1}_{tk}(\tau) &= \max\left\{ 0,\,  F_{tk}(\tau)/r_{tk} - (1 - r_{tk})/r_{tk} \right\}
\end{align*}
Combining Equation \ref{eq:F1tk_ineq} with the requirement that $0 \leq F^{1}_{tk}(\tau) \leq 1$, we see that
\[
  \overline{F}_{tk}^1(\tau) \leq F^{1}_{tk}(\tau) \leq \underline{F}_{tk}^1(\tau)
\]
Hence $\overline{F}^1_{tk}$ first-order stochastically dominates $F^{1}_{tk}$ which in turn first-order stochastically dominates $\underline{F}_{tk}^1$. 
It follows that
\[
 \int \tau \underline{F}_{tk}^1(d\tau) \leq \int \tau F^{1}_{tk}(d\tau) \leq \int \tau\overline{F}_{tk}^1(d\tau)
\]
But notice that 
\[
  \underline{m}_{tk} = \int \tau \underline{F}_{tk}^1(d\tau), \quad 
  \mu^1_{k} = \int \tau F^{1}_{tk}(d\tau), \quad 
  \overline{m}_{tk} = \int \tau\overline{F}_{tk}^1(d\tau)
\]
so we have $\underline{m}_{tk} \leq \mu^1_{k} \leq \overline{m}_{tk}$ which contradicts $\mu^1_{k} \notin [\underline{m}_{tk}, \overline{m}_{tk}]$.

Now suppose that $\mu^1_{k} \in \left[\underline{m}_{tk}, \overline{m}_{tk} \right]$.
Since $y$ is assumed to follow a continuous distribution conditional on $(T,z)$, $m_{tk}$ is continuous on its domain and takes on all values in $\left[ \underline{m}_{tk}, \overline{m}_{tk} \right]$ by the intermediate value theorem.
Thus, there exists a $\xi^*$ such that $m_{tk}(\xi^*) = \mu^1_{k}$.
Now let $f_{tk}(\tau) = dF_{tk}(\tau)/d\tau$ which is non-negative by the assumption that $y$ is continuously distributed.
Define the densities
\[
  f^1_{tk}(\tau) = \frac{f_{tk}(\tau)\times \mathbf{1}\left\{ \tau \in I_{tk}(\xi^*) \right\}}{r_{tk}}, \quad
  f^0_{tk}(\tau) = \frac{f_{tk}(\tau) \times \mathbf{1}\left\{ \tau \in I_{tk}(\xi^*) \right\}}{1 - r_{tk}}.
\]
Clearly $f_{tk}^1\geq 0$ and $f^0_{tk} \geq 0$.
Integrating, 
\begin{align*}
  \int_{\mathbb{R}} f_{tk}^1(\tau) \; d\tau &= \frac{1}{r_{tk}}\int_{I_{tk}(\xi^*)} f_{tk}(\tau)\; d\tau = 1\\
  \int_{\mathbb{R}} f_{tk}^0(\tau) \; d\tau &= \frac{1}{1 - r_{tk}}\int_{I^C_{tk}(\xi^*)} f_{tk}(\tau)\; d\tau = 1
\end{align*}
where $I_{tk}^C$ is the complement of $I_{tk}$.
And, by construction
\[
  r_{tk} \int_A f_{tk}^1(\tau) \; d\tau + (1 - r_{tk}) \int_A f_{tk}^0(\tau) \; d\tau = \int_A f_{tk}(\tau)\; d\tau
\]
for any set $A$. 
Finally,
\[
  \int_{\mathbb{R}} \tau f_{tk}^1(\tau) \; d\tau = \frac{1}{r_{tk}} \int_{I_{tk}(\xi^*)} \tau f_{tk}(\tau)\; d\tau = m(\xi^*) = \mu_{tk}.
\]

\todo[inline]{Some discussion putting all of the pieces together and explaining what the identified set looks like. In particular, what are the ``edge'' cases? Can't rule out $\alpha_0 = \alpha_1 = 0$.}

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The preceding theorem assumes that $y$ is continuously distributed, which is natural in an additively separable model and holds in our simulations and empirical examples below.
If this is not the case then non-differential measurement error may impose further restrictions, although the basic idea of the argument is similar.
We also imposed $\mathbb{E}[y|\mathbf{x},T = 0, z=k] \neq \mathbb{E}[y|\mathbf{x},T=1, z=k]$ for any $k$, which holds generically.
If the two means are exactly equal for some $k$, then the intersection of $\left\{ (\alpha_0, \alpha_1)\colon \alpha_0 = p_k \mbox{ or } \alpha_1 = 1-p_k \right\}$ with the bounds for $\alpha_0, \alpha_1$ from Theorem \ref{thm:sharpI} must be added to the identified set.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lem}
  Let $\eta_2(\mathbf{x}) = \mbox{Cov}(y^2,z|\mathbf{x})$ and $\tau_1(\mathbf{x}) = \mbox{Cov}(yT,z|\mathbf{x})$.
Then, Under Assumptions \ref{assump:model}, \ref{assump:misclassification} and \ref{assump:2ndMoment}, 
\[
    \eta_2(\mathbf{x}) =  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x})
\]
where $\pi(\mathbf{x})$ is as defined in Lemma \ref{lem:eta1} and $\theta_1(\mathbf{x})$ and $\theta_2(\mathbf{x})$ are given by Equations \ref{eq:theta1_def}--\ref{eq:theta2_def}.
  \label{lem:eta2}
\end{lem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of Lemma \ref{lem:eta2}]
  Throughout this argument we suppress dependence on $\mathbf{x}$ for simplicity. 
  By Assumption \ref{assump:model} (i) and the basic properties of covariance, 
\begin{align*}
    \eta_2 &= \beta^2 \mbox{Cov}(T^*,z) + 2 \beta\left[ c\, \mbox{Cov}(T^*,z) + \mbox{Cov}(T^*\varepsilon,z)  \right] + 2c \, \mbox{Cov}(\varepsilon,z) + \mbox{Cov}(\varepsilon^2,z)\\
  \tau_1 &= c \pi + \mbox{Cov}(T\varepsilon,z) + \beta \mbox{Cov}(TT^*,z)
\end{align*}
using the fact that $T^*$ is binary. 
Now, by Assumptions \ref{assump:model} (iii) and \ref{assump:2ndMoment} we have $\mbox{Cov}(\varepsilon,z) = \mbox{Cov}(\varepsilon^2,z) = 0$.
And, using Assumptions \ref{assump:misclassification} (i) and (ii), one can show that $\mbox{Cov}(TT^*,z) = (1 - \alpha_1)\mbox{Cov}(T^*,z)$ and $\mbox{Cov}(T^*,z) = \pi/(1 - \alpha_0 - \alpha_1)$.
Hence, 
\begin{align*}
  \eta_2 &= \theta_1\left( \beta 
+ 2 c \right) \pi + 2\beta \mbox{Cov}(T^*\varepsilon,z) \\
  2 \tau_1 \theta_1 - \pi \theta_2 &= \left[2\theta_1 c + 2 \theta_1^2 (1 - \alpha_1) - \theta_2\right]\pi + 2\theta_1 \mbox{Cov}(T\varepsilon,z) 
\end{align*}
but since $\theta_2 = \theta_1^2 \left[ (1 - \alpha_1) + \alpha_0 \right]$, we see that $[2\theta_1^2(1 - \alpha_1) - \theta_2] = \theta_1 \beta$.
Thus, it suffices to show that $\beta \mbox{Cov}(T^*\varepsilon,z) = \theta_1 \mbox{Cov}(T\varepsilon,z)$.
This equality is trivially satisfied when $\beta=0$, so suppose that $\beta \neq 0$. 
In this case it suffices to show that $(1 - \alpha_0 - \alpha_1) \mbox{Cov}(T^*\varepsilon,z) = \mbox{Cov}(T\varepsilon,z)$.
Define $m^*_{tk} = \mathbb{E}\left[ \varepsilon|T^*=t,z=k \right]$ and $p^*_k = \mathbb{P}(T^*=1|z=k)$.
Then, by iterated expectations, Bayes' rule, and Assumption \ref{assump:misclassification} (iii)
\begin{align*}
  \mbox{Cov}(T^*\varepsilon,z) &=q(1 - q)\left(p_1^* m_{11}^*  - p_0^*m_{10}^* \right) \\
  \mbox{Cov}(T\varepsilon,z) &= q(1 - q)\left\{ (1 - \alpha_1)\left[ p_1^* m_{11}^* - p_0^* m_{10}^* \right] + \alpha_0\left[ (1 - p_1^*) m_{01}^* - (1 - p_0^*)m_{00}^* \right] \right\} 
\end{align*}
But by Assumption \ref{assump:model} (iii), $\mathbb{E}[\varepsilon|z=k] = m_{1k}^*p_{k}^* + m_{0k}^*(1 - p_k^*)=0$ and thus we obtain $m_{0k}^*(1 - p_k^*)= - m_{1k}^* p_k^*$.
Therefore  $(1 - \alpha_0 - \alpha_1) \mbox{Cov}(T^*\varepsilon,z) = \mbox{Cov}(T\varepsilon,z)$ as required.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\todo[inline]{We probably want to have a corollary that establishes identification in the one-sided version of the problem. Related to this, we probably want to show that we are not in general identified from second moments even if the instrument takes on many values. This could be a real pain but on the plus side we would end up deriving the sharp identified set. One slightly confusing point is that we are now making the 2nd moment IV assumption separately from the 2nd moment non-differential measurement error assumption but we need both to get the second moment bounds for $\alpha_0$ and $\alpha_1$.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lem}
  Let $\eta_3(\mathbf{x}) = \mbox{Cov}(y^3,z|\mathbf{x})$ and $\tau_2(\mathbf{x}) = \mbox{Cov}(Ty^2,z|\mathbf{x})$.
  Then, under Assumptions \ref{assump:model}, \ref{assump:misclassification}, \ref{assump:2ndMoment} and \ref{assump:3rdMoment},
\[
  \eta_3(\mathbf{x}) =  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})
\]
where $\tau_1(\mathbf{x})$ and $\pi(\mathbf{x})$ are as defined in Lemma \ref{lem:eta2}, and $\theta_1(\mathbf{x}), \theta_2(\mathbf{x}), \theta_3(\mathbf{x})$ are defined in Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}.
  \label{lem:eta3}
\end{lem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of Lemma \ref{lem:eta3}]
  Throughout this argument we suppress dependence on $\mathbf{x}$ for simplicity.
  Since $T^*$ is binary, if follows from the basic properties of covariance that,
\begin{align*}
  \eta_3 &= \mbox{Cov}\left[ (c + \varepsilon)^3,z \right] + 3 \beta \mbox{Cov}[(c + \varepsilon)^2 T^*, z] + 3 \beta^2 \mbox{Cov}[(c + \varepsilon)T^*,z] + \beta^3 \mbox{Cov}(T^*,z)\\
  \tau_2 &= \mbox{Cov}\left[ (c + \varepsilon)^2 T, z \right] + 2 \beta \mbox{Cov}\left[ (c + \varepsilon)TT^*,z \right] + \beta^2 \mbox{Cov}(TT^*,z)
\end{align*}
By Assumptions \ref{assump:model} (iii), \ref{assump:2ndMoment}, and \ref{assump:3rdMoment} (ii) , $\mbox{Cov}\left[ (c + \varepsilon)^3,z \right] = 0$.
Expanding, 
\begin{align*}
  \eta_3 %&= 3 \beta \mbox{Cov}[(c + \varepsilon)^2 T^*, z] + 3 \beta^2 \mbox{Cov}[(c + \varepsilon)T^*,z] + \beta^3 \mbox{Cov}(T^*,z)\\
  %&= 3\beta\left[  c^2 \mbox{Cov}(T^*,z) + 2c \,\mbox{Cov}(T^*\varepsilon,z) + \mbox{Cov}(T^*\varepsilon^2, z)\right] + 3 \beta^2\left[ c\, \mbox{Cov}(T^*, z) + \mbox{Cov}(T^*\varepsilon, z)\right] + \beta^3 \mbox{Cov}(T^*, z)\\
  &= 3 \beta \mbox{Cov}(T^*\varepsilon^2,z) + \left(3 \beta^2 + 6c\beta \right)\mbox{Cov}(T^*\varepsilon,z) + \left( \beta^3 + 3c\beta^2 + 3c^2\beta \right)\mbox{Cov}(T^*, z)\\
  \tau_2 &= c^2 \mbox{Cov}(T,z) + \beta(\beta + 2c) \mbox{Cov}(TT^*,z) + \mbox{Cov}(T\varepsilon^2,z) + 2c \mbox{Cov}(T\varepsilon,z) + 2\beta\,\mbox{Cov}(TT^*\varepsilon,z)
\end{align*}
Now, define $s^*_{tk} = \mathbb{E}[\varepsilon^2|T^*=t, z=k]$ and $p_k^* = \mathbb{P}(T^*=1|z=k)$.
By iterated expectations, Bayes' rule, and Assumption \ref{assump:3rdMoment} (i), 
\begin{align*}
  \mbox{Cov}(T^*\varepsilon^2, z) %&= q(1 - q) \left\{ \mathbb{E}[T^*\varepsilon^2|z=1] - \mathbb{E}[T^*\varepsilon^2|z=0] \right\} \\
  %&= q(1 - q) \left\{ \mathbb{E}_{T^*|z=1}\left[T^*\mathbb{E}\left(\varepsilon^2|T^*, z=1\right)\right] - \mathbb{E}_{T^*|z=0}\left[T^*\mathbb{E}\left(\varepsilon^2|T^*,z=0\right)\right] \right\} \\
  &= q(1 - q)(p^*_1 s^*_{11} - p^*_0 s^*_{10}) \\
  \mbox{Cov}(T\varepsilon^2, z) %&= q(1 - q) \left\{ \mathbb{E}[T\varepsilon^2|z=1] - \mathbb{E}[T\varepsilon^2|z=0] \right\} \\
  %&= q(1 - q) \left\{ \mathbb{E}_{T|z=1}\left[T\mathbb{E}\left(\varepsilon^2|T, z=1\right)\right] - \mathbb{E}_{T|z=0}\left[T\mathbb{E}\left(\varepsilon^2|T,z=0\right)\right] \right\} \\
  %&= q(1 - q)(p_1 s_{11} - p_0 s_{10}) \\
  &= q(1 - q)\left\{ (1 - \alpha_1)\left[p^*_1 s_{11}^* - p_0^* s_{10}^*\right] + \alpha_0 \left[ (1 - p_1^*)s_{01}^* - (1 - p_0^*) s_{00}^*\right] \right\}
\end{align*}
By Assumption \ref{assump:2ndMoment}, $\mathbb{E}[\varepsilon^2|z=1] = \mathbb{E}[\varepsilon^2|z=0]$ and thus, by iterated expectations we have
$p_1^* s_{11}^* - p_0^* s^*_{10} =  - \left[(1 - p_1^*)s_{01}^* - (1 - p_0^*)s_{00}^* \right]$
which implies 
\begin{equation}
  \mbox{Cov}(T\varepsilon^2,z) = (1 - \alpha_0 - \alpha_1)\mbox{Cov}(T^*\varepsilon^2,z).
  \label{eq:TEpsilonSquared}
\end{equation}
Similarly by iterated expectations and Assumptions \ref{assump:misclassification} (i)--(ii)
\begin{equation}
  \mbox{Cov}(TT^*\varepsilon, z) = q(1 - q)(1 - \alpha_1)(p_1^* m_{1k}^* - p_0^* m_{10}^*) = (1 - \alpha_1) \mbox{Cov}(T^*\varepsilon, z) 
  \label{eq:TTstarEpsilon}
\end{equation}
where $m_{tk}^*$ is defined as in the proof of Lemma \ref{lem:eta2}.
As shown in the proof of Lemma \ref{lem:eta2}, 
\begin{align*}
  \mbox{Cov}(TT^*,z) &= (1 - \alpha_1) \mbox{Cov}(T^*,z)\\ 
  \mbox{Cov}(T^*,z) &= \pi / (1 - \alpha_0 - \alpha_1)\\
  \mbox{Cov}(T^*\varepsilon,z) &= \mbox{Cov}(T\varepsilon,z) / (1 - \alpha_0 - \alpha_1)
\end{align*}
and combining these equalities with Equations \ref{eq:TEpsilonSquared} and \ref{eq:TTstarEpsilon}, it follows that
\begin{align*}
  \tau_2 &=  2\left[(1 - \alpha_1)(c + \beta) - c \alpha_0\right]\mbox{Cov}(T^*\varepsilon,z) + \left[(1 - \alpha_1)(c + \beta)^2 - c^2 \alpha_0 \right]\mbox{Cov}(T^*,z)\\
  &\quad \quad +(1 - \alpha_0 - \alpha_1)\mbox{Cov}(T^*\varepsilon^2,z) \\
  \tau_1 &= (1 - \alpha_0 - \alpha_1)\mbox{Cov}(T^*\varepsilon,z) + \left[(1 - \alpha_1)(c + \beta) - c \alpha_0\right] \mbox{Cov}(T^*,z)
\end{align*}
using $\tau_1 = c\pi + \mbox{Cov}(T\varepsilon,z) + \beta\mbox{Cov}(TT^*,z)$ as shown in the proof of Lemma \ref{lem:eta2}.
Thus, 
\[
  3\tau_2 \theta_1 - 3 \tau_1 \theta_2 + \pi \theta_3 = K_1 \mbox{Cov}(T^*\varepsilon^2,z) + K_2 \mbox{Cov}(T^*\varepsilon, z) + K_3 \mbox{Cov}(T^*,z)
\]
where
\begin{align*}
  K_1 &\equiv 3 \theta_1(1 - \alpha_0 - \alpha_1)\\ 
  K_2 &\equiv 6\theta_1 \left[(1 - \alpha_1)(c +\beta) - c\alpha_0\right] - 3\theta_2 (1 - \alpha_0 - \alpha_1) \\
  %&= (1 - \alpha_0 - \alpha_1) \left\{6\theta_1\left[c + \theta_1 (1 - \alpha_1)\right] - 3\theta_2 \right\}\\
  %&= (1 - \alpha_0 - \alpha_1) \theta_1\left\{6\left[c + \theta_1 (1 - \alpha_1)\right] - 3\theta_1\left[(1 - \alpha_1) + \alpha_0\right]\right\}\\
  %&= \beta\left\{6\left[c + \theta_1 (1 - \alpha_1)\right] - 3\theta_1\left[(1 - \alpha_1) + \alpha_0\right]\right\}\\
  %&= \beta\left[6c + 6\theta_1 (1 - \alpha_1) - 3\theta_1(1 - \alpha_1) - 3 \alpha_0\right]\\
  %&= \beta\left[6c + 3\theta_1 (1 - \alpha_1)  - 3 \alpha_0\right]\\
  %&= \beta\left[6c + 3\theta_1 (1 - \alpha_0 - \alpha_1)\right]\\
  %= 3\beta^2 + 6c\beta\\
  K_3 &\equiv 3\theta_1 \left[ (1 - \alpha_1)(c + \beta)^2 - c^2 \alpha_0 \right] - 3\theta_2 \left[ (1 - \alpha_1)(c + \beta) - c\alpha_0 \right] + \theta_3(1 - \alpha_0 - \alpha_1)
%  &= (1 - \alpha_0 - \alpha_1)\left\{3\theta_1\left[ c^2 + \theta_1(2c + \beta)(1 - \alpha_1) \right] - 3\theta_2\left[ c + \theta_1(1 - \alpha_1) \right] + \theta_3\right\}\\
%  &= \beta\left\{ 3\left[ c^2 + \theta_1(2c + \beta)(1 - \alpha_1) \right] - 3 \theta_1\left[(1 - \alpha_1) + \alpha_0\right]\left[ c + \theta_1(1 - \alpha_1) \right] + \theta_1^2\left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0 (1 - \alpha_1) \right] \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1(1 - \alpha_1)(2c + \beta) - 3 \theta_1\left[(1 - \alpha_1) + \alpha_0\right]\left[ c + \theta_1(1 - \alpha_1) \right] + \theta_1^2\left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0 (1 - \alpha_1) \right] \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[(1 - \alpha_1)(2c + \beta) - \left( 1 + \alpha_0 - \alpha_1 \right) c\right] - 3 \theta_1^2(1 + \alpha_0 - \alpha_1)(1 - \alpha_1) + \theta_1^2\left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0 (1 - \alpha_1) \right] \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[(1 - \alpha_1)(2c + \beta) - \left( 1 + \alpha_0 - \alpha_1 \right) c\right]  + \theta_1^2\left[ (1 - \alpha_0 - \alpha_1)^2 - 3(1 - \alpha_1)(1 + \alpha_0 - \alpha_1) + 6\alpha_0 (1 - \alpha_1) \right] \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[(1 - \alpha_1)(2c + \beta) - \left( 1 + \alpha_0 - \alpha_1 \right) c\right]  + \theta_1^2\left[ (1 - \alpha_0 - \alpha_1)^2 + 3(1 - \alpha_1)(2 \alpha_0 - 1 - \alpha_0 + \alpha_1) \right] \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[(1 - \alpha_1)(2c + \beta) - \left( 1 + \alpha_0 - \alpha_1 \right) c\right]  + \theta_1^2\left[ (1 - \alpha_0 - \alpha_1)^2 + 3(1 - \alpha_1)(\alpha_0 + \alpha_1 - 1) \right] \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[(1 - \alpha_1)(2c + \beta) - \left( 1 + \alpha_0 - \alpha_1 \right) c\right]  + \theta_1^2\left[ (1 - \alpha_0 - \alpha_1)^2 - 3(1 - \alpha_1)(1 - \alpha_0 - \alpha_1) \right] \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[(1 - \alpha_1)(2c + \beta) - \left( 1 + \alpha_0 - \alpha_1 \right) c\right]  + \theta_1^2(1 - \alpha_0 - \alpha_1)\left[ (1 - \alpha_0 - \alpha_1) - 3(1 - \alpha_1) \right] \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[(1 - \alpha_1)(2c + \beta) - \left( 1 + \alpha_0 - \alpha_1 \right) c\right]  + \beta \theta_1\left[ (1 - \alpha_0 - \alpha_1) - 3(1 - \alpha_1) \right] \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[(1 - \alpha_1)(2c + \beta) - \left( 1 + \alpha_0 - \alpha_1 \right) c\right]  + \beta^2 - 3\beta \theta_1(1 - \alpha_1) \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[c(2 - 2\alpha_1 - 1 - \alpha_0 +\alpha_1) + \beta(1 - \alpha_1)\right]  + \beta^2 - 3\beta \theta_1(1 - \alpha_1) \right\}\\
%  &= \beta\left\{ 3 c^2 + 3\theta_1\left[c(1 - \alpha_0  - \alpha_1) + \beta(1 - \alpha_1)\right]  + \beta^2 - 3\beta \theta_1(1 - \alpha_1) \right\}\\
%  &= \beta\left\{ 3 c^2 + 3c\beta + 3\beta\theta_1(1 - \alpha_1)  + \beta^2 - 3\beta \theta_1(1 - \alpha_1) \right\}\\
%&= \beta( 3 c^2 + 3c\beta + \beta^2)\\
%&= \beta^3 + 3c\beta^2 + 3c^2\beta
\end{align*}
Clearly $K_1 = 3\beta$.
Substituting the definitions of $\theta_1, \theta_2$, and $\theta_3$ from Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}, tedious but straightforward algebra likewise shows that $K_2 = 3\beta^2 + 6c\beta$ and $K_3 = \beta^3 + 3c\beta^2 + 3c^2\beta$.
Therefore the coefficients of $\eta_3$ equal those of $3\tau_2 - 3\tau_1 \theta_2 + \pi \theta_3$ and the result follows.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{cor}
  \label{cor:theta_ident}
  Suppose that $\pi(\mathbf{x}), \tau_1(\mathbf{x}), \tau_2(\mathbf{x}),  \eta_1(\mathbf{x}), \eta_2(\mathbf{x})$, and $\eta_3(\mathbf{x})$ -- defined in Lemmas ??? -- are identified.
  Then, under the conditions of Lemmas ???, $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$, and $\theta_3(\mathbf{x})$ -- defined in Equations \ref{eq:theta1_def}--\ref{eq:theta3_def} -- are identified if and only if $\pi(\mathbf{x}) \neq 0$.
\end{cor}
\begin{proof}[Proof of Corollary \ref{cor:theta_ident}]
  For ease of notation we suppress dependence on $\mathbf{x}$ throughout this argument.
  Lemmas ??? yield a triangular system of linear equations in $\theta_1, \theta_2, \theta_3$.
%\begin{align*}
% \pi\theta_1 &= \eta_1 \\
% 2\tau_1 \theta_1 - \pi \theta_2 &= \eta_2 \\
% 3\tau_2 \theta_1 - 3\tau_1 \theta_2 + \pi\theta_3 &= \eta_3
%\end{align*}
By inspection, the determinant of the system is $-\pi^3$ so a unique solution exists if and only if $\pi \neq 0$. 
In particular,
\begin{align*}
  \theta_1 &= \eta_1 / \pi\\
  \theta_2 &= 2\tau_1\eta_1/\pi^2 - \eta_2/\pi\\
  \theta_3 &= \eta_3/\pi - 3(\tau_2\eta_1 + \tau_1\eta_2)/\pi^2 + 6\tau_1^2\eta_1/\pi^3.
\end{align*}
\end{proof}


\begin{thm}[Identification of $\beta$, $\alpha_0$, $\alpha_1$]
  \label{thm:main_ident}
\end{thm}
\begin{proof}[Proof of Theorem \ref{thm:main_ident}]
  For ease of notation we suppress dependence on $\mathbf{x}$ throughout this argument.
  So long as $\beta \neq 0$, we can rearrange Equations \ref{eq:theta2_def} and \ref{eq:theta3_def} to obtain 
  \begin{align}
    \label{eq:quadraticA}
  A &= \theta_2/\theta_1^2 = 1 + (\alpha_0 - \alpha_1)  \\
  \label{eq:quadraticB}
  B &= \theta_3/\theta_1^3 = (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1)
  \end{align}
  Equation \ref{eq:quadraticA} gives $(1 - \alpha_1)= A - \alpha_0$.
  Hence $(1 - \alpha_0 - \alpha_1) = A - 2\alpha_0$ and $\alpha_0(1 - \alpha_1) = \alpha_0(A - \alpha_0)$.
  Substituting into Equation \ref{eq:quadraticB} and simplifying, $(A^2 - B) + 2A \alpha_0 - 2\alpha_0^2=0$.
  Substituting for $\alpha_0$ analogously yields a quadratic in $(1 - \alpha_1)$ with \emph{identical} coefficients.
It follows that one root of $(A^2-B) + 2Ar - 2r^2=0$ is $\alpha_0$ and the other is $1 - \alpha_1$.
Solving,
  \begin{equation}
    r = \frac{A}{2} \pm \sqrt{3 A^2 - 2B} = \frac{1}{\theta_1^2}\left(\frac{\theta_2}{2} \pm  \sqrt{3\theta_2^2  - 2\theta_1 \theta_3}\right).
  \end{equation}
By Equations \ref{eq:theta2_def} and \ref{eq:theta3_def}, 
  \begin{align*}
    3\theta^2_2 - 2\theta_1 \theta_3 &= 3 \left[ \theta_1^2 \left( 1 + \alpha_0 - \alpha_1 \right) \right]^2 - 2 \theta_1 \left\{ \theta_1^3 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0 (1 - \alpha_1) \right] \right\} \\
    &= \theta_1^4 \left\{ 3(1 + \alpha_0 - \alpha_1)^2 - 2 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1) \right] \right\}.
  \end{align*}
Expanding the first term we find that 
  \begin{align*}
    3(1 + \alpha_0 - \alpha_1)^2 
    %&= 3\left[ 1 + (\alpha_0 - \alpha_1) \right]^2 
    &= 3\left[ 1 + 2(\alpha_0 - \alpha_1) + (\alpha_0 - \alpha_1)^2 \right]\\
    &= 3 + 6\alpha_0 - 6\alpha_1 + 3 \alpha_0^2 + 3 \alpha_1^2 - 6\alpha_0\alpha_1 
  \end{align*}
and expanding the second 
  \begin{align*}
    2\left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0(1 - \alpha_1) \right]
    %&= 2\left\{ \left[ 1 - (\alpha_0 + \alpha_1) \right]^2 + 6\alpha_0(1 - \alpha_1) \right\}\\
  &=2\left[ 1 - 2(\alpha_0 + \alpha_1) + (\alpha_0 + \alpha_1)^2 + 6\alpha_0 - 6 \alpha_0 \alpha_1 \right]\\
    %&= 2 - 4(\alpha_0 + \alpha_1) + 2(\alpha_0 + \alpha_1)^2 + 12\alpha_0 - 12 \alpha_0 \alpha_1 \\
    &= 2 + 8\alpha_0 - 4\alpha_1 + 2\alpha_0^2 +  2\alpha_1^2 - 8 \alpha_0 \alpha_1.
  \end{align*}
Therefore
  \begin{align*}
    3\theta^2_2 - 2\theta_1 \theta_3 
    %&= \theta_1^4 \left\{ 3(1 + \alpha_0 - \alpha_1)^2 - 2 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1) \right] \right\}\\
    &= \theta_1^4 \left\{ 1 - 2 \alpha_0 - 2 \alpha_1 + \alpha_0^2 - \alpha_1^2 + 2\alpha_0 \alpha_1 \right\}\\
    &= \theta_1^4 \left[ (1 - \alpha_0 - \alpha_1)^2 \right]
  \end{align*}
which is strictly greater than zero since $\theta_1 \neq 0$ and $\alpha_0 + \alpha_1 \neq 0$.
It follows that both roots of the quadratic are real.
Moreover, $3\theta_2^2/\theta_1^4 - 2\theta_3/\theta_1^3$ identifies $(1 - \alpha_0 - \alpha_1)^2$.
Substituting into Equation \ref{eq:theta1_def}, it follows that $\beta$ is identified up to sign.
If $\alpha_0 + \alpha_1 < 1$ then $\mbox{sign}(\beta) = \mbox{sign}(\theta_1)$ so that both the sign and magnitude of $\beta$ are identified.
If $\alpha_0 + \alpha_1 < 1$ then $1 - \alpha_1 > \alpha_0$ so $(1 - \alpha_1)$ is the larger root of $(A^2 - B) + 2Ar - 2r^2=0$ and $\alpha_0$ is the smaller root.
\end{proof}

\section{Notes on Bugni, Canay \& Shi (2017)}

\todo[inline]{I've played around with this a bit for our example and I don't think it's going to work. The problematic step is the construction of the profiled test statistic. If we want, for example, to carry out inference for $\beta$, we need to estimate $\alpha_0$ and $\alpha_1$ under the null $\beta = \beta_0$. But if $\beta_0$ is small, the moment equalities contain almost no information about $\alpha_0$ and $\alpha_1$ so the optimization problem goes haywire.}

The notation follows the paper nearly verbatim, although I have made a few minor changes to the notation to avoid ambiguity.
Regarding the sequence $\kappa_n$, used to carry out moment selection, the paper only requires that $\kappa_n \rightarrow \infty$ and $\kappa_n/\sqrt{n} \rightarrow 0$ as in Andrews \& Soares.
The leading example, and indeed the choice recommended by Andrews \& Soares, is the BIC-type penalty $\kappa_n = \sqrt{\ln n}$ so in all the expressions below I replace $\kappa_n$ with this quantity.

\paragraph{Setup} Partially identified model defined by $p$ moment  inequalities and $k - p$ equalities:
\begin{align*}
  E_F\left[ m_j(W_i,\theta) \right] &\geq 0 \mbox{ for } j = 1, \cdots, p\\
  E_F\left[ m_j(W_i,\theta) \right] &= 0 \mbox{ for } j = p+1, \cdots, k
\end{align*}
where $W_1, \hdots, W_n \sim \mbox{ iid } F$.
Function $m = (m_1, \hdots, m_k)$ is known and $\theta \in \Theta$ is a finite-dimensional parameter.
The goal is to construct confidence sets for $\theta$.


\paragraph{The Problem} 
The existing literature tests the \emph{joint} hypothesis $\theta = \theta_0$ and inverts this test to construct a confidence set (CS) for $\theta$.
In applied work, we often want inference for a \emph{subset} of $\theta$ and the usual approach is to \emph{project} a joint confidence set for $\theta$.
For example if $\theta = (\alpha, \beta, \gamma, \delta)$, a projection confidence set for $\alpha$ is the set of all values for which we can find \emph{some} triple $(\beta_0, \gamma_0, \delta_0)$, allowed to depend on $\alpha$,  such that $\theta = (\alpha, \beta_0, \gamma_0, \delta_0)$ is not rejected.
This approach is computationally intensive: even if we are only interested in one element of $\theta$ we have to invert the test over the high-dimensional \emph{joint} parameter space.
Projection inference can also have poor power compared to the joint inference upon which it is based.

\paragraph{This Paper} 
The authors propose the \emph{minimum resampling test}: a direct test of
\[
  H_0\colon \lambda(\theta) = \lambda_0 \, \mbox{ vs. } \, H_1\colon \lambda(\theta) \neq \lambda_0
\]
where $\lambda$ is a \emph{function} of $\theta$.
The leading case is where $\lambda$ is simply a sub-vector or individual coordinate of $\theta$.
The test controls size uniformly, has high power, and is in general more computationally efficient than projection inference whenever $\lambda$ is of lower dimension that $\theta$. 


\paragraph{Decision Rule for Minimum Resampling (MR) Test}
The test rejects $H_0\colon \lambda(\theta) = \lambda_0$ when a \emph{profiled test statistic} $T_n(\lambda_0)$ is large.
The rejection rule is
\[
  \text{Reject if} \quad T_n(\lambda_0) > \widehat{c}_n^{MR}(\lambda_0, 1-\alpha) 
\]
where $\alpha$ is the significance level and $\widehat{c}_n^{MR}(\lambda_0, 1-\alpha)$ is the MR critical value, which is calculated by a bootstrap procedure described below.

\paragraph{Notation}
\begin{align*}
  \bar{m}_{n,j}(\theta) &\equiv \frac{1}{n}\sum_{i=1}^n m_j(W_i,\theta), \mbox{ for } j = 1, \hdots, k\\
  \widehat{\sigma}^2_{n,j} &\equiv \frac{1}{n}\left[ m_j(W_i,\theta) - \bar{m}_j(W_i,\theta) \right]^2, \mbox{ for } j = 1, \hdots, k\\
[x]_{-} &\equiv \min\left\{ x, 0 \right\}\\
  Q_n(\theta) &= \underbrace{\sum_{i=1}^p \left[ \frac{\sqrt{n}\bar{m}_{n,j}(\theta)}{\widehat{\sigma}_{n,j}(\theta)} \right]_{-}^{2}}_{\text{inequalities}} + \underbrace{\sum_{j=p+1}^{k} \left\{ \frac{\sqrt{n}\bar{m}_{n,j}(\theta)}{\widehat{\sigma}_{n,j}(\theta)} \right\}^2}_{\text{equalities}}\\
  \Theta(\lambda_0) &\equiv \left\{ \theta \in \Theta\colon \lambda(\theta) = \lambda_0 \right\}\\
  T_n(\lambda_0) &= \inf_{\theta \in \Theta(\lambda_0)} Q_n(\theta)
\end{align*}

\paragraph{Calculating the MR Critical Value}
The MR critical value $\widehat{c}_n(\lambda_0, 1 - \alpha)$ is calculated from \emph{two different approximations} to the sampling distribution of $T_n(\lambda_0)$: \emph{discarded resampling} (DR) and \emph{penalized resampling} (PR), specifically
\begin{align*}
  \widehat{c}_n^{MR}(\lambda_0, 1 - \alpha) &\equiv \mbox{(conditional) } 1 - \alpha \mbox{ quantile of } T_n^{MR}(\lambda_0) \\
  T_n^{MR}(\lambda_0) &\equiv \min\left\{ T_n^{DR}(\lambda_0), T_n^{PR}(\lambda_0) \right\}
\end{align*}
where we approximate the distribution of $T_n^{MR}$ by simulation, as described in more detail below.
Both $T_n^{DR}$ and $T_n^{PR}$ are constructed from $v^*_{n,j}(\theta)$, defined as: 
\begin{align*}
  v^*_{n,j}(\theta) &= \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \xi_i \left[ \frac{m_j(W_i,\theta) - \bar{m}_{n,j}(\theta)}{\widehat{\sigma}_{n,j}(\theta)} \right], \, \mbox{for } j = 1, \hdots, p \\
  \xi_1, \cdots, \xi_n &\sim \mbox{ iid } N(0,1) \mbox{ independent of the data } W_1, \cdots, W_n.
\end{align*}
In particular, both the $T_n^{DR}$ and $T_{n}^{PR}$ take the form
\[
  \inf_{\theta \in \widetilde{\Theta}}\left(\underbrace{\sum_{j=1}^p \left[ v_{n,j}^*(\theta) + s_j(\theta) \right]_{-}^2}_{\text{inequalities}} + \underbrace{\sum_{j=p+1}^{k}\left\{   v^*_{n,j}(\theta) + s_j(\theta)\right\}^2}_{\text{equalities}}  \right)
\]
where $v^*_{n,j}(\theta)$ is as defined in the preceding pair of displayed equations, $\widetilde{\Theta}$ is some set, and $s_j(\theta)$ is a \emph{slackness function}.
The difference between DR and PR is the choice of $\widetilde{\Theta}$ and $s_j(\theta)$ as explained in the following paragraphs.

\paragraph{Discarded Resampling Statistic: $T_n^{DR}(\lambda_0)$}
\begin{align*}
  T_n^{DR}(\lambda_0) &\equiv \inf_{\theta \in \widehat{\Theta}_I(\lambda_0)}\left(\sum_{j=1}^p \left[ v_{n,j}^*(\theta) + \varphi_j(\theta) \right]_{-}^2 + \sum_{j=p+1}^{k}\left\{   v^*_{n,j}(\theta) + \varphi_j(\theta)\right\}^2  \right)\\
  \widehat{\Theta}_I(\lambda_0) &\equiv \left\{ \theta\in \Theta(\lambda_0)\colon Q_n(\theta) \leq T_n(\lambda_0) \right\}\\
  \varphi_j(\theta) &= \left\{
  \begin{array}{ll}
    \infty & \mbox{if } \quad \sqrt{n}\, \bar{m}_{n,j}(\theta)/[ \widehat{\sigma}_{n,j}(\theta) \sqrt{\ln n}] > 1 \quad \mbox{ and } j \leq p\\
    0 & \mbox{if } \quad \sqrt{n}\, \bar{m}_{n,j}(\theta)/[ \widehat{\sigma}_{n,j}(\theta) \sqrt{\ln n}]  \leq 1 \quad \mbox{ or } j > p
  \end{array}
  \right.
\end{align*}
Note that $\widehat{\Theta}_I(\lambda_0)$ contains the values of $\theta$ that \emph{minimize} $T_n(\lambda_0) = \inf_{\theta \in \Theta(\lambda_0)} Q_n(\theta)$.
Note further that $\varphi_j$ only affects the moment \emph{inequalities}: it is zero for $j > p$.

\paragraph{Penalized Resampling Test Statistic: $T_n^{PR}(\lambda_0)$}
\begin{align*}
  T_n^{PR}(\lambda_0) &\equiv \inf_{\theta \in \Theta(\lambda_0)}\left(\sum_{j=1}^p \left[ v_{n,j}^*(\theta) + \ell_j(\theta) \right]_{-}^2 + \sum_{j=p+1}^{k}\left\{   v^*_{n,j}(\theta) + \ell_j(\theta)\right\}^2  \right)\\
  \ell_j(\theta) &= \sqrt{n}\, \bar{m}_{n,j}(\theta)/\left[ \widehat{\sigma}_{n,j}(\theta) \sqrt{\ln n} \right] 
\end{align*}
Note that to calculate $T_n^{PR}(\lambda_0)$ we minimize over $\Theta(\lambda_0)$ -- the set of all $\theta$ for which $\lambda(\theta) = \lambda_0$.
Note further that $\ell_j(\theta)$ affects \emph{both} the moment \emph{equalities} and inequalities.

\paragraph{Computational Cost}
In practice we generate draws for the test statistics using a large number $B$ of simulation replications.
These are essentially bootstrap draws, but we bootstrap the limit distribution rather than the data.
In particular, each of these bootstrap replications makes $n$ iid standard normal draws: $\xi_1, \hdots, \xi_n$ which are used in the calculation of \emph{both} $T_n^{DR}$ and $T_n^{PR}$.
(It is important to make sure that we use the same draws for both!)
The computational cost of the MR test comes from the $2B +1$ optimization problems it requires us to solve.
First, we need to calculate the test statistic $T_n(\lambda_0)$ based on the \emph{actual data} -- this requires us to solve one minimization problem.
Then for each of the $B$ bootstrap replications we need to calculate $T_n^{DR}$ and $T_n^{PR}$ -- this requires us to solve two minimization problems.
The authors claim that this is in general more efficient than projection inference which requires us to search over a high-dimensional space.
Presumably the gains depend sensitively on how efficiently one can carry out the required minimizations.


\paragraph{Sub-vector Inference}
The leading application of the MR test is to sub-vector inference.
In this case we only want to carry out a test for some subset of the coordinates of $\theta$.
Let $\theta_s$ denote this sub-vector.
Then $\Theta(\lambda_0) = \left\{ \theta\in \Theta \colon \theta_s = \lambda_0 \right\}$. 
A particularly simple example is inference for a single coordinate of $\theta$, say $\theta_1$.
In this case, the optimization problems are over all \emph{other} coordinates of $\theta$ besides $\theta_1$ since $\theta_1 = \lambda_0$ under the null that we are testing.

\paragraph{Steps to Carry out the MR Test}
For simplicity, in this section I assume that we wish to carry out inference on a sub-vector of $\theta$.
Let $\theta = (\beta, \gamma)$ where $\beta$ and $\gamma$ could be scalars or vectors.
The parameters of interest are $\beta$ and our null hypothesis is $\beta = \beta_0$.
Let $\Gamma$ denote the parameter space for $\gamma$.
I write any function of $\theta$ explicitly as a function of $(\beta, \gamma)$ so that any calculation carried out under the null is evaluated at $(\beta_0, \gamma)$
\begin{enumerate}
  \item Define the following functions of $\gamma$ under the null $\beta = \beta_0$
    \begin{align*}
      \bar{m}_n(\gamma) &= \frac{1}{n} \sum_{i=1}^n m(W_i,\beta_0,\gamma) \\
      \widehat{D}_n(\gamma) &= \mbox{diag}\left\{ \frac{1}{n} \sum_{i=1}^{n} \left[ m(W_i,\beta_0, \gamma) - \bar{m}_n(\gamma) \right]\left[ m(W_i,\beta_0, \gamma) - \bar{m}_n(\gamma) \right]'\right\} \\
      v(\gamma) &= \sqrt{n} \widehat{D}^{-1/2}_n(\gamma)\bar{m}_n(\gamma) \\
      Q(\gamma) &=  \sum_{j=1}^p \left[ v_j(\gamma) \right]^2_{-} + \sum_{j=p+1}^k v_j^2(\gamma) 
    \end{align*}
  \item Calculate the test statistic using the observed data $\{W_1, \hdots, W_n\}$
    \[
      T_n = \min_{\gamma \in \Gamma} Q(\gamma) 
    \]
  \item Calculate $\widehat{\Gamma}_I$ -- the ``estimated set of minimizers'' of $Q(\gamma)$ 
    \[
      \widehat{\Gamma}_I = \left\{\gamma \in \Gamma \colon Q(\gamma) \leq T_n \right\} 
    \]
  \item For each of $B$ independent bootstrap replications, do the following:
    \begin{enumerate}[(i)]
      \item Draw $\xi_1, \xi_2, \hdots, \xi_n \sim \mbox{ iid } N(0,1)$
      \item Construct $v^*(\gamma)$ -- the re-centered, bootstrap version  of $v(\gamma)$ -- as follows
        \[
          v^*(\gamma) = n^{-1/2} \widehat{D}_n^{-1/2}(\gamma) \sum_{i=1}^n \xi_i \left[ m(W_i, \beta_0, \gamma) - \bar{m}_n(\gamma) \right]
        \]
    \end{enumerate}
\end{enumerate}


\section{May 18th, 2017}
Today I figured out that the Bugni, Canay \& Shi idea isn't going to work for our example since the optimization problem involved in computing the profiled test statistic is very badly behaved under the null $\beta = \beta_0$ when $\beta_0$ is small.
This means that we'll have to go back to doing joint inference over $(\alpha_0, \alpha_1, \beta)$ using Andrews \& Soares.
Here are a few ideas:
\begin{enumerate}
  \item I think it could be faster to use the ``asymptotic version'' of Andrews and Soares.
    The asymptotic version involves making normal draws instead of bootstrapping the sample.
    Is there a tradeoff in terms of accuracy?
    Presumably it won't matter much for reasonably large samples.
  \item $\theta_1$ is strongly identified regardless of the values of $\beta$, $\alpha_0$ and $\alpha_1$.
    This means that if we wanted to do inference for $\alpha_0, \alpha_1$ we could use a plug-in estimator of $\theta_1$ provided that we correct the asymptotic variance matrix estimator as described in section 10.2 of Andrews and Soares.
  \item Might it be interesting to look at a test for the presence of mis-classification? This would entail testing the null that $\alpha_0 = \alpha_1 = 0$.
    This is a null for which the moment inequalities cannot provide any information so presumably we could just use Stock and Wright's GMM-AR test.
    It might also be interesting to try this with Mahajan or Lewbel and show the size distortions that arise if one does not use the AR test.
  \item Our implementation of Andrews and Soares should use preliminary estimators for $\boldsymbol{\kappa}$ and the parameters associated with exogenous covariates.
    We can use the simplest possible estimator for $\boldsymbol{\kappa}$ under the null: three sample means, e.g.\ $\kappa_1 = \bar{y} - \beta_0 \bar{T}/(1 - \alpha_0 - \alpha_1)$.
    Similarly, for the coefficient on the exogenous covariates you just have a regression under the null in which $y - \beta_0 T/(1 - \alpha_0 - \alpha_1)$ replaces $y$.
  \item Is there a clever way to use the fact that $\theta_1$ is strongly identified to help us carry out inference for $\beta$?
   Specifically, suppose we construct a confidence set for $\alpha_0, \alpha_0$ by profiling out $\theta_1$. 
   Can we use this to back out inference for $\beta$?
\end{enumerate}

\section{May 18--19, 2017: More on Andrews \& Soares}
\paragraph{Recall the notation from above:}
\[
  \mathbb{E} \left[ m_j(\mathbf{w}_i,\theta_0) \right]
  \left\{
  \begin{array}{cc}
    \geq 0 & \mbox{for } j = 1, \cdots, p\\
    = 0 & \mbox{for } j = p + 1, \cdots,k \mbox{ where } k = p + v
  \end{array}
  \right.
\]

\[
  \bar{m}_n(\theta) = \left[
  \begin{array}{c}
    \bar{m}_{n,1}(\theta)\\
    \vdots \\
    \bar{m}_{n,k}(\theta)\\
  \end{array}
\right], \quad
\bar{m}_{n,j} = \frac{1}{n} \sum_{i=1}^{n} m_j(\mathbf{w}_i, \theta) \mbox{ for } j = 1, \cdots, k
\]
Now, let $\Sigma(\theta_0)$ denote the asymptotic variance of $\sqrt{n}\; \bar{m}_n(\theta)$.
We estimate this quantity using $\widehat{\Sigma}_n(\theta)$.
For iid observations, as in our example, the estimator is
\[
  \widehat{\Sigma}_n(\theta) = \frac{1}{n} \sum_{i=1}^n \left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]\left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]', \quad 
  m(\mathbf{w}_i, \theta) = \left[
  \begin{array}{c}
    m_1(\mathbf{w}_i, \theta)\\
    \vdots \\
    m_k(\mathbf{w}_i, \theta)\\
  \end{array}
\right]
\]
The test statistic takes the form $T_n(\theta) = S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right)$ for some real-valued function $S$.
The example we will use is $S_1$, defined by
\[
  S_1(m, \Sigma) = \sum_{j=1}^p [m_j/\sigma_j]^2_- + \sum_{j=p+1}^{p+v} (m_j/\sigma_j)^2
\]
where $m = (m_1, \cdots, m_k)'$, $\sigma_j^2$ is the $j$th diagonal element of $\Sigma$, and
\[
  [x]_- = \left\{
  \begin{array}{cc}
    x, & \mbox{if } x <0\\
    0, & \mbox{if } x \geq 0
  \end{array}
\right.
\]


\paragraph{Calculating the Critical Value for the \emph{Asymptotic} Version of GMS}
Above we described the calculation of the \emph{bootstrap} critical value for a test of $\theta = \theta_0$. 
We now present an alternative method that uses iid standard normal draws to ``bootstrap the limit experiment.''

\begin{enumerate}
  \item Calculate $\bar{m}_n(\theta_0)$, $\widehat{\Sigma}_n(\theta_0)$, and $T_n(\theta_0) = S\left( \sqrt{n}\; \bar{m}_n(\theta_0), \widehat{\Sigma}_n(\theta_0) \right)$
  \item Calculate the following quantities:
    \begin{align*}
      \widehat{\Omega}_n(\theta_0) &= \mbox{Diag}^{-1/2}\left(\widehat{\Sigma}_n(\theta_0)  \right)\left(\widehat{\Sigma}_n(\theta_0)\right)\mbox{Diag}^{-1/2}\left( \widehat{\Sigma}_n(\theta_0) \right)\\
      \xi_n(\theta_0) &= \mbox{Diag}^{-1/2}\left( \widehat{\Sigma}_n(\theta_0) \right) \sqrt{n}\; \bar{m}_n(\theta_0) / \sqrt{\ln n}\\
      \varphi\left( \xi_n(\theta_0), \widehat{\Omega}_n(\theta_0) \right) &= \left\{
      \begin{array}{cc}
        0, & \mbox{if } \xi_j \leq 1 \mbox{ and } j \leq p \\
        \infty & \mbox{if } \xi_j > 1 \mbox{ or } j > p\\
      \end{array}
      \right.
    \end{align*}
  \item Draw $Z_1^*, \hdots, Z_R^* \sim \mbox{ iid } N(0_k, I_k)$ for $R$ large.
  \item The critical value is the $1 - \alpha$ quantile of $\left\{S\left( \widehat{\Omega}^{1/2}(\theta_0) Z_r^* + \varphi\left( \xi_n(\theta_0), \widehat{\Omega}_n(\theta_0) \right), \widehat{\Omega}_n(\theta_0) \right)\right\}_{r=1}^R$
\end{enumerate}

\paragraph{Clearer Explanation of the Asymptotic GMS Critical Value Calculation}
The preceding paragraph used the explanation from Andrews \& Soares but it's a little obscure. 
Here's a simpler and clearer explanation.
Recall that the statistics from Andrews \& Soares (2010) satisfy
\begin{align*}
  T_n &= S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right) = S\left( \widehat{D}_n^{-1/2}(\theta) \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Omega}_n(\theta) \right)\\
  \widehat{D}_n(\theta) &= \mbox{diag}\left( \widehat{\Sigma}\left( \theta \right) \right)\\
  \widehat{\Omega}_n(\theta) &= \widehat{D}_n^{-1/2}(\theta)\, \widehat{\Sigma}(\theta)\, \widehat{D}_n^{-1/2}(\theta)
\end{align*}
The procedure for the asymptotic version of the GMS test is as follows:
\begin{enumerate}
  \item Calculate $\sqrt{n}\; \bar{m}_n\left( \theta_0 \right)$ and $\widehat{\Sigma}(\theta_0)$ under the null hypothesis $H_0\colon \theta = \theta_0$.
  \item Calculate the test statistic $T_n(\theta_0) = S_1\left(\sqrt{n}\; \bar{m}_n\left( \theta_0 \right), \widehat{\Sigma}\left( \theta_0 \right)  \right)$.
  \item Determine which inequality moment conditions are ``far from binding'' under $H_0\colon \theta = \theta_0$
    \begin{itemize}
      \item Let $j \in J = \{ 1, \cdots, p\}$ index the inequality moment conditions.
      \item Let $\widehat{\sigma}_{n,j}(\theta_0)^2$ denote the $(j,j)$ element of $\widehat{\Sigma}(\theta_0)$
      \item For each $j\in J$ calculate the ``t-statistic'' $ t_{n,j} = \sqrt{n}\; \bar{m}_j(\theta_0)/\widehat{\sigma}_{n,j}(\theta_0)$
      \item Let $\mathcal{FB}$ denote the subset of $J$ for which $t_{n,j} >\sqrt{\log n}$.
        These are the inequality moment conditions that are ``far from binding'' under $H_0\colon\theta = \theta_0$. 
    \end{itemize}
  \item Calculate the asymptotic critical value by simulation as follows:
    \begin{itemize}
      \item Draw $R$ standard normal $k$-vectors: $Z^*_1, \hdots, Z^*_R \sim \mbox{ iid } N(0_k, I_k)$
      \item Construct the $k$-vector $\varphi(\theta_0)$ from its coordinates $\varphi_j$ as follows:
        \[
          \varphi_j(\theta_0) = \left\{
          \begin{array}{ll}
            \infty & j\in \mathcal{FB} \mbox{ and } j\leq p\\
            0 & j>p \mbox{ or } 
          \end{array}
          \right.
        \]
      \item Set $M^*_{n,r}(\theta_0) = \widehat{\Omega}_n^{1/2}(\theta_0) Z_r^*$, yielding $R$ iid $N(0_k, \widehat{\Omega}_n)$ draws.
      \item Calculate $T^{*(r)}_n(\theta_0) = S_1\left(M^*_{n,r}(\theta_0) + \varphi(\theta_0), \;\widehat{\Omega}_n(\theta_0)\right)$ for $r=1, 2, \hdots, R$. Notice that adding $\varphi(\theta_0)$ is equivalent to dropping any moment conditions that we have determined are far from binding before calculating the test statistic.
      \item Set $\widehat{c}_n(\theta_0, 1 - \delta)$ equal to the $1 - \delta$ quantile of $\left\{ T_n^{*(r)}(\theta_0) \right\}_{r=1}^R$.
    \end{itemize}
  \item Reject $H_0\colon \theta = \theta_0$ if $T_n(\theta_0) > \widehat{c}_n(\theta_0, 1-\delta)$
  \item To construct a $(1 - \delta)\times 100\%$ confidence set, invert the test of $H_0\colon \theta = \theta_0$ for $\theta_0 \in \Theta$.
\end{enumerate}


\paragraph{Preliminary Estimation of an Identified Parameter}
Suppose that the moment equations take the form $m_j(W_i, \theta, \tau)$ where $\tau$ is identified under the null that $\theta = \theta_0$.
Let $\widehat{\tau}_n(\theta_0)$ be a consistent, asymptotically normal estimator for $\tau$ under the null that $\theta = \theta_0$.
As long as $\sqrt{n} \; \bar{m}_n(\theta, \widehat{\tau}_n(\theta))$ is asymptotically normal we can plug in our estimator of $\tau$ and carry out the GMS test almost exactly as above.
Only one small change is required: the variance matrix estimator $\widehat{\Sigma}_n(\theta_0)$ needs to be adjusted to take account of the fact that $\tau$ has been estimated.
This is fairly straightforward using standard calculations for moment condition models.
The explanation we give here follows Hall (2005) although the notation is changed to match Andrews and Soares.

Let $m(W_i, \theta, \tau)$ be a vector of moment functions that we will use to test $\theta = \theta_0$.
Some of these moment functions will enter as equalities and others as inequalities but this is totally immaterial for the derivation to follow.
Let $h(W_i, \theta, \tau)$ be another collection of moment functions that will be used to construct the estimator $\widehat{\tau}_n(\theta)$ but not to test $\theta = \theta_0$.
Let $g(W_i, \theta, \tau) = (m', h')'$ denote the full set of moment functions.
This argument will use a mean-value expansion and hence will rely on $\tau$ being on the interior of the parameter space.
So the idea here is that $\theta$ contains the ``problematic'' parameters, those that are weakly identified or may lie on a boundary, while $\tau$ contains the ``well-behaved'' parameters.
Define: 
\begin{align*}
  \bar{g}_n(\theta, \tau) &= 
  \left[
  \begin{array}{c}
    \bar{m}_n(\theta, \tau)\\
    \bar{h}_n(\theta, \tau)\\
  \end{array}
\right] = 
  \frac{1}{n}\sum_{i=1}^n g(W_i, \theta, \tau) \\
G_n(\theta, \tau) &= \left[
\begin{array}{c}
  M_n(\theta, \tau)\\
  H_n(\theta, \tau)
\end{array}
\right] = n^{-1}\sum_{i=1}^n \partial g(W_i, \theta, \tau)/\partial \tau'
\end{align*}
Throughout this argument, we will hold $\theta$ \emph{fixed} at $\theta_0$.
Under $\theta = \theta_0$ we have the following GMM estimator for $\tau(\theta_0)$, the true value of $\tau$ assuming that $\theta = \theta_0$,
\[
  \widehat{\tau}_n(\theta_0) = \underset{\tau \in \mathscr{T}}{\arg \min} \; \bar{h}_n(\theta_0, \tau)'\, \Xi_n \, \bar{h}_n(\theta_0, \tau)
\]
where $\Xi_n \rightarrow_p \Xi$.
To simplify the notation, we suppress dependence on $\theta$.
Unless otherwise specified, \emph{every function} is evaluated at $\theta = \theta_0$, e.g.\ 
\[
  \widehat{\tau}_n = \widehat{\tau}_n(\theta_0), \quad \bar{g}_n(\tau) = \bar{g}_n(\theta_0, \tau), \quad G_n(\tau) = G_n(\theta_0, \tau)
\]
Now, mean-value expanding $\bar{g}_n$, viewed as a function of $\tau$ \emph{only}, around $\tau_0 \equiv \tau(\theta_0)$,
\begin{equation}
  \bar{g}_n\left(\widehat{\tau}_n\right) = \bar{g}_n(\tau_0) + G_n(\widehat{\tau}_n, \tau_0, \lambda_n) \left( \widehat{\tau}_n - \tau_0 \right)
  \label{eq:gn_MVE}
\end{equation}
where the $i$th row of $G_n(\widehat{\tau}_n, \tau_0, \lambda_n)$ equals $i$th row of $G_n(\widetilde{\tau}_n^{(i)})$ and 
\[
\widetilde{\tau}_n^{(i)} = \lambda_{n,i}\tau_0 + (1 - \lambda_{n,i}) \widehat{\tau}_n
\] 
for some $0 \leq \lambda_{n,i} \leq 1$.
The preceding mean-value expansion is for $\bar{g}_n = (\bar{m}_n', \bar{h}_n')'$.
Restricting our attention to the sub-vector $\bar{h}_n$, we have
\[
  \bar{h}_n(\widehat{\tau}_n) = \bar{h}_n(\tau_0) + H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)(\widehat{\tau}_n - \tau_0) 
\]
where $\lambda^h_n$ is the sub-vector of $\lambda_n$ that corresponds to $H_n$. 
Pre-multiplying by $H_n(\widehat{\tau}_n)'\Xi_n$, we have
\[
  H_n(\widehat{\tau}_n)'\Xi_n\bar{h}_n(\widehat{\tau}_n) = H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\tau_0) + H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)(\widehat{\tau}_n - \tau_0).
\]
But by the first-order conditions for $\widehat{\tau}_n$, we have  $H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\widehat{\tau}_n) = 0$ and thus
\[
  H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\tau_0) + H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)(\widehat{\tau}_n - \tau_0) = 0.
\]
Rearranging,
\begin{equation}
  (\widehat{\tau}_n - \tau_0) = -\left[ H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)\right]^{-1} H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\tau_0).
  \label{eq:tau_solution}
\end{equation}
Now, substituting Equation \ref{eq:tau_solution} into Equation \ref{eq:gn_MVE},
\begin{align*}
  \bar{g}_n\left(\widehat{\tau}_n\right) &= \bar{g}_n(\tau_0) + G_n(\widehat{\tau}_n, \tau_0, \lambda_n) \left( \widehat{\tau}_n - \tau_0 \right)\\
  &= \bar{g}_n(\tau_0) - G_n(\widehat{\tau}_n, \tau_0, \lambda_n) \left[ H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)\right]^{-1} H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\tau_0)
\end{align*}
And since $\bar{g}_n = (\bar{m}_n', \bar{h}_n')$ and $G_n = (M_n', H_n')'$, 
\begin{align*}
  \left[
  \begin{array}{cc}
    \sqrt{n}\;\bar{m}_n(\widehat{\tau}_n)\\
    \sqrt{n}\;\bar{h}_n(\widehat{\tau}_n)
  \end{array}
\right] &=
  \left[
  \begin{array}{cc}
    \sqrt{n}\;\bar{m}_n(\tau_0)\\
    \sqrt{n}\;\bar{h}_n(\tau_0)
  \end{array}
\right] - 
\left[
\begin{array}{c}
  M_n(\widehat{\tau}_n, \tau_0, \lambda_n^m)\\
  H_n(\widehat{\tau}_n, \tau_0, \lambda_n^h)
\end{array}
\right]
\left[ H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)\right]^{-1} H_n(\widehat{\tau}_n)'\Xi_n \left[\sqrt{n}\;\bar{h}_n(\tau_0)\right]\\
  &= \left[
  \begin{array}{cc}
    \sqrt{n}\;\bar{m}_n(\tau_0)\\
    \sqrt{n}\;\bar{h}_n(\tau_0)
  \end{array}
\right] - 
\left[
\begin{array}{r}
M(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi \\
H(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi 
\end{array}
\right]
\sqrt{n}\;\bar{h}_n(\tau_0) + o_p(1)
\end{align*}
where $M(\tau) = E[\partial m(W_i, \theta_0, \tau)/\partial \tau']$ and $H(\tau) = E[\partial h(W_i, \theta_0, \tau)/\partial \tau']$.
We are only interested in the behavior of $\sqrt{n} \; m_n(\widehat{\tau}_n)$.
Restricting attention to this sub-vector,
\begin{align*}
  \sqrt{n}\; \bar{m}_n(\widehat{\tau}_n) &= \sqrt{n}\; \bar{m}_n(\tau_0) - M(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi \times \sqrt{n}\; \bar{h}_n(\tau_0) + o_p(1) \\
  &=  
  \left[
  \begin{array}{cc}
    \mathbf{I}_k & -M(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi 
  \end{array}
\right]
\left[
\begin{array}{c}
  \sqrt{n}\; \bar{m}_n(\tau_0)\\ 
  \sqrt{n}\; \bar{h}_n(\tau_0)
\end{array}
\right] + o_p(1) \\
&=  
\left[
\begin{array}{cc}
  \mathbf{I}_k & B(\tau_0)
\end{array}
\right] \sqrt{n}\; \bar{g}_n(\tau_0)
\end{align*}
where $\mathbf{I}_k$ is the $k\times k$ identity matrix and $B(\tau_0) = M(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi$.
We now have all the ingredients we need to calculate our variance matrix adjustment.
Restoring explicit dependence on $\theta_0$, we have 
\[
  \sqrt{n}\; \bar{m}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right) =  \left[
  \begin{array}{cc}
    \mathbf{I}_k & B\left( \theta_0, \tau(\theta_0) \right) 
\end{array}
\right] \; \sqrt{n}\; \bar{g}_n\left( \theta_0, \tau(\theta_0) \right)
\]
By an appropriate Central Limit Theorem, $\sqrt{n}\; \bar{g}_n\left( \theta_0, \tau(\theta_0) \right)$ is asymptotically normal with variance matrix $\mathcal{V}\left( \theta_0, \tau(\theta_0) \right)$, partitioned as follows:
\[
  \mathcal{V}\left( \theta_0, \tau(\theta_0) \right) = 
  \left[
  \begin{array}{cc}
    \mathcal{V}_{mm}\left( \theta_0, \tau(\theta_0) \right) & \mathcal{V}_{mh}\left( \theta_0, \tau(\theta_0) \right) \\
    \mathcal{V}_{hm}\left( \theta_0, \tau(\theta_0) \right) & \mathcal{V}_{hh}\left( \theta_0, \tau(\theta_0) \right) \\
  \end{array}
\right].
\]
Thus, we calculate the asymptotic variance matrix $\Sigma\left(\theta_0, \widehat{\tau}_n(\theta_0)\right)$ of $\sqrt{n}\; \bar{m}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right)$ as follows, 
suppressing dependence on $\theta$ and $\tau$ for simplicity:
\begin{align*}
\Sigma &= 
\left[
\begin{array}{cc}
  \mathbf{I}_k & B
\end{array}
\right] 
\left[
\begin{array}{cc}
  \mathcal{V}_{mm} & \mathcal{V}_{mh}\\
  \mathcal{V}_{hm} & \mathcal{V}_{hh}
\end{array}
\right]
\left[
\begin{array}{c}
\mathbf{I}_k \\ B'
\end{array}
\right]
\end{align*}
So to estimate $\Sigma\left( \theta_0, \tau(\theta_0) \right)$ we require estimators of $B\left( \theta_0, \tau(\theta_0) \right)$ and $\mathcal{V}\left( \theta_0, \tau(\theta_0) \right)$.
In our example, $\tau(\theta_0)$ is just-identified, leading to the following simplification:
\[
  B = -M\left( H'\Xi H\right)^{-1} H'\Xi 
  = -M\left( H' H\right)^{-1} H' 
  = -M H^{-1} (H')^{-1} H' = -M H^{-1}.
\]
Thus, we can construct the desired variance matrix estimator $\widehat{\Sigma}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right)$ as follows:
\begin{align*}
  \widehat{\mathcal{V}}_n\left( \theta_0\right) &= \frac{1}{n} \sum_{i=1}^n \left[ g\left(W_i, \theta_0, \widehat{\tau}_n(\theta_0)\right) - \bar{g}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right) \right] \left[ g\left(W_i, \theta_0, \widehat{\tau}_n(\theta_0)\right) - \bar{g}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right) \right]'\\
  \widehat{M}_n\left( \theta_0 \right) &= \frac{1}{n} \sum_{i=1}^n \frac{\partial m\left( W_i, \theta_0, \widehat{\tau}_n(\theta_0) \right)}{\partial \tau'}\\
  \widehat{H}_n\left( \theta_0 \right) &= \frac{1}{n} \sum_{i=1}^n \frac{\partial h\left( W_i, \theta_0, \widehat{\tau}_n(\theta_0) \right)}{\partial \tau'}\\
  \widehat{A}_n(\theta_0) &= \left[
  \begin{array}{cc}
    \mathbf{I}_k &  -\widehat{M}_n(\theta_0) \widehat{H}_n^{-1}(\theta_0)\\
  \end{array}
\right]\\
\widehat{\Sigma}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right) &= \widehat{A}_n(\theta_0) \widehat{\mathcal{V}}_n(\theta_0) \widehat{A}_n(\theta_0)'
\end{align*}
Notice that if $M = 0$, so that the $m$ moment functions do not depend on $\tau$, then this reduces to the expression from Andrews \& Soares \emph{without} preliminary estimation of identified parameters.

\section{May 20th -- A\&S (2010) w/ Preliminary Estimates}
\todo[inline]{Notice that in this special case we have $\theta_2 = \beta \theta_1$. Since $\theta_1$ is strongly identified, this means that we could concentrate it out and conduct inference for $\beta$ alone.
But is this a good idea? It would mean that the inequalities don't give us any information. We should possibly try this out in simulations and see what happens. Note that the third moment condition simplifies in this example as well, so that $\theta_3 = \beta^2 \theta_1$. We could also try using this to see how valuable the over-identifying information can be. Could be interesting to try the same thing using quantiles to get over-identifying moment equalities. Can one carry out A\&S (2010) using a continuum of moment equalities and inequalities? Could at least try it out with deciles.}

\paragraph{Simple Example:}
We now return to the simple example of Andrews \& Soares from above, in which we assume that $\alpha_0 = 0$ and use only the ``weak'' bounds for $\alpha_1$.
The difference is that we will now estimate both intercepts: $\kappa_1$ and $\kappa_2$.
The moment equalities in this case are:
\[
  \mathbb{E}\left[ u_1(\kappa_1, \theta_1) \right] = 0, \quad
  \mathbb{E}\left[ u_1(\kappa_1, \theta_1) z \right] = 0, \quad
  \mathbb{E}\left[ u_2(\kappa_2, \theta_1, \theta_2) \right] = 0, \quad
  \mathbb{E}\left[ u_2(\kappa_2, \theta_1, \theta_2) z \right] = 0
\]
where
\begin{align*}
u_1(\kappa_1, \theta_1) &= y - \kappa_1 - \theta_1 T & \theta_1 &= \beta/(1 - \alpha_1)\\
  u_2(\kappa_2, \theta_1, \theta_2) &= y^2 - \kappa_2 - \theta_1 2 yT + \theta_2 T & \theta_2 &= \beta^2/(1 - \alpha_1) = \beta \theta_1
\end{align*}

Using the notation of the preceding section, the $h$ block of moment equalities used for preliminary estimation of $\tau = (\kappa_1, \kappa_2)$ is 
\[
  h(W_i, \theta, \kappa) = \left[
  \begin{array}{l}
    u_{1,i}(\kappa_1,\theta_1)\\
    u_{2,i}(\kappa_2, \theta_1, \theta_2)\\
  \end{array}
\right] = 
\left[
\begin{array}{l}
  y_i - \kappa_1 - \theta_1 T_i\\
  y^2_i - \kappa_2 - \theta_1 2 y_iT_i + \theta_2 T_i
\end{array}
\right]
\]
while the $m$ block of moment inequalities and equalities used for inference are
\[
  m(W_i, \theta, \kappa) = \left[
  \begin{array}{l}
    (1 - \alpha_1) - T_i(1 - z_i)/(1 - q)\\
    (1 - \alpha_1) - T_iz_i/q\\
    u_{1,i}(\kappa_1, \theta_1) z_i \\
    u_{2,i}(\kappa_2, \theta_1, \theta_2) z_i
  \end{array}
\right]
   = \left[
  \begin{array}{l}
    (1 - \alpha_1) - T_i(1 - z_i)/(1 - q)\\
    (1 - \alpha_1) - T_iz_i/q\\
    (y_i - \kappa_1 - \theta_1 T_i) z_i\\
    (y_i^2 - \kappa_2 - \theta_1 2 y_i T_i + \theta_2 T_i) z_i 
  \end{array}
\right]
\]
Under the null $(\beta, \alpha_1) = (\beta^0, \alpha_1^0)$, or equivalently $(\theta_1, \theta_2) = (\theta_1^0, \theta_2^0)$, we can estimate $\kappa_1$ and $\kappa_2$:
\begin{align*}
  \widehat{\kappa}_1(\theta_0) &= \frac{1}{n} \sum_{i=1}^n y_i - \theta_1^0 T_i\\
  \widehat{\kappa}_2(\theta_0) &= \frac{1}{n}\sum_{i=1}^n y^2_i - \theta_1^0 2y_i T_i + \theta_2^0 T_i 
\end{align*}
Note that this estimator is just identified, so the simplification $B = -MH^{-1}$ obtains.
To calculate this quantity, we first need the expected derivative matrices: 
\[
  H = \mathbb{E}\left[ \frac{\partial h}{\partial \kappa'} \right] = 
  \left[
  \begin{array}{rr}
    -1 & 0 \\
    0 & -1
  \end{array}
\right] = -\mathbf{I}_2
\]
and 
\[
  M = \mathbb{E}\left[ \frac{\partial m}{\partial \kappa'} \right] = 
  \left[
  \begin{array}{rr}
    0 & 0 \\ 
    0 & 0 \\ 
    -q & 0 \\
    0 & -q 
  \end{array}
\right] = 
-q \left[
\begin{array}{c}
  \mathbf{0}_{2} \\ \mathbf{I}_2
\end{array}
\right]
\]
which imply 
\[
  B = -MH^{-1} = -q \left[
  \begin{array}{cc}
    \mathbf{0}_2\\ \mathbf{I}_2
  \end{array}
\right] \mathbf{I}_2^{-1} = -q \left[
\begin{array}{c}
  \mathbf{0}_{2} \\ \mathbf{I}_2
\end{array}
\right]
\]
Notice that in our example $B$ does not depend on parameters so it does not have to be re-computed for different null hypotheses.
And since $A = \left[
\begin{array}{cc}
  I & B
\end{array}
\right]$,
the same is true of $A$.
In contrast, $\widehat{\mathcal{V}}_n(\theta_0)$ \emph{does} depend on the null:
\[
  \widehat{\mathcal{V}}_n(\theta_0) = \frac{1}{n}\sum_{i=1}^n 
  \left[
  \begin{array}{c}
    m(W_i, \theta_0, \widehat{\kappa}(\theta_0)) - \bar{m}_n(\theta_0, \widehat{\kappa}(\theta_0))\\
    h(W_i, \theta_0, \widehat{\kappa}(\theta_0)) - \bar{h}_n(\theta_0, \widehat{\kappa}(\theta_0))
  \end{array}
\right]
  \left[
  \begin{array}{c}
    m(W_i, \theta_0, \widehat{\kappa}) - \bar{m}_n(\theta_0, \widehat{\kappa})\\
    h(W_i, \theta_0, \widehat{\kappa}) - \bar{h}_n(\theta_0, \widehat{\kappa})
  \end{array}
\right]'
\]
Finally, we set $\widehat{\Sigma}_n\left( \theta_0, \widehat{\kappa}(\theta_0) \right) = A \widehat{\mathcal{V}}_n A'$ and use this in the Andrews \& Soares (2010) GMS test to correctly account for the fact that $\kappa_1$ and $\kappa_2$ have been estimated in a preliminary GMM step.

%\section{May 21st, 2017 -- Adding Second Moment Bounds}
%Continuing from the preceding section, we now add the second moment bounds for $\alpha_1$ from above:
%\begin{align*}
%  p_k (1 - p_k)(1 - p_k - \alpha_1) \left\{ (1  - \alpha_1)(1 - p_k)\mathbb{E}\left[ y^2|T=0,z_k \right] - \alpha_1 p_k \mathbb{E}[y^2|T=1,z_k \right\}  \\
%    > \left\{ (1 - \alpha_1)(1 - p_k)\mathbb{E}[y|T=0,z_k] - \alpha_1 p_k \mathbb{E}[y|T=1,z_k] \right\}^2
%\end{align*}
%where $k = 0, 1$.
%We continue, for the moment, to assume that $\alpha_0 = 0$.
%Recall that
%\begin{align*}
%  \mathbb{E}\left[ y^r |T=0, z=k\right] &= \mathbb{E}\left[ y^r(1 - T) \mathbf{1}(z=k) \right]/\left[ (1 - p_k) \mathbb{P}(z=k)\right]\\
%  \mathbb{E}\left[ y^r |T=1, z=k\right] &= \mathbb{E}\left[ y^r T \mathbf{1}(z=k) \right]/\left[p_k \mathbb{P}(z=k)\right]
%\end{align*}
%from which we obtain:
%\begin{align*}
%  (1 - p_k)\mathbb{E}[y|T=0, z=0] &= \mathbb{E}[y(1 - T)(1 - z)]/(1 -q)\\
%  p_k \mathbb{E}[y|T=1, z=0] &= \mathbb{E}[yT(1 - z)]/(1 -q)\\
%  (1 - p_k)\mathbb{E}[y^2|T=0, z=0] &= \mathbb{E}[y^2(1 - T)(1 - z)]/(1 -q)\\
%  p_k\mathbb{E}[y^2|T=1, z=0] &= \mathbb{E}[y^2T(1 - z)]/(1 - q)
%\end{align*}
%Thus, the bound for $k = 0$ is
%\begin{align*}
% (1-q) p_0 (1 - p_0)(1 - p_0 - \alpha_1) \left\{ (1  - \alpha_1)\mathbb{E}\left[ y^2(1-T)(1-z) \right] - \alpha_1 \mathbb{E}[y^2 T(1-z)] \right\}  \\
%    > \left\{ (1 - \alpha_1)\mathbb{E}[y(1-T)(1-z)] - \alpha_1 \mathbb{E}[yT(1-z)] \right\}^2 
%\end{align*}
%Proceeding similarly for $k = 1$ we obtain
%\begin{align*}
%  (1 - p_k)\mathbb{E}[y|T=0, z=1] &= \mathbb{E}[y(1 - T)z]/q\\
%  p_k \mathbb{E}[y|T=1, z=1] &= \mathbb{E}[yTz]/q\\
%  (1 - p_k)\mathbb{E}[y^2|T=0, z=1] &= \mathbb{E}[y^2(1 - T)z]/q\\
%  p_k\mathbb{E}[y^2|T=1, z=1] &= \mathbb{E}[y^2Tz]/q
%\end{align*}
%and hence
%\begin{align*}
% q p_1 (1 - p_1)(1 - p_1 - \alpha_1) \left\{ (1  - \alpha_1)\mathbb{E}\left[ y^2(1-T)z \right] - \alpha_1 \mathbb{E}[y^2 Tz] \right\}  \\
%    > \left\{ (1 - \alpha_1)\mathbb{E}[y(1-T)z] - \alpha_1 \mathbb{E}[yTz] \right\}^2 
%\end{align*}
%To convert these bounds into moment functions of the form required by Andrews \& Soares, we introduce additional parameters for we will carry out a preliminary moment-based estimation step: 
%\begin{align*}
%  \mathbb{E}[p_0 - T(1 - z)/(1 - q)] &= 0\\
%  \mathbb{E}[p_1 - Tz/q] &= 0\\
%  \mathbb{E}[\mu_{00} - y(1 - T)(1 - z)] &= 0\\
%  \mathbb{E}[\mu_{10} - yT(1 - z)] &= 0\\
%  \mathbb{E}[\mu_{01} - y(1 - T)z] &= 0\\
%  \mathbb{E}[\mu_{11} - yTz] &= 0
%\end{align*}
%We will continue, however, to treat $q=\mathbb{P}(z=1)$ as fixed in repeated sampling.
%In terms of the newly introduced parameters $\mu_{00}, \mu_{10}, \mu_{01}$, and $\mu_{11}$, the second moment bounds become
%\begin{align*}
%  \mathbb{E}\left[(1 - p_0 - \alpha_1)y^2(1 - z)(1 - T - \alpha_1)   - \frac{\left\{ (1 - \alpha_1)\mu_{00} - \alpha_1 \mu_{10} \right\}^2}{p_0(1-p_0)(1-q)} \right] &\geq 0 \\
%   \mathbb{E}\left[(1 - p_1 - \alpha_1)y^2 z(1 - T - \alpha_1)   - \frac{\left\{ (1 - \alpha_1)\mu_{01} - \alpha_1 \mu_{11} \right\}^2}{p_1(1-p_1)q} \right]&\geq 0 
%\end{align*}
%Notice that the parameters $p_0, p_1, \mu_{00}, \mu_{01}, \mu_{10}$, and $\mu_{11}$ enter these expressions in a fairly complicated way.
%This means that the adjustment to the asymptotic variance matrix will be more involved.
%Fortunately, each of these parameters should be fairly well estimated in practice.
%Notice further that we have taken care to write these inequalities so that the \emph{second term} within the expectation is always \emph{non-positive} and there is no danger of division by zero provided that the instrument not degenerate.
%\todo[inline]{Still need to check these inequalities against the version from the other paper. They should always provide additional information beyond that in the ``weak'' bounds, however. The reason for this is hinted at in the appendix to sick-instruments, but I'll clarify it here. Consider the equality version of the condition: it's a quadratic so there are two roots: $r_1 < r_2$. (How can we be sure the roots aren't equal?) From the weak bounds, we get the sign of the leading term of the quadratic, telling us that it opens upwards. This means that the inequality is satisfied for $\alpha_1 \leq r_2$ and for $\alpha_1 \geq r_1$.
%  But we also know that there's a point in between $r_1$ and $r_2$ that is exactly the boundary for one of the weak bounds, meaning that $r_2$ must \emph{violate} one of the weak bounds. Hence the second moment bound is $\alpha_1 \leq r_1$ and this must be an improvement over the weak bound. Note that we still need the weak bound to rule out one branch of the quadratic: in practice we have to include to make sure that we don't end up in the wrong branch of the quadratic. It would be helpful to understand when the second moment bound becomes degenerate and reduces to the first moment bound. Presumably this has something to do with $\beta$ being small, but I think we need to make sure we understand this fully before preceding. For example, it might be the case that when $\beta$ is zero the second moment bounds are equal to the weak bounds, but the second moment bounds are necessarily higher in variance, so we should still want to include the first moment bounds I think.}


\section{May 22nd, 2017 -- Unsolved Mystery}
A while back we noticed that the CDF bounds appeared to identify the true $\alpha_0$ and $\alpha_1$ in our normal simulation.
At one point I tried to figure out if this was really true and if so why.
I think I wrote out a few notes on paper but didn't make much progress.
It would be good to eventually figure this out.
There are a few calculations for our simulation design above on pages 44--45 of these notes.
Maybe these could be used to figure it out.
I think the CDF bounds may be worth coming back to since one can impose them without making our stronger assumptions about the IV.
This means they could be used with Mahajan or Lewbel, for example.
One needn't use all of them in practice: something like quintile bounds could be useful.

\section{June 1--3, 2017 -- Second Moment Bounds}
Camilo triple-checked the conditional variance inequalities from our sick-instrument paper: they're definitely correct.
Moreover, in spite of the fact that we defined $u = \varepsilon + c$ in that paper, they continue to hold exactly as written since $c$ is a constant.
To see why this is the case, let $u = c + \varepsilon$.
Then
\begin{align*}
  \mbox{Var}\left( u|T,z \right) &= \mathbb{E}(u^2|T,z) - \left[ E(u|T,z) \right]^2\\
&= \mathbb{E}(\varepsilon^2|T,z) + 2c\mathbb{E}(\varepsilon|T,z) + c^2 - \left[ c + \mathbb{E}(\varepsilon|T,z)  \right]^2\\
&= \mbox{Var}(\varepsilon|T,z)
\end{align*}
Camilo then re-wrote the inequalities as follows:
\begin{align*}
  \mathbb{E}\left[ (1- p_k - \alpha_1) y^2 \mathbf{1}\left\{ z=k \right\}(1 - T - \alpha_1) - \mathbb{P}(z=k)\left\{ \alpha_1 p_k \mu_{1k} - (1 - \alpha_1)(1 - p_k)\mu_{0k} \right\}^2 \right] &>0 \\
  \mathbb{E}\left[ (p_k - \alpha_0) y^2 \mathbf{1}\left\{ z=k \right\}(T - \alpha_0) - \mathbb{P}(z=k)\left\{ (1 - \alpha_0) p_k \mu_{1k} - \alpha_0(1 - p_k)\mu_{0k} \right\}^2 \right] &>0 
\end{align*}
where $p_k = \mathbb{P}(T=1|z=k)$ and $\mu_{tk} = \mathbb{E}(y|T=t,z=k)$.
I derived one of these expressions as well (the $\alpha_1$ inequality) and my derivation matches.
To incorporate these inequalities into our inference procedure, we will need auxiliary moment conditions to estimate $\mu_{tk}$ and $p_k$.
We will continue, however, to treat $\mathbb{P}(z=k)$ as fixed in repeated sampling.

We can re-write the preceding expressions in a slightly simpler form that also makes the derivatives we'll need to take to compute the covariance matrix adjustment for the GMS test much simpler. 
Recall that 
\begin{align*}
  \mu_{0k} &= \mathbb{E}\left( y|T=0, z_k \right) = \frac{\mathbb{E}\left[ y(1 - T) \mathbf{1}(z=k) \right]}{(1 - p_k) \mathbb{P}(z=k)}\\ \\
  \mu_{1k} &= \mathbb{E}\left( y|T=1, z_k \right) = \frac{\mathbb{E}\left[ yT \mathbf{1}(z=k) \right]}{p_k \mathbb{P}(z=k)}
\end{align*}
Rearranging,
\begin{align*}
  \mathbb{P}(z=k)(1-p_k)\mu_{0k} &=  \mathbb{E}\left[ y(1 - T) \mathbf{1}(z=k) \right]\\
  \mathbb{P}(z=k)p_k\mu_{1k} &= \mathbb{E}\left[ yT \mathbf{1}(z=k) \right]
\end{align*}
Thus, defining
\begin{align*}
  m_{0k} &= \mathbb{P}(z=k)(1-p_k)\mu_{0k} =  \mathbb{E}\left[ y(1 - T) \mathbf{1}(z=k) \right]\\
  m_{1k} &= \mathbb{P}(z=k)p_k\mu_{1k} = \mathbb{E}\left[ yT \mathbf{1}(z=k) \right]
\end{align*}
by multiplying both sides by $\mathbb{P}(z=k)$ we can write the second moment inequalities as
\begin{align*}
  \mathbb{E}\left[\mathbb{P}(z=k) (1- p_k - \alpha_1) y^2 \mathbf{1}\left\{ z=k \right\}(1 - T - \alpha_1) - \left\{ \alpha_1 m_{1k} - (1 - \alpha_1)m_{0k} \right\}^2 \right] &>0 \\
  \mathbb{E}\left[ \mathbb{P}(z=k)(p_k - \alpha_0) y^2 \mathbf{1}\left\{ z=k \right\}(T - \alpha_0) - \left\{ (1 - \alpha_0) m_{1k} - \alpha_0 m_{0k} \right\}^2 \right] &>0 
\end{align*}
Thus, the six quantities for which we require preliminary estimators are
\begin{align*}
  m_{0k} &= \mathbb{E}\left[ y(1 - T) \mathbf{1}(z=k) \right]\\
  m_{1k} &= \mathbb{E}\left[ yT \mathbf{1}(z=k) \right]\\
  p_k &= \mathbb{E}[T\mathbf{1}(z=k)] / \mathbb{P}(z=k)
\end{align*}
for $k = 0, 1$.
Again, we will treat $\mathbb{P}(z=k)$ as fixed in repeated sampling.
Since $m_{tk}$ and $p_k$ are just sample means, they should be very precisely estimated.

\subsection*{Are the 2nd Moment Bounds Tighter?} 
The second moment bounds given above are equivalent to those from the sick-instruments paper, namely
\begin{align}
  \label{ineq:a0}
  (p_k - \alpha_0) \left[ (1 - \alpha_0)p_k \sigma^2_{1k} - \alpha_0 (1 - p_k)\sigma_{0k}^2 \right] &\geq \alpha_0 (1 - \alpha_0)p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2\\
  \label{ineq:a1}
  (1 - p_k - \alpha_1) \left[ (1 - \alpha_1)(1 - p_k) \sigma^2_{0k} - \alpha_1 p_k\sigma_{1k}^2 \right] &\geq \alpha_1 (1 - \alpha_1)p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2
\end{align}
where I have re-written $\bar{y}_{tk}$ as $\mu_{tk}$ to match the notation we use in our derivations above and allowed for the possibility that the bounds are not strict.
(This is a degenerate situation in which an unobservable variance is zero, but I just want to consider all possibilities!)
We will now argue that these bounds must generically be tighter than the ``weak'' bounds that use only $p_k$.
This holds even if $\beta = 0$.
We will proceed by analyzing the quadratic equations along which the preceding expressions hold with equality.
Throughout the following argument we assume that $p_k \neq 0,1$ and that \emph{at least one} of $\sigma^2_{0k}, \sigma^2_{1k}$ is strictly positive. 

\paragraph{The Bounds for $\alpha_0$}
Rearranging, we can write Inequality \ref{ineq:a0} as $\varphi_k(\alpha_0) \geq 0$ where
\begin{align*}
  \varphi_k(\alpha_0) &= A_k \alpha_0^2 + B^0_k \alpha_0 + C^0_k \\
  A_k &= p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2 + (1 - p_k) \sigma_{0k}^2 + p_k \sigma_{1k}^2\\
  B_k^0 &= - \left[ \sigma_{1k}^2 p_k(1 + p_k) + p_k (1 - p_k)\sigma_{0k}^2 + p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2 \right]\\
  C_k^0 &= p_k^2 \sigma_{1k}^2. 
\end{align*}
Since we assume that $p_k \neq 0,1$ and that at least one of $\sigma_{0k}^2,\sigma^2_{1k}$ is positive $A_k > 0$.
Thus the quadratic function $\varphi_k(\alpha_0)$ opens upwards.
Now, if $\alpha_0 = 0$ then the RHS of Inequality \ref{ineq:a0} becomes zero while the LHS becomes $p_k^2 \sigma_{1k}^2$.
Thus, Inequality \ref{ineq:a0} is always satisfied when $\alpha_0 = 0$.
Similarly $\alpha_0 = 1$, the RHS is again zero while the LHS becomes $(1 - p_k)^2\sigma_{0k}^2$.
Thus, inequality \ref{ineq:a0} is always satisfied when $\alpha_0 = 1$.
Since $\alpha_0$ is a probability it follows that, so long as $\varphi_k$ has two distinct roots $r_1 < r_2$, Inequality \ref{ineq:a0} is satisfied if and only if $\alpha_0 \in [0, r_1]$ or $\alpha_0 \in [r_2, 1]$.
We now show that $\varphi$ generically has two distinct roots, that the bound involving $r_2$ is extraneous, that that $r_1$ is generically strictly smaller than $p_k$ so that the second moment bound is tighter than the weak bound $\alpha_0 \leq p_k$ unless $\mu_{1k} = \mu_{0k}$ in which case the weak and second moment bounds coincide.

If $\alpha_0 = p_k$, the LHS of \ref{ineq:a0} becomes zero.
There are two cases.
Suppose first that $\mu_{1k} \neq \mu_{0k}$.
In this case, the RHS of the inequality becomes becomes $p_k^2(1-p_k)^2(\mu_{1k} - \mu_{0k})^2 >0$ so the inequality is violated.
Thus, $\mu_{1k} \neq \mu_{0k}$ implies that $\varphi_k$ has two distinct roots and that $p_k$ is strictly \emph{between} them.
Since the weak bound gives us $\alpha_0 < p_k$, the bound arising from the larger of the two roots is extraneous.
Now suppose that $\mu_{1k}=\mu_{0k}$.
In this case the RHS and LHS of the inequality are both zero so $\alpha_0 = p_k$ is a root of $\varphi_k$.
When $\mu_{1k}=\mu_{0k}$, the coefficients of $\varphi_k$ become
\begin{align*}
  a &= (1 - p_k) \sigma_{0k}^2 + p_k \sigma_{1k}^2 = \sigma_{0k}^2 + p_k(\sigma_{1k}^2 - \sigma_{0k}^2)\\
  b &= - \left[ \sigma_{1k}^2 p_k(1 + p_k) + p_k (1 - p_k)\sigma_{0k}^2 \right] = -\left[ p_k\sigma_{1k}^2 + p_k \sigma_{0k}^2 + p_k^2(\sigma_{1k}^2 - \sigma_{0k}^2)  \right] = -p_k\left(\sigma_{1k}^2 + a  \right) \\
  c &= p_k^2 \sigma_{1k}^2. 
\end{align*}
Thus, we find that
\begin{align*}
  b^2 - 4ac &= p_k^2 (\sigma_{1k}^2 + a)^2 - 4a p_k^2 \sigma_{1k}^2 = p_k^2 \left[ \sigma_{1k}^4 + 2a \sigma_{1k}^2 + a^2 - 4a \sigma_{1k}^2 \right]\\
  &= p_k^2\left( \sigma_{1k}^2 - a \right)^2
\end{align*}
so the roots of the quadratic are
\[
  \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{p_k(\sigma_{1k}^2 + a) \pm p_k(\sigma_{1k}^2 - a)}{2a} = \left\{\frac{p_k \sigma_{1k}^2}{a}, \, p_k  \right\}
\]
Substituting the definition of $a$,
\[
  \frac{p_k\sigma_{1k}^2}{a} = \frac{p_k \sigma_{1k}^2}{p_k \sigma_{1k}^2 + (1 - p_k) \sigma_{0k}^2}
\]
Rearranging, $p_k$ is the \emph{smaller root} when $\sigma_{0k}^2<\sigma_{1k}^2$ and the \emph{larger root} when the inequality is reversed.
When $\sigma_{0k}^2 = \sigma_{1k}^2$, the two roots are equal.

\paragraph{Bounds for $\alpha_1$}
Proceeding analogously, we can write Inequality \ref{ineq:a1} as $\psi_k(\alpha_1)\geq 0$ where
\begin{align*}
  \psi_k(\alpha_1) &= A_k \alpha_1^2 + B_k^1 \alpha_1 + C_k^1\\ 
  A_k &= p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2 + (1 - p_k) \sigma_{0k}^2 + p_k \sigma_{1k}^2\\
  B^1_{k} &= - \left[p_k (1 - p_k) \sigma_{1k}^2 + (1 - p_k)(2 - p_k)\sigma_{0k}^2 + p_k(1 - p_k)(\mu_{1k} - \mu_{0k})^2 \right]\\
  C^1_{k} &= (1 - p_k)^2 \sigma_{0k}^2.
\end{align*}
Note that the coefficient on the quadratic term is \emph{the same} for $\psi_k$ and $\varphi_k$. 
When $\mu_{1k} = \mu_{0k}$, the coefficients of $\psi_k$ become
\begin{align*}
  a &= (1 - p_k)\sigma_{0k}^2 + p_k \sigma_{1k}^2 = \sigma_{0k}^2 + p_k(\sigma_{1k}^2 - \sigma_{0k}^2)\\
  b &= -\left[ p_k(1 - p_k)\sigma_{1k}^2 + (1 - p_k)(2 - p_k)\sigma_{0k}^2 \right] = -(1 - p_k)\left[ p_k(\sigma_{1k}^2 - \sigma_{0k}^2) + 2 \sigma_{0k}^2 \right] = -(1 - p_k)(\sigma_{0k}^2 + a)\\
  c &= (1 - p_k)^2 \sigma_{0k}^2
\end{align*}

\begin{align*}
  b^2 - 4ac &= (1 - p_k)^2(\sigma_{0k}^2 + a)^2 - 4 a (1 - p_k)^2 \sigma_{0k}^2 = (1 - p_k^2)\left[ (\sigma_{0k}^4 + 2a \sigma_{0k}^2 + a^2) - 4 a \sigma_{0k}^2 \right] \\
  &= (1 - p_k)^2 \left( \sigma_{0k}^4 - 2 a \sigma_{0k}^2 + a^2 \right) = (1 - p_k)^2 (\sigma_{0k}^2 - a)^2
\end{align*}

\[
  \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{(1 - p_k)(\sigma_{0k}^2 + a) \pm (1 - p_k)(\sigma_{0k}^2 - a)}{2a} = \left\{ \frac{(1 - p_k)\sigma_{0k}^2}{a},\; (1 - p_k) \right\}
\]

\[
  \frac{(1 - p_k)\sigma_{0k}^2}{a} = \frac{(1 - p_k)\sigma_{0k}^2}{(1 - p_k)\sigma_{0k}^2 + p_k \sigma_{1k}^2}
\]
Rearranging, $1 - p_k$ is the \emph{smaller root} when $\sigma_{1k}^2<\sigma_{0k}^2$ and the \emph{larger root} when the inequality is reversed.
When $\sigma_{0k}^2 = \sigma_{1k}^2$, the two roots are equal.

\paragraph{What can we say about the case where $\mu_{1k} = \mu_{0k}$?}
Even if $\mu_{1k} = \mu_{0k}$, the second moment bounds are generically strictly better than the weak bounds.
Suppose first that $\sigma_{0k}^2 \neq \sigma_{1k}^2$.
If $\sigma_{0k}^2 < \sigma_{1k}^2$ then $p_k$ is the smaller root of $\varphi_k$ but $(1 - p_k)$ is the \emph{larger} root of $\psi_k$ so the second moment bound for $\alpha_1$ is strictly better than the weak bound $\alpha_1 \leq 1 - p_k$.
If instead $\sigma_{1k}^2 < \sigma_{0k}^2$, then $(1 - p_k)$ is the smaller root of $\psi_k$ but $p_k$ is the \emph{larger} root of $\varphi_k$ so the second moment bound for $\alpha_0$ is strictly better than the weak bound $\alpha_0 \leq p_k$.
Only in the non-generic case where $\mu_{0k} = \mu_{1k}$ \emph{and} $\sigma^2_{0k} = \sigma_{1k}^2$ for all $k$ do the weak bounds and second moment bounds coincide.
This turns out to imply that the second moment bounds remain informative \emph{even if the treatment effect is zero}.
If $\beta = 0$ then $y = c + \varepsilon$ so $\mu_{tk} = c + \mathbb{E}(\varepsilon|T=t,z=k)$ and hence
\[
  0 = \mu_{1k} - \mu_{0k} = \mathbb{E}(\varepsilon|T=1,z_k) - \mathbb{E}(\varepsilon|T=0,z_k)
\]
Thus, if $\mu_{0k} = \mu_{1k}$ then we must have $\mathbb{E}(\varepsilon|T=1,z_k) = \mathbb{E}(\varepsilon|T=0,z_k)$ for all $k$.
But by iterated expectations and the assumption of non-differential measurement error we have
\begin{align*}
  \mathbb{E}(\varepsilon|T=1, z_k) &= \mathbb{E}(\varepsilon|T^*=1,z_k)\mathbb{P}(T^*=1|T=1,z_k) + \mathbb{E}(\varepsilon|T^*=0,z_k)\mathbb{P}(T^*=0|T=1,z_k)\\
  \mathbb{E}(\varepsilon|T=0, z_k) &= \mathbb{E}(\varepsilon|T^*=1,z_k)\mathbb{P}(T^*=1|T=0,z_k) + \mathbb{E}(\varepsilon|T^*=0,z_k)\mathbb{P}(T^*=0|T=0,z_k)
\end{align*}
Since $\mathbb{E}(\varepsilon|T=1,z_k) = \mathbb{E}(\varepsilon|T=0,z_k)$, these equations are a linear system of the form
\begin{align*}
  c &= p x + (1 - p)y\\
  c &= q x + (1 - q)y
\end{align*}
where 
\begin{align*}
  p &= \mathbb{P}(T^*=1|T=1,z_k) = (1 - \alpha_1) \frac{p_k^*}{p_k}\\
  q &= \mathbb{P}(T^*=1|T=0,z_k) = \alpha_1 \left(\frac{p_k^*}{1 - p_k}\right)\\
  x &= \mathbb{E}(\varepsilon|T^*=1,z_k)\\
  y &= \mathbb{E}(\varepsilon|T^*=0,z_k)
\end{align*}
Thus, unless $p = q$, we must have $\mathbb{E}(\varepsilon|T^*=1,z_k) = \mathbb{E}(\varepsilon|T^*=0,z_k)$ for all $k$.
But since $z$ is a valid instrument, $\mathbb{E}(\varepsilon|z_k) = 0$ for all $k$ and thus, by iterated expectations
\begin{align*}
  0 &= \mathbb{E}(\varepsilon|z_k) = \mathbb{E}_{T^*|z_k}\left[ \mathbb{E}(\varepsilon|T^*,z_k) \right]\\
  &= p^*_k \mathbb{E}(\varepsilon|T^*=1,z_k) + (1 - p_k^*) \mathbb{E}(\varepsilon|T^*=0,z_k)\\
  &= \left[ p_k^* + (1 - p_k^*) \right] \mathbb{E}(\varepsilon|T^*=1,z_k)\\
  &= \mathbb{E}(\varepsilon|T^*=1,z_k)\\
  &= \mathbb{E}(\varepsilon|T^*=0,z_k)
\end{align*}
for all $k$. 
Thus, even if $\beta = 0$ it will \emph{still} not in general be true that $\mu_{1k} = \mu_{0k}$.
For this to be the case we require the additional condition that $\mathbb{E}(\varepsilon|T^*_t,z = k) = 0$ for all $t,k$.
In other words, we require $z$ and $T^*$ to be \emph{jointly} first moment independent of $\varepsilon$.
This implies $\mathbb{E}(\varepsilon|T^*)=0$, i.e.\ that $T^*$ is \emph{exogenous}.

Now suppose that $\beta = 0$ \emph{and} $z,T^*$ are jointly mean independent of $\varepsilon$.
The second moment bounds are \emph{still} strictly better than the weak bounds provided that $\sigma_{0k}^2 \neq \sigma_{1k}^2$.
From a proof in the appendix of the sick instruments paper,
\begin{eqnarray*}
  \sigma^2_{1k}&=&  \frac{(1 - \alpha_1) p_k^* }{p_k} s^{*2}_{1k} + \frac{\alpha_0(1 - p_k^*)}{p_k} s^{*2}_{0k} + \frac{\alpha_0 ( 1 - \alpha_1) (1 - p_k)^2 \left( \mu_{1k} - \mu_{0k} \right)^2}{(p_k - \alpha_0)(1 - p_k - \alpha_1)}\\
  \sigma^2_{0k} &=&  \frac{\alpha_1 p_k^* }{1 - p_k} s^{*2}_{1k} + \frac{(1 - \alpha_0)(1 - p_k^*)}{1 - p_k} s^{*2}_{0k} + \frac{\alpha_1 ( 1 - \alpha_0) p_k^2 \left( \mu_{1k} - \mu_{0k} \right)^2}{(p_k - \alpha_0)(1 - p_k - \alpha_1)} 
\end{eqnarray*}
where $s^{*2}_{tk} = \mbox{Var}(\varepsilon|T^*=t, z=k)$.\footnote{See the explanation above for why $\mbox{Var}(u|T^*,z) = \mbox{Var}(\varepsilon|T^*,z)$.}
If $\mu_{1k} = \mu_{0k}$ this reduces to 
\begin{eqnarray*}
  \sigma^2_{1k}&=&  \frac{(1 - \alpha_1) p_k^* }{p_k} s^{*2}_{1k} + \frac{\alpha_0(1 - p_k^*)}{p_k} s^{*2}_{0k} \\
  \sigma^2_{0k} &=&  \frac{\alpha_1 p_k^* }{1 - p_k} s^{*2}_{1k} + \frac{(1 - \alpha_0)(1 - p_k^*)}{1 - p_k} s^{*2}_{0k}
\end{eqnarray*}
In other words,
\begin{eqnarray*}
  \sigma^2_{1k}&=&  \mathbb{P}(T^*=1|T=1,z_k) s^{*2}_{1k} + \mathbb{P}(T^*=0|T=1,z_k) s^{*2}_{0k} \\
  \sigma^2_{0k} &=&  \mathbb{P}(T^*=1|T=0,z_k) s^{*2}_{1k} + \mathbb{P}(T^*=0|T=0,z_k) s^{*2}_{0k}
\end{eqnarray*}
So if $\sigma_{0k}^2 = \sigma_{1k}^2$, we have a linear system of the form
\begin{align*}
  c &= p x + (1 - p)y\\
  c &= q x + (1 - q)y
\end{align*}
as above, where $p$ and $q$ are defined as before but $x = s^{*2}_{1k}$ and $y = s^{*2}_{0k}$.
Again, unless $p = q$, we must have $x = y$ which in this case means $s^{*2}_{0k} = s^{*2}_{1k}$ for all $k$.
Now, since 
\[
  s^{*2}_{tk} = \mathbb{E}(\varepsilon^2|T^*=t,z_k) - \left[ \mathbb{E}(\varepsilon|T^*=t,z_k) \right]^2
\]
under the assumption that $\mathbb{E}(\varepsilon|T^*=0,z_k) = \mathbb{E}(\varepsilon|T^*=1,z_k)$, we see that
\[
  s_{1k}^{*2} - s_{0k}^{*2} = \mathbb{E}(\varepsilon^2|T^*=1,z_k) - \mathbb{E}(\varepsilon^2|T^*=0,z_k)
\]
so that $s_{1k}^{*2} = s_{0k}^{*2}$ if and only if 
\[
   \mathbb{E}(\varepsilon^2|T^*=1,z_k) = \mathbb{E}(\varepsilon^2|T^*=0,z_k)
\]
Suppose this is the case.
Recall that we have assumed $\mathbb{E}\left( \varepsilon^2|z_k \right) = \mathbb{E}(\varepsilon^2)$ for all $k$.
Thus, by iterated expectations,
\begin{align*}
  \mathbb{E}(\varepsilon^2) &= \mathbb{E}\left( \varepsilon^2|z_k \right) = \mathbb{E}_{T^*|z_k}\left[ \mathbb{E}\left( \varepsilon|T^*,z_k \right) \right]\\
  &= p_k^* \mathbb{E}\left( \varepsilon^2|T^*=1, z_k \right) + (1 - p_k^*) \mathbb{E}\left( \varepsilon^2|T^*=0, z_k \right)\\
  &= \left[p_k^* + (1 - p_k^*)\right] \mathbb{E}\left( \varepsilon^2|T^*=1, z_k \right)\\
  &= \mathbb{E}\left( \varepsilon^2|T^*=1, z_k \right)\\
  &= \mathbb{E}\left( \varepsilon^2|T^*=0, z_k \right)
\end{align*}
and thus $\mathbb{E}(\varepsilon^2|T^*=t,z=k)=\mathbb{E}(\varepsilon^2)$ for all $t,k$.
This implies $\mathbb{E}(\varepsilon^2|T^*)=\mathbb{E}(\varepsilon^2)$.

To summarize, we have shown the following:
\begin{enumerate}
  \item Unless $\mu_{1k} = \mu_{0k}$ \emph{and} $\sigma_{1k}^2 = \sigma^2_{0k}$ for all $k$, then the second moment bounds are strictly tighter for at least one of $\alpha_0$ and $\alpha_1$.
  \item If $\beta = 0$, the second moment bounds are still generically tighter.
    In this case, the only way to obtain $\mu_{0k} = \mu_{1k}$ and $\sigma_{0k}^2 = \sigma_{1k}^2$ for all $k$ is if $\mathbb{E}(\varepsilon|T^*,z)=0$ and $\mathbb{E}(\varepsilon^2|T^*,z) = \mathbb{E}(\varepsilon^2)$.
    These conditions in turn imply $\mathbb{E}(\varepsilon|T^*)=0$ and $\mathbb{E}(\varepsilon^2|T^*) = \mathbb{E}(\varepsilon^2)$.
    So even if $\beta = 0$, the second moment bounds are tighter unless $T^*$ is exogenous \emph{and} $\varepsilon$ is homoskedastic with respect to $T^*$.
\end{enumerate}

\section{June 4, 2017: Inference Procedure (no covariates)}
We now describe the full inference procedure for our model, including preliminary estimation of strongly identified parameters and using both the weak and second moment bounds.
We place no restriction on $\alpha_0$ and $\alpha_1$, but assume for the moment that there are no covariates.


\paragraph{Overview and Notation}
We test the null hypothesis $\boldsymbol{\theta} = \boldsymbol{\theta}_0$ for a vector $\boldsymbol{\theta}$ of weakly identified parameters based on a set of equality moment conditions $m^E$ and inequality moment conditions $m^I$
\[
  \mathbb{E}\left[ m^E\left(W_i, \boldsymbol{\theta}_0, \widehat{\boldsymbol{\gamma}}_0\right) \right] = \mathbf{0}, \quad \mathbb{E}\left[ m^I(W_i, \boldsymbol{\theta}_0, \widehat{\boldsymbol{\gamma}}_0) \right] \geq \mathbf{0}
\]
where $\widehat{\boldsymbol{\gamma}}_0 = \widehat{\boldsymbol{\gamma}}(\theta_0)$ is preliminary estimator of the strongly identified parameters $\boldsymbol{\gamma}$ constructed from the moment equalities $\mathbb{E}\left[ h(W_i, \boldsymbol{\theta}_0) - \gamma \right]=0$.

\paragraph{Moment Equality Conditions}
The moment functions $m^E$ are defined as 
\begin{align*}
  m^E(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=  \mathbf{u}_i(\boldsymbol{\theta}, \boldsymbol{\kappa}) z_i\\
  \mathbf{u}_i(\boldsymbol{\theta}, \boldsymbol{\kappa}) &= 
  \left[
  \begin{array}{l}
 y_i - \kappa_1 - \theta_1 T_i\\
 y^2_i - \kappa_2 - \theta_1 2 y_i T_i + \theta_2 T_i\\
 y^3_i - \kappa_3 - \theta_1 3y_i^2 T + \theta_2 3y_iT_i - \theta_3 T_i
  \end{array}
\right]
\end{align*}
where $\boldsymbol{\kappa}' = (\kappa_1, \kappa_2, \kappa_3)$ and  $\boldsymbol{\theta}' = (\theta_1, \theta_2, \theta_3)$ with
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right].
\end{align*}

\paragraph{Moment Functions for Preliminary Estimator}
The moment functions $h$ for the preliminary estimator of $\boldsymbol{\gamma}' = (\mathbf{p}', \boldsymbol{\nu}', \boldsymbol{\kappa}')$ are
\begin{align*}
  h(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &= h(W_i,\boldsymbol{\theta}) - \boldsymbol{\gamma} =  \left[
  \begin{array}{l}
    h_p(W_i) - \mathbf{p}\\
    h_\nu(W_i) - \boldsymbol{\nu}\\
    h_\kappa(W_i,\boldsymbol{\theta}) - \boldsymbol{\kappa}
  \end{array}
\right]\\
h_p(W_i) - \mathbf{p} &= \left[
\begin{array}{l}
  T_i (1-z_i)/(1-q)\\ 
  T_i z_i / q 
\end{array}
\right] - \left[
\begin{array}{c}
  p_0 \\ p_1
\end{array}
\right]\\
h_\nu(W_i) - \boldsymbol{\nu} &= \left[
\begin{array}{l}
  y_i (1 - T_i) (1-z_i) / \sqrt{1-q}\\ 
  y_i T_i (1-z_i) / \sqrt{1-q} \\
  y_i (1 - T_i) z_i / \sqrt{q} \\
  y_i T_i z_i/\sqrt{q} 
\end{array}
\right] - \left[
\begin{array}{c}
  \nu_{00} \\
  \nu_{10} \\
  \nu_{01} \\
  \nu_{11} 
\end{array}
\right]\\
h_\kappa(W_i, \boldsymbol{\theta}) - \boldsymbol{\kappa} &= \mathbf{u}_i(\boldsymbol{\theta}, \boldsymbol{\kappa})
\end{align*}
where $\mathbf{p}'= (p_0, p_1)$, $\boldsymbol{\nu}' = (\nu_{00}, \nu_{10}, \nu_{01}, \nu_{11})$, and $\boldsymbol{\kappa'} = (\kappa_1, \kappa_2, \kappa_3)$.

\paragraph{The Preliminary Estimator}
We estimate $\boldsymbol{\gamma}$ under the null $\boldsymbol{\theta} = \boldsymbol{\theta}_0$ using the just-identified system of moment functions $h$.
Specifically, 
\begin{align*}
  \widehat{\boldsymbol{\gamma}}_0' &= \widehat{\boldsymbol{\gamma}}(\boldsymbol{\theta}_0)' = \left[
\begin{array}{ccc}
  \widehat{\mathbf{p}}' & \widehat{\boldsymbol{\nu}}' &  \widehat{\boldsymbol{\kappa}}(\boldsymbol{\theta}_0)'
\end{array}
\right]\\
\widehat{\mathbf{p}} &= 
\left[
\begin{array}{c}
  \widehat{p}_0 \\ \widehat{p}_1
\end{array}
\right] = \frac{1}{n} \sum_{i=1}^n h_p(W_i) =
\frac{1}{n}\sum_{i=1}^n \left[
\begin{array}{l}
  T_i (1-z_i)/(1-q)\\
  T_i z_i / q
\end{array}
\right] \\
\widehat{\boldsymbol{\nu}} &= \left[
\begin{array}{c}
  \widehat{\nu}_{00} \\ 
  \widehat{\nu}_{10} \\ 
  \widehat{\nu}_{01} \\ 
  \widehat{\nu}_{11}
\end{array}
\right] = \frac{1}{n}\sum_{i=1}^n h_\nu(W_i) = \frac{1}{n} \sum_{i=1}^n \left[
\begin{array}{l}
  y_i (1 - T_i) (1-z_i) / \sqrt{1-q} \\
  y_i T_i (1-z_i) / \sqrt{1-q} \\
  y_i (1 - T_i) z_i / \sqrt{q} \\
  y_i T_i z_i/\sqrt{q} 
\end{array}
\right]\\
\widehat{\boldsymbol{\kappa}}(\boldsymbol{\theta}_0) &= 
\left[
\begin{array}{c}
  \widehat{\kappa}_1(\boldsymbol{\theta}_0)\\
  \widehat{\kappa}_2(\boldsymbol{\theta}_0)\\
  \widehat{\kappa}_3(\boldsymbol{\theta}_0)
\end{array}
\right] = \frac{1}{n} \sum_{i=1}^n\left[
  \begin{array}{l}
 y_i - \theta_1^0 T_i\\
 y^2_i - \theta_1^0 2 y_i T_i + \theta_2^0 T_i\\
 y^3_i - \theta_1^0 3y_i^2 T + \theta_2^0 3y_iT_i - \theta_3^0 T_i
  \end{array}
\right]
\end{align*}
where $\boldsymbol{\theta}_0 = (\theta_1^0, \theta_2^0, \theta_3^0)$.
The preliminary estimators of $\boldsymbol{p}$ and $\boldsymbol{\nu}$ do not depend on $\boldsymbol{\theta}_0$.

\paragraph{Inequality Moment Conditions}
The inequality moment functions $m^I$ are
\begin{align*}
  m^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
  \left[
  \begin{array}{l}
    m_1^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) \\ 
    m_2^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) 
  \end{array}
\right] \\
  m_1^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
  \left[
  \begin{array}{r}
  p_0 - \alpha_0 \\
  (1 - p_0) - \alpha_1  \\
  p_1 - \alpha_0 \\
  (1 - p_1) - \alpha_1  
  \end{array}
\right]\\
m_2^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
\left[
\begin{array}{r}
  (p_0 - \alpha_0) \widetilde{y}_i^2(\alpha_0,z=0) - d^2(\alpha_0,z=0)\\
  (1- p_0 - \alpha_1) \widetilde{y}^2_i(\alpha_1,z=0) - d^2(\alpha_1,z=0)\\
  (p_1 - \alpha_0) \widetilde{y}^2_i(\alpha_0,z=1) - d^2(\alpha_0,z=1)\\
  (1- p_1 - \alpha_1) \widetilde{y}_i^2(\alpha_1,z=1) -  d^2(\alpha_1,z=1)
\end{array}
\right]
\end{align*}
where we define the shorthand
\begin{align*}
  \widetilde{y}_i^2(\alpha_0, z=0) &= y_i^2 (1 - z_i)(T_i - \alpha_0)\\ 
  \widetilde{y}_i^2(\alpha_1, z=0) &= y^2 (1-z_i)(1 - T_i - \alpha_1) \\
  \widetilde{y}_i^2(\alpha_0, z=1) &= y^2_i z_i(T_i - \alpha_0) \\
  \widetilde{y}_i^2(\alpha_1, z=1) &= y_i^2 z_i(1 - T_i - \alpha_1)
\end{align*}
and analogously
\begin{align*}
  d(\alpha_0,z=0) &=(1 - \alpha_0) \nu_{10} - \alpha_0 \nu_{00} \\ 
  d(\alpha_1, z=0) &=\alpha_1 \nu_{10} - (1 - \alpha_1)\nu_{00} \\
  d(\alpha_0, z=1) &=(1 - \alpha_0) \nu_{11} - \alpha_0 \nu_{01} \\
  d(\alpha_1, z=1) &=\alpha_1 \nu_{11} - (1 - \alpha_1)\nu_{01}
\end{align*}




\paragraph{Correcting the Asymptotic Variance Matrix}
To account for the preliminary estimation of $\boldsymbol{\gamma}$ we need to make an adjustment to the estimator of the asymptotic covariance matrix of the moment conditions $m^I$ and $m^E$, as explained above.
Let $m' = (m^{'I}, m^{'E})$ denote the full collection of moment conditions.
Because our estimator of $\boldsymbol{\gamma}$ is just identified, the quantity required to adjust the covariance matrix estimator is $B = -MH^{-1}$ where 
\[
  H = \mathbb{E}\left[ \frac{\partial h(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \boldsymbol{\gamma}'} \right], \,
  M = \mathbb{E}\left[ \frac{\partial m(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \boldsymbol{\gamma}'} \right]  
\]
But in our example $H = -\mathbf{I}$ so that 
\[
  B = M = \mathbb{E}\left[
\begin{array}{ccc}
  \displaystyle \frac{\partial m_1^I(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'} & \mathbf{0}_{4\times 4} & \mathbf{0}_{4 \times 3}\\ \\
  \displaystyle \frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'} &
\displaystyle \frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \boldsymbol{\nu}'} &
\mathbf{0}_{4\times 3} \\ \\
\mathbf{0}_{3 \times 2} & \mathbf{0}_{3\times 4} & 
\displaystyle \frac{\partial m^E\left( W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0 \right)}{\partial \boldsymbol{\kappa}'}
\end{array}
\right]
\]
where 
\begin{align*}
\mathbb{E}\left[\frac{\partial m_1^I(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'}\right] &= 
\left[
\begin{array}{rrr}
  1 & 0 \\
  -1 & 0 \\
  0 & 1 \\
  0 & -1 \\
\end{array}
\right] \\
\mathbb{E}\left[\frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'} \right]
&= \mathbb{E}\left[
\begin{array}{rr}
  \widetilde{y}_i^2(\alpha_0, z=0) & 0 \\
  -\widetilde{y}_i^2(\alpha_1, z=0) & 0 \\
  0 & \widetilde{y}_i^2(\alpha_0, z=1)\\ 
  0 & -\widetilde{y}_i^2(\alpha_1, z=1)
\end{array}
\right]\\
\mathbb{E}\left[\frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \boldsymbol{\nu}'} \right]
 &= 
 \left[
 \begin{array}{rr}
   Q_1 & \mathbf{0}_{2\times 2}\\
   \mathbf{0}_{2\times 2} & Q_2
 \end{array}
 \right]\\
 Q_1 &=\left[
\begin{array}{rr}
   2\alpha_0\, d(\alpha_0, z=0)& -2(1 - \alpha_0)\, d(\alpha_0, z=0)\\
   2(1 - \alpha_1)\, d(\alpha_1, z=0)& -2\alpha_1\, d(\alpha_1, z=0) \\
\end{array}
\right]\\
Q_2 &= \left[
\begin{array}{cc}
  2 \alpha_0 \, d(\alpha_0, z=1) &
  -2 (1 - \alpha_0) \, d(\alpha_0, z=1)\\
  2 (1 - \alpha_1)\, d(\alpha_1, z=1) &
  -2\alpha_1 \, d(\alpha_1, z=1)
\end{array}
\right] \\
\mathbb{E}\left[\frac{\partial m^E\left( W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0 \right)}{\partial \boldsymbol{\kappa}'}\right] &= -q\, \mathbf{I}_3
\end{align*}
and $\alpha_0,\alpha_1$ are understood to be hypothesized values under the null $\boldsymbol{\theta} = \boldsymbol{\theta}_0$.

\section{Improved Notation -- July 20th}
This section expands some of our whiteboard notes from the first week of July.
In our efforts to implement the various proposals for sub-vector inference (KMS and BCS), we discovered some more compact notation with which to express our problem and, in particular, the derivatives of the moment conditions.
Although it looks like we will end up using Andrews \& Soares rather than one of these subvector procedures, the notation is much nicer that what we had before so we might as well use it.
It should make it easier to write out the expressions used in the preliminary estimation of $\kappa$, etc.

\[
  \mathbf{w} = 
  \left[
  \begin{array}{c}
    W_1\\ W_2\\ W_3\\ W_4\\ W_5\\ W_6
  \end{array}
\right] = 
  \left[
  \begin{array}{c}
    T\\ y \\ yT\\  y^2 \\ y^2 T \\ y^3
  \end{array}
\right], \quad \boldsymbol{\mu}_w = \mathbb{E}[\mathbf{w}]
\]

\begin{align*}
  \boldsymbol{\Psi}(\boldsymbol{\theta}) &= \left[
\begin{array}{ccc}
  \boldsymbol{\psi}_1(\boldsymbol{\theta}) &
  \boldsymbol{\psi}_2(\boldsymbol{\theta}) &
  \boldsymbol{\psi}_3(\boldsymbol{\theta})
\end{array}
\right]\\
  \boldsymbol{\psi}_1'(\boldsymbol{\theta}) &= \left[
  \begin{array}{cccccc}
   -\theta_1 & 1 & 0 & 0 & 0 & 0 
  \end{array}
\right] \\
  \boldsymbol{\psi}_2'(\boldsymbol{\theta}) &= \left[
  \begin{array}{cccccc}
   \theta_2 & 0 & -2\theta_1 & 1 & 0 & 0
  \end{array}
\right] \\
  \boldsymbol{\psi}_3'(\boldsymbol{\theta}) &= \left[
  \begin{array}{cccccc}
   -\theta_3 & 0 & 3\theta_2 & 0 & -3\theta_1 & 1
  \end{array}
\right] 
\end{align*}

\begin{align*}
  \kappa_1 + u_{i1}(\boldsymbol{\theta}, \boldsymbol{\kappa}) &=  \boldsymbol{\psi}_1'(\boldsymbol{\theta}) \mathbf{w}_i  =  y_i - \theta_1 T_i \\
  \kappa_2 + u_{i2}(\boldsymbol{\theta}, \boldsymbol{\kappa}) &= \boldsymbol{\psi}_2'(\boldsymbol{\theta}) \mathbf{w}_i = y_i^2 - 2\theta_1 y_iT_i + \theta_2 T_i \\
  \kappa_3 + u_{i3}(\boldsymbol{\theta},\boldsymbol{\kappa}) &= \boldsymbol{\psi}_3'(\boldsymbol{\theta}) \mathbf{w}_i = y_i^3 - 3\theta_1 y^2_i T_i + 3 \theta_2 y_iT_i - \theta_3 T_i 
\end{align*}

\noindent Moment equalities
\begin{align*}
  m^E_j(\mathbf{w}_i, \boldsymbol{\theta}, \kappa_j) =  \boldsymbol{\psi}_j'(\boldsymbol{\theta}) \mathbf{w}_i - \kappa_j \\
  \mathbb{E}[m^E_j(\mathbf{w}_i, \boldsymbol{\theta}, \kappa_j) ] = 0
\end{align*}

\noindent Population version of preliminary estimator and plugin
\begin{align*}
  \kappa_j(\boldsymbol{\theta}) &= \mathbb{E}[\boldsymbol{\psi}_j'(\boldsymbol{\theta})\mathbf{w}] = \boldsymbol{\psi}_j'(\boldsymbol{\theta}) \boldsymbol{\mu}_w\\
m_j^E\left(\mathbf{w}_i, \boldsymbol{\theta}, \kappa_j(\boldsymbol{\theta})\right) &= \left[\boldsymbol{\psi}_j'(\boldsymbol{\theta}) \mathbf{w}_i - \kappa_j(\boldsymbol{\theta})\right] z_i = \boldsymbol{\psi}_j'(\boldsymbol{\theta}) (\mathbf{w}_i - \boldsymbol{\mu}_w)z_i
\end{align*}
sample analogues of the same
\begin{align*}
  \widehat{\kappa}_j(\boldsymbol{\theta}) &= \boldsymbol{\psi}_j'(\boldsymbol{\theta}) \bar{\mathbf{w}}\\ m_j^E\left(\mathbf{w}_i, \boldsymbol{\theta}, \kappa_j(\boldsymbol{\theta})\right) &= \left[ \boldsymbol{\psi}'_j(\boldsymbol{\theta}) \mathbf{w}_i - \widehat{\kappa}_j(\boldsymbol{\theta}) \right] =  \boldsymbol{\psi}_j'(\boldsymbol{\theta}) (\mathbf{w}_i - \bar{\mathbf{w}}_i) z_i
\end{align*}

\noindent We may not end up needing all of these derivatives, but just in case:

\noindent If $\boldsymbol{\theta}$ is a $(d\times 1)$ vector of parameters and $\mathbf{b}(\boldsymbol{\theta})$ is $(r\times 1)$ vector of functions, then 
\[
  \underbrace{\frac{\partial}{\partial \boldsymbol{\theta}'} \left[ \frac{\mathbf{b}(\boldsymbol{\theta})'\mathbf{c}}{\sqrt{\mathbf{b}(\boldsymbol{\theta})'\mathbf{M}\mathbf{b}(\boldsymbol{\theta})}} \right]}_{(1 \times d)}
  = \underbrace{\frac{\partial}{\partial \mathbf{b}'} \left[ \frac{\mathbf{b}'\mathbf{c}}{\sqrt{\mathbf{b}'\mathbf{M}\mathbf{b}}} \right]}_{(1\times r)} \underbrace{\frac{\partial}{\partial \boldsymbol{\theta}'} \mathbf{b}(\boldsymbol{\theta})}_{(r\times d)}
\]
by the chain rule and, using basic properties of matrix derivatives,
\[
  \frac{\partial}{\partial \mathbf{b}'} \left[ \frac{\mathbf{b}'\mathbf{c}}{\sqrt{\mathbf{b}'\mathbf{M}\mathbf{b}}}   \right] = \frac{1}{\sqrt{\mathbf{b}'\mathbf{M}\mathbf{b}}}\left[ \mathbf{c}' - \frac{\mathbf{b}' \mathbf{c}\mathbf{b}' \mathbf{M}}{\mathbf{b}'\mathbf{M}\mathbf{b}} \right]
\]

\section{New Idea: Prelim.\ Estimation of $\theta_1$ (2017-07-22)}
As long as $z$ is a strong instrument, which we will assume, $\theta_1$ is strongly identified.
Recall that $\beta$ does not enter our inequality moment conditions.
And since,
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right]
\end{align*}
we see that $\beta$ only enters the equality moment conditions through $\theta_1$.
This means that we can completely eliminate $\beta$ from the system if we substitute a preliminary estimator for $\theta_1$.
After doing this, our equalities and inequalities will be functions of $\alpha_0$ and $\alpha_1$ only.
The same idea could be used with the BBS/KRS/FL/Mahajan moment conditions.
I initially thought this could be a useful way to carry out inference for $\alpha_0$ and $\alpha_1$, for example to test $\alpha_0 = \alpha_1 = 0$, in a way that accounts for boundary problems and weak identification.
If one only desires inference for $\alpha_0$ and $\alpha_1$ it is clearly overkill to $m_1^E$ among the moment equalities used in the GMS inference procedure, since $\theta_1$ does not suffer from a weak identification problem.
Including it would be just like including the moment conditions for $\mathbf{\kappa}$ -- computationally expensive and unnecessary since these parameters are strongly identified.

Because we are not especially interested in carrying out inference for $\alpha_0$ and $\alpha_1$, I initially dismissed this idea.
It was unclear to me how a confidence region for $(\alpha_0, \alpha_1)$ could be used to carry out inference for $\beta$.
But I think there's actually a very simple way to do this.
Suppose we have a valid $(1 - \delta)\times 100\%$ confidence interval $(\mbox{LCL}, \mbox{UCL})$ for $\theta_1$, then
\begin{align*}
  \mathbb{P}\left( \mbox{LCL} \leq \theta_1 \leq \mbox{UCL} \right) &= 1 - \delta\\
  \mathbb{P}\left( \mbox{LCL} \leq \frac{\beta}{1 - \alpha_0 - \alpha_1} \leq \mbox{UCL} \right) &= 1 - \delta\\
  \mathbb{P}\left( \left[ 1 - \alpha_0 - \alpha_1 \right] \times \mbox{LCL} \leq \beta \leq \left[ 1 - \alpha_0 - \alpha_1 \right] \times \mbox{UCL} \right) &= 1 - \delta
\end{align*}
It is trivial to construct a valid confidence interval for $\theta_1$ under our assumption that $z$ is a strong instrument: we can used use the usual IV confidence interval.
And the preceding equalities show that a valid confidence interval for $\theta_1$ immediately implies a valid confidence interval for $\beta$ \emph{conditional} on knowledge of the sum $\alpha_0 + \alpha_1$.
This is exactly analogous to the situation in my job market paper, where I had a way to construct a valid confidence interval for $\mu$ conditional on knowledge of $\tau$, so we can use the same solution here: Bonferroni.
The procedure will be to construct a $(1 - \delta_1)\times 100\%$ confidence set for $\alpha_0 + \alpha_1$ and then take the max and min of the $(1 - \delta_2)\times 100\%$ confidence sets for $\beta|\alpha_0 + \alpha_1$, yielding a valid $>= (1 - \delta_1 - \delta_2)\times 100\%$ confidence set for $\beta$.
Bonferroni necessarily induces conservatism, but so does projection.
And unlike projection, which we can't tune, Bonferroni allows us to choose $\delta_1$ and $\delta_2$ to reduce the degree of conservatism.
Moreover, by reducing the dimension over which we must carry out projection there should be an attendant reduction in the conservatism that comes from projection itself.
So I think in the end, Bonferroni could end up being less conservative than our planned projection of a joint confidence set for $(\alpha_0, \alpha_1, \beta)$ into a marginal set for $\beta$.

Another consideration is computational cost.
Bonferroni should be much cheaper to compute since we only have to carry out the GMS procedure in two dimensions $(\alpha_0, \alpha_1)$-space.
Indeed, we do not in fact need a confidence set for $(\alpha_0, \alpha_1)$ -- we only need a confidence set for their \emph{sum}.
This means that we do not even need to carry out a two-dimensional grid search.
What we really need is projection inference for $\alpha_0 + \alpha_1$.
I have several ideas for how we can speed this up.
First, after eliminating $m_1^E$, the only moment equalities are $m_2^E$ and $m_3^E$. 
We would expect that $m_2^E$ is the more precisely estimated of these since second moments should have less sampling variation than third moments.
Since the second moment equality identifies a line of the form $\alpha_1 - \alpha_0 = K$, I think we can reduce the dimensionality even further be searching at or near this line.
In other words, we probably do not want to start by constructing the full grid of $\alpha_0, \alpha_1$ values since we have a good guess for the region that will not be rejected, provided we're inside the region where the moment inequalities do not bind.
And once we've identified a pair $(\alpha_0, \alpha_1)$ that sum to a particular value, say $B$, we don't need to check any more grid points on the line $\alpha_0 + \alpha_1 = B$, since we're only interested in inference for the sum.

Another idea worth considering is speeding up or eliminating the bootstrap entirely.
Given the observed data, the GMS procedure defines a region in $(\alpha_0, \alpha_1)$ space where the moment inequalities are ``far from binding'' and are hence excluded.
Indeed, the structure of our problem tells us that there is a rectangular region $(0, \underline{\alpha}_0) \times (0, \underline{\alpha}_1)$ in which \emph{all} of the moment conditions will be deemed far from binding.
The region itself may depend on the $(\alpha_0, \alpha_1)$ pair being tested via the variance matrix but this doesn't matter: at a given pair $(\alpha_0, \alpha_1)$ we can determine whether we're in a case where GMS throws out \emph{all} of the moment conditions.
In any case, the pair $(\alpha_0 = 0, \alpha_1 = 0)$ should always be in the region where GMS tosses out the inequalities.
To put it another way, we know in advance that we cannot rule this point out from the inequalities given their one-sided nature.
If we are testing a point $(\alpha_0, \alpha_1)$ where the inequalities are all excluded  then, since we are using the asymptotic version of the GMS test, there is in fact no need to use the bootstrap normal draws.
We can work out the \emph{exact} distribution of the test statistic under the null and it will be fully determined by a \emph{single} value that we estimate from data under the null.
This is because the test statistic is simply a sum of squares of two correlated, zero-mean normals (we've eliminated one of the moment equalities by using a preliminary estimator of $\theta_1$).
But because the test statistic divides through by the variance of each component, we can assume WLOG that the normals are standard normal.
The only unknown is the correlation between them, but our code already estimates this to transform the normal draws for carrying out the asymptotic version of the bootstrap.
So we just need to know the distribution of the sum of squares of two correlated standard normals.
This can be shown (see the StackOverflow question) to equal a weighted sum of two independent Chi-squared random variables and there is an R package that calculates $\mathbb{P}(X>x)$ for such a distribution.
If we were to use the random normal draws instead we would simply be approximating this distribution by simulation.

Now, suppose instead that we are testing a pair $(\alpha_0, \alpha_1)$ in the region where at least one moment inequality is \emph{not} far from binding.
In this case we cannot directly calculate the distribution of the test statistic.
However, we can easily find an \emph{upper bound} on the critical value without using our normal draws.
To do this, we construct a test statistic that dominates the MMM statistic.
Suppose that $J$ moment inequalities are deemed to \emph{not} be far from binding.
Then these $J$ are used in the calculation of the critical value for the MMM test statistic.
Notice that the moment inequalities only contribute to the MMM statistic when they are violated, in which case they enter as a sum of squares.
Otherwise they contribute zero.
Thus, the MMM test statistic is \emph{bounded above} by the sum of squares of $J+2$ correlated normals: $J$ for the inequalities that were not discarded, and two for the equalities.
Again, the MMM test statistic does not depend on variances, only correlations.
So this dominating statistic (one that treats the inequalities \emph{symmetrically}) is a sum of squares of $J+2$ correlated standard normals.
This can be shown to be equivalent to a weighted sum of $J+2$ independent Chi-squared random variables where the weights depends on the correlations.
This means that we can potentially take a computational shortcut even when $(\alpha_0,\alpha_1)$ lies in a region where at least one moment inequality binds.
To calculate the bootstrap critical value, we need the correlation matrix of the moment conditions anyway.
But before doing the multiplications that transform all the normal draws for the bootstrap, we could first calculate the critical value for the \emph{dominating} test statistic.
Call this the ``upper bound'' critical value.
The upper bound critical value is exactly correct when all of the moment inequalities are deemed far from binding; otherwise it is ``too large.''
However, if we don't care getting the p-value for the test and merely the accept/reject decision itself we could first compare the test statistic to the upper bound critical value.
If we would reject, then there is no need to calculate the actual critical value, since we know that we would reject.
In these cases we can skip the calculations for the bootstrap.
Since these comprise almost the whole cost of the procedure, this should be a substantial savings.
Effectively, it will allow us to rule out pairs $(\alpha_0, \alpha_1)$ strongly rejected by the data using very simple calculations.

\paragraph{What should we do now?}
\begin{enumerate}
  \item Preliminary estimation of $\theta_1$ changes the variance matrix of the remaining moment conditions.
    We need to do the calculations for this.
    It will involve taking some derivatives, but I don't think they're too bad since $\theta_1$ always enters multiplicatively.
  \item I should write up the precise calculations needed for the ``upper bound'' critical value. 
    The same calculations are used in the case where it is an upper bound as when it is exact.
  \item We need to think a bit more about how to cleverly search over values for $(\alpha_0, \alpha_1)$ to get the projection confidence set for $\alpha_0 + \alpha_1$.
    I suppose there are some calculations to do concerning the line along with we can make the second moment equality zero.
  \item We should think about how to use this method with BBS/KRS/FL/Mahajan and also how to use it for Lewbel.
    We don't have the unconditional moment equalities for Lewbel yet so I suppose we need to derive these.
  \item We're going to have to say something about covariates.
    At the least I think we should derive the moment conditions one would use under the assumption that $\alpha_0, \alpha_1, \beta$ don't depend on $\mathbf{x}$ as in Frazis and Loewenstein.
    This requires some additional preliminary estimation and hence some additional derivatives.
    I suppose we'll also need to write up a brief description of what one could do if one is unwilling to make these assumptions.
    I wonder if the fact that we're now doing Bonferroni simplifies anything here.
    For example, suppose one did an IV series estimator with interactions between the exogenous variable and the treatment.
    What is the probability limit of IV ignoring the mis-classification?
    Is there some simple way to proceed? 
    I think there actually might be.
    There are presumably two cases: one in which the $\alpha_0, \alpha_1$ don't depend on $\mathbf{x}$ but $\beta$ does, and another in which everything depends on $\mathbf{x}$. 
    Presumably the first case is easier, possibly even trivial.
    It would be nice if this were the case.
    I think there's also a question of whether you want to learn $\beta(\mathbf{x})$ or $\mathbb{E}[\beta(\mathbf{x})]$.
  \item We also need to carry out the standard GMM calculations for naive estimation of our model along with that of Lewbel, BBS/KRS/FL/Mahajan so we can do simulations comparing the performance of the methods.
\end{enumerate}

\section{Preliminary Estimation of $\theta_1$ (2017--07--23)}

\paragraph{Notation}
Recall that
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right]
\end{align*}
Now, define $\boldsymbol{\alpha} = (\alpha_0, \alpha_1)'$ and $\eta_2(\boldsymbol{\alpha})$, $\eta_3(\boldsymbol{\alpha})$ as
\begin{align*}
  \eta_2(\boldsymbol{\alpha}) &= 1 + (\alpha_0 - \alpha_1)\\
  \eta_3(\boldsymbol{\alpha}) &= (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0(1 - \alpha_1)
\end{align*}
so that we have
\begin{align*}
  \theta_2 &= \eta_2(\boldsymbol{\alpha}) \theta_1^2\\
  \theta_3 &= \eta_3(\boldsymbol{\alpha})\theta_1^3
\end{align*}
and we will use the shorthand $\eta_2, \eta_3$ suppressing dependence on $\alpha_0, \alpha_1$.



\paragraph{Moment Equality Conditions}
Let $\boldsymbol{\gamma}$ denote the parameters for which we will substitute preliminary estimators.
The elements of $\boldsymbol{\gamma}$ that are relevant for the GMS moment equalities $m^E$ are $\theta_1, \kappa_2$ and $\kappa_3$.
The GMS moment functions $m^E$ are defined as 
\begin{align*}
  m_2^E(\mathbf{w}_i, \boldsymbol{\alpha}, \boldsymbol{\gamma}) &= \left[y_i^2 - \kappa_2 - \theta_1 2y_i T_i + \eta_2(\boldsymbol{\alpha}) (\theta_1^2 T_i)\right] z_i \\
  m_3^E(\mathbf{w}_i, \boldsymbol{\alpha}, \boldsymbol{\gamma}) &= \left[ y_i^3 - \kappa_3 - \theta_1 3 y_i^2 T_i +  \eta_2(\boldsymbol{\alpha})(3\theta_1^2y_iT_i) -  \eta_3(\boldsymbol{\alpha})(\theta_1^3 T_i)\right]z_i
\end{align*}



\paragraph{Moment Functions for Preliminary Estimators}
The preliminary estimators of the parameters $\mathbf{p}$ and $\mathbf{\nu}$ from the moment inequality conditions, defined above, are are unchanged.
However, the preliminary estimators for the moment equality conditions are affected by the fact that we will now use a preliminary estimator of $\theta_1$.
Note that $\kappa_1$ does not enter the GMS moment equalities $m^E$ although it does enter the moment equality used to identify $\theta_1$.
Moreover, estimating $\kappa_2$ and $\kappa_3$ under the null $\mathbf{\alpha} = \alpha^0$ in turn requires a preliminary estimator of $\theta_1$.
The moment equalities we will use to construct preliminary estimators to plug into $m^E$, under the null $\boldsymbol{\alpha} = \boldsymbol{\alpha}^0$, are $\mathbb{E}[h^E(\mathbf{w}_i, \boldsymbol{\alpha}^0, \theta_1, \boldsymbol{\kappa})]=0$ where
\[
  h^E(\mathbf{w}_i, \boldsymbol{\alpha}^0, \theta_1, \boldsymbol{\kappa}) = 
  \left[
  \begin{array}{l}
 y_i - \kappa_1 - \theta_1 T_i\\
  y_i^2 - \kappa_2 - \theta_1 2y_i T_i + \eta_2(\boldsymbol{\alpha}^0) (\theta_1^2 T_i) \\
 y_i^3 - \kappa_3 - \theta_1 3 y_i^2 T_i +  \eta_2(\boldsymbol{\alpha}^0)(3\theta_1^2y_iT_i) -  \eta_3(\boldsymbol{\alpha}^0)(\theta_1^3 T_i)\\
 (y_i - \kappa_1 - \theta_1 T_i)z_i
  \end{array}
\right] 
\]
Notice that the three ``intercept'' moment equalities appear first while the ``slope'' moment equality for the IV estimator appears last.
Since this system is just identified, the GMM weighting matrix is irrelevant and the GMM estimator will simply equal the method of moments estimator: we can always set find parameter values that make the sample analogues of all four moment conditions \emph{exactly} equal to zero which in turn makes the GMM criterion function exactly zero.
Setting the first and last moment conditions equal to zero is equivalent to using the standard TSLS estimator of $\theta_1$ and $\kappa_1$.
This means that GMM estimation of these four moment conditions will not attempt to use information about the higher powers of $\theta_1$ in the second and third moment equalities to estimate $\theta_1$.
If we simply plug the TSLS estimator for $\theta_1$ into the bottom two equalities, we are still free to choose $\kappa_2$ and $\kappa_3$ to make the corresponding sample analogues exactly zero, so this is what GMM estimation will do.

The moment equalities used to construct preliminary estimators for the moment inequality conditions are unchanged from above, namely $\mathbb{E}\left[ h_p(\mathbf{w})_i - \mathbf{p} \right] = 0$ and $\mathbb{E}\left[ h_\nu(\mathbf{w}_i) - \boldsymbol{\nu} \right] = 0$ where
\begin{align*}
h_p(W_i) - \mathbf{p} &= \left[
\begin{array}{l}
  T_i (1-z_i)/(1-q)\\ 
  T_i z_i / q 
\end{array}
\right] - \left[
\begin{array}{c}
  p_0 \\ p_1
\end{array}
\right]\\
h_\nu(W_i) - \boldsymbol{\nu} &= \left[
\begin{array}{l}
  y_i (1 - T_i) (1-z_i) / \sqrt{1-q}\\ 
  y_i T_i (1-z_i) / \sqrt{1-q} \\
  y_i (1 - T_i) z_i / \sqrt{q} \\
  y_i T_i z_i/\sqrt{q} 
\end{array}
\right] - \left[
\begin{array}{c}
  \nu_{00} \\
  \nu_{10} \\
  \nu_{01} \\
  \nu_{11} 
\end{array}
\right]
\end{align*}
and we define $\mathbf{p}'= (p_0, p_1)$ and $\boldsymbol{\nu}' = (\nu_{00}, \nu_{10}, \nu_{01}, \nu_{11})$.
Stacking all the moment equalities used for preliminary estimation under the null, we have $\mathbb{E}[h(\mathbf{w}_i, \boldsymbol{\alpha}^0, \boldsymbol{\gamma})] = 0$ where
\[
  h\left( \mathbf{w}_i, \boldsymbol{\alpha}_0,\boldsymbol{\gamma} \right) = \left[
  \begin{array}{l}
    h^I(\mathbf{w}_i, \mathbf{p}, \boldsymbol{\nu}) \\
    h^E(\mathbf{w}_i, \boldsymbol{\alpha}^0, \theta_1, \boldsymbol{\kappa}) 
  \end{array}
\right]
\]

\[
  h^I\left( \mathbf{w}_i, \mathbf{p}, \boldsymbol{\nu} \right) = \left[
  \begin{array}{l}
    h_p(\mathbf{w}_i) - \mathbf{p} \\
    h_\nu(\mathbf{w}_i) - \boldsymbol{\nu} 
  \end{array}
\right]
\]

\paragraph{Preliminary Estimators:} The preliminary estimators of $\mathbf{p}$ and $\boldsymbol{\nu}$ are sample means:
\[
  \widehat{\mathbf{p}} = \frac{1}{n}\sum_{i=1}^n h_p(\mathbf{w}_i), \quad
  \widehat{\boldsymbol{\nu}} = \frac{1}{n}\sum_{i=1}^n h_\nu(\mathbf{w}_i) 
\]
And, as argued a few paragraphs back, the preliminary estimator for $\theta_1$ is ordinary IV: 
\[
  \widehat{\theta}_1 = \frac{\sum_{i=1}^n (y_i - \bar{y})(z_i - \bar{z})}{\sum_{i=1}^n (T_i - \bar{T})(z_i - \bar{z})}
\]
while those for $\kappa_2$ and $\kappa_3$ are sample means constructed by plugging in $\widehat{\theta}_1$ under the null: 
\begin{align*}
  \widehat{\kappa}_2(\boldsymbol{\alpha}^0) &= \frac{1}{n} \sum_{i=1}^n \left[y_i^2 - 2\widehat{\theta}_1 y_i T_i + \widehat{\theta}_2(\boldsymbol{\alpha}^0) T_i \right]\\
  \widehat{\kappa}_2(\boldsymbol{\alpha}^0) &= \frac{1}{n} \sum_{i=1}^n 
  \left[y_i^3 - 3\widehat{\theta}_1  y_i^2 T_i +  3\widehat{\theta}_2(\boldsymbol{\alpha}^0) y_iT_i -  \widehat{\theta}_3(\boldsymbol{\alpha}^0)T_i\right]\\
  \widehat{\theta}_2(\boldsymbol{\alpha}^0) &= (\widehat{\theta}_1)^2 \, \eta_2(\boldsymbol{\alpha}^0)\\
  \widehat{\theta}_3(\boldsymbol{\alpha}^0) &= (\widehat{\theta}_1)^3\, \eta_3(\boldsymbol{\alpha}^0)
\end{align*}

Notice that $\kappa_1$ enters neither $m^E$ nor $m^I$.
This means that we do not need to substitute a preliminary estimator of $\kappa_1$ into the GMS moment conditions.
However, implicitly we \emph{do} need to estimate $\kappa_1$ in order to estimate $\theta_1$.
Moreover, the general derivation we carried out above for adjusting a covariance matrix to account for preliminary estimation becomes much simpler if the number in the setting where $h$ consists of one moment equality for each parameter we need to pre-estimate. 
For this reason, we will consider $\kappa_1$ to be among the parameters for which we need preliminary estimators.
The final result will still be correct because the derivatives of $m$ with respect to $\kappa_1$ will all be zero, but we will correctly account for the way in which estimation of $\kappa_1$ affects the asymptotic variance of $\widehat{\theta}$.
The estimator for $\kappa_1$ is as follows:
\[
  \widehat{\kappa}_1 = \frac{1}{n} \sum_{i=1}^n y_i - \widehat{\theta}_1 T_i
\]
Note that this does not depend on the null hypothesis.
Using the notation $\boldsymbol{\psi}_j$, $\mathbf{w}$, etc.\ defined in the preceding section, we can express the estimators of $\boldsymbol{\kappa}$ more compactly as
\begin{align*}
  \widehat{\kappa}_1 &= \widehat{\boldsymbol{\psi}}_1' \bar{\mathbf{w}}\\
  \widehat{\kappa}_2(\boldsymbol{\alpha}^0) &= \widehat{\boldsymbol{\psi}}_2'(\boldsymbol{\alpha}^0) \bar{\mathbf{w}}\\
  \widehat{\kappa}_3(\boldsymbol{\alpha}^0) &= \widehat{\boldsymbol{\psi}}_3'(\boldsymbol{\alpha}^0) \bar{\mathbf{w}}\\
  \widehat{\boldsymbol{\kappa}} &= \left[
  \begin{array}{l}
    \widehat{\kappa}_1 \\
    \widehat{\kappa}_2(\boldsymbol{\alpha}^0)\\
    \widehat{\kappa}_3(\boldsymbol{\alpha}^0)
  \end{array}
\right] = \widehat{\boldsymbol{\Psi}}'(\boldsymbol{\alpha}^0) \bar{\mathbf{w}}
\end{align*}
where, 
\begin{align*}
  \bar{\mathbf{w}} &= n^{-1} \sum_{i=1}^n \mathbf{w}_i \\
  \mathbf{w}_i' &= \left[
  \begin{array}{cccccc}
    T_i & y_i & y_i T_i & y_i^2 & y^2_i T_i & y_i^3
  \end{array}
\right]\\
\widehat{\boldsymbol{\Psi}}(\boldsymbol{\alpha}^0) &= \left[
\begin{array}{lll}
  \widehat{\boldsymbol{\psi}}_1 &
  \widehat{\boldsymbol{\psi}}_2(\boldsymbol{\alpha^0}) &
  \widehat{\boldsymbol{\psi}}_3(\boldsymbol{\alpha^0})
\end{array}
\right]\\
\widehat{\boldsymbol{\psi}}_1' &= \left[
  \begin{array}{cccccc}
    -\widehat{\theta}_1 & 1 & 0 & 0 & 0 & 0 
  \end{array}
\right] \\
  \widehat{\boldsymbol{\psi}}_2'(\boldsymbol{\alpha}^0) &= \left[
  \begin{array}{cccccc}
    \widehat{\theta}_2(\boldsymbol{\alpha}^0) & 0 & -2\widehat{\theta}_1 & 1 & 0 & 0
  \end{array}
\right] \\
\widehat{\boldsymbol{\psi}}_3'(\boldsymbol{\alpha}^0) &= \left[
  \begin{array}{cccccc}
    -\widehat{\theta}_3(\boldsymbol{\alpha}^0) & 0 & 3\widehat{\theta}_2(\boldsymbol{\alpha}^0) & 0 & -3\widehat{\theta}_1 & 1
  \end{array}
\right] \\
  \widehat{\theta}_2(\boldsymbol{\alpha}^0) &= (\widehat{\theta}_1)^2 \, \eta_2(\boldsymbol{\alpha}^0)\\
  \widehat{\theta}_3(\boldsymbol{\alpha}^0) &= (\widehat{\theta}_1)^3\, \eta_3(\boldsymbol{\alpha}^0)\\
  \eta_2(\boldsymbol{\alpha}) &= 1 + (\alpha_0 - \alpha_1)\\
  \eta_3(\boldsymbol{\alpha}) &= (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0(1 - \alpha_1)
\end{align*}

\paragraph{Cleaner Notation for Equality Moment Conditions} 
\[
m^E = \left[
  \begin{array}{cc}
    m_2^E \\ m_3^E 
  \end{array}
\right], \quad m_j^E(\mathbf{w}_i, \cdots) = \left[ \boldsymbol{\psi}_j'(\theta_1, \boldsymbol{\alpha}^0) \mathbf{w}_i - \kappa_j \right]z_i 
\]

\paragraph{Cleaner Notation for $h^E$} 
\[
  h^E(\mathbf{w}_i, \cdots) = \left[
 \begin{array}{l}
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i - \kappa_1\\
   \boldsymbol{\psi}'_2(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_2\\
   \boldsymbol{\psi}'_3(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_3\\ 
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i z_i - \kappa_1 z_i
 \end{array}
 \right] = \left[
 \begin{array}{l}
   \boldsymbol{\Psi}'(\theta_1, \boldsymbol{\alpha}^0) \mathbf{w}_i - \boldsymbol{\kappa}\\
   \left\{\boldsymbol{\psi}_1'(\theta_1)\mathbf{w}_i - \kappa_1\right\}z_i
 \end{array}
 \right]
\]

\paragraph{Cleaner Notation for Equality Moment Conditions with Preliminary Estimators}
Let $\widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)$ denote our preliminary estimator of $\widehat{\gamma}$ under the null $\boldsymbol{\alpha} = \boldsymbol{\alpha}^0$. 
Then using the notation of the preceding paragraph,
\begin{align*}
  m_2^E\left(\mathbf{w}_i, \boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) &= \left[y_i^2 - \widehat{\kappa}_2(\boldsymbol{\alpha}^0) - 2 \widehat{\theta}_1 y_i T_i + (\widehat{\theta_1})^2 \eta_2(\boldsymbol{\alpha}) T_i\right] z_i \\
  &= \widehat{\boldsymbol{\psi}}_2'(\boldsymbol{\alpha}^0)(\mathbf{w}_i z_i) - \widehat{\boldsymbol{\psi}}_2'(\boldsymbol{\alpha}^0) (\bar{\mathbf{w}} z_i)\\
  &= \widehat{\boldsymbol{\psi}}_2'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})z_i \\
  m_3^E(\mathbf{w}_i, \boldsymbol{\alpha}, \boldsymbol{\gamma}) &= \left[ y_i^3 - \widehat{\kappa}_3(\boldsymbol{\alpha}^0) - 3\widehat{\theta_1}  y_i^2 T_i + 3 (\widehat{\theta}_1)^2 \eta_2(\boldsymbol{\alpha})y_iT_i -  (\widehat{\theta}_1)^3\eta_3(\boldsymbol{\alpha}) T_i\right]z_i\\
  &= \widehat{\boldsymbol{\psi}}_3'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})z_i 
\end{align*}
So we see that the data enters in a very simple and transparent way, as does the null hypothesis and the preliminary estimator $\widehat{\theta}_1$ of the IV estimand $\theta_1$.

\paragraph{Cleaner Notation for $h_E$ with preliminary estimators}
When we construct the joint covariance matrix of $m$ and $h$, we will substitute the preliminary estimators of $\gamma$ into the sample analogue for $h$.
As in the preceding paragraph, we can simplify the notation for the $h_E$ block as follows:
\[
  h_E\left(\mathbf{w}_i, \boldsymbol{\alpha}^0, \widehat{\gamma}(\boldsymbol{\alpha}^0)\right) = \left[
  \begin{array}{l}
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})\\ 
    \widehat{\boldsymbol{\psi}}'_2(\boldsymbol{\alpha}^0) (\mathbf{w}_i - \bar{\mathbf{w}})\\ 
    \widehat{\boldsymbol{\psi}}'_3(\boldsymbol{\alpha}^0) (\mathbf{w}_i - \bar{\mathbf{w}})\\
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})z_i
  \end{array}
\right] = \left[
\begin{array}{l}
  \widehat{\boldsymbol{\Psi}}'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})\\
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})z_i
\end{array}
\right]
\]
Notice that only the bottom element of $h_E$ involves $z$: this corresponds to the moment condition that identifies $\theta_1$.


\paragraph{Inequality Moment Conditions}
The inequality moment functions $m^I$ are unchanged from above, namely
\begin{align*}
  m^I(\mathbf{w}_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
  \left[
  \begin{array}{l}
    m_1^I(\mathbf{w}_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) \\ 
    m_2^I(\mathbf{w}_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) 
  \end{array}
\right] \\
m_1^I(\mathbf{w}_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
  \left[
  \begin{array}{r}
  p_0 - \alpha_0 \\
  (1 - p_0) - \alpha_1  \\
  p_1 - \alpha_0 \\
  (1 - p_1) - \alpha_1  
  \end{array}
\right]\\
m_2^I(\mathbf{w}_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
\left[
\begin{array}{r}
  (p_0 - \alpha_0) \widetilde{y}_i^2(\alpha_0,z=0) - d^2(\alpha_0,z=0)\\
  (1- p_0 - \alpha_1) \widetilde{y}^2_i(\alpha_1,z=0) - d^2(\alpha_1,z=0)\\
  (p_1 - \alpha_0) \widetilde{y}^2_i(\alpha_0,z=1) - d^2(\alpha_0,z=1)\\
  (1- p_1 - \alpha_1) \widetilde{y}_i^2(\alpha_1,z=1) -  d^2(\alpha_1,z=1)
\end{array}
\right]
\end{align*}
where we define the shorthand
\begin{align*}
  \widetilde{y}_i^2(\alpha_0, z=0) &= y_i^2 (1 - z_i)(T_i - \alpha_0)\\ 
  \widetilde{y}_i^2(\alpha_1, z=0) &= y^2 (1-z_i)(1 - T_i - \alpha_1) \\
  \widetilde{y}_i^2(\alpha_0, z=1) &= y^2_i z_i(T_i - \alpha_0) \\
  \widetilde{y}_i^2(\alpha_1, z=1) &= y_i^2 z_i(1 - T_i - \alpha_1)
\end{align*}
and analogously
\begin{align*}
  d(\alpha_0,z=0) &=(1 - \alpha_0) \nu_{10} - \alpha_0 \nu_{00} \\ 
  d(\alpha_1, z=0) &=\alpha_1 \nu_{10} - (1 - \alpha_1)\nu_{00} \\
  d(\alpha_0, z=1) &=(1 - \alpha_0) \nu_{11} - \alpha_0 \nu_{01} \\
  d(\alpha_1, z=1) &=\alpha_1 \nu_{11} - (1 - \alpha_1)\nu_{01}
\end{align*}


\paragraph{Correcting the Asymptotic Variance Matrix}
We now have all the ingredients needed to account for the preliminary estimation of $\boldsymbol{\gamma} = (\mathbf{p}', \boldsymbol{\nu}', \boldsymbol{\kappa}', \theta_1)$ where $\boldsymbol{\kappa}' = (\kappa_1, \kappa_2, \kappa_3)$ under the null $\boldsymbol{\alpha} = \boldsymbol{\alpha}^0$.
As above, let $m' = (m^{'I}, m^{'E})$ denote the full collection of moment conditions.
Because our estimator of $\boldsymbol{\gamma}$ is just identified, the quantity required to adjust the covariance matrix estimator is $B = -MH^{-1}$ where 
\[
  H = \mathbb{E}\left[ \frac{\partial h\left(\mathbf{w}_i, \boldsymbol{\alpha}^0, \boldsymbol{\gamma}(\boldsymbol{\alpha}^0)\right)}{\partial \boldsymbol{\gamma}'} \right], \,
  M = \mathbb{E}\left[ \frac{\partial m(\mathbf{w}_i, \boldsymbol{\alpha}^0, \boldsymbol{\gamma}(\boldsymbol{\alpha}^0))}{\partial \boldsymbol{\gamma}'} \right]  
\]
It is no longer the case that $H = -\mathbf{I}$ because the $h_E$ block of moment conditions is more complicated:
\[
  h^E(\mathbf{w}_i, \boldsymbol{\alpha}^0, \theta_1, \boldsymbol{\kappa}) = 
  \left[
  \begin{array}{l}
 y_i - \kappa_1 - \theta_1 T_i\\
  y_i^2 - \kappa_2 - \theta_1 2y_i T_i + \eta_2(\boldsymbol{\alpha}^0) (\theta_1^2 T_i) \\
 y_i^3 - \kappa_3 - \theta_1 3 y_i^2 T_i +  \eta_2(\boldsymbol{\alpha}^0)(3\theta_1^2y_iT_i) -  \eta_3(\boldsymbol{\alpha}^0)(\theta_1^3 T_i)\\
 (y_i - \kappa_1 - \theta_1 T_i)z_i
  \end{array}
\right] 
\]
Define 
\[
  H = \left[
  \begin{array}{c}
    H^I \\ H^E
  \end{array}
\right], 
\quad H^I = \frac{\partial h^I}{\partial \boldsymbol{\gamma}'}, \quad
\quad H^E = \frac{\partial h^E}{\partial \boldsymbol{\gamma}'}
\]
we have
\[
  H^I = \left[
  \begin{array}{cccc}
    \displaystyle \frac{\partial h^I}{\partial \mathbf{p}'} & 
    \displaystyle \frac{\partial h^I}{\partial \boldsymbol{\nu}'} &
    \displaystyle \frac{\partial h^I}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial h^I}{\partial \theta_1}
  \end{array}
\right] = \left[
\begin{array}{cc}
  -\mathbf{I}_{6\times 6} & \mathbf{0}_{6\times 4}
\end{array}
\right]
\]
and
\[
  H^E = \left[
  \begin{array}{cccc}
    \displaystyle \frac{\partial h^E}{\partial \mathbf{p}'} & 
    \displaystyle \frac{\partial h^E}{\partial \boldsymbol{\nu}'} &
    \displaystyle \frac{\partial h^E}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial h^E}{\partial \theta_1}
  \end{array}
\right] = \left[
\begin{array}{cc}
  \mathbf{0}_{4\times 6} & \mathbf{R}
\end{array}
\right], \quad
\mathbf{R} = \left[
\begin{array}{rrrr}
  -1 & 0 & 0 
  & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & -1 & 0 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & 0 & -1 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   -q & 0 & 0 &  \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}z]\\
\end{array}
\right]
\]
and hence
\[
  H = \left[
  \begin{array}{rr}
    -\mathbf{I}_{6\times 6} & \mathbf{0}_{6\times 4}\\
    \mathbf{0}_{4\times 6} & \mathbf{R}
  \end{array}
\right] \implies H^{-1} =
  \left[\begin{array}{rr}
    -\mathbf{I}_{6\times 6} & \mathbf{0}_{6\times 4}\\
    \mathbf{0}_{4\times 6} & \mathbf{R}^{-1}
\end{array}\right]
\]
So $H$ is invertible if and only if $\mathbf{R}$ is invertible.
From the definition of $\mathbf{w}$ a few lines of algebra show that the determinant of $\mathbf{R}$ is equal to $\mbox{Cov}(T,z)$.
Thus, so long as we have a strong instrument, there will be no problem inverting $H$ \emph{regardless} of the values of $\alpha_0,\alpha_1$ under the null hypothesis!

The matrix $M$ is fairly similar to the case without preliminary estimation of $\theta_1$.
In particular,
\[
  M = \mathbb{E}\left[
\begin{array}{cccc}
  \displaystyle \frac{\partial m_1^I(\mathbf{w}_i,\mathbf{p}^0)}{\partial \mathbf{p}'} & \mathbf{0}_{4\times 4} & \mathbf{0}_{4 \times 3} & \mathbf{0}_{4\times 1}\\ \\
  \displaystyle \frac{\partial m^I_2(\mathbf{w}_i, \mathbf{p}^0, \boldsymbol{\nu}^0)}{\partial \mathbf{p}'} &
  \displaystyle \frac{\partial m^I_2(\mathbf{w}_i, \mathbf{p}^0, \boldsymbol{\nu}^0)}{\partial \boldsymbol{\nu}'} &
  \mathbf{0}_{4\times 3} & \mathbf{0}_{4\times 1}\\ \\
\mathbf{0}_{2 \times 2} & \mathbf{0}_{2\times 4} & 
\displaystyle \frac{\partial m^E\left( \mathbf{w}_i, \boldsymbol{\alpha}^0, \boldsymbol{\kappa}^0, \theta_1^0 \right)}{\partial \boldsymbol{\kappa}'} & \displaystyle \frac{\partial m^E\left( \mathbf{w}_i, \boldsymbol{\alpha}^0, \boldsymbol{\kappa}^0,\theta_1^0\right)}{\partial \theta_1} 
\end{array}
\right]
\]
where 
\begin{align*}
\mathbb{E}\left[\frac{\partial m_1^I(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'}\right] &= 
\left[
\begin{array}{rrr}
  1 & 0 \\
  -1 & 0 \\
  0 & 1 \\
  0 & -1 \\
\end{array}
\right] \\
\mathbb{E}\left[\frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'} \right]
&= \mathbb{E}\left[
\begin{array}{rr}
  \widetilde{y}_i^2(\alpha_0, z=0) & 0 \\
  -\widetilde{y}_i^2(\alpha_1, z=0) & 0 \\
  0 & \widetilde{y}_i^2(\alpha_0, z=1)\\ 
  0 & -\widetilde{y}_i^2(\alpha_1, z=1)
\end{array}
\right]\\
\mathbb{E}\left[\frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \boldsymbol{\nu}'} \right]
 &= 
 \left[
 \begin{array}{rr}
   Q_1 & \mathbf{0}_{2\times 2}\\
   \mathbf{0}_{2\times 2} & Q_2
 \end{array}
 \right]\\
 Q_1 &=\left[
\begin{array}{rr}
   2\alpha_0\, d(\alpha_0, z=0)& -2(1 - \alpha_0)\, d(\alpha_0, z=0)\\
   2(1 - \alpha_1)\, d(\alpha_1, z=0)& -2\alpha_1\, d(\alpha_1, z=0) \\
\end{array}
\right]\\
Q_2 &= \left[
\begin{array}{cc}
  2 \alpha_0 \, d(\alpha_0, z=1) &
  -2 (1 - \alpha_0) \, d(\alpha_0, z=1)\\
  2 (1 - \alpha_1)\, d(\alpha_1, z=1) &
  -2\alpha_1 \, d(\alpha_1, z=1)
\end{array}
\right] \\
\mathbb{E}\left[\frac{\partial m^E\left( \mathbf{w}_i, \boldsymbol{\alpha}^0,  \boldsymbol{\kappa}^0, \theta_1^0 \right)}{\partial \boldsymbol{\kappa}'}\right] &= \left[
\begin{array}{ccc}
  0 & -q & 0 \\
  0 & 0 & -q
\end{array}
\right]\\ 
\mathbb{E}\left[\frac{\partial m^E\left( \mathbf{w}_i, \boldsymbol{\alpha}^0,  \boldsymbol{\kappa}^0, \theta_1^0 \right)}{\partial \theta_1}\right] &=
\left[
\begin{array}{c}
  \displaystyle\left( \frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1} \right)'\\ 
  \displaystyle\left( \frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1} \right)' 
\end{array}
\right] \mathbb{E}[\mathbf{w}_iz_i]
\end{align*}
and $\alpha_0,\alpha_1$ are understood to be hypothesized values under the null.
Notice that we take derivatives with respect to $\kappa_1$ but these are all equal to zero.

\paragraph{Simpler Special Case: Only First Moment Inequalities}
If we only use the first moment inequalities, the preceding expressions become much simpler since we only need preliminary estimators of $\boldsymbol{\kappa}$ and $\theta_1$.
Hence,
\[
  h = h^E  =  \left[
 \begin{array}{l}
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i - \kappa_1\\
   \boldsymbol{\psi}'_2(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_2\\
   \boldsymbol{\psi}'_3(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_3\\ 
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i z_i - \kappa_1 z_i
 \end{array}
 \right] = \left[
 \begin{array}{l}
   \boldsymbol{\Psi}'(\theta_1, \boldsymbol{\alpha}^0) \mathbf{w}_i - \boldsymbol{\kappa}\\
   \left\{\boldsymbol{\psi}_1'(\theta_1)\mathbf{w}_i - \kappa_1\right\}z_i
 \end{array}
 \right]
\]
and $m' = (m^{I'}_1, m^{E'})$ where
\begin{align*}
m^E &= \left[
  \begin{array}{cc}
    m_2^E \\ m_3^E 
  \end{array}
\right], \quad m_j^E(\mathbf{w}_i, \cdots) = \left[ \boldsymbol{\psi}_j'(\theta_1, \boldsymbol{\alpha}^0) \mathbf{w}_i - \kappa_j \right]z_i  \\
m_1^I(\mathbf{w}_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
  \left[
  \begin{array}{r}
  T(1 - z)/(1 - q) - \alpha_0 \\
  (1 - T)(1 - z)/(1 - q) - \alpha_1  \\
  Tz/q - \alpha_0 \\
  (1 - T)z/q - \alpha_1  
  \end{array}
\right]
\end{align*}
Now $H$ simplifies to
\[
  H = \left[
  \begin{array}{cc}
    \displaystyle \frac{\partial h^E}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial h^E}{\partial \theta_1}
  \end{array}
\right] = \mathbf{R}, \quad \mathbf{R} = \left[
\begin{array}{rrrr}
  -1 & 0 & 0 
  & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & -1 & 0 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & 0 & -1 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   -q & 0 & 0 &  \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}z]\\
\end{array}
\right]
\]
And $M$ becomes
\[
  M = \mathbb{E}\left[ \frac{\partial m}{\partial \boldsymbol{\gamma}'}  \right]= 
  \mathbb{E}\left[
  \begin{array}{rr}
    \displaystyle \frac{\partial m^I_1}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial m^I_1}{\partial \theta_1}\\ \\
    \displaystyle \frac{\partial m^E}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial m^E}{\partial \theta_1}
  \end{array}
\right]  =
  \mathbb{E}\left[
  \begin{array}{rr}

    \mathbf{0}_{4\times 3} & \mathbf{0}_{4\times 1}\\ \\
    \displaystyle \frac{\partial m^E}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial m^E}{\partial \theta_1}
  \end{array}
\right] 
\]
where
\begin{align*}
\mathbb{E}\left[\frac{\partial m^E\left( \mathbf{w}_i, \boldsymbol{\alpha}^0,  \boldsymbol{\kappa}^0, \theta_1^0 \right)}{\partial \boldsymbol{\kappa}'}\right] &= \left[
\begin{array}{ccc}
  0 & -q & 0 \\
  0 & 0 & -q
\end{array}
\right]\\ 
\mathbb{E}\left[\frac{\partial m^E\left( \mathbf{w}_i, \boldsymbol{\alpha}^0,  \boldsymbol{\kappa}^0, \theta_1^0 \right)}{\partial \theta_1}\right] &=
\left[
\begin{array}{c}
  \displaystyle\left( \frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1} \right)'\\ 
  \displaystyle\left( \frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1} \right)' 
\end{array}
\right] \mathbb{E}[\mathbf{w}_iz_i]
\end{align*}
so that $B = -M \mathbf{R}^{-1}$ and $A = \left[
\begin{array}{cc}
\mathbf{I} & B
\end{array}
\right]$.
Finally, accounting for preliminary estimation of $\boldsymbol{\kappa}$ and $\theta_1$, the variance matrix estimator is $\widehat{\Sigma}_n\left(\boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) = A \widehat{\mathcal{V}}_n \left(\boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) A'$ where
\[
  \widehat{\mathcal{V}}_n\left(\boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) = \frac{1}{n}\sum_{i=1}^n 
  \left[
  \begin{array}{c}
    m(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{m}_n(\alpha^0, \widehat{\gamma}(\alpha^0))\\
    h(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{h}_n(\alpha^0, \widehat{\gamma}(\alpha^0))
  \end{array}
\right]
  \left[
  \begin{array}{c}
    m(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{m}_n(\alpha^0, \widehat{\gamma}(\alpha^0))\\
    h(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{h}_n(\alpha^0, \widehat{\gamma}(\alpha^0))
  \end{array}
\right]'
\]
Where, 
\begin{align*}
  m_1^I(\mathbf{w}_i, \boldsymbol{\alpha}^0) &=
  \left[
  \begin{array}{r}
  T_i(1 - z_i)/(1 - q) - \alpha_0 \\
  (1 - T_i)(1 - z_i)/(1 - q) - \alpha_1  \\
  T_iz_i/q - \alpha_0 \\
  (1 - T_i)z/q - \alpha_1  
  \end{array}
\right]\\
  m^E\left(\mathbf{w}_i, \boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) &= 
  \left[
  \begin{array}{c}
 \widehat{\boldsymbol{\psi}}_2'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})z_i \\
   \widehat{\boldsymbol{\psi}}_3'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})z_i 
  \end{array}
\right]\\
  h_E\left(\mathbf{w}_i, \boldsymbol{\alpha}^0, \widehat{\gamma}(\boldsymbol{\alpha}^0)\right) &= \left[
  \begin{array}{l}
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})\\ 
    \widehat{\boldsymbol{\psi}}'_2(\boldsymbol{\alpha}^0) (\mathbf{w}_i - \bar{\mathbf{w}})\\ 
    \widehat{\boldsymbol{\psi}}'_3(\boldsymbol{\alpha}^0) (\mathbf{w}_i - \bar{\mathbf{w}})\\
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})z_i
  \end{array}
\right] = \left[
\begin{array}{l}
  \widehat{\boldsymbol{\Psi}}'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})\\
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})z_i
\end{array}
\right]
\end{align*}

\section{More on Bonferroni -- (2017--07-25)}
The Bonferroni idea we sketched above is implemented as follows.
We first construct a $(1 - \delta_1)\times 100 \%$ joint confidence set for $(\alpha_0, \alpha_1)$ and then project this to get a $(1 - \delta_1)\times 100 \%$ confidence region $\mathscr{S}$ for $s = (1 - \alpha_0 - \alpha_1)$.
We then construct a $(1 - \delta_2)\times 100\%$ confidence interval for $\theta_1$ from the output of the usual IV estimator.
let $(\mbox{LCL}, \mbox{UCL})$ denote this IV confidence interval.
Since
\begin{align*}
  \mathbb{P}\left( \mbox{LCL} \leq \theta_1 \leq \mbox{UCL} \right) &= 1 - \delta_2\\
  \mathbb{P}\left( \mbox{LCL} \leq \frac{\beta}{1 - \alpha_0 - \alpha_1} \leq \mbox{UCL} \right) &= 1 - \delta_2\\
  \mathbb{P}\left( \left[ 1 - \alpha_0 - \alpha_1 \right] \times \mbox{LCL} \leq \beta \leq \left[ 1 - \alpha_0 - \alpha_1 \right] \times \mbox{UCL} \right) &= 1 - \delta_2
\end{align*}
The Bonferroni interval
\[
\left[\inf_{s \in \mathscr{S}} (s \times \mbox{LCL}),\, \sup_{s \in \mathscr{S}} (s \times \mbox{UCL})\right] 
\]
is a valid $(1 - \delta_1 - \delta_2) \times 100\%$ confidence interval for $\beta$.
But we do not actually have to evaluate each $s \in \mathscr{S}$ since the $\inf$ and $\sup$ must clearly occur at one the endpoints of $\mathscr{S}$.
Defining $\underbar{s} =\inf \mathscr{S}$ and $\bar{s} = \sup \mathscr{S}$, the Bonferroni interval is equivalent to
\[
  \left[\min\left\{\underbar{s} \times \mbox{LCL}, \bar{s} \times \mbox{LCL}  \right\}, \; 
\max\left\{\underbar{s} \times \mbox{UCL}, \bar{s} \times \mbox{UCL}  \right\} \right]
\]

The Bonferroni interval above has several advantages over projection inference for $\beta$ based on a joint confidence set for $(\alpha_0, \alpha_1, \beta)$.
These should make it quite appealing for practitioners.
First, if a standard $(1 - \delta_1 - \delta_2)\times 100\%$ IV confidence interval for $\theta_1$ includes zero, so does the Bonferroni interval. 
Recall that the standard IV test of $\theta_1 = 0$ is also a valid test of $\beta = 0$ at the same significance level. 
This means that we will never be in a situation where the higher moment equalities lead us to reject $\beta = 0$ when we would have been unable to reject $\beta = 0$ based on first moments alone.
In contrast, even if $\mbox{Cov}(y,z)$ were exactly equal to zero, the full projection-based inference for $\beta$ could lead us to reject $\beta=0$ if $\mbox{Cov}(y^2,z)\neq 0$ or $\mbox{Cov}(y^3,z) \neq 0$.
Given that lower moments of $y$ should be more precisely estimated, a rejection of this form would strongly suggest that something is wrong with our model, for example that the higher moment assumptions for the instrument are incorrect.
The Bonferroni procedure is immune to this problem because it can only reject $\beta = 0$ based on first moment information.
When the first moment information conflicts with the higher moment information, we prefer the former.

Second, projection necessarily involves some degree of conservatism.\footnote{Although I suppose we could always consider going back to look at BCS or KMS\ldots}
Our procedure projects from a two-dimensional space to a one-dimensional space -- we convert a confidence region for $(\alpha_0, \alpha_1)$ to a confidence interval for $(1 - \alpha_0 - \alpha_1)$ -- while the alternative projects form a three-dimensional space to a one-dimensional space.
Thus we would generally expect the projection conservatism to be less severe in our procedure.
Bonferroni, of course, introduces its own source of conservatism.
Often, however, the alternative hypothesis of greatest interest is $\beta = 0$.
Our Bonferroni procedure rejects this hypothesis if and only if a standard IV test of $\beta=0$ does so at the $(1 - \delta_2)$ so the source of lower power is both quite transparent and under our direct control via the choice of $\delta_2$ relative to $\delta_1$.
Compare this to the full three-dimensional projection, in which we gain power from the higher moment conditions but lose power from the dimension reduction, making the overall result less clear.

Third, our Bonferroni procedure is very simple computationally.
Constructing a grid of values over which to carry out projection is already very costly, though feasible, in three dimensions.
The alternative is to trust the results of a derivative-free constrained optimizer on a problem that can be highly non-smooth.
Note that the parameter space for $\beta$ is \emph{unbounded}.
In contrast, searching over a grid for $(\alpha_0, \alpha_1)$ in two dimensions is easy, particularly since the parameter space is bounded: $\alpha_0, \alpha_1>0$ and we have assumed $\alpha_0 + \alpha_1 < 1$.



\section{BBS/FL/Mahajan MCs -- 2017-07-28}

\paragraph{A Better Parameterization}
It turns out to be much simpler if we use a different parameterization from the one given in the argument from above.
Define, instead, the following quantities:
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \kappa_1 &= c - \alpha_0 \theta_1 \\
  \theta_2 &= -\theta_1 \alpha_0(1 - \alpha_1)\\
  \theta_3 &= \theta_1 (1 + \alpha_0 - \alpha_1)
\end{align*}
Notice that $\theta_1$ and $\kappa_1$ are \emph{exactly} as in our moment equations but the other quantities differ.
Using these definitions, the moment conditions from above become
\[
  \mathbb{E}\left\{ \left[
    \begin{array}{c}
    y - \kappa_1 - \theta_1 T\\
    (y - \kappa_1) T - \theta_2 - \theta_3 T
  \end{array}
\right]\otimes
\left[
\begin{array}{c}
1 \\ z
\end{array}
\right]\right\} = \mathbf{0}
\]
Note that these moment conditions are completely uninformative about $(\alpha_0, \alpha_1)$ when $\beta = 0$.

\paragraph{The Method of Moments Estimator}
There is a closed-form for the GMM estimator in this example. 
To calculate it, we first need to solve the moment equations from the preceding paragraph.
To this end, define:
\begin{align*}
  A \equiv \theta_3/\theta_1 &= 1 + \alpha_0 - \alpha_1 \\
  B \equiv -\theta_2/\theta_1 &= \alpha_0 (1 - - \alpha_1)
\end{align*}
Then, eliminating $(1 - \alpha_1)$ and $\alpha_0$, respectively, we obtain:
\begin{align*}
  \alpha_0^2 - A\alpha_0 + B &= 0\\
  (1 - \alpha_1)^2 - A(1 - \alpha_1) + B &= 0
\end{align*}
These are exactly the same quadratic, namely $x^2 - Ax + B = 0$ so one root of this equation is $\alpha_0$ while the other is $(1 - \alpha_1)$.
The discriminant is
\begin{align*}
  D &= A^2 - 4 B = \left( \frac{\theta_3}{\theta_1} \right)^2 + 4\left(\frac{\theta_2}{\theta_1} \right) = \left[ (1 - \alpha_1) + \alpha_0 \right]^2 - 4\left[ \alpha_0 (1 - \alpha_1) \right]\\
  &= (1 - \alpha_0 - \alpha_1)^2
\end{align*}
so that both roots are real as long as $\alpha_0 + \alpha_1 \neq 0$.
To solve for $\alpha_0$ and $\alpha_1$ we need to calculate the roots of $x^2 - Ax + B =0$, namely
\[
  x = \frac{1}{2}\left( A \pm \sqrt{D} \right)
\]
One of these roots is $\alpha_0$ and the other is $1 - \alpha_1$.
By assumption, however, $\alpha_0 + \alpha_1 < 1$ and thus $\alpha_0 < 1 - \alpha_1$ so we know that the smaller of the two roots is $\alpha_0$ and the larger is $1 - \alpha_1$.

We can now estimate $\vartheta = (\alpha_0, \alpha_1, \theta)$.
The procedure is as follows.
First, use the standard IV estimator for $\kappa_1$ and $\theta_1$, namely
\begin{align*}
  \widehat{\theta}_1 &= \mbox{Cov}(y,z)/\mbox{Cov}(T,z)\\
  \widehat{\kappa}_1 &= \overline{y} - \widehat{\theta}_1 \overline{T}
\end{align*}
Then substitute $\widehat{\kappa}_1$ into the remaining moment conditions and again use the standard IV estimator treating $(y - \widehat{\kappa}_1)T$ as the outcome variable:
\begin{align*}
  \widehat{\theta}_3 &= \mbox{Cov}\left[ (y-\widehat{\kappa}_1)T, z \right]/ \mbox{Cov}(T, z)\\
  \widehat{\theta}_2 &= (\overline{yT} - \widehat{\kappa}_1 \overline{T}) - \widehat{\theta}_3 \overline{T}  
\end{align*}
These estimators make the sample analogue of the GMM moment conditions \emph{exactly zero}.
To estimate $\beta$ we use the fact that $\beta = \theta_1 (1 - \alpha_0 - \alpha_1)$:
\[
  \widehat{\beta} =  \widehat{\theta}_1 \sqrt{\widehat{D}} = \widehat{\theta}_1 \sqrt{\left( \frac{\widehat{\theta}_3}{\widehat{\theta}_1} \right)^2 + 4 \left( \frac{\widehat{\theta}_2}{\widehat{\theta}_1} \right)} = \mbox{sign}(\widehat{\theta}_1) \sqrt{\left(\widehat{\theta}_3\right)^2 +4 \widehat{\theta}_1 \widehat{\theta}_2}
\]
where the final equality comes from the fact that, since $\alpha_0 + \alpha_1 < 1$, the sign of $\theta_1$ equals that of $\beta$.
To estimate $\alpha_0$ and $\alpha_1$ we calculate
\begin{align*}
  \widehat{r}_1 &= \frac{1}{2}\left( \widehat{A} + \sqrt{\widehat{D}} \right)\\
  \widehat{r}_2 &= \frac{1}{2}\left( \widehat{A} - \sqrt{\widehat{D}} \right)\\
  \widehat{A} &= \widehat{\theta}_3 / \widehat{\theta}_1\\
  \widehat{D} &= (\widehat{\theta}_3 / \widehat{\theta}_1)^2 + 4\widehat{\theta}_2/\widehat{\theta}_1
\end{align*}
and set $\widehat{\alpha}_0 = \min\left\{ \widehat{r}_1, \widehat{r}_2 \right\}$ and $\widehat{\alpha}_1 = \max\left\{ \widehat{r}_1, \widehat{r}_2 \right\}$.

\paragraph{Boundary Problems Etc.}
The above procedure will always yields estimates for $\theta_1, \kappa_1, \theta_2$ and $\theta_3$.
The estimators of $\beta, \alpha_0$ and $\alpha_1$, on the other hand, do not exist when $\widehat{D} < 0$, or equivalently when 
\[
  (\widehat{\theta}_3 / \widehat{\theta}_1)^2 + 4\widehat{\theta}_2/\widehat{\theta}_1 < 0
\]
While the first term in this sum is always positive, the second can easily be negative.
This could result either from sampling variation or model mis-specification.
But even when $\widehat{D} < 0$, the proposed estimators do not enforce the constraints $0 \leq \alpha_0 \leq 1$, $0 \leq \alpha_1 \leq 1$ and $\alpha_0 + \alpha_1 < 1$.
In our simulation study we will need to decide how to proceed when $\widehat{D}$ is negative or any of these constraints is violated.

\paragraph{What do other papers say about this problem?}
BBS, Mahajan and Lewbel don't say anything about the non-standard inference problem.
KRS use a non-linear GMM estimator that constrains $\alpha_0, \alpha_1$ to be between zero and one, but don't account for this in their inference procedure.
Frazis and Loewenstein point out the boundary value problem in relation both to the requirement that $\alpha_0$ and $\alpha_1$ should be valid probabilities and in relation to certain percentile bounds that they derive in their paper.
The percentile bounds use an exogenous regressor $X$ and rely on the fact that $\alpha_0 \leq \mathbb{P}(T=1|X) \leq 1 - \alpha_1$ for all $X$ under the assumption that mis-classification is unrelated to $X$.
This allows one to look at covariate ``bins'' $S$ for which $P(T=1|X\in S)$ is very large or very small and use this information to get tighter bounds for the mis-classification probabilities.
This is essentially a simple non-parametric version of the maximum likelihood approach used by Hausman, Abrevaya and Scott-Morton.
Note that these bounds are unavailable to us if we wish to allow $X$ to affect the mis-classification probabilities.
Frazis and Loewenstein propose to carry out inference that respects both the parameter space restrictions for $\alpha_0$ and $\alpha_1$ along with their percentile bounds in two ways.
In the first they employ a pseudo-Bayesian approach adapted from a paper by Geweke.
In this procedure, they estimate the unrestricted GMM estimator $\widehat{\Pi}$ of the \emph{reduced form parameters} along with its associated asymptotic variance matrix estimator $V(\widehat{\Pi})$.
They then make draws from a $N(\widehat{\Pi}, V(\widehat{\Pi}))$ distribution and discard those that violate $0 \leq \alpha_0 \leq 1$, $0 \leq \alpha_1 \leq 1$ or their estimated percentile bounds.
The draws that remain are treated as a posterior for the purposes of estimation and inference.
They also propose a specification test of their model via the inequality restrictions.
In particular, they determine the least-favorable critical value for a test of the null hypothesis that none of their inequalities is violated.
There doesn't appear to be anything wrong with the specification test but the inference procedure based on Geweke seems quite ad-hoc and they do not argue for why we should expect this to give valid frequentist inference.
They don't provide any simulation evidence either.
It's actually a pretty interesting idea though and we may want to try it out.


\paragraph{Standard GMM Inference}
We will show that the usual GMM inference based on the estimator we have just defined can break down very badly in this model.
To do so, we'll need the GMM asymptotic variance matrix estimator.
This in turn requires some derivatives.
Define $\boldsymbol{\vartheta}' = (\alpha_0, \alpha_1, \beta)$ and
\[
  g = \left[
  \begin{array}{c}
    y - \kappa_1 - \theta_1 T\\
    (y - \kappa_1 - \theta_1 T)z
  \end{array}
\right], \quad
G = \mathbb{E}
\left[
\begin{array}{cc}
  \nabla_{\vartheta}' g & \nabla_{\kappa_1}' g
\end{array}
\right]
\]
and
\[
  h = \left[
  \begin{array}{c}
    (y - \kappa_1)T - \theta_2 - \theta_3 T\\
    \left\{  (y - \kappa_1)T - \theta_2 - \theta_3 T \right\} z
  \end{array}
\right], \quad
H = \mathbb{E}
\left[
\begin{array}{cc}
  \nabla_{\vartheta}' h & \nabla_{\kappa_1}' h
\end{array}
\right]
\]
and let $f = (f', g')'$ and $F = (G', H')'$.
Then required derivative matrices are
\[
  G = 
  \left[
  \begin{array}{cc}
    -\left(\displaystyle \frac{\partial \theta_1}{\partial\boldsymbol{\vartheta}'}  \right)\mathbb{E}[T] & -1\\ \\
    -\left( \displaystyle \frac{\partial \theta_1}{\partial\boldsymbol{\vartheta}'}  \right)\mathbb{E}[Tz] & -\mathbb{E}[z]
  \end{array}
\right] = 
  \left[
  \begin{array}{cc}
    -p\left(\displaystyle \frac{\partial \theta_1}{\partial\boldsymbol{\vartheta}'}  \right) & -1\\ \\
    -p_1q\left( \displaystyle \frac{\partial \theta_1}{\partial\boldsymbol{\vartheta}'}  \right) & -q
  \end{array}
\right]
\]
and 
\[
  H = 
  \left[
  \begin{array}{cc}
    -\left\{ \displaystyle \frac{\partial \theta_2}{\partial \boldsymbol{\vartheta}'} +  \frac{\partial \theta_3}{\partial \boldsymbol{\vartheta}'} \mathbb{E}[T]\right\}& - \mathbb{E}[T] \\ \\
    -\left\{ \displaystyle \frac{\partial \theta_2}{\partial \boldsymbol{\vartheta}'} \mathbb{E}[z] +  \frac{\partial \theta_3}{\partial \boldsymbol{\vartheta}'} \mathbb{E}[Tz]\right\}& - \mathbb{E}[Tz] 
  \end{array}
\right] = 
  \left[
  \begin{array}{cc}
    -\left\{ \displaystyle \frac{\partial \theta_2}{\partial \boldsymbol{\vartheta}'} +  p\left(\frac{\partial \theta_3}{\partial \boldsymbol{\vartheta}'}\right) \right\}& -p \\ \\
    -\left\{ \displaystyle q\left(\frac{\partial \theta_2}{\partial \boldsymbol{\vartheta}'} \right) +  p_1 q\left(\frac{\partial \theta_3}{\partial \boldsymbol{\vartheta}'}\right) \right\}& -p_1 q
  \end{array}
\right]
\]
where the derivatives that enter these expressions are
\begin{align*}
  \frac{\partial \theta_1}{\partial \boldsymbol{\vartheta}'} &= \left[
  \begin{array}{ccc}
    \displaystyle \frac{\beta}{(1 - \alpha_0 - \alpha_1)^2} &
    \displaystyle \frac{\beta}{(1 - \alpha_0 - \alpha_1)^2} &
    \displaystyle \frac{1}{1 - \alpha_0 - \alpha_1}
  \end{array}
\right] =\frac{1}{1 - \alpha_0 - \alpha_1} \left[
\begin{array}{ccc}
  \theta_1 & \theta_1 & 1
\end{array}
\right]\\
  \frac{\partial \theta_2}{\partial \boldsymbol{\vartheta}'} &= 
  -\alpha_0(1 - \alpha_1) \left( \frac{\partial \theta_1}{\partial \boldsymbol{\vartheta}'} \right) + \left[
  \begin{array}{ccc}
    -\theta_1(1 - \alpha_1) &
    \theta_1 \alpha_0 & 
    0
  \end{array}
\right]\\
  \frac{\partial \theta_3}{\partial \boldsymbol{\vartheta}'} &= (1 + \alpha_0 - \alpha_1) \left( \frac{\partial \theta_1}{\partial \boldsymbol{\vartheta}'} \right) + \left[
  \begin{array}{ccc}
    \theta_1 & - \theta_1 & 0
  \end{array}
\right]
\end{align*}

Now, because we have a just-identified system of moment equalities, standard results for GMM imply that the method of moments estimator of the full parameter vector $\boldsymbol{\gamma} = (\alpha_0, \alpha_1, \beta, \kappa_1)$ has the following large sample distribution:
\[
  \sqrt{n}\left( \widehat{\gamma} - \gamma \right) \rightarrow_d -F^{-1} M, \quad \sqrt{n}f_n(\gamma) \rightarrow_d M
\]
where $M \sim N(0, \Omega)$ and
\[
  \Omega = \lim_{n\rightarrow \infty}\mbox{Var}\left[ n^{-1/2}\sum_{i=1}^n f(\mathbf{w}_i; \gamma) \right]
\]
Under the assumption of iid observations, the preceding simplifies to $\Omega = \mbox{Var}\left[ f(\mathbf{w}_i;\gamma) \right]$.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Derivatives for GMM -- 2017-07-28}
We will show that the usual GMM estimator of our just-identified system of moment equalities is badly-behaved when $\beta$ is small.
To do this, we require some derivatives that will allow us to calculate the asymptotic variance of the GMM estimator.

As above, define the following quantities:
\begin{align*}
  \mathbf{w}' &= (T, y, yT, y^2, y^2 T, y^3) \\
  \boldsymbol{\Psi} &= \left[
  \begin{array}{ccc}
    \psi_1 & \psi_2 & \psi_3
  \end{array}
\right]\\
  \psi_1' &= (-\theta_1, 1, 0, 0, 0, 0)\\
  \psi_2' &= (\theta_2, 0, -2\theta_1, 1, 0, 0)\\
  \psi_3' &= (-\theta_3, 0, 3\theta_2, 0, -3\theta_1, 1)\\
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 (1 + \alpha_0 - \alpha_1) \\
  \theta_3 &= \theta_1^3 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0(1 - \alpha_1) \right]
\end{align*}
As in the preceding section, let $\boldsymbol{\vartheta}' = (\alpha_0, \alpha_1, \beta)$, but define the blocks of moment functions as follows:
\[
  f = \left[
  \begin{array}{c}
  g(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\kappa})\\
    h(\mathbf{w}_i, \boldsymbol{\vartheta}, \boldsymbol{\kappa})
  \end{array}
\right] = 
\left[
\begin{array}{c}
  \left\{\boldsymbol{\Psi}(\boldsymbol{\vartheta})'\mathbf{w}_i - \boldsymbol{\kappa}\right\} z_i \\
  \boldsymbol{\Psi}(\boldsymbol{\vartheta})'\mathbf{w}_i - \boldsymbol{\kappa} 
\end{array}
\right]
\]
and 
\[
  F = \mathbb{E}[\nabla f ] = \mathbb{E}\left[
  \begin{array}{cc}
    \nabla_{\vartheta'} g & \nabla_{\kappa'} g \\
    \nabla_{\vartheta'} h & \nabla_{\kappa'} h
  \end{array}
\right] = 
\left[
\begin{array}{cc}
  G_{\vartheta} & -q\mathbb{I} \\
  H_{\vartheta} & -\mathbb{I}
\end{array}
\right]
\]
Now, defining
\[
  D^\Psi = \left[
  \begin{array}{c}
    \frac{\partial\psi_1}{\partial \boldsymbol{\vartheta}'}\\
    \frac{\partial\psi_2}{\partial \boldsymbol{\vartheta}'}\\
    \frac{\partial\psi_3}{\partial \boldsymbol{\vartheta}'}
  \end{array}
\right]
\]
we see that
\[
  G_\vartheta = \mathbb{E}[\mathbf{w}_i z_i]' D^\Psi, \quad H_\vartheta = \mathbb{E}[\mathbf{w}_i]' D^\Psi
\]
The remaining derivatives needed to construct these objects are as follows:
\[
  \frac{\partial \psi_1}{\partial \boldsymbol{\vartheta}'} = \left[
  \begin{array}{r}
    \displaystyle -\frac{\partial\theta_1}{\partial \boldsymbol{\vartheta}'}\\ \\
    \mathbf{0}_{5\times 3}
  \end{array}
\right], \quad
  \frac{\partial \psi_2}{\partial \boldsymbol{\vartheta}'} = \left[
  \begin{array}{r}
    \displaystyle \frac{\partial\theta_2}{\partial \boldsymbol{\vartheta}'}\\  \\
    \mathbf{0}_{1\times 3}\\ \\ 
    \displaystyle -2\frac{\partial\theta_1}{\partial \boldsymbol{\vartheta}'}\\ \\
    \mathbf{0}_{3\times 3}
  \end{array}
\right], \quad
  \frac{\partial \psi_3}{\partial \boldsymbol{\vartheta}'} = \left[
  \begin{array}{r}
    -\displaystyle \frac{\partial\theta_3}{\partial \boldsymbol{\vartheta}'}\\  \\
    \mathbf{0}_{1\times 3}\\ \\ 
    \displaystyle 3\frac{\partial\theta_2}{\partial \boldsymbol{\vartheta}'}\\ \\
    \mathbf{0}_{1\times 3} \\ \\ 
    \displaystyle -3\frac{\partial\theta_1}{\partial \boldsymbol{\vartheta}'}\\ \\
    \mathbf{0}_{1\times 3} 
  \end{array}
\right]
\]

\begin{align*}
  \frac{\partial \theta_1}{\partial \boldsymbol{\vartheta}'} &= 
  \left[
  \begin{array}{ccc}
    \displaystyle \frac{\beta}{(1 - \alpha_0 - \alpha_1)^2} &
    \displaystyle \frac{\beta}{(1 - \alpha_0 - \alpha_1)^2} &
    \displaystyle \frac{1}{1 - \alpha_0 - \alpha_1}
  \end{array}
\right] = \frac{1}{1 - \alpha_0 - \alpha_1}\left[
\begin{array}{ccc}
  \theta_1 & \theta_1 & 1
\end{array}
\right]\\ \\ 
  \frac{\partial \theta_2}{\partial \boldsymbol{\vartheta}'} &= 2\theta_1(1 + \alpha_0 - \alpha_1) \left( \frac{\partial \theta_1}{\partial\boldsymbol{\vartheta}'} \right) +  \theta_1^2 \left[
  \begin{array}{ccc}
    1& -1 & 0
  \end{array}
\right] \\ \\ 
  \frac{\partial \theta_3}{\partial \boldsymbol{\vartheta}'} &= 3\theta_1^2 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1) \right] \left( \frac{\partial \theta_1}{\partial \boldsymbol{\vartheta}'} \right) +  \theta_1^3 \mathbf{v}'\\
  \mathbf{v}' &= \left[
  \begin{array}{ccc}
    \left\{-2(1 - \alpha_0 - \alpha_1) + 6(1 - \alpha_1)\right\} & \left\{-2(1 - \alpha_0 - \alpha_1) - 6\alpha_0\right\} & 0
  \end{array}
\right]
\end{align*}

Now, because we have a just-identified system of moment equalities, standard results for GMM imply that the method of moments estimator of the full parameter vector $\boldsymbol{\gamma} = (\alpha_0, \alpha_1, \beta, \kappa_1)$ has the following large sample distribution:
\[
  \sqrt{n}\left( \widehat{\gamma} - \gamma \right) \rightarrow_d -F^{-1} M, \quad \sqrt{n}f_n(\gamma) \rightarrow_d M
\]
where $M \sim N(0, \Omega)$ and
\[
  \Omega = \lim_{n\rightarrow \infty}\mbox{Var}\left[ n^{-1/2}\sum_{i=1}^n f(\mathbf{w}_i; \gamma) \right]
\]
Under the assumption of iid observations, the preceding simplifies to $\Omega = \mbox{Var}\left[ f(\mathbf{w}_i;\gamma) \right]$.

\section{What to include in our simulations? 2017-07-29}
\begin{enumerate}
  \item We need a ``baseline'' GMM procedure. It's difficult to decide what this should be. On the one hand we want it to be fairly simple. On the other hand, we don't want it to be a ``straw man.''
    Here's a relatively simple idea that avoids full non-linear GMM and is broadly comparable to our Bonferroni GMS procedure (in that we use preliminary estimators along with the inequalities)
    \begin{enumerate}
      \item Calculate the unconstrained method of moment estimator if it exists.
      \item If the unconstrained estimator fails to exist or does not satisfy the bounds, then fix $\theta_1$ and $\kappa$ at the GMM estimates and minimize the sum of squares of the profile GMM criterion over $\alpha_0, \alpha_1$ subject to the constraint.
      \item Report standard GMM inference conditional on the estimated parameters, regardless of whether the estimates are the unconstrained solultion or the constrained solution.
    \end{enumerate}
  \item There are multiple ways to carry out our GMS procedure: Bonferroni vs full projection, first moment inequalities, second moment inequalities, CDF inequalities, etc.
    There are many directions we could go, but some of these are really extensions.
    Perhaps the baseline should be first moment inequalities only.
  \item If we can show that the Bonferroni idea is not dominated by the full projection idea, then I think we can dispense with the full projection procedure.
  \item I like the Geweke idea used by Frazis and Loewenstein now that I understand it, but I don't have any intuition about how well it can be expected to perform in practice.
    This should be fairly easy to code since it doesn't rely on any of those ``nasty'' derivatives of the $\theta$ parameters with respect to $\alpha_0, \alpha_1$ and $\beta$.
    The basic idea would be as follows:
    \begin{enumerate}
      \item Calculate the GMM estimates of the reduced form parameters $\theta$ and $\kappa$ and the associated asymptotic variance matrix estimator. 
        Call the vector of reduced form estimates $\widehat{\Pi}$ and the associated asymptotic variance matrix $V(\widehat{\Pi})$.
      \item  Make a large number of draws $\Pi^{(1)}, \cdots, \Pi^{(N)}$ from a $N\left(\widehat{\Pi}, V(\widehat{\Pi})\right)$ distribution.
      \item Keep only those draws $\Pi^{(i)}$ that satisfy the constraints and restrictions on the parameter space.
        This could potentially be a very small number of draws or perhaps even zero draws.
      \item For each draw that satisfies the bounds, solve for the structural parameters $\alpha_0, \alpha_1, \beta$.
        Treat the result as a posterior draw for the structural parameters.
    \end{enumerate}
\end{enumerate}


\section{CDF Bounds: 2017--08--03}
In our simulation experiments thus far, we have found that using the second moment inequalities doesn't seem to help very much if at all.
This seems to be because these bounds are much less precisely estimated than the ``weak'' bounds.
But if we use only the weak bounds, we are not actually using the non-differential measurement error assumption in any of our inequality conditions.
We now know that the FL/Mahajan/Lewbel non-differential measurement error assumption \emph{does} yield partially identifying information.
However this information is difficult to express in terms of unconditional moment equalities because it involves conditional expectations of indicator functions whose arguments are quantiles of the data that are in turn functions of $\alpha_0$ and $\alpha_1$.

If we instead impose the stronger assumption that $\varepsilon$ is conditionally independent of $T$ given $(T^*,z)$ we obtain bounds that are much easier to use.
Note that FL/Mahajan/Lewbel make a mean assumption precisely because this is what they need for identification.
But it is difficult to imagine a situation in which one is willing to make the mean assumption without actually believing the stronger independence assumption. 
Let $F_{tk}$ denote the conditional CDF of $Y$ given $(T=t,z=k)$ and define $F^*_{t^*k}$ analogously given $(T^*=t^*,z=k)$.
Using the this independence condition along with Bayes' rule and the law of total probability, 
\begin{align*}
  F_{0k}(\tau) &= \left[ \frac{(1 - \alpha_0)(1 - p_k^*)}{1 - p_k} \right] F_{0k}^*(\tau) + \left[ \frac{\alpha_1 p_k^*}{1 - p_k} \right] F_{1k}^*(\tau) \\
  F_{1k}(\tau) &= \left[ \frac{\alpha_0(1 - p_k^*)}{p_k} \right] F_{0k}^*(\tau) + \left[ \frac{(1 - \alpha_1) p_k^*}{p_k} \right] F_{1k}^*(\tau) 
\end{align*}
for all $\tau$ and all $k$.
Solving this system, we find that
\begin{align*}
  F_{0k}^*(\tau) &= \left[ \frac{(1 - p_k)(1 - \alpha_1)}{1 - p_k - \alpha_1} \right] F_{0k}(\tau) - \left[ \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right] F_{1k}(\tau) \\
  F_{1k}^*(\tau) &= -\left[ \frac{\alpha_0(1 - p_k)}{p_k - \alpha_0} \right] F_{0k}(\tau) + \left[ \frac{(1 - \alpha_0)p_k}{p_k - \alpha_0} \right] F_{1k}(\tau)
\end{align*}
Since CDFs must be non-decreasing and bounded between $0$ and $1$, these equalities impose restrictions on $(\alpha_0, \alpha_1)$ given the observed data.
It is difficult to work with the first restriction but easy to work with the second.
As our model is point identified, we need not use all the restrictions: the only point of including these moment inequalities is to stabilize the estimator and provide better inference in small samples.
After some algebra, we see that $0\leq F^*_{tk}(\tau) \leq 1$ for all $\tau$ if and only if
\begin{align*}
  (1 - p_k) F_{0k}(\tau) - \alpha_1 F_k(\tau) &\geq 0\\
  p_k F_{1k}(\tau) - \alpha_0 F_k(\tau) &\geq 0 \\
(1 - p_k - \alpha_1) - \left[(1 - p_k) F_{0k}(\tau) - \alpha_1 F_k(\tau)\right] &\geq 0 \\
  (p_k - \alpha_0) - \left[p_k F_{1k}(\tau) - \alpha_0 F_k(\tau)\right] &\geq 0
\end{align*}
for all $\tau$ and $k$, where we define $F_k(\tau) = \mathbb{P}(y \leq \tau|z=k)$.
After yet more algebra, we can write these as unconditional moment restrictions as follows:
\begin{align*}
  \mathbb{E}\left[ \mathbf{1}(y\leq \tau) \mathbf{1}(z=k)(1 - T - \alpha_1) \right] &\geq 0\\
  \mathbb{E}\left[ \mathbf{1}(y > \tau) \mathbf{1}(z=k)(1 - T - \alpha_1) \right] &\geq 0\\
  \mathbb{E}\left[ \mathbf{1}(y\leq \tau) \mathbf{1}(z=k)(T - \alpha_0) \right] &\geq 0\\
  \mathbb{E}\left[ \mathbf{1}(y > \tau) \mathbf{1}(z=k)(T-\alpha_0) \right] &\geq 0
\end{align*}
for all $\tau$ and all $k$.
With a binary $z$ we obtain 8 inequalities for each quantile $\tau$.
Notice that none of these requires any preliminary estimators and, moreover, that the inequalities involve only $\alpha_0$ and $\alpha_1$.
Thus, including these in the GMS procedure is fairly straightforward.
Moreover, we can simplify our notation for the first moment inequalities and avoid the awkward point about conditioning on $q$ in one fell swoop!
To this end, define
\[
  m_1^I(\mathbf{w}_i, \alpha_0, \alpha_1) = \left[
  \begin{array}{l}
    (1 - z_i)(T_i - \alpha_0) \\
    (1 - z_i)(1 - T_i - \alpha_1) \\
    z_i(T - \alpha_0) \\
    z_i (1 - T_i - \alpha_1) \\
  \end{array}
\right]
\]
Note that the preceding contains \emph{exactly the same information} as our earlier definition of the first moment inequalities from above. 
By multiplying both sides of the inequalities by $q$ or $(1 - q)$, respectively, we can completely eliminate the need for $q$.
Now, we can construct the CDF bounds simply by pre-multiplying by $\mathbf{1}(y_i\leq \tau)$ or $\mathbf{1}(y_i > \tau)$.
In other words: \emph{the first moment inequalities are a special case of the CDF bounds!} 
To obtain them from the CDF bounds either simply choose a $\tau$ that is strictly greater than or equal to all of the observed $y_i$ and multiply $m_1^I$ by $\mathbf{1}(y_i \leq \tau)$ or choose a $\tau$ that is strictly \emph{less} than all of the observed $y_i$ and multiply $m_1^I$ by $\mathbf{1}(y_i > \tau)$.

\section{Final Version of GMS -- 2017--08-11}
Use the first-moment bounds for $(\alpha_0, \alpha_1)$ along with the restrictions implied by non-differential measurement error.
Pre-estimate $\theta_1$, $\kappa$, and quantiles of the distribution of $y$ that enter the non-differential measurement error bounds.

\paragraph{Unconditional Moment Inequalities from Non-differential Measurement Error}

First define 
\[
  \mu_k(\alpha_0) = \left( \frac{1}{p_k - \alpha_0} \right) \left[ p_k \mathbb{E}(y|T=1,z_k) - \alpha_0 \mathbb{E}(y|z_k \right]
\]
This equals $\mathbb{E}(y|T^*=1,z_k)$ under non-differential measurement error, provided we evaluate at the true value of $\alpha_0$. 
By iterated expectations,
\[
  \mu_k(\alpha_0) = \left[ \frac{1}{(p_k - \alpha_0)\mathbb{P}(z_k)} \right] \mathbb{E}\left[ y\mathbf{1}(z=k)(T - \alpha_0) \right]
\]
Similarly, define
\begin{align*}
  \underline{\mu}_{tk}(\alpha_0, \alpha_1) &= \mathbb{E}\left[ y|y\leq \underline{q}_{tk}, T=t, z=k\right] \\
  \overline{\mu}_{tk}(\alpha_0, \alpha_1) &= \mathbb{E}\left[ y|y > \overline{q}_{tk}, T=t, z=k\right]
\end{align*}
for $0 < r_{tk} < 1$, where $F_{tk}$ is the conditional CDF of $y|(T=t,z_k)$ and
\begin{align*}
  \underline{q}_{tk} &= F^{-1}_{tk}(r_{tk})\\
  \overline{q}_{tk} &= F^{-1}_{tk}(1 - r_{tk})\\
  r_{0k} &= \frac{\alpha_1}{1 - p_k} \left( \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)\\
  r_{1k} &= \frac{1 - \alpha_1}{p_k} \left( \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)
\end{align*}
When $r_{tk} = 0$, neither $\underline{\mu}_{tk}$ nor $\overline{\mu}_{tk}$ is well-defined.
When $r_{tk} = 1$, both quantities are well-defined but $\underline{\mu}_{tk} = \overline{\mu}_{tk} = \mathbb{E}(y|T=t, z=k)$.
By iterated expectations,
\begin{align*}
  \underline{\mu}_{tk}(\alpha_0, \alpha_1) &= \frac{\mathbb{E}\left[ y \mathbf{1}(y \leq \underline{q}_{tk})\mathbf{1}(T=t)\mathbf{1}(z=k) \right]}{r_{tk}\mathbb{P}(T=t|z=k) \mathbb{P}(z=k)}\\ \\
  \overline{\mu}_{tk}(\alpha_0, \alpha_1) &= \frac{\mathbb{E}\left[ y \mathbf{1}\left(y > \overline{q}_{tk}\right)\mathbf{1}(T=t)\mathbf{1}(z=k) \right]}{r_{tk}\mathbb{P}(T=t|z=k) \mathbb{P}(z=k)}
\end{align*}
The moment inequalities are
\begin{align*}
  m_{2,tk}^I(W, \boldsymbol{\alpha}, \mathbf{q}) = \left[
  \begin{array}{c}
  \mu_k(\alpha_0) - \underline{\mu}_{tk}(\alpha_0, \alpha_1) \\
  \overline{\mu}_{tk}(\alpha_0, \alpha_1) - \mu_k(\alpha_0)  
  \end{array}
\right] \geq \mathbf{0}
\end{align*}
for each $(t,k) \in \left\{ 0,1 \right\}\times \left\{ 0,1 \right\}$.
We stack these conditions as follows:
\[
m^I_2(W, \boldsymbol{\alpha}, \mathbf{q}) = \left[
\begin{array}{c}
  m^I_{2,00}(W,\boldsymbol{\alpha},\mathbf{q})\\
  m^I_{2,10}(W,\boldsymbol{\alpha},\mathbf{q})\\
  m^I_{2,01}(W,\boldsymbol{\alpha},\mathbf{q})\\
  m^I_{2,11}(W,\boldsymbol{\alpha},\mathbf{q}) \\
\end{array}
\right]
\]
Simplifying, we find that for $t=0$
\begin{align*}
  \mu_k(\alpha_0) - \underline{\mu}_{0k}(\alpha_0, \alpha_1) &= \mathbb{E}\left[ y \mathbf{1}\left( z=k \right)(T - \alpha_0) - \left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right) y \mathbf{1}(y \leq \underline{q}_{0k}) \mathbf{1}(z=k) (1 - T)\right]\\
  \overline{\mu}_{0k}(\alpha_0, \alpha_1) - \mu_k(\alpha_0) &=
  \mathbb{E}\left[ y \mathbf{1}(y > \overline{q}_{0k})\mathbf{1}(z=k) (1 - T) \left( \frac{1 - \alpha_0 - \alpha_1}{\alpha_1} \right) - y \mathbf{1}(z=k)(T - \alpha_0) \right]
\end{align*}
and for $t=1$
\begin{align*}
  \mu_k(\alpha_0) - \underline{\mu}_{1k}(\alpha_0, \alpha_1) &= \mathbb{E}\left[ y \mathbf{1}\left( z=k \right)(T - \alpha_0) - \left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right) y \mathbf{1}(y \leq \underline{q}_{1k}) \mathbf{1}(z=k) T\right]\\
  %&= \mathbb{E}\left[ y \mathbf{1}\left( z=k \right)\left\{(T - \alpha_0) - T\left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right)  \mathbf{1}(y \leq \underline{q}_{1k}) \right\}\right]\\
  \overline{\mu}_{1k}(\alpha_0, \alpha_1) - \mu_k(\alpha_0) &=
  \mathbb{E}\left[ y \mathbf{1}(y > \overline{q}_{1k})\mathbf{1}(z=k) T \left( \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_1} \right) - y \mathbf{1}(z=k)(T - \alpha_0) \right]
\end{align*}

\paragraph{Moment Equalities for Preliminary Estimation of $\mathbf{q}$}
\begin{align*}
  &\mathbb{E}\left[ \mathbf{1}(y \leq \underline{q}_{tk}) |T=t,z=k \right] - r_{tk} = 0 \\
  &\mathbb{E}\left[ \mathbf{1}(y \leq \overline{q}_{tk}) |T=t,z=k \right] - (1 - r_{tk}) = 0
\end{align*}
We can write these as 
\[
  \mathbb{E}[h^I(W,\boldsymbol{\alpha}, \mathbf{q})] = \mathbb{E}
  \left[
  \begin{array}{c}
    h^I_{0}(W,\boldsymbol{\alpha}, \mathbf{q})\\
    h^I_{1}(W,\boldsymbol{\alpha}, \mathbf{q})
  \end{array}
\right] = \mathbf{0}
\]
where
\[
  h_k^I(W,\boldsymbol{\alpha},\mathbf{q}) = \left[
  \begin{array}{c}
    \mathbf{1}(y \leq \underline{q}_{0k}) \mathbf{1}(z=k)(1 - T) 
    - \displaystyle \left( \frac{\alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z=k)(T-\alpha_0)\\ \\
    \mathbf{1}(y \leq \overline{q}_{0k}) \mathbf{1}(z=k)(1 - T)
    - \displaystyle \left( \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z=k)(1 - T-\alpha_0)\\ \\
    \mathbf{1}(y \leq \underline{q}_{1k}) \mathbf{1}(z=k)T
    - \displaystyle \left( \frac{1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z=k)(T-\alpha_0)\\ \\
    \mathbf{1}(y \leq \overline{q}_{1k}) \mathbf{1}(z=k)T 
    - \displaystyle \left( \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} \right) \mathbf{1}(z=k)(1 - T-\alpha_0)\\ \\
  \end{array}
\right]
\]

\paragraph{Accounting for Preliminary Estimation}
\[
m = \left[
\begin{array}{c}
  m^I \\ m^E
\end{array}
\right], \quad
m^I = \left[
\begin{array}{c}
  m^I_1 \\ m^I_2
\end{array}
\right], \quad
m^E = \left[
  \begin{array}{cc}
    m_2^E \\ m_3^E 
  \end{array}
\right], \quad m_j^E(\mathbf{w}_i, \cdots) = \left[ \boldsymbol{\psi}_j'(\theta_1, \boldsymbol{\alpha}^0) \mathbf{w}_i - \kappa_j \right]z_i 
\]

\[
  m_1^I(\mathbf{w}_i, \alpha_0, \alpha_1) = \left[
  \begin{array}{l}
    (1 - z_i)(T_i - \alpha_0) \\
    (1 - z_i)(1 - T_i - \alpha_1) \\
    z_i(T - \alpha_0) \\
    z_i (1 - T_i - \alpha_1) \\
  \end{array}
\right], \quad
m^I_2(W, \boldsymbol{\alpha}, \mathbf{q}) = \left[
\begin{array}{c}
  m^I_{2,00}(W,\boldsymbol{\alpha},\mathbf{q})\\
  m^I_{2,10}(W,\boldsymbol{\alpha},\mathbf{q})\\
  m^I_{2,01}(W,\boldsymbol{\alpha},\mathbf{q})\\
  m^I_{2,11}(W,\boldsymbol{\alpha},\mathbf{q}) \\
\end{array}
\right]
\]
\[
  m_{2,tk}^I(W, \boldsymbol{\alpha}, \mathbf{q}) = \left[
  \begin{array}{c}
  \mu_k(\alpha_0) - \underline{\mu}_{tk}(\alpha_0, \alpha_1) \\
  \overline{\mu}_{tk}(\alpha_0, \alpha_1) - \mu_k(\alpha_0)  
  \end{array}
\right] 
\]



\[
  h  = \left[
  \begin{array}{c}
    h^I_0 \\ h^I_1 \\ h^E
  \end{array}
\right], \quad
  h^E  =  \left[
 \begin{array}{l}
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i - \kappa_1\\
   \boldsymbol{\psi}'_2(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_2\\
   \boldsymbol{\psi}'_3(\theta_1,\boldsymbol{\alpha}^0)\mathbf{w}_i - \kappa_3\\ 
   \boldsymbol{\psi}'_1(\theta_1)\mathbf{w}_i z_i - \kappa_1 z_i
 \end{array}
 \right] = \left[
 \begin{array}{l}
   \boldsymbol{\Psi}'(\theta_1, \boldsymbol{\alpha}^0) \mathbf{w}_i - \boldsymbol{\kappa}\\
   \left\{\boldsymbol{\psi}_1'(\theta_1)\mathbf{w}_i - \kappa_1\right\}z_i
 \end{array}
 \right]
\]
Now, let $\boldsymbol{\gamma}' = (\mathbf{q}', \boldsymbol{\kappa}', \theta_1)$ where $\mathbf{q}' = ( \underline{q}_{00}, \overline{q}_{00}, \underline{q}_{10}, \overline{q}_{10}, \underline{q}_{01}, \overline{q}_{01}, \underline{q}_{11}, \overline{q}_{11})$, and $\boldsymbol{\kappa}' = (\kappa_1, \kappa_2, \kappa_3)$.
Now define,
\begin{align*}
  B^E &= -M^E (H^E)^{-1}\\
  M^E &= \mathbb{E}\left[
  \begin{array}{cc}
    \displaystyle \frac{\partial m^E}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial m^E}{\partial \theta_1}
  \end{array}
\right]\\
  H^E &= \left[
  \begin{array}{cc}
    \displaystyle \frac{\partial h^E}{\partial \boldsymbol{\kappa}'} &
    \displaystyle \frac{\partial h^E}{\partial \theta_1}
  \end{array}
\right] = \mathbf{R}
\end{align*}
where
\[
\mathbf{R} = \left[
\begin{array}{rrrr}
  -1 & 0 & 0 
  & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & -1 & 0 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   0 & 0 & -1 & \displaystyle \left(\frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}]\\
   -q & 0 & 0 &  \displaystyle \left(\frac{\partial \boldsymbol{\psi}_1}{\partial \theta_1}\right)'\mathbb{E}[\mathbf{w}z]\\
\end{array}
\right]
\]
and
\[
\mathbb{E}\left[\frac{\partial m^E}{\partial \boldsymbol{\kappa}'}\right] = \left[
\begin{array}{ccc}
  0 & -q & 0 \\
  0 & 0 & -q
\end{array}
\right], \quad
\mathbb{E}\left[\frac{\partial m^E}{\partial \theta_1}\right] =
\left[
\begin{array}{c}
  \displaystyle\left( \frac{\partial \boldsymbol{\psi}_2}{\partial \theta_1} \right)'\\ 
  \displaystyle\left( \frac{\partial \boldsymbol{\psi}_3}{\partial \theta_1} \right)' 
\end{array}
\right] \mathbb{E}[\mathbf{w}_iz_i]
\]

\todo[inline]{Change from here down\ldots}
so that $B = -M \mathbf{R}^{-1}$ and $A = \left[
\begin{array}{cc}
\mathbf{I} & B
\end{array}
\right]$.
Finally, accounting for preliminary estimation of $\boldsymbol{\kappa}$ and $\theta_1$, the variance matrix estimator is $\widehat{\Sigma}_n\left(\boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) = A \widehat{\mathcal{V}}_n \left(\boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) A'$ where
\[
  \widehat{\mathcal{V}}_n\left(\boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) = \frac{1}{n}\sum_{i=1}^n 
  \left[
  \begin{array}{c}
    m(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{m}_n(\alpha^0, \widehat{\gamma}(\alpha^0))\\
    h(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{h}_n(\alpha^0, \widehat{\gamma}(\alpha^0))
  \end{array}
\right]
  \left[
  \begin{array}{c}
    m(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{m}_n(\alpha^0, \widehat{\gamma}(\alpha^0))\\
    h(\mathbf{w}_i, \alpha^0, \widehat{\gamma}(\alpha^0)) - \bar{h}_n(\alpha^0, \widehat{\gamma}(\alpha^0))
  \end{array}
\right]'
\]
Where, 
\begin{align*}
  m_1^I(\mathbf{w}_i, \boldsymbol{\alpha}^0) &=
  \left[
  \begin{array}{r}
  T_i(1 - z_i)/(1 - q) - \alpha_0 \\
  (1 - T_i)(1 - z_i)/(1 - q) - \alpha_1  \\
  T_iz_i/q - \alpha_0 \\
  (1 - T_i)z/q - \alpha_1  
  \end{array}
\right]\\
  m^E\left(\mathbf{w}_i, \boldsymbol{\alpha}^0, \widehat{\boldsymbol{\gamma}}(\boldsymbol{\alpha}^0)\right) &= 
  \left[
  \begin{array}{c}
 \widehat{\boldsymbol{\psi}}_2'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})z_i \\
   \widehat{\boldsymbol{\psi}}_3'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})z_i 
  \end{array}
\right]\\
  h_E\left(\mathbf{w}_i, \boldsymbol{\alpha}^0, \widehat{\gamma}(\boldsymbol{\alpha}^0)\right) &= \left[
  \begin{array}{l}
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})\\ 
    \widehat{\boldsymbol{\psi}}'_2(\boldsymbol{\alpha}^0) (\mathbf{w}_i - \bar{\mathbf{w}})\\ 
    \widehat{\boldsymbol{\psi}}'_3(\boldsymbol{\alpha}^0) (\mathbf{w}_i - \bar{\mathbf{w}})\\
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})z_i
  \end{array}
\right] = \left[
\begin{array}{l}
  \widehat{\boldsymbol{\Psi}}'(\boldsymbol{\alpha}^0)(\mathbf{w}_i - \bar{\mathbf{w}})\\
    \widehat{\boldsymbol{\psi}}'_1 (\mathbf{w}_i - \bar{\mathbf{w}})z_i
\end{array}
\right]
\end{align*}



\end{document}
