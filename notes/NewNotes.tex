\documentclass[12pt]{article}
\usepackage{../frankstyle}

\title{Notes for Paper on Mis-measured, Binary, Endogenous Regressors}
\author{Francis J.\ DiTraglia \& Camilo Garc\'{i}a-Jimeno}

\begin{document}

\maketitle

\section{Model and Notation}

\paragraph{Probabilities}
\begin{eqnarray*}
p^*_{tk} &=& P(T^*=t, Z=k)\\
p_{tk} &=& P(T=t, Z=k)\\
p^*_k &=& P(T^* = 1|Z = k)\\
p_k &=& P(T = 1|Z = k)\\
q &=& P(Z = 1)
\end{eqnarray*}

\begin{eqnarray*}
  p^*_{00} &=& P(T^* = 0|Z=0)P(Z=0) = (1 - p_0^*)(1 - q) =  \left( \frac{1 - p_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{10} &=& P(T^* = 1|Z=0)P(Z=0) = p_0^*(1 - q) =  \left( \frac{p_0 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{01} &=& P(T^* = 0|Z=1)P(Z=1) = (1 - p_1^*)q =  \left( \frac{1 - p_1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) q\\
  p^*_{11} &=& P(T^* = 1|Z=1)P(Z=1) = p_1^*(1 - q)  =  \left( \frac{p_1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)q
\end{eqnarray*}

\paragraph{CDFs}
For $t, Z \in \left\{ 0,1 \right\}$ define
\begin{eqnarray*}
F_{tk}^*(\tau) &=&  P(Y \leq \tau|T^* = t, Z = k) \\
F_{tk}(\tau) &=&  P(Y \leq \tau|T = t, Z = k)\\
F_k(\tau) &=& P(Y \leq \tau | Z=k) 
\end{eqnarray*}
Note that the second two are observed for all $t,k$ while the first is never observed since it depends on the unobserved RV $T^*$.


\section{Weakest Bounds on $\alpha_0, \alpha_1$}
Assume that $\alpha_0 + \alpha_1 < 1$ that $T$ is independent of $Z$ conditional on $T^*$.
These standard assumptions turn out to yield informative bounds on $\alpha_0$ and $\alpha_1$ without \emph{any further restrictions of any kind}.
In particular, we assume nothing about the validity of the instrument $Z$ and nothing about the relationship between the mis-classification error and the outcome $Y$: we impose only that the mis-classification error rates do not depend on $z$ and that the mis-classification is not so bad that $1 - T$ is a better measure of $T^*$ than $T$. 

By the Law of Total Probability and the assumption that $T$ is conditionally independent of $Z$ given $T^*$,
\begin{eqnarray*}
  p_k &=& P(T=1|Z=k,T^*=0) (1 - p_k^*) + P(T=1|Z=k,T^*=1)p_k^*\\
  &=& P(T=1|T^*=0)(1 - p_k^*) + P(T=1|T^*=1)p_k^*\\
  &=& \alpha_0 (1 - p_k^*) + (1 - \alpha_1) p_k^*\\
  &=& \alpha_0 +(1 - \alpha_0 - \alpha_1) p_k^* 
\end{eqnarray*}
and similarly 
\begin{eqnarray*}
  1 - p_k &=& P(T=0|Z=k,T^*=0) (1 - p_k^*) + P(T=0|Z=k,T^*=1)p_k^*\\
  &=& P(T=0|T^*=0)(1 - p_k^*) + P(T=0|T^*=1)p_k^*\\
  &=& (1 - \alpha_0)(1 - p_k^*) + \alpha_1 p_k^*\\
  &=& \alpha_1 + (1 - p_k^*)(1 - \alpha_0 - \alpha_1)
\end{eqnarray*}
and hence
\begin{eqnarray*}
  p_k - \alpha_0 &=& (1 - \alpha_0 - \alpha_1)p_k^*\\
  (1 - p_k) - \alpha_1 &=& (1 - \alpha_0 - \alpha_1)(1 - p_k^*)
\end{eqnarray*}
Now, since $p_k^*$ and $(1 - p_k^*)$ are probabilities they are between zero and one which means that the sign of $p_k - \alpha_0$ as well as that of $(1 - p_k) - \alpha_1$ are both determined by that of $1 - \alpha_0 - \alpha_1$.
Accordingly, provided that $1 - \alpha_0 - \alpha_1 < 1$, we have
\begin{eqnarray*}
  \alpha_0 &<& p_k\\
  \alpha_1 &<& (1 - p_k)
\end{eqnarray*}
so long as $p_k^*$ does not equal zero or one, which is not a realistic case for any example that we consider.
Since these bounds hold for all $k$, we can take the tightest bound over all values of $Z$.

\section{Stronger Bounds for $\alpha_0, \alpha_1$}
Now suppose we add the assumption that $T$ is conditionally independent of $Y$ given $T^*$. 
This is essentially the non-differential measurement error assumption although it is slightly stronger than the version used by Mahajan (2006) who assumes only conditional mean independence.
This assumption allows us to considerably strengthen the bounds from the preceding section by exploiting information contained in the conditional distribution of $Y$ given $T$ and $Z$.
The key ingredient is a relationship that we can derive between the unobservable distributions $F_{tk}^*$ and the observable distributions $F_{tk}$ using this new conditional independence assumption.
To begin, note that by Bayes' rule we have
\begin{eqnarray*}
  P(T^*=1|T=1, Z=k) &=& P(T=1 | T^*=1) \left(\frac{p_k^*}{p_k}\right) = (1 - \alpha_1)\left( \frac{p_k^*}{p_k} \right)\\
  P(T^*=1|T=0, Z=k) &=& P(T=0 | T^*=1) \left(\frac{p_k^*}{1 - p_k}\right) = \alpha_1 \left( \frac{p_k^*}{1 - p_k} \right)\\
  P(T^*=0|T=1, Z=k) &=& P(T=1 | T^*=0) \left(\frac{1 - p_k^*}{p_k}\right) = \alpha_0 \left( \frac{1 - p_k^*}{p_k} \right)\\
  P(T^*=0|T=0, Z=k) &=& P(T=0 | T^*=0) \left(\frac{1 - p_k^*}{1 - p_k}\right) = (1 - \alpha_0)\left( \frac{1 - p_k^*}{1 - p_k} \right)
\end{eqnarray*}
Now, by the conditional independence assumption
\begin{eqnarray*}
  P(Y\leq \tau|T^* = 0, T=t , Z = k) = P(Y \leq \tau|T^*=0, Z =k) = F_{0k}^*(\tau)\\
  P(Y\leq \tau|T^* = 1, T=t , Z = k) = P(Y \leq \tau|T^*=1, Z =k) = F_{1k}^*(\tau)
\end{eqnarray*}
Finally, putting everything together using the Law of Total Probability, we find that
\begin{eqnarray*}
  (1 - p_k) F_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  p_k F_{1k}(\tau) = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$.
Defining the shorthand 
\begin{eqnarray*}
  \widetilde{F}_{0k}(\tau)&\equiv& (1 - p_k) F_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\equiv& p_k F_{1k}(\tau) 
\end{eqnarray*}
this becomes
\begin{eqnarray}
  \label{eq:F0kTilde}
  \widetilde{F}_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  \label{eq:F1kTilde}
  \widetilde{F}_{1k}(\tau)  = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray}
Now, solving Equation \ref{eq:F0kTilde} for $p_k^* F_{1k}^*(\tau)$ we have
\[
  p_{k}^* F_{1k}^*(\tau) = \frac{1}{\alpha_1}\left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) (1 - p_k^*) F_{0k}^*(\tau)\right]
\]
Substituting this into Equation \ref{eq:F1kTilde},
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{\alpha_1} \left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) ( 1 - p_k^*)F_{0k}^*(\tau) \right]\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \alpha_0 - \frac{(1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \frac{\alpha_0 \alpha_1 - (1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ (1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ 1 - \alpha_1 -  \alpha_0 }{\alpha_1} \right]\left( \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) F_{0k}^*(\tau)\\
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) -  \frac{1 - p_k - \alpha_1}{\alpha_1}  F_{0k}^*(\tau)
  \label{eq:F1kTildeAlpha1}
\end{equation}
Equation \ref{eq:F1kTildeAlpha1} relates the observable $\widetilde{F}_{1k}(\tau)$ to the mis-classification error rate $\alpha_1$ and the unobservable CDF $F_{0k}^*\left( \tau \right)$.
Since $F_{0k}^*(\tau)$ is a CDF, however, it lies in the interval $\left[ 0,1 \right]$.
Accordingly, substituting $0$ in place of $F^*_{0k}(\tau)$ gives 
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_leq_a1}
\end{equation}
while substituting $1$ gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau) - \frac{1 - p_k - \alpha_1}{\alpha_1}
  \label{eq:F1ktilde_F0kTilde_geq_a1}
\end{equation}
Rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a1}
\begin{eqnarray*}
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau)\\
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& \widetilde{F}_{0k}(\tau) - \alpha_1 \widetilde{F}_{0k}(\tau)\\
 \alpha_1 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right]&\leq& \widetilde{F}_{0k}(\tau) 
\end{eqnarray*}
since $\alpha_1 \in [0,1]$ and therefore
\begin{equation}
  \alpha_1  \leq \frac{\widetilde{F}_{0k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = (1 - p_k) \left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
  \label{eq:Alpha1_Bound1}
\end{equation}
since $\widetilde{F}_{1k}(\tau) + \widetilde{F}_{1k}(\tau) \geq 0$.
Proceeding similarly for Equation \ref{eq:F1ktilde_F0kTilde_geq_a1},
\begin{eqnarray*}
  \alpha_1 \widetilde{F}_{1k}(\tau) &\geq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau) - (1 - p_k - \alpha_1)\\
  \alpha_1 \left[\widetilde{F}_{1k}(\tau) + \widetilde{F}_{0k}(\tau) - 1\right] &\geq& \widetilde{F}_{0k}(\tau) - (1 - p_k)\\
  -\alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\geq& -\left[1 - \widetilde{F}_{0k}(\tau) - p_k \right]\\
  \alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\leq& 1 - \widetilde{F}_{0k}(\tau) - p_k 
\end{eqnarray*}
Now since $\widetilde{F}_{1k}(\tau) = p_k F_{1k}(\tau) \leq p_k$ and $\widetilde{F}_{0k}(\tau) = (1 - p_k) F_{0k}(\tau) \leq (1 - p_k)$ it follows that $1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \geq 0$ and hence
\begin{equation}
  \alpha_1 \leq \frac{1 - \widetilde{F}_{0k}(\tau) - p_k}{1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau)} = (1 - p_k) \left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha1_Bound2}
\end{equation}
The bounds given in Equations \ref{eq:Alpha1_Bound1} and \ref{eq:Alpha1_Bound2} relate $\alpha_1$ to observable quantities \emph{only} and hold for all values of $\tau$ for which their respective denominators are non-zero.
Moreover, these bounds hold for any value $k$ that the instrument takes on.

We can proceed similarly for $\alpha_0$.
First solve Equation \ref{eq:F0kTilde} for $(1 - p_k^*)F^*_{0k}(\tau)$:
\[
  (1 - p_k^*)F^*_{0k}(\tau) = \frac{1}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right]
\]
and then substitute into Equation \ref{eq:F1kTilde}:
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \frac{\alpha_0}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right] + (1 - \alpha_1) p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ (1 - \alpha_1) - \frac{\alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{(1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_0}   \right] \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} F_{1k}^*(\tau) 
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) +  \frac{p_k - \alpha_0}{1 - \alpha_0} F_{1k}^*(\tau) 
\end{equation}
Now we can again obtain two bounds by substituting the smallest and largest possible values of $F_{1k}^*(\tau)$.
Substituting zero gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_geq_a0}
\end{equation}
while substituting one gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \frac{p_k - \alpha_0}{1 - \alpha_0}
  \label{eq:F1ktilde_F0kTilde_leq_a0}
\end{equation}
Now, rearranging Equation \ref{eq:F1ktilde_F0kTilde_geq_a0}, 
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \widetilde{F}_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] 
\end{eqnarray*}
since $1 - \alpha_0 \geq 0$.
Therefore,
\begin{equation}
  \alpha_0 \leq \frac{\widetilde{F}_{1k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{F_{1k}(\tau)}{F_{k}(\tau)}\right]
  \label{eq:Alpha0_Bound1}
\end{equation}
since $\left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] \geq 0$.
Similarly, rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a0}
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\leq& \alpha_0\widetilde{F}_{0k}(\tau) + p_k - \alpha_0\\
  \widetilde{F}_{1k}(\tau) - p_k &\leq& \alpha_0\left[\widetilde{F}_{0k}(\tau)  + \widetilde{F}_{1k}(\tau) - 1 \right] \\
  -\left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\leq& -\alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] \\
  \left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\geq& \alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] 
\end{eqnarray*}
Therefore
\begin{equation}
\alpha_0 \leq \frac{1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)}{1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha0_Bound2}
\end{equation}

\paragraph{Putting Everything Together} 
For all $k$ we have
\begin{equation}
  \alpha_0 \leq p_k \min_\tau\left\{\left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{1k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq p_k 
\end{equation}
\begin{equation}
  \alpha_1 \leq (1 - p_k) \min_\tau \left\{\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{0k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq (1 - p_k) 
\end{equation}
Note that these bounds can only improve upon those derived in the previous section since the ratio of CDFs tends to one as $\tau \rightarrow \infty$.
To derive these tighter bounds we have made no assumption regarding the relationship between $Z$ and the error term $\varepsilon$.
These bounds use only the assumption that $\alpha_0 + \alpha_1 < 1$, and the assumption that $T$ is conditionally independent of $Z,Y$ given $T^*$.
Notice that that the bounds are related.
In particular,
\[
  p_k \left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
\]
and 
\[
p_k \left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
\]


\section{Even Stronger Bounds on $\alpha_0, \alpha_1$}
Try applying the stochastic dominance conditions from our simulation study.

\section{Independent Instrument}
Assume that $Z \perp U$.  
The model is $Y = \beta T^* + U$ and
\[ F_{U}(\tau) = P(U \leq\tau) = P(Y - \beta T^* \leq \tau)\]
but if $Z$ is independent of $U$ then it follows that
\begin{eqnarray*}
F_U(\tau) &=&  F_{U|Z=k}(\tau) = P(U\leq \tau |Z=k) = P(Y  - \beta T^* \leq \tau |Z=k)\\
&=&  P(Y \leq \tau |T^* = 0, Z = k)(1 - p_k^*) + P(Y\leq \tau + \beta| T^* = 1, Z = k)p_k^* \\
&=& (1 - p_k^*) F^*_{0k}(\tau) + p_k^* F^*_{1k}(\tau + \beta)
\end{eqnarray*} 
for all $k$ by the Law of Total Probability.
Similarly, 
\[ F_k(\tau) = (1 - p_k^*) F_{0k}^*(\tau)  + p_k^* F_{1k}^*(\tau)\]
and rearranging
\[  (1 - p_k^*) F_{0k}^*(\tau)  = F_k(\tau) - p_k^* F_{1k}^*(\tau)\]
Substituting this expression into the equation for $F_U(\tau)$ from above, we have
\[F_U(\tau) = F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]\]
for all $k$ and all $\tau$.
Evaluating at two values $k$ and $\ell$ in the support of $Z$ and equating 
\[ F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right] =  F_\ell(\tau) + p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right]\]
or equivalently
\begin{equation}
 F_k(\tau) - F_\ell(\tau) =  p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right] - p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]  
 \label{eq:CDFs1}
\end{equation}
for all $\tau$.
Now we simply need to re-express all of the ``star'' quantities, namely $p_k^*, p_\ell^*$ and $F_{1k}^*, F_{1\ell}^*$ in terms of $\alpha_0, \alpha_1$ and the \emph{observable} probability distributions $F_{1k}$ and $F_{1\ell}$ and observable probabilities $p_k, p_\ell$.
To do this, we use the fact that
\begin{eqnarray*}
  F_{0k}(\tau) &=& \frac{1 - \alpha_0}{1 - p_k} (1 - p^*_k)F_{0k}^*(\tau) + \frac{\alpha_1}{1 - p_k}p_k^* F_{1k}^*(\tau)\\ \\
  F_{1k}(\tau) &=& \frac{ \alpha_0}{p_k}(1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{p_k}p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$ by Bayes' rule.
Solving these equations,
\begin{equation*}
  p_k^* F_{1k}^*(\tau) = \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} p_k F_{1k}(\tau) - \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} (1 - p_k) F_{0k}(\tau) 
\end{equation*}
for all $k$.
Combining this with Equation \ref{eq:CDFs1}, we find that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Now, define
\[
  \Delta^\tau_{tk}(\beta) = F_{tk}(\tau + \beta) - F_{tk}(\tau) = E\left[ \frac{\mathbf{1}\left\{ T = t, Z = k \right\}}{p_{tk}}\left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right) \right]
\]
and note that we can express $F_k(\tau) - F_\ell(\tau)$ similarly as 
\[
  F_k(\tau)  - F_{\ell}(\tau) = E\left[ \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right) \right]
\]
Using this notation, we can write the preceding as
\begin{equation*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_{\ell}(\tau) \right] = \alpha_0\left[ (1 - p_k) \Delta^\tau_{0k}(\beta) - (1 - p_\ell) \Delta^\tau_{0\ell}(\beta) \right] - (1 - \alpha_0)\left[ p_k \Delta^\tau_{1k}(\beta) - p_\ell \Delta^\tau_{1\ell}(\beta) \right]
\end{equation*}
or in moment-condition form
\begin{align*}
   E\Bigg[ &(1 - \alpha_0 - \alpha_1) \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right)  - 
   \left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right)\Bigg\{ \\
   &\alpha_0 \bigg((1 - p_k)\frac{\mathbf{1}\left\{ T = 0, Z = k \right\}}{p_{0k}} - 
    (1 - p_\ell)\frac{\mathbf{1}\left\{ T = 0, Z = \ell \right\}}{p_{0\ell}}\bigg)\\
   &-(1 - \alpha_0) \bigg( p_k\frac{\mathbf{1}\left\{ T = 1, Z = k \right\}}{p_{1k}} - 
 p_\ell \frac{\mathbf{1}\left\{ T = 1, Z = \ell \right\}}{p_{1\ell}}\bigg) \Bigg\}\Bigg] = 0
\end{align*}
Each value of $\tau$ yields a moment condition.

\section{Special Case: $\alpha_0 = 0$}
In this case the expressions from above simplify to
\begin{align}
  (1 - \alpha_1)\left[ F_k(\tau) - F_\ell(\tau)\right] = \left[ p_\ell F_{1\ell}(\tau + \beta) 
 - p_k  F_{1k}(\tau+ \beta) 
 - p_\ell F_{1\ell}(\tau) 
 + p_k F_{1k}(\tau) \right]
 \label{eq:specialCDF}
\end{align}
for all $\tau$.
Now, provided that all of the CDFs are differentiable we have\footnote{There must be a way to generalize this using Lebesgue.}
\begin{align*}
  e^{i\omega \tau}(1 - \alpha_1)\left[f_k(\tau) - f_\ell(\tau)\right] = e^{i\omega \tau}\left[ p_\ell f_{1\ell}(\tau + \beta) - p_k  f_{1k}(\tau+ \beta) - p_\ell f_{1\ell}(\tau) + p_k f_{1k}(\tau) \right]
\end{align*}
where we have pre-multiplied both sides by $e^{i\omega \tau}$.
Finally, integrating both sides with respect to $\tau$ over $(-\infty, \infty)$, we have
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] = \left\{  \int_{-\infty}^{\infty} e^{i\omega \tau} \left[p_\ell f_{1\ell}(\tau + \beta) - p_k f_{1k}(\tau+ \beta)\right] \; d\tau - p_\ell \varphi_{1\ell}(\omega) + p_k \varphi_{1k}(\omega) \right\}
\end{align*}
where $\varphi_k$ is the conditional characteristic function of $Y$ given $Z=k$ and $\varphi_{1k}$ is the conditional characteristic function of $Y$ given $T=1, Z=k$.
Finally, 
\begin{align*}
  \int_{-\infty}^{\infty} e^{i\omega \tau} p_\ell f_{1\ell}(\tau + \beta) \; d\tau &=  e^{ i\omega \beta } p_\ell \int_{u = -\infty + \beta}^{u = \infty + \beta} e^{ i\omega u }f_{1\ell}(u)\; du \\
  &= e^{-i\omega \beta } p_\ell \varphi_{1\ell}(\omega)
\end{align*}
using the substitution $u = \tau + \beta$.
Changing subscripts, the same holds for $k$ and thus
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  e^{-i\omega \beta}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] +  \left[p_k \varphi_{1k}(\omega) -  p_\ell \varphi_{1\ell}(\omega)\right]
\end{align*}
which, after collecting terms, simplifies to
\begin{align}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  \left(e^{-i\omega \beta} - 1\right)\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] 
  \label{eq:CharacteristicSpecial}
\end{align}
for all $\omega$.  
Equation \ref{eq:CharacteristicSpecial} contains exactly the same information as Equation \ref{eq:specialCDF} but gives us a more convenient way to prove identification since $\beta$ enters in a simpler way.
Leibniz's formula for the $r$th derivative of a product of two functions $f$ and $g$ is:
\begin{align*}
  (fg)^{(r)} = \sum_{s=0}^r {r \choose s} f^{(s)}g^{(r-s)}
\end{align*}
where $f^{(r)}$ denotes the $r$th derivative of the function $f$ and $g^{(r-s)}$ denotes the $(r-s)$th derivative of the function $g$.
Applying this to the RHS, $R(\omega)$ of Equation \ref{eq:CharacteristicSpecial} gives
\begin{align*}
  \frac{d}{d\omega^r}R(\omega)
  &=  \sum_{s=0}^r {r \choose s} \frac{d}{d\omega^s}\left( e^{-i\omega\beta} - 1\right)\frac{d}{d\omega^{r - s}}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] \\
  &= \left( e^{-i\omega \beta} - 1 \right) \left[ p_\ell \varphi_{1\ell}^{(r)}(\omega) - p_k \varphi_{1k}^{r}(\varphi) \right] + e^{-i\omega\beta} \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(\omega) - p_k \varphi^{(r-s)}_{1k}(\omega) \right] 
\end{align*}
where we split off the $s=0$ term because our generic expression for the $s$th derivative of $(e^{-i\omega\beta} - 1)$ only applies for $s\geq 1$.
Evaluating at zero:
\begin{align*}
  \frac{d}{d\omega^r}R(0)
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Combining this with the LHS of Equation \ref{eq:CharacteristicSpecial}, also differentiated $r$ times and evaluated at zero, we have
\begin{align*}
  (1 - \alpha_1) \left[ \varphi_{k}^{(r)}(0) - \varphi_{\ell}^{(r)}(0) \right] 
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Now, recall that if $\varphi(\omega)$ is the characteristic function of $Y$ then $\varphi^{(r)}(0) = i^r E[Y^r]$ provided that the expectation exists where $\varphi^{(r)}$ denotes the $r$th derivative of $\varphi$.
The same applies for the conditional characteristic functions we consider here.
Hence, provided that the $r$th moments exist, 
\footnotesize
\begin{align*}
  i^r(1 - \alpha_1)\left\{ E[Y^r|Z=k] - E[Y^r|Z=\ell]\right\} = \sum_{s=1}^r {r \choose s} (-i\beta)^s i^{r-s}\left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
After simplifying the terms involving $i$ and cancelling them from both sides, 
\small
\begin{align*}
  (1 - \alpha_1)\left(E[Y^r|Z=k] - E[Y^r|Z=\ell]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
again provided that the moments exist.
Abbreviating the conditional expectations according to $E[Y^r|Z=k] = E_k[Y^r]$ and $E[Y^r|T=t,Z=k] = E_{tk}[Y^r]$, this becomes
\begin{equation}
  (1 - \alpha_1)\left(E_k[Y^r] - E_\ell[Y^r]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{r-s}\right] - p_k E_{1k}\left[ Y^{r-s}\right] \right)
  \label{eq:MomentsSpecial}
\end{equation}
Equation \ref{eq:MomentsSpecial} can be used to generate moment equations that are implied by the Equation \ref{eq:CharacteristicSpecial} and the equivalent representation in terms of CDFs: Equation \ref{eq:specialCDF}.
Assuming that the conditional first moments exist, we can evaluate Equation \ref{eq:MomentsSpecial} at $r=1$, yielding
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y] - E_\ell[Y]\right) &= \sum_{s=1}^1 {1 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{1-s}\right] - p_k E_{1k}\left[ Y^{1-s}\right] \right)\\
  &=  - \beta\left( p_\ell - p_k \right) 
\end{align*}
Rearranging, this gives us the expression for the probability limit of the Wald estimator
\begin{equation}
  \mathcal{W} \equiv \frac{E_{k}[Y]- E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_1} 
  \label{eq:WaldSpecial}
\end{equation}
Evaluating Equation \ref{eq:MomentsSpecial} at $r = 2$, we have
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y^2] - E_\ell[Y^2]\right) &= \sum_{s=1}^2 {2 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{2-s}\right] - p_k E_{1k}\left[ Y^{2-s}\right] \right)\\
  &= 2\beta\left( p_k E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta^2\left( p_k - p_{\ell} \right)
\end{align*}
Rearranging, we have
\begin{equation}
  E_k[Y^2] - E_\ell[Y^2] 
  =  \frac{\beta}{1 - \alpha_1}\left[2\left( p_k  E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta(p_k - p_\ell)\right]
  \label{eq:SpecialSquared}
\end{equation}
Substituting Equation \ref{eq:WaldSpecial}, we can replace $\beta/(1-\alpha_1)$ with a function of observables only, namely $\mathcal{W}$.
Solving, we find that 
\begin{align}
  \beta &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} 
  \label{eq:BetaSpecial}
\end{align}
This allows us to state low-level sufficient conditions for identification:
\begin{enumerate}[(a)]
  \item $\alpha_1 < 1$
  \item $p_k \neq p_\ell$ 
  \item $E_k[Y] \neq E_\ell[Y]$ 
  \item $E_{1k}[|Y|], E_{1\ell}[|Y|], E_k[|Y^2|], E_\ell[|Y^2|] < \infty$.
\end{enumerate}
Note that, although $\beta = 0$ is always a solution of Equation \ref{eq:specialCDF} this solution is ruled out by the assumption that $E_k[Y] \neq E_\ell[Y]$ via Equation \ref{eq:WaldSpecial}.
The mis-classification error rate $\alpha_1$ is likewise uniquely identified under these assumptions.
Substituting $\beta/\mathcal{W} = 1-\alpha_1$ into Equation \ref{eq:BetaSpecial}
\begin{align*}
  (1 - \alpha_1) &= \left\{ \frac{p_k - p_\ell}{E_k[Y] - E_\ell[Y]} \right\}\left\{\frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} \right\}\\
  &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} - (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\}
\end{align*}
and thus
\begin{align*}
  \alpha_1
  &= 1 + (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\} - \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} 
\end{align*}

\section{Identification in the General Case}

\section{Characteristic Functions}
Recall from above that in the general case an independent instrument combined with non-differential measurement error implies that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Using the same steps as in the preceding section, we can convert this expression into characteristic function form by differentiating each side, multiplying by $e^{i\omega\tau}$ and then integrating with respect to $\tau$, yielding
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left[ \varphi_k(\omega) - \varphi_{\ell}(\omega) \right] &= \alpha_0 \left\{ (1 - p_k)\left(e^{-i\omega\beta} - 1\right)\varphi_{0k}(\omega) - (1 - p_\ell)\left( e^{-i\omega\beta} - 1\right) \varphi_{0\ell}(\omega)  \right\}\\
  &\quad - (1 - \alpha_0) \left\{ p_k\left(e^{-i\omega\beta} - 1 \right)\varphi_{1k}(\omega) - p_\ell \left( e^{-i\omega\beta} - 1\right) \varphi_{1\ell}(\omega) \right\}
\end{align*}
which simplifies to
\begin{align*}
  \varphi_k(\omega) - \varphi_{\ell}(\omega) &= \left( e^{-i\omega\beta} - 1 \right)\left(\frac{\alpha_0\left[ (1 - p_k)\varphi_{0k}(\omega) - (1-p_\ell)\varphi_{0\ell}(\omega) \right]  - (1 - \alpha_0)\left[ p_k \varphi_{1k}(\omega) - p_\ell \varphi_{1\ell}(\omega) \right]}{1 - \alpha_0 - \alpha_1}\right)
\end{align*}
As above, we will differentiate both sides of this expression $r$ times and evaluate at $\omega = 0$.
Steps nearly identical to those given above yield
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left(  E_k[Y^r] - E_\ell[Y^r]\right) 
  &= \alpha_0 \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{ (1 - p_k) E_{0k}[Y^{r-s}] - (1 - p_\ell) E_{0\ell}[Y^{r-s}] \right\}\\
  &\quad - (1 - \alpha_0) \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{p_k E_{1k}[Y^{r-s}] - p_\ell E_{1\ell}[Y^{r-s}] \right\}
\end{align*}

\paragraph{First Moments}
Taking $r = 1$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left( E_k[Y] - E_{\ell}[Y] \right) = \beta (p_k - p_\ell)
\end{align*}
Simplifying,
\begin{equation}
  \mathcal{W} \equiv \frac{E_k[Y] - E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_0 - \alpha_1}
  \label{eq:Wald}
\end{equation}

\paragraph{Second Moments}
Now, taking $r = 2$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left( E_{k}[Y^2] - E_{\ell}[Y^2] \right) &=
  \alpha_0\left\{ \left[ (1 - p_k) E_{0k}[Y] - (1 - p_\ell) E_{0\ell} \right] - \beta^2\left( p_k - p_\ell \right) \right\}\\
  &\quad  -(1 - \alpha_0)\left\{ -2\beta\left( p_k E_{1k}[Y] - p_{\ell}E_{1\ell}[Y] \right) + \beta^2\left( p_k - p_\ell \right) \right\}\\
  &= -2\beta \alpha_0\left\{ (1 - p_k)E_{0k}[Y] - (1 - p_\ell) E_{0\ell}[Y] p_k E_{1k}[Y] + p_{\ell}E_{1\ell}[Y]\right\} \\ 
  &\quad +2\beta \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) 
  - (p_k - p_\ell)\beta^2\left( \alpha_0 + 1 - \alpha_0 \right)\\
  &= -2\beta\left\{ \alpha_0 \left( E_k[Y] - E_\ell[Y] \right) - \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) \right\} - \beta^2(p_k - p_\ell)
\end{align*}
Now, simplifying
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\beta \alpha_0 \left(\frac{E_k[Y]-E_k[Y]}{p_k - p_\ell}\right) + 2\beta \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) - \beta^2
\end{align*}
and substituting Equation \ref{eq:Wald} to eliminate $\beta$, this becomes
\small
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 (1 - \alpha_0 - \alpha_1)\mathcal{W}^2 + 2\mathcal{W}(1 - \alpha_0 - \alpha_1) \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) \\
  &\quad \quad - (1 - \alpha_0 - \alpha_1)^2 \mathcal{W}^2\\
  \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 \mathcal{W}^2 + 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2
\end{align*}
\normalsize
And thus, simplifying
\begin{align*}
  -2\alpha_0 \mathcal{W}^2 - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2 &= \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)- 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  \\
  \alpha_1  - \alpha_0   &= 1 +  \left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\mathcal{W}^2(p_k - p_\ell)} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{\mathcal{W}(p_k - p_\ell)} \right] 
\end{align*}
and therefore
\begin{equation}
  \alpha_1  - \alpha_0  = 1 +  (p_k - p_\ell)\left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\left( E_k[Y] - E_\ell[Y] \right)^2} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{E_k[Y] - E_\ell[Y]} \right] 
\end{equation}

\paragraph{``Product'' Moments}
Recall that in our initial draft of the paper we worked with moments such as $E[TY|Z=k], E[TY|Z=\ell]$ and $E[TY^2|Z=k], E[TY^2|Z=\ell]$.
In the notation of this document, we can express these quantities as follows:
\begin{align*}
  E[TY^r|z=k] &= E[TY^r|T=1,z=k]p_k + E[TY^r|T=0,z=k](1 - p_k)\\
  &= p_k E[Y^r|T=1,z=k] + 0\\
  &= p_k E_{1k}[Y^r]
\end{align*}
for any $r$. 
We will use this relationship to motivate some shorthand notation below.

\paragraph{Some Shorthand}
The notation above is becoming very cumbersome and we haven't even looked at the third moments yet! 
To make life easier, define the following: 
\begin{align*}
  \widetilde{y^r_{1k}} &= p_k E_{1k}[Y^r] \\
  \widetilde{y^r_{0k}} &= (1 - p_k) E_{1k}[Y^r] \\
  \Delta \overline{y^r} &= E_k[Y^r] - E_\ell[Y^r]\\
  \Delta \overline{Ty^r} &= p_k E_{1k}[Y^r] - p_\ell E_{1\ell}[Y^r] = \widetilde{y^r_{1k}} - \widetilde{y^r_{1k}}\\
  \mathcal{W} &= (E_k[Y] - E_\ell[Y]) / (p_k - p_\ell)
\end{align*}
for all $r$.
When no $r$ superscript is given this means $r=1$.
Note, moreover, that when $r =0$ we have $\widetilde{y_{1k}^0} = p_k$ and $\widetilde{y_{0k}^0} = (1 - p_k)$.
Thus $\Delta \overline{Ty^0} = p_k - p_\ell$.
In contrast, $\Delta y^0 = 0$.

Among other things, this notation will make it easier for us to link the derivations here to our earlier derivations from the first draft of the paper that used slightly different notation and did not work explicitly with the independence of the instrument.

\paragraph{Simplifying the Moment Equalities}
Using the final two pieces of notation defined in the preceding section, we can re-rewrite the collection of moment equalities arising from the characteristic function equations as
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left[\alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right) \right]
\end{align*}
Now, simplifying the terms in the square brackets,
\begin{align*}
  \alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)
  &= \alpha_0\left[ \left( \widetilde{y_{0k}^{r-s}} + \widetilde{y_{1k}^{r-s}} \right) - \left( \widetilde{y_{0\ell}^{r-s}} + \widetilde{y_{1\ell}^{r-s}} \right)  \right] - \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)\\
  &= \alpha_0\left( E_k[Y^{r-s}] - E_\ell[Y^{r-s}] \right) - \Delta \overline{Ty^{r-s}}\\
  &= \alpha_0 \Delta \overline{y^{r-s}} - \Delta\overline{Ty^{r-s}}
\end{align*}
and hence
\begin{align}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{r-s}} - \Delta\overline{Ty^{r-s}} \right) 
  \label{eq:MomentEqualitiesSimplified}
\end{align}

\paragraph{Third Moments}
Evaluating Equation \ref{eq:MomentEqualitiesSimplified} at $r=3$ 
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^3} 
  &= \sum_{s=1}^3 {3 \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{3-s}} - \Delta\overline{Ty^{3-s}} \right) \\
  &= -3\beta\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta^2\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^3 (p_k - p_\ell)
\end{align*}

\paragraph{Solving the System}
Using $\mathcal{W} = \beta/(1 - \alpha_0 - \alpha_1)$ we can re-write the third moment expression as follows
\begin{align*}
  \Delta \overline{y^3} &= -3\mathcal{W}\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta \mathcal{W}\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^2 \mathcal{W} (p_k - p_\ell)\\
  \frac{\Delta \overline{y^3}}{\mathcal{W} (p_k - p_\ell)} 
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} }{p_k - p_\ell}\right) \\
  \frac{\Delta \overline{y^3} - 3\mathcal{W}\Delta\overline{y^2T}}{\mathcal{W}(p_k - p_\ell)}
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2}  }{p_k - p_\ell}\right) 
\end{align*}
Now, translating the second moment equation into the shorthand notation defined above, we have

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{New Results from September 2016}

\subsection{Is the Treatment Effect Identified when $Y$ is Discrete?}
Suppose that $Y$ is discrete with arbitrary support.
This allows for the possibility that $Y$ is binary.
Using our standard argument,
\begin{align*}
  P(Y|T^*=0, z_k) &= P(Y|T=0, z_k) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ P(Y|T=0,z_k) - P(Y|T=1,z_k) \right] \\
  P(Y|T^*=1, z_k) &= P(Y|T=1, z_k) + \left( \frac{\alpha_0 (1-p_k)}{ p_k - \alpha_0} \right)\left[ P(Y|T=1,z_k) - P(Y|T=0,z_k) \right]
\end{align*}
under non-differential measurement error.
Now, if we assume that $Z$ is conditionally independent of $Y$ given $T^*$, then we have $P(Y,Z|T^*) = P(Z|T^*)P(Y|T^*)$ or equivalently $P(Y|T^*,Z) = P(Y|T^*)$.
I originally derived this in terms of the first condition, but here I'll present both ways.

\subsubsection{Derivation Using $P(Y,Z|T^*) = P(Y|T^*)P(Z|T^*)$}
We have
\begin{align*}
  P(Z|T^*=1) = P(Z)\left[ \frac{P(T=1|Z) - \alpha_0}{P(T=1) - \alpha_0} \right]\\
  P(Z|T^*=0) = P(Z)\left[ \frac{P(T=0|Z) - \alpha_1}{P(T=0) - \alpha_1} \right]
\end{align*}
Now, by our usual argument
\begin{align*}
  P(Y,Z|T^*=0) = P(Y,Z|T=0) + \left( \frac{\alpha_1 p}{1 - p - \alpha_1} \right)\left[ P(Y,Z|T=0) - P(Y,Z|T=1) \right]\\ 
  P(Y,Z|T^*=1) = P(Y,Z|T=1) + \left( \frac{\alpha_0 (1-p)}{p - \alpha_0} \right)\left[ P(Y,Z|T=1) - P(Y,Z|T=0) \right]\
\end{align*}
which simplifies to
\begin{align*}
  P(Y,Z|T^*=1) &= \frac{P(Y,Z,T=1) - \alpha_0 P(Y,Z)}{P(T=1) - \alpha_0}\\
  P(Y,Z|T^*=0) &= \frac{P(Y,Z,T=0) - \alpha_1 P(Y,Z)}{P(T=0) - \alpha_1}
\end{align*}
Similarly
\begin{align*}
  P(Y|T^*=1) &= \left( \frac{1 - \alpha_0}{p-\alpha_0} \right) p P(Y|T=1) - \left(\frac{\alpha_0}{p - \alpha_0}\right)(1 - p) P(Y|T=0)\\
  P(Y|T^*=0) &= \left( \frac{-\alpha_1}{1 - p-\alpha_1} \right) p P(Y|T=1) - \left(\frac{1 - \alpha_1}{1 - p - \alpha_1}\right)(1 - p) P(Y|T=0)
\end{align*}
which simplifies to
\begin{align*}
  P(Y|T^*=1) &= \frac{P(Y,T=1) - \alpha_0 P(Y)}{P(T=1) - \alpha_0}\\
  P(Y|T^*=0) &= \frac{P(Y,T=0) - \alpha_1 P(Y)}{P(T=0) - \alpha_1}
\end{align*}
Thus, imposing $P(Y,Z|T^*) = P(Z|T^*)P(Y|T^*)$ we have two restrictions for every pair $(y,z)$ in the joint support of $(Y,Z)$: one corresponding to $T^*=0$ and the other to $T^*=1$, namely
\begin{align*}
  \left[ P(T=1) - \alpha_0 \right]\left[ P(Y,Z,T=1) - \alpha_0 P(Y,Z) \right] &= P(Z)\left[ P(T=1|Z)- \alpha_0 \right]\left[ P(Y,T=1) - \alpha_0 P(Y) \right]\\
  \left[ P(T=0) - \alpha_1 \right]\left[ P(Y,Z,T=0) - \alpha_1 P(Y,Z) \right] &= P(Z)\left[ P(T=0|Z)- \alpha_1 \right]\left[ P(Y,T=0) - \alpha_1 P(Y) \right]
\end{align*}
The first of these gives a quadratic in $\alpha_0$ and observables only while the second gives a quadratic in $\alpha_1$ and observables only.
Both quadratices have the same coefficient on the squared variable: $P(Y,Z) - P(Y)P(Z)$.
This means that the quadratic term vanishes when $Y$ and $Z$ are independent.

The first equation can be re-written in polynomial form:
\begin{align*}
\alpha_0^2 \left[ P(Y) - P(Y|Z) \right] - \alpha_0 \left[ P(Y,T=1) - P(Y,T=1|Z) + P(T=1|Z)P(Y) - P(T=1)P(Y|Z) \right] \\
+ \left[ P(T=1|Z)P(Y,T=1) - P(T=1)P(Y,T=1|Z) \right] = 0
\end{align*}

The second equation can be re-written in polynomial form:
\begin{align*}
\alpha_1^2 \left[ P(Y) - P(Y|Z) \right] - \alpha_1 \left[ P(Y,T=0) - P(Y,T=0|Z) + P(T=0|Z)P(Y) - P(T=0)P(Y|Z) \right] \\
+ \left[ P(T=0|Z)P(Y,T=0) - P(T=0)P(Y,T=0|Z) \right] = 0
\end{align*}

In both cases we make use of the fact that
\begin{align*}
\frac{ P(Y,Z,T)}{P(Z)} = P(Y,T|Z)
\end{align*}

After some algebra, the discriminant ($D = b^2 - 4ac$) for the $\alpha_0$ roots can be written as:
\begin{align*}
\left( P(Y,T=1) - P(Y)P(T=1|Z) \right)^2 + \left( P(Y,T=1|Z) - P(T=1)P(Y|Z) \right)^2 \\
+ 4P(Y|Z)P(T=1|Z)P(Y,T=1) - 2P(Y|Z)P(T=1|Z)P(Y)P(T=1)\\
+ 4P(Y,T=1|Z)P(Y)P(T=1) - 2P(Y,T=1|Z)P(Y)P(T=1|Z)\\
 - 2P(Y,T=1)P(Y,T=1|Z) - 2P(Y,T=1)P(Y|Z)P(T=1)
\end{align*}

The question is whether the last six terms can be factored in a way that shows that the whole discriminant is always positive. 

\subsubsection{Derivation Using $P(Y|T^*,Z) = P(Y|T^*)$}

Let's write this in a way that allows for $Y$ to follow an arbitrary distribution.
Let
\begin{align*}
F^*_{tk}(\tau) &= P(Y \leq \tau|T^*=t, z_k)\\
F_{tk}(\tau) &= P(Y \leq \tau|T=t, z_k)
\end{align*}
Now, by the assumption of non-differential measurement error,
\begin{align*}
  p_k F_{1k}(\tau) &= (1 - \alpha_1) p_k^* F_{1k}^*(\tau) + \alpha_0 (1 - p_k^*)F_{0k}^*(\tau)\\
  (1 - p_k) F_{0k}(\tau) &= \alpha_1 p_k^* F_{1k}^*(\tau) + (1 - \alpha_0) (1 - p_k^*)F_{0k}^*(\tau)
\end{align*}
Solving the linear system as above, we find that
\begin{align*}
  F_{0k}^*(\tau) &= F_{0k}(\tau) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ F_{0k}(\tau) - F_{1k}(\tau) \right]\\
  F_{1k}^*(\tau) &= F_{1k}(\tau) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ F_{1k}(\tau) - F_{0k}(\tau) \right]\\
\end{align*}
The conditional independence assumption is equivalent to
\begin{align*}
  F^*_{0k}(\tau) &= F^*_{0\ell}(\tau)\\
  F^*_{1k}(\tau) &= F^*_{1\ell}(\tau)
\end{align*}
for all $\tau$.
(If $Y$ is discrete, it's simpler to work with the analogous statement for probability mass functions.)
Imposing this on the solutions for $F_{0k}^*$ and $F_{1k}^*$ gives,
\begin{align*}
  F_{0k}(\tau) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ F_{0k}(\tau) - F_{1k}(\tau) \right] &= F_{0\ell}(\tau) + \left( \frac{\alpha_1 p_\ell}{1 - p_\ell - \alpha_1} \right)\left[ F_{0\ell}(\tau) - F_{1\ell}(\tau) \right] \\
  F_{1k}(\tau) + \left( \frac{\alpha_0(1 - p_k)}{p_k - \alpha_0}\right)\left[ F_{1k}(\tau) - F_{0k}(\tau) \right]  &= F_{1\ell}(\tau) + \left( \frac{\alpha_0(1 - p_\ell)}{p_\ell - \alpha_0}\right)\left[ F_{1\ell}(\tau) - F_{0\ell}(\tau) \right] 
\end{align*}
Simplifying these should give quadratics in $\alpha_0$ and $\alpha_1$ with the same solutions as those from above derived under the alternative version of the independence condition.


\subsection{Can we relax the measurement error assumptions?}
Suppose that we continue to assume that $P(Y|T^*,T,z) = P(Y|T^*,z)$ but relax the assumption that $P(T|T^*,z) = P(T|T^*)$.
Define:
\begin{align*}
  \alpha_{0k} &= P\left( T=1|T^*=1, z_k \right)\\
  \alpha_{1k} &= P\left( T=1|T^*=0, z_k \right)
\end{align*}
As before, the Wald estimator converges in probability to
\[
  \mathcal{W} = \frac{E[Y|z_k]-E[Y|z_\ell]}{p_k - p_\ell}
\]
but the relationship between $p_1 - p_0$ and the unobserved $p^*_1 - p^*_0$ changes.
By the law of total probability
\begin{align*}
  p_k &= P(T=1|z_k) = P(T=1|T^*=1,z_k)P(T^*=1|z_k) + P(T=1|T^*=0,z_k)P(T^*=0|z_k)\\
  &= (1 - \alpha_{1k})p_k^* + \alpha_{0k}(1 - p^*_k) = p_k^*(1 - \alpha_{0k} - \alpha_{1k}) + \alpha_{0k}
\end{align*}
and thus
\[
  p_k^* = \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}},
  \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_{1k}}{1 - \alpha_{0k} - \alpha_{1k}}.
\]
Thus, we have
\begin{align*}
  p^*_k - p^*_\ell &= \left( \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}} \right) - \left( \frac{p_0 - \alpha_{0\ell}}{1 - \alpha_{0\ell} - \alpha_{1\ell}} \right)\\
  &= \frac{\left( p_k - \alpha_{0k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right) - \left( p_0 - \alpha_{0\ell} \right)\left( 1 - \alpha_{0k} - \alpha_{1k} \right)}{\left( 1 - \alpha_{0k} - \alpha_{1k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right)}
\end{align*}



\subsection{Is there a LATE interpretation of our results?}
Let $J \in \left\{ a, c, d, n \right\}$ index an individual's \emph{type}: always-taker, complier, defier, or never-taker.
Let $\pi_a, \pi_c, \pi_d, \pi_n$ denote the population proportions of always-takers, compliers, defiers, and never-takers.
The unconfounded type assumption is $P(J=j|z=1) = P(J=j|z=0)$.
Combined with the law of total probability, this gives
\begin{align*}
  p^*_1 &= P(T^*=1|z=1) = \pi_a + \pi_c \\
  1 - p^*_1 &= P(T^*=0|z=1) = \pi_d + \pi_n \\
  p^*_0 &= P(T^*=1|z=0) = \pi_d + \pi_a \\
  1-p^*_0 &= P(T^*=0|z=0) = \pi_n + \pi_c 
\end{align*}
Imposing no-defiers, $\pi_d = 0$, these expressions simplify to
\begin{align*}
  p^*_1 &=  \pi_a + \pi_c \\
  1 - p^*_1 &=  \pi_n \\
  p^*_0 &=  \pi_a \\
  1-p^*_0 &=  \pi_n + \pi_c 
\end{align*}
Solving for $\pi_c$, we see that
\begin{align*}
  \pi_c &= p_1^* - p_0^*\\
  \pi_a &= p_0^*\\
  \pi_n &= 1 - p_1^*
\end{align*}

Now, let $Y(1)$ indicate the potential outcome when $T^*=1$ and $Y(0)$ indicate the potential outcome when $T^*=0$.
The standard LATE assumptions (no defiers, mean exclusion, unconfounded type) imply
\begin{align*}
  \mathbb{E}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{E}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{E}\left[ Y(1)|J=c \right] \\
  \mathbb{E}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=n \right]\\
  \mathbb{E}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{E}\left[ Y(1)|J=a \right]\\
  \mathbb{E}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{E}\left[ Y(0)|J=n \right]
\end{align*}



\subsubsection{LATE Version of Theorem 2 from the Draft}
\begin{align*}
  \Delta\overline{yT} &= \mathbb{E}\left( yT|z=1 \right) - \mathbb{E}\left( yT|z=0 \right) \\
  &= (1 - \alpha_1) \left[ p_1^* \mathbb{E}\left( y|T^*=1, z=1 \right) - p_0^* \mathbb{E}\left(y|T^*=1, z=0\right) \right] \\
  & \; \; \quad \quad + \alpha_0 \left[ (1 - p_1^*)\mathbb{E}\left( y|T^*=0, z=1\right) - (1 - p_0^*)\mathbb{E}\left(y|T^*,z=0 \right) \right]
\end{align*}
So we find that
\begin{align*}
  \Delta\overline{yT} &= (p_1^* - p_0^*)\left\{ (1 - \alpha_1) \mathbb{E}\left[ Y(1)|J=c \right] - \alpha_0\mathbb{E}\left[ Y(0)|J=c \right] \right\}\\
  &= (1 - \alpha_1) \left\{ \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1} (p_1 - p_0) \right\} + (p_1  - p_0) \mathbb{E}\left[ Y(0)|J=c \right]
\end{align*}
Recall that the analogous expression in the homogeneous treatment effect case is
\begin{align*}
  \Delta\overline{yT} &= (1 - \alpha_1) \mathcal{W} (p_1 - p_0) + \mu_{10}^*\\
  &= (1 - \alpha_1) \left(\frac{\beta}{1 - \alpha_0 - \alpha_1}\right) (p_1 - p_0) + (p_1 - \alpha_0)m_{11}^* - (p_0 - \alpha_0)m_{10}^*
\end{align*}
while the expression for the difference of variances is 
\begin{align*}
  \Delta\overline{y^2} &= \beta \mathcal{W}(p_1 - p_0) + 2\mathcal{W} \mu_{10}^*
\end{align*}
From above we see that the analogue of $\mu_{10}^*$ in the heterogeneous treatment effects setting is $(p_1 - p_0)E\left[ Y(0)|J=c \right]$ and since the LATE is $\mathbb{E}\left[ Y(1) - Y(0) |J=c\right]$, the analogue of $\mathcal{W}$ is
\[
  \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1}
\]
so \emph{if} we could establish that 
\[
  \Delta\overline{y^2} =  \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0) |J=c \right]
\]
in the heterogeneous treatment effects case, the proof of Theorem 2 would go through immediately.
Now, if we assume an exclusion restriction on the \emph{second} moment of $y$ an argument almost identical to the standard LATE derivation gives
\[
  \Delta\overline{y^2} = \frac{\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right]}{p_1^* - p_0^*} = \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right] 
\]
so we see that the necessary and sufficient condition for our proof to go through is 
\[
  \mathbb{E}\left[ Y^2(1) - Y^2(0)|J=c \right] = \mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0)|J=c \right]
\]
Rearranging, this in turn is equivalent to
\[
  \mbox{Var}\left[ Y(1)|J=c \right] = \mbox{Var}\left[ Y(0)|J=c \right]
\]

\subsubsection{Independence Restriction}
If we strengthen the exclusion restriction from $E[Y(T^*,z)] = E[Y(T^*)]$ to $Y(T^*,z) = Y(T^*)$, 
\begin{align*}
  \mathbb{P}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{P}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{P}\left[ Y(1)|J=c \right] \\
  \mathbb{P}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=n \right]\\
  \mathbb{P}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{P}\left[ Y(1)|J=a \right]\\
  \mathbb{P}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{P}\left[ Y(0)|J=n \right].
\end{align*}
Now suppose impose $P(Y|T^*,z) = P(Y|T^*)$ to identify $\alpha_0$ and $\alpha_1$.
What would this require us to assume about the potential outcomes?
Evaluating at each combination of $(T^*,z)$, the assumption is equivalent to
\begin{align*}
  P(Y|T^*=1,z=1) &= P(Y|T^*=1,z=0) = P(Y|T^*=1)\\
  P(Y|T^*=0,z=1) &= P(Y|T^*=0,z=0) = P(Y|T^*=0)
\end{align*}
Since
\[
  P(Y|T^*) = P(Y|T^*,z=1)P(z=1|T^*) + P(Y|T^*,z=0)\left[ 1 - P(z=1|T^*) \right]
\]
it suffices to establish that
\begin{align*}
  P(Y|T^*=1,z=0) &= P(Y|T^*=1,z=1) \\
  P(Y|T^*=0,z=1) &= P(Y|T^*=0,z=0)  
\end{align*}
From the expressions given at the beginning of this section, these are equivalent to 
\begin{align*}
  \mathbb{P}\left[ Y(1)|J=a \right] &=\left( \frac{p_0^*}{p_1^*} \right) \mathbb{P}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{P}\left[ Y(1)|J=c \right] \\ 
  \mathbb{P}\left[ Y(0)|J=n \right] &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=n \right]
\end{align*}
which are in turn equivalent to
\begin{align*}
  \mathbb{P}\left[ Y(1)|J=a \right] &= \mathbb{P}\left[ Y(1)|J=c \right] \\
  \mathbb{P}\left[ Y(0)|J=n \right] &=  \mathbb{P}\left[ Y(0)|J=c \right] 
\end{align*}
Thus, under the LATE assumptions from Kitagawa etc.\, $P(Y|T^*,z)=P(Y|T^*)$ is equivalent to assuming that the distribution of $Y(1)$ is the same for always-takers and compliers, and that the distribution of $Y(0)$ is the same for never-takers and compliers.
If we are prepared to assume this, then $\alpha_0$ and $\alpha_1$ are identified and can be used to identify a LATE. 

\subsection{Partial Identification Under Weak Independence Assumption}
Suppose we only make the LATE independence assumption $Y(T^*,z) = Y(T^*)$ rather than the conditional independence assumption $P(Y<\tau|T^*,z_k) = P(Y<\tau|T^*,z_\ell)$.
Then we still obtain
\begin{align*}
  \mathbb{P}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{P}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{P}\left[ Y(1)|J=c \right] \\
  \mathbb{P}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=n \right]\\
  \mathbb{P}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{P}\left[ Y(1)|J=a \right]\\
  \mathbb{P}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{P}\left[ Y(0)|J=n \right]
\end{align*}
From above, we also know that
\begin{align*}
  P(Y|T^*=0,z_k) &= P(Y|T=0, z_k) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ P(Y|T=0,z_k) - P(Y|T=1,z_k) \right]\\
  P(Y|T^*=1,z_k) &= P(Y|T=1, z_k) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ P(Y|T=1,z_k) - P(Y|T=0,z_k) \right]
\end{align*}
The notation is getting a bit unwieldy so let $\pi^*_{tk}(y)= P(Y=y|T^*=t,z_k)$ and similarly define $\pi_{tk}(y) = P(Y=y|T=t,z_k)$.
Using this new notation, we have
\begin{align*}
  (1 - p_k - \alpha_1) \pi^*_{0k}(y) &= (1 - p_k - \alpha_1) \pi_{0k}(y) + \alpha_1 p_k \left[ \pi_{0k}(y) - \pi_{1k}(y) \right]\\
  (p_k - \alpha_0) \pi_{1k}^*(y) &= (p_k - \alpha_0) \pi_{1k}(y) + \alpha_0 (1 - p_k)\left[ \pi_{1k}(y) - \pi_{0k}(y) \right]
\end{align*}
Writing these out for all values of $k$,
\begin{align*}
  (p_1 - \alpha_0) \pi_{11}^*(y) &= (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  (1 - p_0 - \alpha_1) \pi^*_{00}(y) &= (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0) \pi_{10}^*(y) &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) \pi^*_{01}(y) &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
Similarly, using the fact that $p_k^* = (p_k - \alpha_0)/(1 - \alpha_0 - \alpha_1)$,  
\begin{align*}
  \pi^*_{11}(y) &= \left( \frac{p_0 - \alpha_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=a \right] + \left( \frac{p_1 - p_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=c \right]\\
  \pi^*_{00}(y) &= \left( \frac{p_1 - p_0}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1 - \alpha_1}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=n \right]\\
  \pi^*_{10}(y) &= P\left[ Y(1)|J=a \right]\\
  \pi^*_{01}(y) &= P\left[ Y(0)|J=n \right] 
\end{align*}
or equivalently,
\begin{align*}
  (p_1 - \alpha_0)\pi^*_{11}(y) &= \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right]\\
  (1 - p_0 - \alpha_1)\pi^*_{00}(y) &=\left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right]\\
  (p_0 - \alpha_0)\pi^*_{10}(y) &= (p_0 - \alpha_0)P\left[ Y(1)|J=a \right]\\
  (1 - p_1 - \alpha_1)\pi^*_{01}(y) &= (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] 
\end{align*}
Equating,
\begin{align*}
  \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
and substituting the third and fourth equalities into the first and second we obtain
\footnotesize
\begin{align*}
   (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Simplifying and re-arranging,
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y) + (1 - p_1)\pi_{01}(y) - (1 - p_0) \pi_{00}(y) }{p_1 - p_0} \right] \\ 
  P\left[Y(0) =y|J=c \right] &= \left[ \frac{(1 - p_0)\pi_{00}(y) - (1 - p_1)\pi_{01}(y)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{(1 - p_0) \pi_{00}(y) - (1 - p_1)\pi_{01}(y) + p_0 \pi_{10}(y) - p_1 \pi_{11}(y)}{p_1 - p_0} \right] \\
  P\left[Y(1) = y|J=a \right] &=  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  P\left[ Y(0) = y|J=n \right] &=  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Notice that the first two equations can be simplified as follows
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{P(Y = y,T=1|z=1) - P(Y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y =y|z=0)}{p_1 - p_0} \right] \\ 
  P\left[Y(0) = y|J=c \right] &= \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] 
\end{align*}
\normalsize
Now, since probabilities must be between zero and one, we obtain the bounds
\begin{align*}
  0 &\leq \left[ \frac{P(Y = y,T=1|z=1) - P(Y = y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y = y|z=0)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] \leq 1 
\end{align*}
\normalsize
which we abbreviate 
\begin{align*}
  0 &\leq \left[ \frac{\Delta P(Y=y,T=1)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \alpha_1 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] - \left[\frac{ \Delta P(Y=y,T=0)}{p_1 - p_0} \right] \leq 1 
\end{align*}
where
\begin{align*}
  \Delta P(Y=y) &= P(Y=y|z=1) - P(Y=y|z=0)\\
  \Delta P(Y=y, T=t) &= P(Y=y, T=t|z=1) - P(Y=y,T=t|z=0).
\end{align*}
To manipulate these bounds, we need to know the sign of $R = \Delta P(Y=y)/(p_1 - p_0)
$. 
Presumably this will be positive for most values of $y$, but it could be negative.
\paragraph{Case I: $R$ is positive.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)}\\
  \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)}
\end{align*}

\paragraph{Case II: $R$ is negative.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} \\
  \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)}
\end{align*}
Note that we \emph{two-sided} bounds for the misclassification probabilities.
These may be trivial in some cases, but I don't think it's obvious that they always will be.
\todo[inline]{Do these bounds have anything to do with the testability of the LATE assumptions? That is, do we get a lower bound for measurement error \emph{precisely when} we would otherwise violate a testable LATE assumption?}

Note that we also obtain bounds from the potential outcome distributions of always-takers and never-takers, namely 
\begin{align*}
  0 &\leq  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right] \leq 1\\
  0&\leq  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right] \leq 1
\end{align*}
but these are redundant.
From the assumption of non-differential measurement error, we already have 
\begin{align*}
  \pi_{0k}^* &= \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \\
  \pi_{1k}^* &= \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) 
\end{align*}
for all $k$ as given at the beginning of this section.
These expressions imply
\begin{align*}
 0 &\leq \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \leq 1 \\
  0 &\leq \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) \leq 1
\end{align*}
Re-arranging, we have
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \pi_{0k} + \alpha_1 p_k \left( \pi_{0k} - \pi_{1k} \right) \leq 1 - p_k - \alpha_1 \\
 0 &\leq p_k \pi_{1k}- \alpha_0\pi_{1k} + \alpha_0 (1 - p_k) \left( \pi_{1k} - \pi_{0k} \right) \leq p_k - \alpha_0
\end{align*}
and thus
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \left[(1 - p_k)\pi_{0k} + p_k \pi_{1k} \right] \leq 1 - p_k - \alpha_1 \\
  0 &\leq p_k \pi_{1k} - \alpha_0\left[p_k\pi_{1k} + (1 - p_k)\pi_{0k}\right] \leq p_k - \alpha_0
\end{align*}
Now consider the first inequality.
Re-arranging the right-hand side we obtain
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)(1 - \pi_{0k})}{1 - \left[ (1 - p_k)\pi_{0k} + p_k \pi_{1k} \right]} = (1 - p_k) \left[ \frac{P(Y=0|T=0, z=k)}{P(Y=0|z=k)} \right]
\end{align*}
and re-arranging the left-hand side we find
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)\pi_{0k}}{(1 - p_k)\pi_{0k} + p_k \pi_{1k}} = (1 - p_k) \left[ \frac{P(Y=1|T=0,z=k)}{P(Y=1|z=k)} \right]
\end{align*}
For the second inequality, the left-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k \pi_{1k}}{p_k \pi_{1k} + (1 - p_k)\pi_{0k}} = p_k \left[ \frac{P(Y=1|T=1,z=k)}{P(Y=1|z_k)} \right] 
\end{align*}
while the right-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k (1 - \pi_{1k})}{1 - \left[ p_k \pi_{1k} + (1 - p_k) \pi_{0k} \right]} = p_k \left[ \frac{P(Y=0|T=1,z=k)}{P(Y=0|z=k)} \right]
\end{align*}
These are analogous to our CDF bounds from above although they may not be tighter than the bounds 
\[
  \alpha_0 \leq p_k, \quad \alpha_1 \leq (1 - p_k)
\]
because we cannot argue, as we did above, about a limit in which the ratio of CDFs approaches one.
As before, however, we can take the tightest bound over $k = 0, 1$.

\subsection{Bounding the LATE}
Even if we didn't know anything about $\alpha_0$ and $\alpha_1$ beyond the fact that they are probabilities, it looks like we could still bound the LATE.
I think we can do this without using the independence of the instrument, that is only using the mean exclusion restriction.
Write out the LATE expressions with the $\alpha_0$ and $\alpha_1$ in them and them just plug in zero and one.
Could then tighten the bounds by imposing additional assumptions to get bounds for $\alpha_0$ and $\alpha_1$, from weakest to strongest.
If you have an independent instrument, you also get bounds for the outcome distributions.
Need to think some more about this\ldots


\subsection{Stochastic Dominance Conditions}
What if we imposed a stochastic ordering, e.g.\ $Y(1) > Y(0)$ for compliers?
Presumably this would give joint bounds for $\alpha_0$ and $\alpha_1$ from the LATE expressions from above.
Alternatively, perhaps one would choose to impose an ordering on the $Y(0)$ distributions for compliers versus never-takers or the $Y(1)$ distributions for the compliers versus always-takers.
This might be interesting in situations where one is concerned that the assumption we need for identification does not in fact hold and should give additional bounds.


\section{Outline For New Draft}
\begin{enumerate}
  \item Introduction / Literature Review
  \item Mahajan/Lewbell-style Assumptions 
    \begin{enumerate}
      \item Setup and Assumptions:
        \begin{itemize}
          \item Homogenous treatment effect model (additively separable)
          \item Conditional mean version of non-differential measurement error assumption.
          \item Conditional mean independence for IV.
        \end{itemize}
      \item Show that the model is not identified, regardless of (discrete) support of IV.
      \item Derive sharp bounds for $\alpha_0, \alpha_1$ and treatment effect.
      \item Show that second and third-moment independence for IV identifies this model? Maybe this isn't interesting in and of itself?
    \end{enumerate}
  \item Independence Assumptions
    \begin{enumerate}
      \item Motivation
    \begin{itemize}
      \item Showed above that stronger assumptions are needed for identification, but the additional moment restrictions seem a bit artificial.
      \item When instruments are derived from economic theory that yields conditional mean independence only, we wouldn't want to use them.
      \item They would make sense, however, in an an RCT or natural experiment.
      \item The whole point in these settings is \emph{not} to rely on functional form assumptions. It would be strange to say that $z$ is an instrument for $y$ but not $\log y$.
      \item This points towards an \emph{independence} assumption for the instrument.
      \item Can make a similar argument for measurement error: seems strange to assume that $T$ is non-differential for $y$ but not $\log y$.
    \end{itemize}
      \item Sharp Bounds for $\alpha_0$ and $\alpha_1$ without valid instrument  
        \begin{itemize}
          \item Assume ``independence'' version of non-differential measurement error.
          \item Derive CDF bounds.
        \end{itemize}
      \item Conditional Independence for Instrument
        \begin{itemize}
          \item Exactly what assumptions do we need here? Just conditional independence for instrument, or are we using ``independence'' form of non-differential measurement error?
          \item Show how this gives us a nice clean way to identify $\alpha_0$ and $\alpha_1$.
          \item Overidentifying restrictions? Test model?
        \end{itemize}
    \end{enumerate}
  \item LATE Model
    \begin{enumerate}
      \item Introduction
        \begin{itemize}
          \item Most of the existing mis-classification literature focuses on a homogeneous treatment effects model.
          \item What if we don't have an additively separable model?
          \item These results complement Ura because we work under the assumption of non-differential measurement error while he asks what can be learned when one is unwilling to make any assumptions about the form of the mis-classification.
        \end{itemize}
      \item Mahajan/Lewbel Setup
        \begin{itemize}
          \item Presumably the partial identification results go through for a LATE.
          \item The second and third moment conditions would require restrictions on form of heterogeneity.
        \end{itemize}
      \item Independence Assumptions
        \begin{itemize}
          \item Presumably the CDF bounds go through as before but need to state exact form of independence assumption in terms of potential outcomes.
          \item Kitagawa-style independence assumption for IV: $Y(T^*,z) = Y(T^*)$. This gives bounds for all quantile treatment effects.
          \item Stochastic dominance conditions?
          \item If we assume $Y(0)$ is the same for never-takers and compliers and $Y(1)$ is the same for always-takers and compliers, then the independence condition identifies the model
        \end{itemize}
    \end{enumerate}
  \item Estimation / Inference
  \item Simulation Study
  \item Empirical Examples
    \begin{itemize}
      \item Try to look at a number of examples under different assumptions to illustrate both point and partial identification results. Don't forget about Oreopoulous: the sample size is so huge that inference isn't a major concern.
    \end{itemize}
\end{enumerate}

\end{document}
