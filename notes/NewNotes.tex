\documentclass[12pt]{article}
\usepackage{../frankstyle}

\title{Notes for Paper on Mis-measured, Binary, Endogenous Regressors}
\author{Francis J.\ DiTraglia \& Camilo Garc\'{i}a-Jimeno}

\begin{document}

\maketitle

\section{Model and Notation}

\paragraph{Probabilities}
\begin{eqnarray*}
p^*_{tk} &=& P(T^*=t, Z=k)\\
p_{tk} &=& P(T=t, Z=k)\\
p^*_k &=& P(T^* = 1|Z = k)\\
p_k &=& P(T = 1|Z = k)\\
q &=& P(Z = 1)
\end{eqnarray*}

\begin{eqnarray*}
  p^*_{00} &=& P(T^* = 0|Z=0)P(Z=0) = (1 - p_0^*)(1 - q) =  \left( \frac{1 - p_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{10} &=& P(T^* = 1|Z=0)P(Z=0) = p_0^*(1 - q) =  \left( \frac{p_0 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)(1 - q)\\
  p^*_{01} &=& P(T^* = 0|Z=1)P(Z=1) = (1 - p_1^*)q =  \left( \frac{1 - p_1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) q\\
  p^*_{11} &=& P(T^* = 1|Z=1)P(Z=1) = p_1^*(1 - q)  =  \left( \frac{p_1 - \alpha_0}{1 - \alpha_0 - \alpha_1} \right)q
\end{eqnarray*}

\paragraph{CDFs}
For $t, Z \in \left\{ 0,1 \right\}$ define
\begin{eqnarray*}
F_{tk}^*(\tau) &=&  P(Y \leq \tau|T^* = t, Z = k) \\
F_{tk}(\tau) &=&  P(Y \leq \tau|T = t, Z = k)\\
F_k(\tau) &=& P(Y \leq \tau | Z=k) 
\end{eqnarray*}
Note that the second two are observed for all $t,k$ while the first is never observed since it depends on the unobserved RV $T^*$.


\section{Weakest Bounds on $\alpha_0, \alpha_1$}
Assume that $\alpha_0 + \alpha_1 < 1$ that $T$ is independent of $Z$ conditional on $T^*$.
These standard assumptions turn out to yield informative bounds on $\alpha_0$ and $\alpha_1$ without \emph{any further restrictions of any kind}.
In particular, we assume nothing about the validity of the instrument $Z$ and nothing about the relationship between the mis-classification error and the outcome $Y$: we impose only that the mis-classification error rates do not depend on $z$ and that the mis-classification is not so bad that $1 - T$ is a better measure of $T^*$ than $T$. 

By the Law of Total Probability and the assumption that $T$ is conditionally independent of $Z$ given $T^*$,
\begin{eqnarray*}
  p_k &=& P(T=1|Z=k,T^*=0) (1 - p_k^*) + P(T=1|Z=k,T^*=1)p_k^*\\
  &=& P(T=1|T^*=0)(1 - p_k^*) + P(T=1|T^*=1)p_k^*\\
  &=& \alpha_0 (1 - p_k^*) + (1 - \alpha_1) p_k^*\\
  &=& \alpha_0 +(1 - \alpha_0 - \alpha_1) p_k^* 
\end{eqnarray*}
and similarly 
\begin{eqnarray*}
  1 - p_k &=& P(T=0|Z=k,T^*=0) (1 - p_k^*) + P(T=0|Z=k,T^*=1)p_k^*\\
  &=& P(T=0|T^*=0)(1 - p_k^*) + P(T=0|T^*=1)p_k^*\\
  &=& (1 - \alpha_0)(1 - p_k^*) + \alpha_1 p_k^*\\
  &=& \alpha_1 + (1 - p_k^*)(1 - \alpha_0 - \alpha_1)
\end{eqnarray*}
and hence
\begin{eqnarray*}
  p_k - \alpha_0 &=& (1 - \alpha_0 - \alpha_1)p_k^*\\
  (1 - p_k) - \alpha_1 &=& (1 - \alpha_0 - \alpha_1)(1 - p_k^*)
\end{eqnarray*}
Now, since $p_k^*$ and $(1 - p_k^*)$ are probabilities they are between zero and one which means that the sign of $p_k - \alpha_0$ as well as that of $(1 - p_k) - \alpha_1$ are both determined by that of $1 - \alpha_0 - \alpha_1$.
Accordingly, provided that $1 - \alpha_0 - \alpha_1 < 1$, we have
\begin{eqnarray*}
  \alpha_0 &<& p_k\\
  \alpha_1 &<& (1 - p_k)
\end{eqnarray*}
so long as $p_k^*$ does not equal zero or one, which is not a realistic case for any example that we consider.
Since these bounds hold for all $k$, we can take the tightest bound over all values of $Z$.

\todo[inline]{Important: using these to bound $\beta$ gives $\beta \in [\mbox{ITT}, \mbox{Wald}]$.}

\section{Stronger Bounds for $\alpha_0, \alpha_1$}
Now suppose we add the assumption that $T$ is conditionally independent of $Y$ given $T^*$. 
This is essentially the non-differential measurement error assumption although it is slightly stronger than the version used by Mahajan (2006) who assumes only conditional mean independence.
This assumption allows us to considerably strengthen the bounds from the preceding section by exploiting information contained in the conditional distribution of $Y$ given $T$ and $Z$.
The key ingredient is a relationship that we can derive between the unobservable distributions $F_{tk}^*$ and the observable distributions $F_{tk}$ using this new conditional independence assumption.
To begin, note that by Bayes' rule we have
\begin{eqnarray*}
  P(T^*=1|T=1, Z=k) &=& P(T=1 | T^*=1) \left(\frac{p_k^*}{p_k}\right) = (1 - \alpha_1)\left( \frac{p_k^*}{p_k} \right)\\
  P(T^*=1|T=0, Z=k) &=& P(T=0 | T^*=1) \left(\frac{p_k^*}{1 - p_k}\right) = \alpha_1 \left( \frac{p_k^*}{1 - p_k} \right)\\
  P(T^*=0|T=1, Z=k) &=& P(T=1 | T^*=0) \left(\frac{1 - p_k^*}{p_k}\right) = \alpha_0 \left( \frac{1 - p_k^*}{p_k} \right)\\
  P(T^*=0|T=0, Z=k) &=& P(T=0 | T^*=0) \left(\frac{1 - p_k^*}{1 - p_k}\right) = (1 - \alpha_0)\left( \frac{1 - p_k^*}{1 - p_k} \right)
\end{eqnarray*}
Now, by the conditional independence assumption
\begin{eqnarray*}
  P(Y\leq \tau|T^* = 0, T=t , Z = k) = P(Y \leq \tau|T^*=0, Z =k) = F_{0k}^*(\tau)\\
  P(Y\leq \tau|T^* = 1, T=t , Z = k) = P(Y \leq \tau|T^*=1, Z =k) = F_{1k}^*(\tau)
\end{eqnarray*}
Finally, putting everything together using the Law of Total Probability, we find that
\begin{eqnarray*}
  (1 - p_k) F_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  p_k F_{1k}(\tau) = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$.
Defining the shorthand 
\begin{eqnarray*}
  \widetilde{F}_{0k}(\tau)&\equiv& (1 - p_k) F_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\equiv& p_k F_{1k}(\tau) 
\end{eqnarray*}
this becomes
\begin{eqnarray}
  \label{eq:F0kTilde}
  \widetilde{F}_{0k}(\tau) = (1 - \alpha_0) (1 - p^*_k)F_{0k}^*(\tau) + \alpha_1 p_k^* F_{1k}^*(\tau)\\ 
  \label{eq:F1kTilde}
  \widetilde{F}_{1k}(\tau)  = \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + (1 - \alpha_1)p_k^* F_{1k}^*(\tau)
\end{eqnarray}
Now, solving Equation \ref{eq:F0kTilde} for $p_k^* F_{1k}^*(\tau)$ we have
\[
  p_{k}^* F_{1k}^*(\tau) = \frac{1}{\alpha_1}\left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) (1 - p_k^*) F_{0k}^*(\tau)\right]
\]
Substituting this into Equation \ref{eq:F1kTilde},
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \alpha_0 (1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{\alpha_1} \left[ \widetilde{F}_{0k}(\tau) - (1 - \alpha_0) ( 1 - p_k^*)F_{0k}^*(\tau) \right]\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \alpha_0 - \frac{(1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) + \left[ \frac{\alpha_0 \alpha_1 - (1 - \alpha_1)(1 - \alpha_0)}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ (1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{\alpha_1} \right](1 - p_k^*) F_{0k}^*(\tau)\\
  &=& \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) - \left[ \frac{ 1 - \alpha_1 -  \alpha_0 }{\alpha_1} \right]\left( \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1} \right) F_{0k}^*(\tau)\\
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{1 - \alpha_1}{\alpha_1} \widetilde{F}_{0k}(\tau) -  \frac{1 - p_k - \alpha_1}{\alpha_1}  F_{0k}^*(\tau)
  \label{eq:F1kTildeAlpha1}
\end{equation}
Equation \ref{eq:F1kTildeAlpha1} relates the observable $\widetilde{F}_{1k}(\tau)$ to the mis-classification error rate $\alpha_1$ and the unobservable CDF $F_{0k}^*\left( \tau \right)$.
Since $F_{0k}^*(\tau)$ is a CDF, however, it lies in the interval $\left[ 0,1 \right]$.
Accordingly, substituting $0$ in place of $F^*_{0k}(\tau)$ gives 
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_leq_a1}
\end{equation}
while substituting $1$ gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{1 - \alpha_1}{\alpha_1}\widetilde{F}_{0k}(\tau) - \frac{1 - p_k - \alpha_1}{\alpha_1}
  \label{eq:F1ktilde_F0kTilde_geq_a1}
\end{equation}
Rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a1}
\begin{eqnarray*}
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau)\\
 \alpha_1 \widetilde{F}_{1k}(\tau) &\leq& \widetilde{F}_{0k}(\tau) - \alpha_1 \widetilde{F}_{0k}(\tau)\\
 \alpha_1 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right]&\leq& \widetilde{F}_{0k}(\tau) 
\end{eqnarray*}
since $\alpha_1 \in [0,1]$ and therefore
\begin{equation}
  \alpha_1  \leq \frac{\widetilde{F}_{0k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = (1 - p_k) \left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
  \label{eq:Alpha1_Bound1}
\end{equation}
since $\widetilde{F}_{1k}(\tau) + \widetilde{F}_{1k}(\tau) \geq 0$.
Proceeding similarly for Equation \ref{eq:F1ktilde_F0kTilde_geq_a1},
\begin{eqnarray*}
  \alpha_1 \widetilde{F}_{1k}(\tau) &\geq& (1 - \alpha_1)\widetilde{F}_{0k}(\tau) - (1 - p_k - \alpha_1)\\
  \alpha_1 \left[\widetilde{F}_{1k}(\tau) + \widetilde{F}_{0k}(\tau) - 1\right] &\geq& \widetilde{F}_{0k}(\tau) - (1 - p_k)\\
  -\alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\geq& -\left[1 - \widetilde{F}_{0k}(\tau) - p_k \right]\\
  \alpha_1 \left[ 1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \right] &\leq& 1 - \widetilde{F}_{0k}(\tau) - p_k 
\end{eqnarray*}
Now since $\widetilde{F}_{1k}(\tau) = p_k F_{1k}(\tau) \leq p_k$ and $\widetilde{F}_{0k}(\tau) = (1 - p_k) F_{0k}(\tau) \leq (1 - p_k)$ it follows that $1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau) \geq 0$ and hence
\begin{equation}
  \alpha_1 \leq \frac{1 - \widetilde{F}_{0k}(\tau) - p_k}{1 - \widetilde{F}_{1k}(\tau) - \widetilde{F}_{0k}(\tau)} = (1 - p_k) \left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha1_Bound2}
\end{equation}
The bounds given in Equations \ref{eq:Alpha1_Bound1} and \ref{eq:Alpha1_Bound2} relate $\alpha_1$ to observable quantities \emph{only} and hold for all values of $\tau$ for which their respective denominators are non-zero.
Moreover, these bounds hold for any value $k$ that the instrument takes on.

We can proceed similarly for $\alpha_0$.
First solve Equation \ref{eq:F0kTilde} for $(1 - p_k^*)F^*_{0k}(\tau)$:
\[
  (1 - p_k^*)F^*_{0k}(\tau) = \frac{1}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right]
\]
and then substitute into Equation \ref{eq:F1kTilde}:
\begin{eqnarray*}
  \widetilde{F}_{1k}(\tau) &=&  \frac{\alpha_0}{1 - \alpha_0}\left[ \widetilde{F}_{0k}(\tau) - \alpha_1 p_k^* F_{1k}^*(\tau)\right] + (1 - \alpha_1) p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ (1 - \alpha_1) - \frac{\alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{(1 - \alpha_1)(1 - \alpha_0) - \alpha_0 \alpha_1}{1 - \alpha_0}   \right] p_k^* F_{1k}^*(\tau) \\
  &=& \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \left[ \frac{1 - \alpha_0 - \alpha_1}{1 - \alpha_0}   \right] \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1} F_{1k}^*(\tau) 
\end{eqnarray*}
and therefore
\begin{equation}
  \widetilde{F}_{1k}(\tau) = \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) +  \frac{p_k - \alpha_0}{1 - \alpha_0} F_{1k}^*(\tau) 
\end{equation}
Now we can again obtain two bounds by substituting the smallest and largest possible values of $F_{1k}^*(\tau)$.
Substituting zero gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \geq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau)
  \label{eq:F1ktilde_F0kTilde_geq_a0}
\end{equation}
while substituting one gives
\begin{equation}
  \widetilde{F}_{1k}(\tau) \leq \frac{\alpha_0}{1 - \alpha_0} \widetilde{F}_{0k}(\tau) + \frac{p_k - \alpha_0}{1 - \alpha_0}
  \label{eq:F1ktilde_F0kTilde_leq_a0}
\end{equation}
Now, rearranging Equation \ref{eq:F1ktilde_F0kTilde_geq_a0}, 
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \widetilde{F}_{0k}(\tau) \\
  \widetilde{F}_{1k}(\tau) &\geq& \alpha_0 \left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] 
\end{eqnarray*}
since $1 - \alpha_0 \geq 0$.
Therefore,
\begin{equation}
  \alpha_0 \leq \frac{\widetilde{F}_{1k}(\tau)}{\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{F_{1k}(\tau)}{F_{k}(\tau)}\right]
  \label{eq:Alpha0_Bound1}
\end{equation}
since $\left[\widetilde{F}_{0k}(\tau) + \widetilde{F}_{1k}(\tau)\right] \geq 0$.
Similarly, rearranging Equation \ref{eq:F1ktilde_F0kTilde_leq_a0}
\begin{eqnarray*}
  (1 - \alpha_0)\widetilde{F}_{1k}(\tau) &\leq& \alpha_0\widetilde{F}_{0k}(\tau) + p_k - \alpha_0\\
  \widetilde{F}_{1k}(\tau) - p_k &\leq& \alpha_0\left[\widetilde{F}_{0k}(\tau)  + \widetilde{F}_{1k}(\tau) - 1 \right] \\
  -\left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\leq& -\alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] \\
  \left[1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)\right] &\geq& \alpha_0\left[1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)  \right] 
\end{eqnarray*}
Therefore
\begin{equation}
\alpha_0 \leq \frac{1 - \widetilde{F}_{1k}(\tau) - ( 1 - p_k)}{1 - \widetilde{F}_{0k}(\tau)  - \widetilde{F}_{1k}(\tau)} = p_k\left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right]
  \label{eq:Alpha0_Bound2}
\end{equation}

\paragraph{Putting Everything Together} 
For all $k$ we have
\begin{equation}
  \alpha_0 \leq p_k \min_\tau\left\{\left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{1k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq p_k 
\end{equation}
\begin{equation}
  \alpha_1 \leq (1 - p_k) \min_\tau \left\{\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right] \wedge \left[\frac{1-F_{0k}(\tau)}{1 - F_k(\tau)} \right]\right\} \leq (1 - p_k) 
\end{equation}
Note that these bounds can only improve upon those derived in the previous section since the ratio of CDFs tends to one as $\tau \rightarrow \infty$.
To derive these tighter bounds we have made no assumption regarding the relationship between $Z$ and the error term $\varepsilon$.
These bounds use only the assumption that $\alpha_0 + \alpha_1 < 1$, and the assumption that $T$ is conditionally independent of $Z,Y$ given $T^*$.
Notice that that the bounds are related.
In particular,
\[
  p_k \left[\frac{F_{1k}(\tau)}{F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{F_{0k}(\tau)}{F_k(\tau)}\right]
\]
and 
\[
p_k \left[\frac{1 - F_{1k}(\tau)}{1 - F_k(\tau)}\right] = 1 - (1-p_k)\left[\frac{1 - F_{0k}(\tau)}{1 - F_k(\tau)}\right]
\]


\section{Even Stronger Bounds on $\alpha_0, \alpha_1$}
Try applying the stochastic dominance conditions from our simulation study.

\section{Independent Instrument}
Assume that $Z \perp U$.  
The model is $Y = \beta T^* + U$ and
\[ F_{U}(\tau) = P(U \leq\tau) = P(Y - \beta T^* \leq \tau)\]
but if $Z$ is independent of $U$ then it follows that
\begin{eqnarray*}
F_U(\tau) &=&  F_{U|Z=k}(\tau) = P(U\leq \tau |Z=k) = P(Y  - \beta T^* \leq \tau |Z=k)\\
&=&  P(Y \leq \tau |T^* = 0, Z = k)(1 - p_k^*) + P(Y\leq \tau + \beta| T^* = 1, Z = k)p_k^* \\
&=& (1 - p_k^*) F^*_{0k}(\tau) + p_k^* F^*_{1k}(\tau + \beta)
\end{eqnarray*} 
for all $k$ by the Law of Total Probability.
Similarly, 
\[ F_k(\tau) = (1 - p_k^*) F_{0k}^*(\tau)  + p_k^* F_{1k}^*(\tau)\]
and rearranging
\[  (1 - p_k^*) F_{0k}^*(\tau)  = F_k(\tau) - p_k^* F_{1k}^*(\tau)\]
Substituting this expression into the equation for $F_U(\tau)$ from above, we have
\[F_U(\tau) = F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]\]
for all $k$ and all $\tau$.
Evaluating at two values $k$ and $\ell$ in the support of $Z$ and equating 
\[ F_k(\tau) + p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right] =  F_\ell(\tau) + p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right]\]
or equivalently
\begin{equation}
 F_k(\tau) - F_\ell(\tau) =  p_\ell^* \left[ F_{1\ell}^*(\tau+ \beta) - F_{1\ell}^*(\tau)\right] - p_k^* \left[ F_{1k}^*(\tau+ \beta) - F_{1k}^*(\tau)\right]  
 \label{eq:CDFs1}
\end{equation}
for all $\tau$.
Now we simply need to re-express all of the ``star'' quantities, namely $p_k^*, p_\ell^*$ and $F_{1k}^*, F_{1\ell}^*$ in terms of $\alpha_0, \alpha_1$ and the \emph{observable} probability distributions $F_{1k}$ and $F_{1\ell}$ and observable probabilities $p_k, p_\ell$.
To do this, we use the fact that
\begin{eqnarray*}
  F_{0k}(\tau) &=& \frac{1 - \alpha_0}{1 - p_k} (1 - p^*_k)F_{0k}^*(\tau) + \frac{\alpha_1}{1 - p_k}p_k^* F_{1k}^*(\tau)\\ \\
  F_{1k}(\tau) &=& \frac{ \alpha_0}{p_k}(1 - p_k^*) F_{0k}^*(\tau) + \frac{1 - \alpha_1}{p_k}p_k^* F_{1k}^*(\tau)
\end{eqnarray*}
for all $k$ by Bayes' rule.
Solving these equations,
\begin{equation*}
  p_k^* F_{1k}^*(\tau) = \frac{1 - \alpha_0}{1 - \alpha_0 - \alpha_1} p_k F_{1k}(\tau) - \frac{\alpha_0}{1 - \alpha_0 - \alpha_1} (1 - p_k) F_{0k}(\tau) 
\end{equation*}
for all $k$.
Combining this with Equation \ref{eq:CDFs1}, we find that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Now, define
\[
  \Delta^\tau_{tk}(\beta) = F_{tk}(\tau + \beta) - F_{tk}(\tau) = E\left[ \frac{\mathbf{1}\left\{ T = t, Z = k \right\}}{p_{tk}}\left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right) \right]
\]
and note that we can express $F_k(\tau) - F_\ell(\tau)$ similarly as 
\[
  F_k(\tau)  - F_{\ell}(\tau) = E\left[ \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right) \right]
\]
Using this notation, we can write the preceding as
\begin{equation*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_{\ell}(\tau) \right] = \alpha_0\left[ (1 - p_k) \Delta^\tau_{0k}(\beta) - (1 - p_\ell) \Delta^\tau_{0\ell}(\beta) \right] - (1 - \alpha_0)\left[ p_k \Delta^\tau_{1k}(\beta) - p_\ell \Delta^\tau_{1\ell}(\beta) \right]
\end{equation*}
or in moment-condition form
\begin{align*}
   E\Bigg[ &(1 - \alpha_0 - \alpha_1) \mathbf{1}\left\{ Y \leq \tau \right\} \left( \frac{\mathbf{1}\left\{ Z = k \right\}}{q_k} - \frac{\mathbf{1}\left\{ Z = \ell \right\}}{q_\ell} \right)  - 
   \left( \mathbf{1}\left\{ Y \leq \tau + \beta \right\} - \mathbf{1}\left\{ Y \leq \tau \right\} \right)\Bigg\{ \\
   &\alpha_0 \bigg((1 - p_k)\frac{\mathbf{1}\left\{ T = 0, Z = k \right\}}{p_{0k}} - 
    (1 - p_\ell)\frac{\mathbf{1}\left\{ T = 0, Z = \ell \right\}}{p_{0\ell}}\bigg)\\
   &-(1 - \alpha_0) \bigg( p_k\frac{\mathbf{1}\left\{ T = 1, Z = k \right\}}{p_{1k}} - 
 p_\ell \frac{\mathbf{1}\left\{ T = 1, Z = \ell \right\}}{p_{1\ell}}\bigg) \Bigg\}\Bigg] = 0
\end{align*}
Each value of $\tau$ yields a moment condition.

\section{Special Case: $\alpha_0 = 0$}
In this case the expressions from above simplify to
\begin{align}
  (1 - \alpha_1)\left[ F_k(\tau) - F_\ell(\tau)\right] = \left[ p_\ell F_{1\ell}(\tau + \beta) 
 - p_k  F_{1k}(\tau+ \beta) 
 - p_\ell F_{1\ell}(\tau) 
 + p_k F_{1k}(\tau) \right]
 \label{eq:specialCDF}
\end{align}
for all $\tau$.
Now, provided that all of the CDFs are differentiable we have\footnote{There must be a way to generalize this using Lebesgue.}
\begin{align*}
  e^{i\omega \tau}(1 - \alpha_1)\left[f_k(\tau) - f_\ell(\tau)\right] = e^{i\omega \tau}\left[ p_\ell f_{1\ell}(\tau + \beta) - p_k  f_{1k}(\tau+ \beta) - p_\ell f_{1\ell}(\tau) + p_k f_{1k}(\tau) \right]
\end{align*}
where we have pre-multiplied both sides by $e^{i\omega \tau}$.
Finally, integrating both sides with respect to $\tau$ over $(-\infty, \infty)$, we have
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] = \left\{  \int_{-\infty}^{\infty} e^{i\omega \tau} \left[p_\ell f_{1\ell}(\tau + \beta) - p_k f_{1k}(\tau+ \beta)\right] \; d\tau - p_\ell \varphi_{1\ell}(\omega) + p_k \varphi_{1k}(\omega) \right\}
\end{align*}
where $\varphi_k$ is the conditional characteristic function of $Y$ given $Z=k$ and $\varphi_{1k}$ is the conditional characteristic function of $Y$ given $T=1, Z=k$.
Finally, 
\begin{align*}
  \int_{-\infty}^{\infty} e^{i\omega \tau} p_\ell f_{1\ell}(\tau + \beta) \; d\tau &=  e^{ i\omega \beta } p_\ell \int_{u = -\infty + \beta}^{u = \infty + \beta} e^{ i\omega u }f_{1\ell}(u)\; du \\
  &= e^{-i\omega \beta } p_\ell \varphi_{1\ell}(\omega)
\end{align*}
using the substitution $u = \tau + \beta$.
Changing subscripts, the same holds for $k$ and thus
\begin{align*}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  e^{-i\omega \beta}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] +  \left[p_k \varphi_{1k}(\omega) -  p_\ell \varphi_{1\ell}(\omega)\right]
\end{align*}
which, after collecting terms, simplifies to
\begin{align}
  (1 - \alpha_1)\left[\varphi_k(\omega) - \varphi_\ell(\omega)\right] =  \left(e^{-i\omega \beta} - 1\right)\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] 
  \label{eq:CharacteristicSpecial}
\end{align}
for all $\omega$.  
Equation \ref{eq:CharacteristicSpecial} contains exactly the same information as Equation \ref{eq:specialCDF} but gives us a more convenient way to prove identification since $\beta$ enters in a simpler way.
Leibniz's formula for the $r$th derivative of a product of two functions $f$ and $g$ is:
\begin{align*}
  (fg)^{(r)} = \sum_{s=0}^r {r \choose s} f^{(s)}g^{(r-s)}
\end{align*}
where $f^{(r)}$ denotes the $r$th derivative of the function $f$ and $g^{(r-s)}$ denotes the $(r-s)$th derivative of the function $g$.
Applying this to the RHS, $R(\omega)$ of Equation \ref{eq:CharacteristicSpecial} gives
\begin{align*}
  \frac{d}{d\omega^r}R(\omega)
  &=  \sum_{s=0}^r {r \choose s} \frac{d}{d\omega^s}\left( e^{-i\omega\beta} - 1\right)\frac{d}{d\omega^{r - s}}\left[ p_\ell \varphi_{1\ell}(\omega) - p_k \varphi_{1k}(\omega) \right] \\
  &= \left( e^{-i\omega \beta} - 1 \right) \left[ p_\ell \varphi_{1\ell}^{(r)}(\omega) - p_k \varphi_{1k}^{r}(\varphi) \right] + e^{-i\omega\beta} \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(\omega) - p_k \varphi^{(r-s)}_{1k}(\omega) \right] 
\end{align*}
where we split off the $s=0$ term because our generic expression for the $s$th derivative of $(e^{-i\omega\beta} - 1)$ only applies for $s\geq 1$.
Evaluating at zero:
\begin{align*}
  \frac{d}{d\omega^r}R(0)
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Combining this with the LHS of Equation \ref{eq:CharacteristicSpecial}, also differentiated $r$ times and evaluated at zero, we have
\begin{align*}
  (1 - \alpha_1) \left[ \varphi_{k}^{(r)}(0) - \varphi_{\ell}^{(r)}(0) \right] 
  &= \sum_{s=1}^r {r \choose s} (-i\beta)^{s}\left[ p_\ell \varphi^{(r-s)}_{1\ell}(0) - p_k \varphi^{(r-s)}_{1k}(0) \right] 
\end{align*}
Now, recall that if $\varphi(\omega)$ is the characteristic function of $Y$ then $\varphi^{(r)}(0) = i^r E[Y^r]$ provided that the expectation exists where $\varphi^{(r)}$ denotes the $r$th derivative of $\varphi$.
The same applies for the conditional characteristic functions we consider here.
Hence, provided that the $r$th moments exist, 
\footnotesize
\begin{align*}
  i^r(1 - \alpha_1)\left\{ E[Y^r|Z=k] - E[Y^r|Z=\ell]\right\} = \sum_{s=1}^r {r \choose s} (-i\beta)^s i^{r-s}\left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
After simplifying the terms involving $i$ and cancelling them from both sides, 
\small
\begin{align*}
  (1 - \alpha_1)\left(E[Y^r|Z=k] - E[Y^r|Z=\ell]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E\left[ Y^{r-s}|T=1, Z=\ell \right] - p_k E\left[ Y^{r-s}|T=1,Z=k \right] \right)
\end{align*}
\normalsize
again provided that the moments exist.
Abbreviating the conditional expectations according to $E[Y^r|Z=k] = E_k[Y^r]$ and $E[Y^r|T=t,Z=k] = E_{tk}[Y^r]$, this becomes
\begin{equation}
  (1 - \alpha_1)\left(E_k[Y^r] - E_\ell[Y^r]\right) = \sum_{s=1}^r {r \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{r-s}\right] - p_k E_{1k}\left[ Y^{r-s}\right] \right)
  \label{eq:MomentsSpecial}
\end{equation}
Equation \ref{eq:MomentsSpecial} can be used to generate moment equations that are implied by the Equation \ref{eq:CharacteristicSpecial} and the equivalent representation in terms of CDFs: Equation \ref{eq:specialCDF}.
Assuming that the conditional first moments exist, we can evaluate Equation \ref{eq:MomentsSpecial} at $r=1$, yielding
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y] - E_\ell[Y]\right) &= \sum_{s=1}^1 {1 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{1-s}\right] - p_k E_{1k}\left[ Y^{1-s}\right] \right)\\
  &=  - \beta\left( p_\ell - p_k \right) 
\end{align*}
Rearranging, this gives us the expression for the probability limit of the Wald estimator
\begin{equation}
  \mathcal{W} \equiv \frac{E_{k}[Y]- E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_1} 
  \label{eq:WaldSpecial}
\end{equation}
Evaluating Equation \ref{eq:MomentsSpecial} at $r = 2$, we have
\begin{align*}
  (1 - \alpha_1)\left(E_k[Y^2] - E_\ell[Y^2]\right) &= \sum_{s=1}^2 {2 \choose s} (-\beta)^s \left( p_{\ell} E_{1\ell}\left[ Y^{2-s}\right] - p_k E_{1k}\left[ Y^{2-s}\right] \right)\\
  &= 2\beta\left( p_k E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta^2\left( p_k - p_{\ell} \right)
\end{align*}
Rearranging, we have
\begin{equation}
  E_k[Y^2] - E_\ell[Y^2] 
  =  \frac{\beta}{1 - \alpha_1}\left[2\left( p_k  E_{1k}[Y] -  p_\ell E_{1\ell}[Y]\right) - \beta(p_k - p_\ell)\right]
  \label{eq:SpecialSquared}
\end{equation}
Substituting Equation \ref{eq:WaldSpecial}, we can replace $\beta/(1-\alpha_1)$ with a function of observables only, namely $\mathcal{W}$.
Solving, we find that 
\begin{align}
  \beta &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} 
  \label{eq:BetaSpecial}
\end{align}
This allows us to state low-level sufficient conditions for identification:
\begin{enumerate}[(a)]
  \item $\alpha_1 < 1$
  \item $p_k \neq p_\ell$ 
  \item $E_k[Y] \neq E_\ell[Y]$ 
  \item $E_{1k}[|Y|], E_{1\ell}[|Y|], E_k[|Y^2|], E_\ell[|Y^2|] < \infty$.
\end{enumerate}
Note that, although $\beta = 0$ is always a solution of Equation \ref{eq:specialCDF} this solution is ruled out by the assumption that $E_k[Y] \neq E_\ell[Y]$ via Equation \ref{eq:WaldSpecial}.
The mis-classification error rate $\alpha_1$ is likewise uniquely identified under these assumptions.
Substituting $\beta/\mathcal{W} = 1-\alpha_1$ into Equation \ref{eq:BetaSpecial}
\begin{align*}
  (1 - \alpha_1) &= \left\{ \frac{p_k - p_\ell}{E_k[Y] - E_\ell[Y]} \right\}\left\{\frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{p_k - p_\ell} - \frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{E_k[Y] - E_\ell[Y]} \right\}\\
  &= \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} - (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\}
\end{align*}
and thus
\begin{align*}
  \alpha_1
  &= 1 + (p_k - p_\ell)\left\{\frac{E_{k}[Y^2] - E_{\ell}[Y^2]}{\left(E_k[Y] - E_\ell[Y]\right)^2} \right\} - \frac{2\left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right)}{E_k[Y] - E_{\ell}[Y]} 
\end{align*}

\section{Identification in the General Case}

\section{Characteristic Functions}
Recall from above that in the general case an independent instrument combined with non-differential measurement error implies that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
Using the same steps as in the preceding section, we can convert this expression into characteristic function form by differentiating each side, multiplying by $e^{i\omega\tau}$ and then integrating with respect to $\tau$, yielding
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left[ \varphi_k(\omega) - \varphi_{\ell}(\omega) \right] &= \alpha_0 \left\{ (1 - p_k)\left(e^{-i\omega\beta} - 1\right)\varphi_{0k}(\omega) - (1 - p_\ell)\left( e^{-i\omega\beta} - 1\right) \varphi_{0\ell}(\omega)  \right\}\\
  &\quad - (1 - \alpha_0) \left\{ p_k\left(e^{-i\omega\beta} - 1 \right)\varphi_{1k}(\omega) - p_\ell \left( e^{-i\omega\beta} - 1\right) \varphi_{1\ell}(\omega) \right\}
\end{align*}
which simplifies to
\begin{align*}
  \varphi_k(\omega) - \varphi_{\ell}(\omega) &= \left( e^{-i\omega\beta} - 1 \right)\left(\frac{\alpha_0\left[ (1 - p_k)\varphi_{0k}(\omega) - (1-p_\ell)\varphi_{0\ell}(\omega) \right]  - (1 - \alpha_0)\left[ p_k \varphi_{1k}(\omega) - p_\ell \varphi_{1\ell}(\omega) \right]}{1 - \alpha_0 - \alpha_1}\right).
\end{align*}
As above, we will differentiate both sides of this expression $r$ times and evaluate at $\omega = 0$.
Steps nearly identical to those given above yield
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left(  E_k[Y^r] - E_\ell[Y^r]\right) 
  &= \alpha_0 \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{ (1 - p_k) E_{0k}[Y^{r-s}] - (1 - p_\ell) E_{0\ell}[Y^{r-s}] \right\}\\
  &\quad - (1 - \alpha_0) \sum_{s=1}^r {r \choose s} (-\beta)^s \left\{p_k E_{1k}[Y^{r-s}] - p_\ell E_{1\ell}[Y^{r-s}] \right\}
\end{align*}

\paragraph{First Moments}
Taking $r = 1$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left( E_k[Y] - E_{\ell}[Y] \right) = \beta (p_k - p_\ell)
\end{align*}
Simplifying,
\begin{equation}
  \mathcal{W} \equiv \frac{E_k[Y] - E_{\ell}[Y]}{p_k - p_\ell} = \frac{\beta}{1 - \alpha_0 - \alpha_1}
  \label{eq:Wald}
\end{equation}

\paragraph{Second Moments}
Now, taking $r = 2$ gives
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left( E_{k}[Y^2] - E_{\ell}[Y^2] \right) &=
  \alpha_0\left\{ \left[ (1 - p_k) E_{0k}[Y] - (1 - p_\ell) E_{0\ell} \right] - \beta^2\left( p_k - p_\ell \right) \right\}\\
  &\quad  -(1 - \alpha_0)\left\{ -2\beta\left( p_k E_{1k}[Y] - p_{\ell}E_{1\ell}[Y] \right) + \beta^2\left( p_k - p_\ell \right) \right\}\\
  &= -2\beta \alpha_0\left\{ (1 - p_k)E_{0k}[Y] - (1 - p_\ell) E_{0\ell}[Y] p_k E_{1k}[Y] + p_{\ell}E_{1\ell}[Y]\right\} \\ 
  &\quad +2\beta \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) 
  - (p_k - p_\ell)\beta^2\left( \alpha_0 + 1 - \alpha_0 \right)\\
  &= -2\beta\left\{ \alpha_0 \left( E_k[Y] - E_\ell[Y] \right) - \left( p_k E_{1k}[Y] - p_\ell E_{1\ell}[Y] \right) \right\} - \beta^2(p_k - p_\ell)
\end{align*}
Now, simplifying
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\beta \alpha_0 \left(\frac{E_k[Y]-E_k[Y]}{p_k - p_\ell}\right) + 2\beta \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) - \beta^2
\end{align*}
and substituting Equation \ref{eq:Wald} to eliminate $\beta$, this becomes
\small
\begin{align*}
  (1 - \alpha_0 - \alpha_1)\left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 (1 - \alpha_0 - \alpha_1)\mathcal{W}^2 + 2\mathcal{W}(1 - \alpha_0 - \alpha_1) \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right) \\
  &\quad \quad - (1 - \alpha_0 - \alpha_1)^2 \mathcal{W}^2\\
  \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)&= -2\alpha_0 \mathcal{W}^2 + 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2
\end{align*}
\normalsize
And thus, simplifying
\begin{align*}
  -2\alpha_0 \mathcal{W}^2 - (1 - \alpha_0 - \alpha_1) \mathcal{W}^2 &= \left(\frac{E_k[Y^2] - E_{\ell}[Y^2]}{p_k - p_\ell} \right)- 2\mathcal{W} \left( \frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{p_k - p_\ell} \right)  \\
  \alpha_1  - \alpha_0   &= 1 +  \left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\mathcal{W}^2(p_k - p_\ell)} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{\mathcal{W}(p_k - p_\ell)} \right] 
\end{align*}
and therefore
\begin{equation}
  \alpha_1  - \alpha_0  = 1 +  (p_k - p_\ell)\left[\frac{E_k[Y^2] - E_{\ell}[Y^2]}{\left( E_k[Y] - E_\ell[Y] \right)^2} \right]-2  \left[\frac{p_{1k}E_{1k}[Y] - p_\ell E_{1\ell}[Y]}{E_k[Y] - E_\ell[Y]} \right] 
\end{equation}

\paragraph{``Product'' Moments}
Recall that in our initial draft of the paper we worked with moments such as $E[TY|Z=k], E[TY|Z=\ell]$ and $E[TY^2|Z=k], E[TY^2|Z=\ell]$.
In the notation of this document, we can express these quantities as follows:
\begin{align*}
  E[TY^r|z=k] &= E[TY^r|T=1,z=k]p_k + E[TY^r|T=0,z=k](1 - p_k)\\
  &= p_k E[Y^r|T=1,z=k] + 0\\
  &= p_k E_{1k}[Y^r]
\end{align*}
for any $r$. 
We will use this relationship to motivate some shorthand notation below.

\paragraph{Some Shorthand}
The notation above is becoming very cumbersome and we haven't even looked at the third moments yet! 
To make life easier, define the following: 
\begin{align*}
  \widetilde{y^r_{1k}} &= p_k E_{1k}[Y^r] \\
  \widetilde{y^r_{0k}} &= (1 - p_k) E_{1k}[Y^r] \\
  \Delta \overline{y^r} &= E_k[Y^r] - E_\ell[Y^r]\\
  \Delta \overline{Ty^r} &= p_k E_{1k}[Y^r] - p_\ell E_{1\ell}[Y^r] = \widetilde{y^r_{1k}} - \widetilde{y^r_{1k}}\\
  \mathcal{W} &= (E_k[Y] - E_\ell[Y]) / (p_k - p_\ell)
\end{align*}
for all $r$.
When no $r$ superscript is given this means $r=1$.
Note, moreover, that when $r =0$ we have $\widetilde{y_{1k}^0} = p_k$ and $\widetilde{y_{0k}^0} = (1 - p_k)$.
Thus $\Delta \overline{Ty^0} = p_k - p_\ell$.
In contrast, $\Delta y^0 = 0$.

Among other things, this notation will make it easier for us to link the derivations here to our earlier derivations from the first draft of the paper that used slightly different notation and did not work explicitly with the independence of the instrument.

\paragraph{Simplifying the Moment Equalities}
Using the final two pieces of notation defined in the preceding section, we can re-rewrite the collection of moment equalities arising from the characteristic function equations as
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left[\alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right) \right]
\end{align*}
Now, simplifying the terms in the square brackets,
\begin{align*}
  \alpha_0 \left( \widetilde{y^{r-s}_{0k}} - \widetilde{y^{r-s}_{0\ell}} \right) - (1 - \alpha_0) \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)
  &= \alpha_0\left[ \left( \widetilde{y_{0k}^{r-s}} + \widetilde{y_{1k}^{r-s}} \right) - \left( \widetilde{y_{0\ell}^{r-s}} + \widetilde{y_{1\ell}^{r-s}} \right)  \right] - \left( \widetilde{y^{r-s}_{1k}} - \widetilde{y^{r-s}_{1\ell}} \right)\\
  &= \alpha_0\left( E_k[Y^{r-s}] - E_\ell[Y^{r-s}] \right) - \Delta \overline{Ty^{r-s}}\\
  &= \alpha_0 \Delta \overline{y^{r-s}} - \Delta\overline{Ty^{r-s}}
\end{align*}
and hence
\begin{align}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^r} 
  &= \sum_{s=1}^r {r \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{r-s}} - \Delta\overline{Ty^{r-s}} \right) 
  \label{eq:MomentEqualitiesSimplified}
\end{align}

\paragraph{Third Moments}
Evaluating Equation \ref{eq:MomentEqualitiesSimplified} at $r=3$ 
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \Delta \overline{y^3} 
  &= \sum_{s=1}^3 {3 \choose s} (-\beta)^s \left( \alpha_0 \Delta\overline{y^{3-s}} - \Delta\overline{Ty^{3-s}} \right) \\
  &= -3\beta\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta^2\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^3 (p_k - p_\ell)
\end{align*}

\paragraph{Solving the System}
Using $\mathcal{W} = \beta/(1 - \alpha_0 - \alpha_1)$ we can re-write the third moment expression as follows
\begin{align*}
  \Delta \overline{y^3} &= -3\mathcal{W}\left( \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} \right) + 3\beta \mathcal{W}\left( \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} \right) + \beta^2 \mathcal{W} (p_k - p_\ell)\\
  \frac{\Delta \overline{y^3}}{\mathcal{W} (p_k - p_\ell)} 
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2} - \Delta\overline{Ty^2} }{p_k - p_\ell}\right) \\
  \frac{\Delta \overline{y^3} - 3\mathcal{W}\Delta\overline{y^2T}}{\mathcal{W}(p_k - p_\ell)}
  &= \beta^2 + 3\beta \left(\frac{ \alpha_0 \Delta\overline{y} - \Delta\overline{Ty} }{p_k - p_\ell}\right) -3\left(\frac{ \alpha_0 \Delta\overline{y^2}  }{p_k - p_\ell}\right) 
\end{align*}
Now, translating the second moment equation into the shorthand notation defined above, we have


\paragraph{Simplifying the Characteristic Function Equation}
From above, we have
\begin{align*}
  \varphi_k(\omega) - \varphi_{\ell}(\omega) &= \left( e^{-i\omega\beta} - 1 \right)\left(\frac{\alpha_0\left[ (1 - p_k)\varphi_{0k}(\omega) - (1-p_\ell)\varphi_{0\ell}(\omega) \right]  - (1 - \alpha_0)\left[ p_k \varphi_{1k}(\omega) - p_\ell \varphi_{1\ell}(\omega) \right]}{1 - \alpha_0 - \alpha_1}\right).
\end{align*}
Using the fact that $\varphi_{k} = p_k \varphi_{1k} + (1 - p_k) \varphi_{0k}$, we can simplify this further, yielding
\[
(1 - \alpha_0 - \alpha_1) = \left( e^{-i\omega \beta} - 1 \right)\left[ \alpha_0 - \xi(\omega)\right] 
\]
where we define
\[
  \xi(\omega) \equiv \frac{\varphi_k(\omega) - \varphi_\ell(\omega)}{p_k \varphi_{1k}(\omega) - p_\ell\varphi_{1\ell}(\omega)}
\]
Now, re-arranging
\[
  (1 - \alpha_1) - \xi(\omega) = e^{-i\omega\beta}\left[ \alpha_0 - \xi(\omega) \right]  
\]
or equivalently
\[
  e^{i\omega\beta}\left[(1 - \alpha_1) - \xi(\omega)\right] =  \alpha_0 - \xi(\omega) 
\]
or
\[
  e^{i\omega \beta} = \frac{\alpha_0 - \xi(\omega)}{(1 - \alpha_1) - \xi(\omega)}
\]
provided the denominator does not vanish.
By taking differences or ratios evaluated at $\omega_1$ and $\omega_2$ we can eliminate $\beta$, $\alpha_0$ or $\alpha_1$ but it's not clear how or if we can prove identification in terms of a restriction on the characteristic functions.

Suppose we consider three values $\omega_1, \omega_2$ and $\omega_3$ for which that yield to distinct, non-zero values $\xi_1, \xi_2$ and $\xi_3$ of $\xi(\omega)$.
\begin{align*}
  e^{i\omega_1\beta}\left[ (1 - \alpha_1) - \xi_1 \right] - e^{i\omega_2\beta}\left[ (1 - \alpha_1) - \xi_2 \right] = \xi_2 - \xi_1
\end{align*}

\subsection{Simplifying the Characteristic CDF Equation}
Recall from above that
\begin{align*}
  (1 - \alpha_0 - \alpha_1) \left[ F_k(\tau) - F_\ell(\tau) \right] &= \alpha_0 \left\{ (1 - p_{k})\left[F_{0k}(\tau + \beta) - F_{0k}(\tau)  \right] - (1 - p_\ell)\left[ F_{0\ell}(\tau + \beta) - F_{0\ell}(\tau)  \right] \right\}\\
  &- (1 - \alpha_0)\left\{ p_k\left[ F_{1k}(\tau + \beta) - F_{1k}(\tau) \right] - p_\ell \left[ F_{1\ell}(\tau+ \beta) - F_{1\ell}(\tau) \right] \right\}
\end{align*}
We can simplify the RHS as follows
\begin{align*}
  \mbox{RHS} &= \alpha_0 \left\{ \left[ F_k(\tau + \beta) - F_\ell(\tau + \beta) \right] - \left[ F_k(\tau) - F_\ell(\tau) \right] \right\}\\
  &- \left\{ \left[ p_k F_{1k}(\tau + \beta) - p_\ell F_{1\ell}(\tau + \beta) \right]  - \left[ p_k F_{1k}(\tau) - p_\ell F_{1\ell}(\tau) \right]\right\}
\end{align*}
Now, define
\begin{align*}
  \Delta(\tau) &= F_k(\tau) - F_\ell(\tau)\\
  \widetilde{\Delta}_1(\tau) &= p_k F_{1k}(\tau) - p_\ell F_{1\ell}(\tau)
\end{align*}
Using this notation, our equation becomes
\[
  (1 - \alpha_0 - \alpha_1) \Delta(\tau) = \alpha_0 \left[ \Delta(\tau + \beta) - \Delta(\tau)\right] - \left[ \widetilde{\Delta}_1(\tau+\beta) - \widetilde{\Delta}_1(\tau)\right]
\]
which simplifies to
\[
    \widetilde{\Delta}_1(\tau+\beta) - \widetilde{\Delta}_1(\tau) = \alpha_0 \Delta(\tau + \beta) - (1 - \alpha_1) \Delta(\tau)
\]

\paragraph{Suppose $\alpha_0 = 0$:}
In this case we obtain
\[
  (1 - \alpha_1)  = \frac{ \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}
\]
Now, evaluating at two values of $\tau$ and taking differences, we find
\[
  \frac{ \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)} - 
  \frac{ \widetilde{\Delta}_1(\tau') - \widetilde{\Delta}_1(\tau' + \beta)}{\Delta(\tau')} = 0
\]

\paragraph{Suppose $\alpha_1 = 0$:}
In this case we obtain
\[
  \alpha_0 = \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + \Delta(\tau)}{\Delta(\tau + \beta)}
\]
Again, taking differences evaluated at two values of $\tau$,
\[
  \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + \Delta(\tau)}{\Delta(\tau + \beta)} - 
  \frac{\widetilde{\Delta}_1(\tau' + \beta) - \widetilde{\Delta}_1(\tau') + \Delta(\tau')}{\Delta(\tau' + \beta)} = 0
\]

\paragraph{Some Equations to Check Numerically}
We can use the same basic idea when either $\alpha_0$ or $\alpha_1$ is known but nonzero.
This isn't realistic in practice, but can be used to check our equations:
\begin{align*}
  \alpha_0 &= \frac{\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau) + (1 - \alpha_1) \Delta(\tau)}{\Delta(\tau + \beta)}\\ \\
  (1 - \alpha_1) &= \frac{\alpha_0 \Delta(\tau + \beta) + \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}
\end{align*}
As above, after substituting the true value of either $\alpha_1$ or $\alpha_0$, we can eliminate the remaining mis-classification probability by evaluating at two quantiles $\tau$, $\tau'$ and taking differences.
\todo[inline]{These appear to work just fine!}

\paragraph{What if $\alpha_0$ and $\alpha_1$ are both unknown?}
Suppose we take differences at two quantiles $\tau$ and $\nu$ to eliminate $\alpha_1$:
\begin{align*}
  \left[\frac{\alpha_0 \Delta(\tau + \beta) + \widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau + \beta)}{\Delta(\tau)}\right]
  - \left[\frac{\alpha_0 \Delta(\nu + \beta) + \widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0 \\ 
  \alpha_0 \left[ \frac{\Delta(\tau+ \beta)}{\Delta(\tau)} - \frac{\Delta(\nu + \beta)}{\Delta(\nu)} \right] - \left[ \frac{\widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau+ \beta)}{\Delta(\tau)} - \frac{\widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0
 \end{align*}

 \paragraph{The Equation that Didn't Work\ldots}
\[
  \frac{[\widetilde{\Delta}_1(\tau + \beta) - \widetilde{\Delta}_1(\tau)] - [\widetilde{\Delta}_1(\tau' + \beta) - \widetilde{\Delta}_1(\tau')]}{\Delta(\tau + \beta) - \Delta(\tau' + \beta)}
- \frac{[\widetilde{\Delta}_1(\nu + \beta) - \widetilde{\Delta}_1(\nu)] - [\widetilde{\Delta}_1(\nu' + \beta) - \widetilde{\Delta}_1(\nu')]}{\Delta(\nu + \beta) - \Delta(\nu' + \beta)} = 0
 \]
 where $\Delta(\nu) = \Delta(\nu')$ and $\Delta(\tau) = \Delta(\tau')$.
%Now, recall that $\Delta(\tau)$ is a difference of CDFs. 
%This means that it its limits as $\tau \rightarrow +\infty$ and as $\tau \rightarrow -\infty$ both equal zero.
%If $Y$ is continuous, then it follows that for any $\tau$ we can always find a $\tau' \neq \tau$ such that $\Delta(\tau) = \Delta(\tau')$.
%Now if we take \emph{another} difference, between pairs  $(\tau, \nu)$ and $(\tau', \nu')$ such that $\Delta(\tau) = \Delta(\tau')$ and $\Delta(\nu) = \Delta(\nu')$, the $\alpha_0$ term disappears:
%\begin{align*}
%  \left[ \frac{\widetilde{\Delta}_1(\tau) - \widetilde{\Delta}_1(\tau+ \beta)}{\Delta(\tau)} - \frac{\widetilde{\Delta}_1(\nu) - \widetilde{\Delta}_1(\nu + \beta)}{\Delta(\nu)}\right] &= 0
% \end{align*}
%But so long as $\widetilde{\Delta}_1(\tau) \neq \widetilde{\Delta}_1(\tau')$ and  $\widetilde{\Delta}_1(\nu) \neq \widetilde{\Delta}_1(\nu')$ the equation itself does not vanish and can be used to solve for $\beta$.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{New Results from September 2016}

\subsubsection{Relationship between observed and unobserved CDFs}
Let
\begin{align*}
F^*_{tk}(\tau) &= P(Y \leq \tau|T^*=t, z_k)\\
F_{tk}(\tau) &= P(Y \leq \tau|T=t, z_k)
\end{align*}
Now, by the assumption of non-differential measurement error,
\begin{align*}
  p_k F_{1k}(\tau) &= (1 - \alpha_1) p_k^* F_{1k}^*(\tau) + \alpha_0 (1 - p_k^*)F_{0k}^*(\tau)\\
  (1 - p_k) F_{0k}(\tau) &= \alpha_1 p_k^* F_{1k}^*(\tau) + (1 - \alpha_0) (1 - p_k^*)F_{0k}^*(\tau)
\end{align*}
Solving the linear system as above, we find that
\begin{align*}
  F_{0k}^*(\tau) &= F_{0k}(\tau) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ F_{0k}(\tau) - F_{1k}(\tau) \right]\\
  F_{1k}^*(\tau) &= F_{1k}(\tau) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ F_{1k}(\tau) - F_{0k}(\tau) \right]\\
\end{align*}

\subsection{Can we relax the measurement error assumptions?}
Suppose that we continue to assume that $P(Y|T^*,T,z) = P(Y|T^*,z)$ but relax the assumption that $P(T|T^*,z) = P(T|T^*)$.
Define:
\begin{align*}
  \alpha_{0k} &= P\left( T=1|T^*=1, z_k \right)\\
  \alpha_{1k} &= P\left( T=1|T^*=0, z_k \right)
\end{align*}
As before, the Wald estimator converges in probability to
\[
  \mathcal{W} = \frac{E[Y|z_k]-E[Y|z_\ell]}{p_k - p_\ell}
\]
but the relationship between $p_1 - p_0$ and the unobserved $p^*_1 - p^*_0$ changes.
By the law of total probability
\begin{align*}
  p_k &= P(T=1|z_k) = P(T=1|T^*=1,z_k)P(T^*=1|z_k) + P(T=1|T^*=0,z_k)P(T^*=0|z_k)\\
  &= (1 - \alpha_{1k})p_k^* + \alpha_{0k}(1 - p^*_k) = p_k^*(1 - \alpha_{0k} - \alpha_{1k}) + \alpha_{0k}
\end{align*}
and thus
\[
  p_k^* = \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}},
  \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_{1k}}{1 - \alpha_{0k} - \alpha_{1k}}.
\]
Thus, we have
\begin{align*}
  p^*_k - p^*_\ell &= \left( \frac{p_k - \alpha_{0k}}{1 - \alpha_{0k} - \alpha_{1k}} \right) - \left( \frac{p_0 - \alpha_{0\ell}}{1 - \alpha_{0\ell} - \alpha_{1\ell}} \right)\\
  &= \frac{\left( p_k - \alpha_{0k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right) - \left( p_0 - \alpha_{0\ell} \right)\left( 1 - \alpha_{0k} - \alpha_{1k} \right)}{\left( 1 - \alpha_{0k} - \alpha_{1k} \right)\left( 1 - \alpha_{0\ell} - \alpha_{1\ell} \right)}
\end{align*}



\subsection{Is there a LATE interpretation of our results?}
Let $J \in \left\{ a, c, d, n \right\}$ index an individual's \emph{type}: always-taker, complier, defier, or never-taker.
Let $\pi_a, \pi_c, \pi_d, \pi_n$ denote the population proportions of always-takers, compliers, defiers, and never-takers.
The unconfounded type assumption is $P(J=j|z=1) = P(J=j|z=0)$.
Combined with the law of total probability, this gives
\begin{align*}
  p^*_1 &= P(T^*=1|z=1) = \pi_a + \pi_c \\
  1 - p^*_1 &= P(T^*=0|z=1) = \pi_d + \pi_n \\
  p^*_0 &= P(T^*=1|z=0) = \pi_d + \pi_a \\
  1-p^*_0 &= P(T^*=0|z=0) = \pi_n + \pi_c 
\end{align*}
Imposing no-defiers, $\pi_d = 0$, these expressions simplify to
\begin{align*}
  p^*_1 &=  \pi_a + \pi_c \\
  1 - p^*_1 &=  \pi_n \\
  p^*_0 &=  \pi_a \\
  1-p^*_0 &=  \pi_n + \pi_c 
\end{align*}
Solving for $\pi_c$, we see that
\begin{align*}
  \pi_c &= p_1^* - p_0^*\\
  \pi_a &= p_0^*\\
  \pi_n &= 1 - p_1^*
\end{align*}

Now, let $Y(1)$ indicate the potential outcome when $T^*=1$ and $Y(0)$ indicate the potential outcome when $T^*=0$.
The standard LATE assumptions (no defiers, mean exclusion, unconfounded type) imply
\begin{align*}
  \mathbb{E}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{E}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{E}\left[ Y(1)|J=c \right] \\
  \mathbb{E}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{E}\left[ Y(0)|J=n \right]\\
  \mathbb{E}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{E}\left[ Y(1)|J=a \right]\\
  \mathbb{E}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{E}\left[ Y(0)|J=n \right]
\end{align*}



\subsubsection{LATE Version of Theorem 2 from the Draft}
\begin{align*}
  \Delta\overline{yT} &= \mathbb{E}\left( yT|z=1 \right) - \mathbb{E}\left( yT|z=0 \right) \\
  &= (1 - \alpha_1) \left[ p_1^* \mathbb{E}\left( y|T^*=1, z=1 \right) - p_0^* \mathbb{E}\left(y|T^*=1, z=0\right) \right] \\
  & \; \; \quad \quad + \alpha_0 \left[ (1 - p_1^*)\mathbb{E}\left( y|T^*=0, z=1\right) - (1 - p_0^*)\mathbb{E}\left(y|T^*,z=0 \right) \right]
\end{align*}
So we find that
\begin{align*}
  \Delta\overline{yT} &= (p_1^* - p_0^*)\left\{ (1 - \alpha_1) \mathbb{E}\left[ Y(1)|J=c \right] - \alpha_0\mathbb{E}\left[ Y(0)|J=c \right] \right\}\\
  &= (1 - \alpha_1) \left\{ \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1} (p_1 - p_0) \right\} + (p_1  - p_0) \mathbb{E}\left[ Y(0)|J=c \right]
\end{align*}
Recall that the analogous expression in the homogeneous treatment effect case is
\begin{align*}
  \Delta\overline{yT} &= (1 - \alpha_1) \mathcal{W} (p_1 - p_0) + \mu_{10}^*\\
  &= (1 - \alpha_1) \left(\frac{\beta}{1 - \alpha_0 - \alpha_1}\right) (p_1 - p_0) + (p_1 - \alpha_0)m_{11}^* - (p_0 - \alpha_0)m_{10}^*
\end{align*}
while the expression for the difference of variances is 
\begin{align*}
  \Delta\overline{y^2} &= \beta \mathcal{W}(p_1 - p_0) + 2\mathcal{W} \mu_{10}^*
\end{align*}
From above we see that the analogue of $\mu_{10}^*$ in the heterogeneous treatment effects setting is $(p_1 - p_0)E\left[ Y(0)|J=c \right]$ and since the LATE is $\mathbb{E}\left[ Y(1) - Y(0) |J=c\right]$, the analogue of $\mathcal{W}$ is
\[
  \frac{\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]}{1 - \alpha_0 - \alpha_1}
\]
so \emph{if} we could establish that 
\[
  \Delta\overline{y^2} =  \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0) |J=c \right]
\]
in the heterogeneous treatment effects case, the proof of Theorem 2 would go through immediately.
Now, if we assume an exclusion restriction on the \emph{second} moment of $y$ an argument almost identical to the standard LATE derivation gives
\[
  \Delta\overline{y^2} = \frac{\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right]}{p_1^* - p_0^*} = \left( \frac{p_1 - p_0}{1 - \alpha_0 - \alpha_1} \right)\mathbb{E}\left[ Y^2(1) - Y^2(0) |J=c \right] 
\]
so we see that the necessary and sufficient condition for our proof to go through is 
\[
  \mathbb{E}\left[ Y^2(1) - Y^2(0)|J=c \right] = \mathbb{E}\left[ Y(1) - Y(0)|J=c \right]\cdot \mathbb{E}\left[ Y(1) + Y(0)|J=c \right]
\]
Rearranging, this in turn is equivalent to
\[
  \mbox{Var}\left[ Y(1)|J=c \right] = \mbox{Var}\left[ Y(0)|J=c \right]
\]


\subsection{Partial Identification Under Independence Assumption}
Suppose we only make the LATE independence assumption $Y(T^*,z) = Y(T^*)$ rather than the conditional independence assumption $P(Y<\tau|T^*,z_k) = P(Y<\tau|T^*,z_\ell)$.
Then we still obtain
\begin{align*}
  \mathbb{P}\left( Y| T^* = 1, z = 1 \right) &= \left( \frac{p_0^*}{p_1^*} \right) \mathbb{P}\left[ Y(1)|J=a \right] + \left( \frac{p_1^* - p_0^*}{p_1^*} \right)\mathbb{P}\left[ Y(1)|J=c \right] \\
  \mathbb{P}\left( Y| T^* = 0, z = 0 \right) &= \left( \frac{p_1^* - p_0^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1^*}{1 - p_0^*} \right)\mathbb{P}\left[ Y(0)|J=n \right]\\
  \mathbb{P}\left( Y| T^* = 1, z = 0 \right) &= \mathbb{P}\left[ Y(1)|J=a \right]\\
  \mathbb{P}\left( Y| T^* = 0, z = 1 \right) &= \mathbb{P}\left[ Y(0)|J=n \right]
\end{align*}
From above, we also know that
\begin{align*}
  P(Y|T^*=0,z_k) &= P(Y|T=0, z_k) + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right)\left[ P(Y|T=0,z_k) - P(Y|T=1,z_k) \right]\\
  P(Y|T^*=1,z_k) &= P(Y|T=1, z_k) + \left( \frac{\alpha_0 (1-p_k)}{p_k - \alpha_0} \right)\left[ P(Y|T=1,z_k) - P(Y|T=0,z_k) \right]
\end{align*}
The notation is getting a bit unwieldy so let $\pi^*_{tk}(y)= P(Y=y|T^*=t,z_k)$ and similarly define $\pi_{tk}(y) = P(Y=y|T=t,z_k)$.
Using this new notation, we have
\begin{align*}
  (1 - p_k - \alpha_1) \pi^*_{0k}(y) &= (1 - p_k - \alpha_1) \pi_{0k}(y) + \alpha_1 p_k \left[ \pi_{0k}(y) - \pi_{1k}(y) \right]\\
  (p_k - \alpha_0) \pi_{1k}^*(y) &= (p_k - \alpha_0) \pi_{1k}(y) + \alpha_0 (1 - p_k)\left[ \pi_{1k}(y) - \pi_{0k}(y) \right]
\end{align*}
Writing these out for all values of $k$,
\begin{align*}
  (p_1 - \alpha_0) \pi_{11}^*(y) &= (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  (1 - p_0 - \alpha_1) \pi^*_{00}(y) &= (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0) \pi_{10}^*(y) &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) \pi^*_{01}(y) &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
Similarly, using the fact that $p_k^* = (p_k - \alpha_0)/(1 - \alpha_0 - \alpha_1)$,  
\begin{align*}
  \pi^*_{11}(y) &= \left( \frac{p_0 - \alpha_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=a \right] + \left( \frac{p_1 - p_0}{p_1 - \alpha_0} \right) P\left[ Y(1)|J=c \right]\\
  \pi^*_{00}(y) &= \left( \frac{p_1 - p_0}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=c \right] + \left( \frac{1 - p_1 - \alpha_1}{1 - p_0 - \alpha_1} \right) P\left[ Y(0)|J=n \right]\\
  \pi^*_{10}(y) &= P\left[ Y(1)|J=a \right]\\
  \pi^*_{01}(y) &= P\left[ Y(0)|J=n \right] 
\end{align*}
or equivalently,
\begin{align*}
  (p_1 - \alpha_0)\pi^*_{11}(y) &= \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right]\\
  (1 - p_0 - \alpha_1)\pi^*_{00}(y) &=\left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right]\\
  (p_0 - \alpha_0)\pi^*_{10}(y) &= (p_0 - \alpha_0)P\left[ Y(1)|J=a \right]\\
  (1 - p_1 - \alpha_1)\pi^*_{01}(y) &= (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] 
\end{align*}
Equating,
\begin{align*}
  \left( p_0 - \alpha_0\right) P\left[ Y(1)|J=a \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + \left( 1 - p_1 - \alpha_1 \right) P\left[ Y(0)|J=n \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
and substituting the third and fourth equalities into the first and second we obtain
\footnotesize
\begin{align*}
   (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right] + \left( p_1 - p_0 \right) P\left[ Y(1)|J=c \right] &=  (p_1 - \alpha_0) \pi_{11}(y) + \alpha_0 (1 - p_1)\left[ \pi_{11}(y) - \pi_{01}(y) \right]\\
  \left( p_1 - p_0 \right) P\left[ Y(0)|J=c \right] + (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right] &=  (1 - p_0 - \alpha_1) \pi_{00}(y) + \alpha_1 p_0 \left[ \pi_{00}(y) - \pi_{10}(y) \right]\\
  (p_0 - \alpha_0)P\left[ Y(1)|J=a \right] &= (p_0 - \alpha_0) \pi_{10}(y) + \alpha_0 (1 - p_0)\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  (1 - p_1 - \alpha_1) P\left[ Y(0)|J=n \right] &= (1 - p_1 - \alpha_1) \pi_{01}(y) + \alpha_1 p_1 \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Simplifying and re-arranging,
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{p_1 \pi_{11}(y) - p_0 \pi_{10}(y) + (1 - p_1)\pi_{01}(y) - (1 - p_0) \pi_{00}(y) }{p_1 - p_0} \right] \\ 
  P\left[Y(0) =y|J=c \right] &= \left[ \frac{(1 - p_0)\pi_{00}(y) - (1 - p_1)\pi_{01}(y)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{(1 - p_0) \pi_{00}(y) - (1 - p_1)\pi_{01}(y) + p_0 \pi_{10}(y) - p_1 \pi_{11}(y)}{p_1 - p_0} \right] \\
  P\left[Y(1) = y|J=a \right] &=  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right]\\
  P\left[ Y(0) = y|J=n \right] &=  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right]
\end{align*}
\normalsize
Notice that the first two equations can be simplified as follows
\footnotesize
\begin{align*}
  P\left[ Y(1) = y|J=c \right] &= \left[ \frac{P(Y = y,T=1|z=1) - P(Y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y =y|z=0)}{p_1 - p_0} \right] \\ 
  P\left[Y(0) = y|J=c \right] &= \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] 
\end{align*}
\normalsize
Now, since probabilities must be between zero and one, we obtain the bounds
\begin{align*}
  0 &\leq \left[ \frac{P(Y = y,T=1|z=1) - P(Y = y,T=1|z=0)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{P(Y = y|z=1) - P(Y = y|z=0)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \left[ \frac{P(Y = y,T=0|z=1) - P(Y = y,T=0|z=1)}{p_1 - p_0} \right] - \alpha_1 \left[ \frac{P(Y=y|z=0) - P(Y=y|z=1)}{p_1 - p_0} \right] \leq 1 
\end{align*}
\normalsize
which we abbreviate 
\begin{align*}
  0 &\leq \left[ \frac{\Delta P(Y=y,T=1)}{p_1 - p_0} \right] - \alpha_0 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] \leq 1\\ 
  0 &\leq \alpha_1 \left[ \frac{\Delta P(Y=y)}{p_1 - p_0} \right] - \left[\frac{ \Delta P(Y=y,T=0)}{p_1 - p_0} \right] \leq 1 
\end{align*}
where
\begin{align*}
  \Delta P(Y=y) &= P(Y=y|z=1) - P(Y=y|z=0)\\
  \Delta P(Y=y, T=t) &= P(Y=y, T=t|z=1) - P(Y=y,T=t|z=0).
\end{align*}
To manipulate these bounds, we need to know the sign of $R = \Delta P(Y=y)/(p_1 - p_0)
$. 
Presumably this will be positive for most values of $y$, but it could be negative.
\paragraph{Case I: $R$ is positive.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)}\\
  \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)}
\end{align*}

\paragraph{Case II: $R$ is negative.}
\begin{align*}
  \frac{\Delta P(Y=y,T=1)}{\Delta P(Y=y)} &\leq \alpha_0 \leq \frac{\Delta P(Y=y,T=1) - (p_1 - p_0)}{\Delta P(Y=y)} \\
  \frac{\Delta P(Y=y,T=0) + (p_1 - p_0)}{\Delta P(Y=y)} &\leq \alpha_1 \leq \frac{\Delta P(Y=y,T=0)}{\Delta P(Y=y)}
\end{align*}
Note that we \emph{two-sided} bounds for the misclassification probabilities.
These may be trivial in some cases, but I don't think it's obvious that they always will be.
\todo[inline]{Do these bounds have anything to do with the testability of the LATE assumptions? That is, do we get a lower bound for measurement error \emph{precisely when} we would otherwise violate a testable LATE assumption?}

Note that we also obtain bounds from the potential outcome distributions of always-takers and never-takers, namely 
\begin{align*}
  0 &\leq  \pi_{10}(y) + \left[\frac{\alpha_0 (1 - p_0)}{p_0 - \alpha_0}\right]\left[ \pi_{10}(y) - \pi_{00}(y) \right] \leq 1\\
  0&\leq  \pi_{01}(y) + \left[\frac{\alpha_1 p_1}{1 - p_1 - \alpha_1} \right] \left[ \pi_{01}(y) - \pi_{11}(y) \right] \leq 1
\end{align*}
but these are redundant.
From the assumption of non-differential measurement error, we already have 
\begin{align*}
  \pi_{0k}^* &= \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \\
  \pi_{1k}^* &= \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) 
\end{align*}
for all $k$ as given at the beginning of this section.
These expressions imply
\begin{align*}
 0 &\leq \pi_{0k} + \left( \frac{\alpha_1 p_k}{1 - p_k - \alpha_1} \right) \left( \pi_{0k} - \pi_{1k} \right) \leq 1 \\
  0 &\leq \pi_{1k} + \left( \frac{\alpha_0 (1 - p_k)}{p_k - \alpha_0} \right) \left( \pi_{1k} - \pi_{0k} \right) \leq 1
\end{align*}
Re-arranging, we have
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \pi_{0k} + \alpha_1 p_k \left( \pi_{0k} - \pi_{1k} \right) \leq 1 - p_k - \alpha_1 \\
 0 &\leq p_k \pi_{1k}- \alpha_0\pi_{1k} + \alpha_0 (1 - p_k) \left( \pi_{1k} - \pi_{0k} \right) \leq p_k - \alpha_0
\end{align*}
and thus
\begin{align*}
  0 &\leq (1 - p_k)\pi_{0k} - \alpha_1 \left[(1 - p_k)\pi_{0k} + p_k \pi_{1k} \right] \leq 1 - p_k - \alpha_1 \\
  0 &\leq p_k \pi_{1k} - \alpha_0\left[p_k\pi_{1k} + (1 - p_k)\pi_{0k}\right] \leq p_k - \alpha_0
\end{align*}
Now consider the first inequality.
Re-arranging the right-hand side we obtain
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)(1 - \pi_{0k})}{1 - \left[ (1 - p_k)\pi_{0k} + p_k \pi_{1k} \right]} = (1 - p_k) \left[ \frac{P(Y=0|T=0, z=k)}{P(Y=0|z=k)} \right]
\end{align*}
and re-arranging the left-hand side we find
\begin{align*}
  \alpha_1 \leq \frac{(1 - p_k)\pi_{0k}}{(1 - p_k)\pi_{0k} + p_k \pi_{1k}} = (1 - p_k) \left[ \frac{P(Y=1|T=0,z=k)}{P(Y=1|z=k)} \right]
\end{align*}
For the second inequality, the left-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k \pi_{1k}}{p_k \pi_{1k} + (1 - p_k)\pi_{0k}} = p_k \left[ \frac{P(Y=1|T=1,z=k)}{P(Y=1|z_k)} \right] 
\end{align*}
while the right-hand side gives
\begin{align*}
  \alpha_0 \leq \frac{p_k (1 - \pi_{1k})}{1 - \left[ p_k \pi_{1k} + (1 - p_k) \pi_{0k} \right]} = p_k \left[ \frac{P(Y=0|T=1,z=k)}{P(Y=0|z=k)} \right]
\end{align*}
These are analogous to our CDF bounds from above although they may not be tighter than the bounds 
\[
  \alpha_0 \leq p_k, \quad \alpha_1 \leq (1 - p_k)
\]
because we cannot argue, as we did above, about a limit in which the ratio of CDFs approaches one.
As before, however, we can take the tightest bound over $k = 0, 1$.

\subsection{Bounding the LATE}
Even if we didn't know anything about $\alpha_0$ and $\alpha_1$ beyond the fact that they are probabilities, it looks like we could still bound the LATE.
I think we can do this without using the independence of the instrument, that is only using the mean exclusion restriction.
Write out the LATE expressions with the $\alpha_0$ and $\alpha_1$ in them and them just plug in zero and one.
Could then tighten the bounds by imposing additional assumptions to get bounds for $\alpha_0$ and $\alpha_1$, from weakest to strongest.
If you have an independent instrument, you also get bounds for the outcome distributions.
Need to think some more about this\ldots


\subsection{Stochastic Dominance Conditions}
What if we imposed a stochastic ordering, e.g.\ $Y(1) > Y(0)$ for compliers?
Presumably this would give joint bounds for $\alpha_0$ and $\alpha_1$ from the LATE expressions from above.
Alternatively, perhaps one would choose to impose an ordering on the $Y(0)$ distributions for compliers versus never-takers or the $Y(1)$ distributions for the compliers versus always-takers.
This might be interesting in situations where one is concerned that the assumption we need for identification does not in fact hold and should give additional bounds.


\section{Outline For New Draft}
\begin{enumerate}
  \item Introduction / Literature Review
    \begin{enumerate}
      \item Why is this an important question?
        \begin{itemize}
          \item Treatments of interest in economics usually endogenous and often binary.
          \item Randomized encouragement designs are common in applied work.
          \item Treatment status is often self-reported.
          \item This problem is much more challenging that people realize.
        \end{itemize}
      \item Why are we different from Ura?
        \begin{itemize}
          \item Main difference is that we, in line with the existing literature, study the case of non-differential measurement error. This allows us to obtain point identification under certain assumptions.
          \item In contrast, Ura considers arbitray forms of mis-classification but as a consequence presents only partial identification results.
          \item Second, while we do provide results for LATE in Section blah, we mainly focus on additively separable model in which heterogeneity is captured by observed covariates while Ura considers only a LATE model. (And also doesn't allow for covariates.) 
        \end{itemize}
    \end{enumerate}
  \item Mahajan/Lewbell-style Assumptions 
    \begin{enumerate}
      \item Setup and Assumptions:
        \begin{itemize}
          \item Homogenous treatment effect model (additively separable)
          \item Conditional mean version of non-differential measurement error assumption.
          \item Conditional mean independence for IV.
        \end{itemize}
      \item Show that the model is not identified, regardless of (discrete) support of IV.
      \item Derive sharp bounds for $\alpha_0, \alpha_1$ and treatment effect.
      \item Show that second and third-moment independence for IV identifies this model? Maybe this isn't interesting in and of itself?
    \end{enumerate}
  \item Independence Assumption
    \begin{enumerate}
      \item Motivation
    \begin{itemize}
      \item Showed above that stronger assumptions are needed for identification, but the additional moment restrictions seem a bit artificial.
      \item When instruments are derived from economic theory that yields conditional mean independence only, we wouldn't want to use them.
      \item They would make sense, however, in an an RCT or natural experiment.
      \item The whole point in these settings is \emph{not} to rely on functional form assumptions. It would be strange to say that $z$ is an instrument for $y$ but not $\log y$.
      \item This points towards an \emph{independence} assumption for the instrument.
      \item Can make a similar argument for measurement error: seems strange to assume that $T$ is non-differential for $y$ but not $\log y$.
    \end{itemize}
      \item Sharp Bounds for $\alpha_0$ and $\alpha_1$ without valid instrument  
        \begin{itemize}
          \item Assume ``independence'' version of non-differential measurement error.
          \item Derive CDF bounds.
        \end{itemize}
      \item Conditional Independence for Instrument
        \begin{itemize}
          \item Exactly what assumptions do we need here? 
          \item Characteristic functions.
          \item Identification conditions?
          \item Overidentifying restrictions? Test model?
        \end{itemize}
    \end{enumerate}
  \item LATE Model
    \begin{enumerate}
      \item Introduction
        \begin{itemize}
          \item Most of the existing mis-classification literature focuses on a homogeneous treatment effects model.
          \item What if we don't have an additively separable model?
          \item These results complement Ura because we work under the assumption of non-differential measurement error while he asks what can be learned when one is unwilling to make any assumptions about the form of the mis-classification.
        \end{itemize}
      \item Mahajan/Lewbel Setup
        \begin{itemize}
          \item Presumably the partial identification results go through for a LATE.
          \item The second and third moment conditions would require restrictions on form of heterogeneity.
            These would seem to be satisfied by a generalized Roy model.
        \end{itemize}
      \item Independence Assumptions
        \begin{itemize}
          \item Presumably the CDF bounds go through as before but need to state exact form of independence assumption in terms of potential outcomes.
          \item Kitagawa-style independence assumption for IV: $Y(T^*,z) = Y(T^*)$. This gives bounds for all quantile treatment effects.
          \item Stochastic dominance conditions?
        \end{itemize}
    \end{enumerate}
  \item Estimation / Inference
  \item Simulation Study
  \item Empirical Examples
    \begin{itemize}
      \item Try to look at a number of examples under different assumptions to illustrate both point and partial identification results. Don't forget about Oreopoulous: the sample size is so huge that inference isn't a major concern.
    \end{itemize}
\end{enumerate}

\section{Weak Identification}

\subsection{Moment Equations}
First we write the moment equations in a more familiar GMM-style form.
\paragraph{First Moment Condition:}
This is simply the IV moment condition: 
\[
  \mbox{Cov}(y,z)/ \mbox{Cov}(T,z) = \beta/(1 - \alpha_0 - \alpha_1)
\]
Rearranging gives a more ``canonical'' GMM form:
\[
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right) \mbox{Cov}(T,z) = 0
\]

\paragraph{Second Moment Condition:}
The equations used to identify $(\alpha_0 - \alpha_1)$ in the paper are 
\begin{align*}
  \mu_{k\ell}^* &= (p_k - \alpha_0) m_{1k}^* - (p_\ell - \alpha_0) m_{1\ell}^* \\
  \Delta \overline{y^2} &= \beta \mathcal{W} (p_k - p_\ell) + 2 \mathcal{W} \mu_{k\ell}^*\\
  \Delta \overline{yT} &= (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) + \mu_{k\ell}^*
\end{align*}
Re-arranging the third equation,
$\mu_{k\ell}^* = \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell)$.
Substituting into the second equation, 
\begin{align*}
  \Delta\overline{y^2} &= \mathcal{W}\left[ \beta(p_k - p_\ell) + 2 \mu_{k\ell}^* \right]\\
  &= \mathcal{W}\left\{ \beta(p_k - p_\ell) + 2\left[ \Delta\overline{yT} - (1 - \alpha_1)\mathcal{W}(p_k - p_\ell) \right]  \right\}\\
  &= \mathcal{W} \left\{ (p_k - p_\ell)\left[ \beta - 2(1 - \alpha_1)\mathcal{W} \right] + 2\Delta\overline{yT} \right\}
\end{align*}
Now, substituting $\mathcal{W} = \beta/(1 - \alpha_0 - \alpha_1)$
\begin{align*}
  \Delta\overline{y^2}&= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{\beta (p_k - p_\ell)\left[ 1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right] + 2 \Delta\overline{yT} \right\}\\
  &= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\Delta\overline{yT} - \beta(p_k - p_\ell)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\}
\end{align*}
We now write this in a more standard form.
Let $w$ be any random variable. 
Then,
\begin{align*}
  \mbox{Cov}(w,z) &= E(wz) - E(w)E(z) = \left[ 1 \times E(w|z=1)q + 0 \times E(w|z=0)(1 - q) \right] - E(w)q\\
  &= q E(w|z=1) - qE(w) = q E(w|z=1) - q\left[ E(w|z=1)q + E(w|z=0)(1 - q) \right]\\
  &= q\left[ E(w|z=1)(1 - q) + E(w|z=0)(1 - q)\right]\\
  &= q(1-q)\left[ E(w|z=1) - E(w|z=0) \right]
\end{align*}
Using this fact, we can express the quantities that appear in the second moment equality in terms of covariances as follows
\[
  \Delta\overline{y^2} = \frac{\mbox{Cov}(y^2,z)}{q(1 - q)}, \quad
  \Delta\overline{yT} = \frac{\mbox{Cov}(yT,z)}{q(1 - q)}, \quad
  (p_k - p_\ell) = \frac{\mbox{Cov}(T,z)}{q(1 - q)}
\]
leading to
\[\frac{\mbox{Cov}(y^2,z)}{q(1-q)}= \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{\frac{2\mbox{Cov}(yT,z)}{q(1-q)} - \beta \frac{\mbox{Cov}(T,z)}{q(1-q)}\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\}
\]
Or, multiplying through by $q(1-q)$ and re-arranging,
\[
  \mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\} = 0
\]

\paragraph{Third Moment Condition:}
The third and final set of moment conditions is
\begin{align*}
    \Delta\overline{y^3} &= \beta^2 [\mathcal{W} (p_k - p_\ell)]  + 3 \beta [\mathcal{W} \mu_{k\ell}^*] + 3 \mathcal{W} \lambda^*_{k\ell}\\
    \Delta\overline{y^2T} &=  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) + 2(1-\alpha_1)\mathcal{W}\mu_{k\ell}^* + \lambda_{k\ell}^*
\end{align*}
  To put this into a more familiar format, we first eliminate $\mu_{k\ell}^*$ using 
\[
  \mu_{k\ell}^* = \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell)
\]
from the derivation of the second moment equation from above, yielding
\begin{align*}
  \Delta\overline{y^3} &= \beta^2 [\mathcal{W} (p_k - p_\ell)]  + 3 \beta \mathcal{W}\left[ \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) \right]  + 3 \mathcal{W} \lambda^*_{k\ell}\\
    \Delta\overline{y^2T} &=  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) + 2(1-\alpha_1)\mathcal{W}\left[ \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) \right] + \lambda_{k\ell}^*
\end{align*}
Re-arranging and factoring the first equation gives
\[
  \Delta\overline{y^3} = \mathcal{W}\left( p_k - p_\ell \right)
  \left\{ \beta^2 + \frac{3\beta \Delta\overline{yT}}{p_k - p_\ell} - 3\beta \mathcal{W} (1 - \alpha_1) + \frac{3\lambda^*_{k\ell}}{p_k - p_\ell} \right\}
\]
Now, by re-arranging the second equation we find that
\begin{align*}
  \lambda_{k\ell}^* &= \Delta\overline{y^2T} -  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) - 2(1-\alpha_1)\mathcal{W}\left[ \Delta \overline{yT} - (1 - \alpha_1) \mathcal{W} (p_k - p_\ell) \right] \\
  &= \Delta\overline{y^2T} -  \beta(1-\alpha_1)\mathcal{W}(p_k - p_\ell) - 2(1-\alpha_1)\mathcal{W} \Delta \overline{yT} + 2(1-\alpha_1)^2\mathcal{W}^2(p_k - p_\ell)
\end{align*}
and thus
\begin{align*}
  \frac{3 \lambda^*_{k\ell}}{p_k - p_\ell} &= 
  3\left(\frac{\Delta\overline{y^2T}}{p_k - p_\ell}\right) -  3\beta(1-\alpha_1)\mathcal{W} - 6(1-\alpha_1)\mathcal{W} \left(\frac{\Delta \overline{yT}}{p_k - p_\ell}\right) + 6(1-\alpha_1)^2\mathcal{W}^2
\end{align*}
so that 
\small
\begin{align*}
  \frac{\Delta\overline{y^3}}{\mathcal{W}(p_k - p_\ell)} &=
  \left\{ \beta^2 - 6\beta\mathcal{W}(1 - \alpha_1) + 6\mathcal{W}^2(1 - \alpha_1)^2 + \left( \frac{\Delta\overline{yT}}{p_k - p_\ell} \right)\left[ 3\beta - 6\mathcal{W}(1 - \alpha_1) \right] + 3\left( \frac{\Delta\overline{y^2T}}{p_k - p_\ell} \right)  \right\}\\
  &= \left\{ \beta^2\left[1 - \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}  + \frac{6(1 - \alpha_1)^2}{(1 - \alpha_0 - \alpha_1)^2}\right]   + 3\beta\left[ 1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right] \left( \frac{\Delta\overline{yT}}{p_k - p_\ell} \right)+ 3\left( \frac{\Delta\overline{y^2T}}{p_k - p_\ell} \right) \right\}
\end{align*}
\normalsize
Simplifying, we find that
\[
  \left[1 - \frac{2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}\right] 
  = \frac{(1 - \alpha_0 - \alpha_1) - 2(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}
  =\frac{1 - \alpha_0 - \alpha_1 - 2 + 2\alpha_1}{1 - \alpha_0 - \alpha_1}
  = \frac{\alpha_1 - \alpha_0 - 1}{1 - \alpha_0 - \alpha_1}
\]
and
\begin{align*}
\left[1 - \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1}  + \frac{6(1 - \alpha_1)^2}{(1 - \alpha_0 - \alpha_1)^2}\right] 
&= 1 - \left[ \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right]\left[ 1 - \frac{1 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right]\\
&= 1 - \left[ \frac{6(1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right]\left[  \frac{(1 - \alpha_0 - \alpha_1) - (1 - \alpha_1)}{1 - \alpha_0 - \alpha_1} \right]\\
&= 1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2}
\end{align*}
so that
\begin{align*}
  \frac{\Delta\overline{y^3}}{\mathcal{W}(p_k - p_\ell)} 
  &= \left\{ \beta^2\left[1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2} \right] - 3\beta\left[ \frac{1 - (\alpha_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] \left( \frac{\Delta\overline{yT}}{p_k - p_\ell} \right)+ 3\left( \frac{\Delta\overline{y^2T}}{p_k - p_\ell} \right) \right\}
\end{align*}
Therefore, re-arranging and multiplying through by $q(1 - q)$,
\footnotesize
\begin{align*}
  \mbox{Cov}(z,y^3) 
  &= \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right)\left\{ \beta^2\left[1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2} \right] \mbox{Cov}(z,T) - 3\beta\left[ \frac{1 - (\alpha_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] \mbox{Cov}(z,yT) + 3\mbox{Cov}(z,y^2T) \right\}
\end{align*}
\normalsize
\paragraph{Full Set of Moment Conditions}
\footnotesize
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \frac{\beta}{1 - \alpha_0 - \alpha_1}\left\{2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z)\left( \frac{1 + \alpha_0 - \alpha_1}{1 - \alpha_0 - \alpha_1} \right)  \right\} &= 0\\
  \mbox{Cov}(y^3,z) - \left( \frac{\beta}{1 - \alpha_0 - \alpha_1} \right)\left\{ \beta^2\left[1 + \frac{6\alpha_0(1 - \alpha_1)}{(1 - \alpha_0 - \alpha_1)^2} \right] \mbox{Cov}(T,z) - 3\beta\left[ \frac{1 - (\alpha_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] \mbox{Cov}(yT,z) + 3\mbox{Cov}(y^2T,z) \right\} &= 0
\end{align*}
\normalsize

\subsection{Simple Special Case: $\alpha_0 = 0$}
Suppose that $\alpha_0$.
Then the model is identified using the first and second moment equalities, which simplify to
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 -\alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \left(\frac{\beta}{1 - \alpha_1}\right)\left[2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z) \right] &= 0
\end{align*}
In this simple special case, it is easy to solve for $\beta$ by substituting the first moment condition into the second:
\[
  \beta = \frac{2 \mbox{Cov}(yT,z)}{\mbox{Cov}(T,z)} - \frac{\mbox{Cov}(y^2,z)}{\mbox{Cov}(y,z)}
\]
\todo[inline]{I checked this equation in our simulation experiment and it is indeed correct} 
Notice that if $\beta \approx 0$ then both $\mbox{Cov}(y^2,z)$ and $\mbox{Cov}(y,z)$ are close to zero so their ratio becomes extremely noisy.

\paragraph{Standard GMM form:}
To express this system in the standard GMM form, we need to agument these moment equalities with expressions for the means of $z, y, y^2, T,$ and $yT$ as follows.
Let $\mathbf{w}_i = (y_i, z_i, T_i)'$, $\theta = (\beta, \alpha_1)'$ and $\gamma = (q, p, \mu, s, r)'$ where
\begin{align*}
  q &= \mathbb{E}\left[z \right] \\
  p &= \mathbb{E}\left[T \right] \\
  \mu &= \mathbb{E}\left[y \right] \\
  s &= \mathbb{E}\left[y^2 \right] \\
  r &= \mathbb{E}\left[yT \right].
\end{align*}
We can express our problem in terms of two blocks of moment conditions, namely
\[
  f(\mathbf{w}; \theta, \gamma) = \left[
  \begin{array}{c}
    g(\mathbf{w}; \theta, \gamma)\\
    h(\mathbf{w}; \gamma)
  \end{array}
\right]
\]
where
\[
  g(\mathbf{w}; \theta, \gamma) = \left[
  \begin{array}{c}
    (zy - q\mu) - \displaystyle\left( \frac{\beta}{1 - \alpha_1} \right)(zT - qp) \\
    (zy^2 - qs) - 2\displaystyle\left( \frac{\beta}{1 -\alpha_1}\right) (zyT - qr) + \left(\frac{\beta^2}{1 - \alpha_1}\right)(zT - qp)  
  \end{array}
\right]
\]
and
\[
  h(\mathbf{w}; \gamma) = 
  \left[
  \begin{array}{c}
    z - q \\ T - p \\ y - \mu \\ y^2 - s \\ yT - r
  \end{array}
\right]
\]
We can view this as a two-step or ``plug-in'' GMM estimation problem where $\widehat{\gamma}$ solves the sample moment condition
\[
  \frac{1}{n}\sum_{i=1}^n h(\mathbf{w}_i; \gamma) = 0
\]
and $\widehat{\theta}$ solves
\[
  \frac{1}{n}\sum_{i=1}^n g(\mathbf{w}_i; \theta, \widehat{\gamma}) = 0.
\]
Unfortunately, in our example the first-step estimation affects the asymptotic variance of the second since an inconsistent estimator of $\gamma$ yields an inconsistent estimator of $\theta$.\footnote{See Newey \& McFadden (1994), Section 6.}
This means that we will have to proceed ``the hard way.''

Under standard regularity conditions, a GMM estimator based on the sample analogue $f_n(\theta, \gamma)$ of $\mathbb{E}[f(\mathbf{w};\theta,\gamma)]=0$ using a weighting matrix $\widehat{W}\rightarrow_p W$ converges in distribution to
\[
  -(F'WF)^{-1}F'W M, \quad M\sim N(0, \Omega) 
\]
where $\sqrt{n}f_n(\theta_0, \gamma_0) \rightarrow_d M$ and $F = \mathbb{E}[\nabla_\theta' f(\mathbf{w};\theta_0, \gamma_0), \nabla_\gamma' f(\mathbf{w}; \theta_0, \gamma_0)]$.
The present example, however, is just-identified which means that $F$ is square and hence
\[
  -(F'WF)^{-1}F'W = F^{-1}W^{-1}(F')^{-1}F'W = -F^{-1}
\]
Now, given the special structure of our example,
\begin{align*}
  F &= \left[
    \begin{array}{cc}
      \mathbb{E}\left\{\nabla'_\theta g(\mathbf{w};\theta_0, \gamma_0)\right\} & 
      \mathbb{E}\left\{\nabla'_\gamma g(\mathbf{w};\theta_0, \gamma_0)\right\}\\
\mathbf{0}& -\mathbf{I}
    \end{array}
  \right] \equiv
  \left[
  \begin{array}{cc}
    G_\theta & G_\gamma\\
    \mathbf{0} & - \mathbf{I}
  \end{array}
\right]
\end{align*}
becuase $h$ does not involve $\theta$ and $\nabla_\gamma' h(\mathbf{w},\gamma) = -\mathbf{I}$.
Inverting, we have
\[
  -F^{-1} = 
  \left[
  \begin{array}{cc}
    -G_\theta & -G_\gamma \\
    \mathbf{0} & \mathbf{I}
  \end{array}
\right]^{-1} =
\left[
\begin{array}{cc}
  -G_{\theta}^{-1} & -G_{\theta}^{-1}G_{\gamma}\\
  \mathbf{0} & \mathbf{I}
\end{array}
\right]
\]
We see from this expression that if $G_\gamma$ were zero, the first step-estimation would not affect the limit distribution of $\widehat{\theta}$.
Differentiating,
\[ 
  \left[
  \begin{array}{cc}
    \nabla g_\beta & \nabla g_{\alpha_1}
  \end{array}
\right] = 
  \left[
  \begin{array}{cc}
    \displaystyle -\left(\frac{zT - qp}{1 - \alpha_1}\right) & \displaystyle -\left\{\frac{\beta(zT - qp)}{(1 - \alpha_1)^2}\right\} \\ \\
    \displaystyle 2\left\{ \frac{\beta(zT - qp) - (zyT - qr)}{1 - \alpha_1}\right\} & \displaystyle \frac{\beta^2(zT - qp) - 2\beta (zyT - qr)}{(1 - \alpha_1)^2}
  \end{array}
\right]
\]
and thus, taking expectations,
\[
  G_{\theta} = 
  \left[
  \begin{array}{cc}
    \displaystyle \frac{-\mbox{Cov}(z,T)}{1 - \alpha_1} & \displaystyle  \frac{-\beta \mbox{Cov}(z,T)}{(1 - \alpha_1)^2} \\ \\
    \displaystyle 2\left\{ \frac{\beta\mbox{Cov}(z,T) - \mbox{Cov}(yT,z) }{1 - \alpha_1} \right\} & \displaystyle \frac{ \beta^2\mbox{Cov}(z,T) - 2\beta \mbox{Cov}(yT,z)}{(1 - \alpha_1)^2} 
  \end{array}
\right]
\]
Now, for $G_\gamma$ we have
\begin{align*}
  G_\gamma &= \mathbb{E}
  \left[
  \begin{array}{ccccc}
    \nabla_q g & \nabla_p \mu & \nabla_\mu g & \nabla_s g & \nabla_r g
  \end{array}
\right] \\
&=
\left[
\begin{array}{ccccc}
  \displaystyle \left( \frac{p\beta}{1 - \alpha_1}  - \mu \right)  & \displaystyle\left(\frac{q\beta}{1 - \alpha_1}\right)& -q & 0 & 0 \\ \\
  \displaystyle\left( \frac{\beta}{1 - \alpha_1} \right)(2r - \beta p) - s &\displaystyle \frac{-q\beta^2}{1 - \alpha_1} & 0 & -q & \displaystyle \frac{2\beta q}{1 - \alpha_1}
\end{array}
\right]
\end{align*}
The next step is to invert $G_\theta$.
First we calculate the determinant.
For the purposes of this calculation, use the shorthand $C = \mbox{Cov}(z,T)$ and $D = \mbox{Cov}(yT,z)$.
We have:
\begin{align*}
  |G_\theta| &= \left[ \frac{-C}{1 - \alpha_1} \right]\left[ \frac{\beta^2 C - 2\beta D}{(1 - \alpha_1)^2} \right] - \left[ \frac{-\beta C}{(1 - \alpha_1)^2} \right]\left[ \frac{2\beta C - 2D}{1 - \alpha_1} \right]\\
  &= \left( \frac{1}{1 - \alpha_1} \right)^3 \left[ 2\beta C D - \beta^2 C^2 + 2\beta^2 C^2 - 2\beta CD  \right]\\
  &= \frac{\beta^2 \mbox{Cov}(z,T)^2}{(1 - \alpha_1)^3} 
\end{align*}
Thus,
\begin{align*}
  G_\theta^{-1} &= \frac{(1 - \alpha_1)^3}{\beta^2 \mbox{Cov}(z,T)^2} 
  \left[
  \begin{array}{cc}
    \displaystyle \frac{ \beta^2\mbox{Cov}(z,T) - 2\beta \mbox{Cov}(yT,z)}{(1 - \alpha_1)^2}  & \displaystyle  \frac{\beta \mbox{Cov}(z,T)}{(1 - \alpha_1)^2} \\ \\
    \displaystyle -2\left\{ \frac{\beta\mbox{Cov}(z,T) - \mbox{Cov}(yT,z) }{1 - \alpha_1} \right\} & \displaystyle\frac{-\mbox{Cov}(z,T)}{1 - \alpha_1}
  \end{array}
\right] \\ \\
  &=\left[
  \begin{array}{cc}
    \displaystyle \left\{\frac{1 - \alpha_1}{\mbox{Cov}(z,T)}\right\}\left\{ 1 - \frac{2\mbox{Cov}(yT,z)}{\beta\mbox{Cov}(z,T)} \right\}& \displaystyle  \frac{1 - \alpha_1}{\beta \mbox{Cov}(z,T)} \\ \\
    \displaystyle \frac{2(1 - \alpha_1)^2}{\beta \mbox{Cov}(z,T)}\left\{ \frac{\mbox{Cov}(yT,z)}{\beta\mbox{Cov}(z,T)} - 1 \right\} & \displaystyle \frac{-(1 - \alpha_1)^2}{\beta^2 \mbox{Cov}(z,T)} 
  \end{array}
\right] \\
\end{align*}
The next step is to calculate $\Omega$:
\begin{align*}
  \Omega = \lim_{n\rightarrow \infty}\mbox{Var}\left[\sqrt{n}f_n(\theta_0, \gamma_0)\right] 
  = \lim_{n\rightarrow \infty}\mbox{Var}\left[\frac{1}{\sqrt{n}} \sum_{i=1}^n f(\mathbf{w}_i; \theta_0, \gamma_0)  \right]
\end{align*}
If $\mathbf{w}_i$ is an iid sequence of RVs, then
\begin{align*}
  \Omega = \lim_{n\rightarrow \infty}\frac{1}{n}\mbox{Var}\left[ \sum_{i=1}^n f(\mathbf{w}_i; \theta_0, \gamma_0)\right] = \lim_{n\rightarrow \infty} \frac{1}{n}\sum_{i=1}^n \mbox{Var}\left[ f(\mathbf{w}_i;\theta_0, \gamma_0) \right] = \mbox{Var}\left[ f(\mathbf{w}_i;\theta_0, \gamma_0) \right]  
\end{align*}
And assuming that our model is correctly specified, so that $\mathbb{E}\left[ f(\mathbf{w}_i;\theta_0, \gamma_0) \right]=0$,
\begin{align*}
  \mbox{Var}\left[ f(\mathbf{w}_i; \theta_0, \gamma_0) \right] 
  &=\mathbb{E}
  \left[
  \begin{array}{cc}
    g(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    g(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)' \\
    h(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    h(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)'
  \end{array}
\right]\\ &\equiv
\left[
\begin{array}{cc}
  \Omega_{gg} & \Omega_{gh}\\
  \Omega_{gh} & \Omega_{hh}
\end{array}
\right]
\end{align*}
We now calculate each block.
\todo[inline]{I don't think this is actually going to give us anything interpretable. The expressions are quite involved and it seems unlikely that they'll cancel in a useful way. This doesn't matter for implementation, of course, since it's easy to calculate the estimate of $\widehat{\Omega}$ by plugging the GMM estimates into the sample moment conditions, taking the outer product, and averaging.}

We are only interested in the asymptotic variance matrix $V_\theta$ of our parameters of interest $\theta = (\beta, \alpha_1)$.
We calculate this as follows:
\begin{align*}
  V_\theta &= \left[
  \begin{array}{cc}
    G_\theta^{-1} & G_{\theta}^{-1}G_\gamma
  \end{array}
\right]
\left[
\begin{array}{cc}
  \Omega_{gg} & \Omega_{gh}\\
  \Omega_{hg} & \Omega_{hh}\\
\end{array}
\right]
\left[
\begin{array}{c}
  \left( G_\theta^{-1} \right)'\\
  \left( G_\theta^{-1}G_{\gamma} \right)'\\
\end{array}
\right] \\
&= 
  \left[\begin{array}{cc}
      G_\theta^{-1}\left(\Omega_{gg} + G_\gamma \Omega_{gh}  \right) & 
      G_{\theta}^{-1}\left( \Omega_{gh} + G_\gamma \Omega_{hh} \right)
  \end{array}
\right]
\left[
\begin{array}{c}
  \left( G_\theta^{-1} \right)'\\
  G_{\gamma}' \left( G_\theta^{-1}\right)'\\
\end{array}
\right]\\
&= G_\theta^{-1}\left(\Omega_{gg} + G_\gamma \Omega_{hg}  \right)\left( G_{\theta}^{-1} \right)' + G_\theta^{-1}\left( \Omega_{gh} + G_\gamma \Omega_{hh}\right)G_\gamma' \left( G_\theta^{-1} \right)'\\
&= G_\theta^{-1}\left(\Omega_{gg} + G_\gamma \Omega_{hg} + \Omega_{gh}G_\gamma' + G_\gamma \Omega_{hh} G_\gamma'\right)\left( G_\theta^{-1} \right)'
\end{align*}

\subsection{Easier(?) Derivation of Simple Special Case: $\alpha_0 = 0$}
Recall that we could eliminate $\alpha_1$ from the moment conditions, yielding,
\[
  \beta = \frac{2 \mbox{Cov}(yT,z)}{\mbox{Cov}(T,z)} - \frac{\mbox{Cov}(y^2,z)}{\mbox{Cov}(y,z)}
\]
We can treat this as our $g$ block of moment conditions with a parameter vector $\theta$ that is simply $\beta$.
This gives
\[
  g(\mathbf{w}; \beta, \gamma) = \left[ 2\left(\frac{zTy - qr}{zT - qp}\right) - \frac{zy^2 - qs}{zy - q\mu} - \beta \right]
\]
The $h$ block of moment conditions is unchanged.
Now, we have
\begin{align*}
  F &= \left[
    \begin{array}{cc}
      \mathbb{E}\left\{\nabla'_\beta g(\mathbf{w};\beta_0, \gamma_0)\right\} & 
      \mathbb{E}\left\{\nabla'_\gamma g(\mathbf{w};\beta_0, \gamma_0)\right\}\\
\mathbf{0}& -\mathbf{I}
    \end{array}
  \right] \equiv
  \left[
  \begin{array}{cc}
    G_\beta & G_\gamma\\
    \mathbf{0} & - \mathbf{I}
  \end{array}
\right] = \left[
\begin{array}{cc}
  -1 & G_\gamma \\
    \mathbf{0} & - \mathbf{I}
\end{array}
\right]
\end{align*}
since the derivative of $g$ with respect to $\beta$ is -1 and that of $h$ with respect to $\gamma$ is $-\mathbf{I}$.
Inverting, 
\[
  -F^{-1} = 
  \left[
  \begin{array}{cc}
    -G_\beta & -G_\gamma \\
    \mathbf{0} & \mathbf{I}
  \end{array}
\right]^{-1} =
\left[
\begin{array}{cc}
  -G_{\beta}^{-1} & -G_{\beta}^{-1}G_{\gamma}\\
  \mathbf{0} & \mathbf{I}
\end{array}
\right] = \left[
\begin{array}{cc}
  1 & G_\gamma \\
  \mathbf{0} & \mathbf{I}
\end{array}
\right]
\]
We calculate $G_\gamma$ as follows: 
\begin{align*}
  G_\gamma &= \mathbb{E}
  \left[
  \begin{array}{ccccc}
    \nabla_q g & \nabla_p g & \nabla_\mu g & \nabla_s g & \nabla_r g
  \end{array}
\right] \\ \\
  \nabla_q g(\mathbf{w};\beta_0, \gamma_0) &= \nabla_q \left[ 2\left(\frac{zTy - qr}{zT - qp}\right) - \frac{zy^2 - qs}{zy - q\mu} - \beta \right]\\
  &= 2\left[ \frac{-r(zT - qp) + p(zTy - qr)}{(zT - qp)^2} \right] - \frac{-s(zy - q\mu) + \mu (zy^2 - qs)}{(zy - q\mu)^2}\\
  &=  \frac{2zT(py - r)}{(zT - qp)^2}  - \frac{zy (\mu y - s)  }{(zy - q\mu)^2}\\ \\
  \nabla_p g(\mathbf{w};\beta_0, \gamma_0) &= \frac{2q(zTy - qr)}{(zT - qp)^2} \\ \\
  \nabla_\mu g(\mathbf{w};\beta_0, \gamma_0) &= \frac{-q(zy^2 - qs)}{(zy - q\mu)^2}\\ \\
  \nabla_s g(\mathbf{w};\beta_0, \gamma_0) &= \frac{q}{zy - q\mu} \\ \\
  \nabla_r g(\mathbf{w};\beta_0, \gamma_0) &= \frac{-2q}{zT - qp}
\end{align*}

The next step is to calculate $\Omega$:
\begin{align*}
  \Omega &= \mbox{Var}\left[ f(\mathbf{w}_i; \theta_0, \gamma_0) \right] 
  =\mathbb{E}
  \left[
  \begin{array}{cc}
    g(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    g(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)' \\
    h(\mathbf{w}_i;\theta_0, \gamma_0)g(\mathbf{w}_i;\theta_0, \gamma_0)' &
    h(\mathbf{w}_i;\theta_0, \gamma_0)h(\mathbf{w}_i;\theta_0, \gamma_0)'
  \end{array}
\right]\\ &\equiv
\left[
\begin{array}{cc}
  \Omega_{gg} & \Omega_{gh}\\
  \Omega_{gh} & \Omega_{hh}
\end{array}
\right]
\end{align*}
We now calculate each block.
\todo[inline]{Still need to do this!}


\subsection{Two-Step Inference Idea}
If $\beta$ is small, then confidence intervals based on the GMM limit distribution from above will perform badly.
But even in this case, inference for the \emph{identified set} $\left[ \beta_{RF}, \beta_{IV} \right]$ should still be well-behaved, so long as the instrument is strong.
The idea of this section is to explore a two-step procedure that chooses between reporting the GMM confidence interval or inference for the identified set based on a pre-test of $\beta_{RF}$.
Presumably conducting valid inference based on such a procedure will require a Bonferroni correction.
The first step, however, is to determine the joint limiting behavior of the reduced form, IV, and GMM estimators.

\paragraph{Reduced Form Estimator}
The reduced form is given by
\begin{align*}
  y &= \gamma_0 + \gamma_1 z + \eta \\
  \gamma_0 &= \mathbb{E}[y|z=0]\\
  \gamma_1 &= \mathbb{E}[y|z=0] - \mathbb{E}[y|z=1]
\end{align*}
Now, we need to write $\eta$ in terms of the ``primitives'' of our model.
The first stage and main equation are
\begin{align*}
  y &= c + \beta T^* + \varepsilon\\
  T^* &= \pi_0 + \pi_1 z + v\\
  \pi_1 &= p_1^* - p_0^*
\end{align*}
which implies
\[
  y = (c + \beta \pi_0) + (\beta \pi_1) z + (\varepsilon + \beta v)
\]
so that
\begin{align*}
\gamma_1 &= \beta(p_1^* - p_0^*)\\
\eta &= \varepsilon + \beta v
\end{align*}
Now, define $W = (\mathbf{1}, \mathbf{z})$ and $\boldsymbol{\gamma} = (\gamma_0, \gamma_1)'$.
Then the reduced form estimator is 
\[
  \widehat{\boldsymbol{\gamma}} = \left(W'W\right)^{-1}  W'\mathbf{y} 
  = \left( W'W \right)^{-1}W'\left( W\boldsymbol{\gamma} + \boldsymbol{\eta} \right) = \boldsymbol{\gamma} + \left( W'W \right)^{-1}W'\boldsymbol{\eta}
\]
and hence
\[
  \sqrt{n}\left( \widehat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}\right) = \left( \frac{W'W}{n} \right)^{-1}\frac{W'\boldsymbol{\eta}}{\sqrt{n}}
\]
Now,
\[
  \left(\frac{W'W}{n}\right)^{-1} = \left[
  \begin{array}{cc}
    1 & \bar{\mathbf{z}} \\
    \bar{\mathbf{z}} & \mathbf{z}'\mathbf{z}/n
  \end{array}
\right]^{-1} \rightarrow_p 
\left[
\begin{array}{cc}
  1 & q \\
  q & q
\end{array}
\right]^{-1}
= \frac{1}{q(1-q)}\left[
\begin{array}{cc}
  q & -q \\
  -q & 1
\end{array}
\right]
\]
and by the Central Limit Theorem,
\[
  \frac{W'\boldsymbol{\eta}}{\sqrt{n}} = \frac{1}{\sqrt{n}}\sum_{i=1}^n \eta_i\left[
  \begin{array}{c}
    1 \\ z_i 
  \end{array}
\right] 
= \frac{1}{\sqrt{n}}\sum_{i=1}^n (\varepsilon_i + \beta v_i)\left[
  \begin{array}{c}
    1 \\ z_i 
  \end{array}
\right] \rightarrow_d N\left(\mathbf{0}, \Sigma \right)
\]
where
\begin{align*}
  \Sigma &= 
  \mathbb{E}\left[
  \begin{array}{cc}
   \eta_i^2 & z_i \eta_i^2\\
   z_i \eta_i^2 & z_i^2 \eta_i^2
  \end{array}
\right]
=
  \mathbb{E}\left[
  \begin{array}{cc}
   \left( \varepsilon_i + \beta v_i  \right)^2 & 
   z_i \left( \varepsilon_i + \beta v_i  \right)^2\\ 
   z_i \left( \varepsilon_i + \beta v_i  \right)^2 &
   z_i^2\left( \varepsilon_i + \beta v_i  \right)^2  
  \end{array}
\right]\\
&=
  \mathbb{E}\left[
  \begin{array}{cc}
   \varepsilon_i^2 + 2\beta \varepsilon_i v_i + \beta^2 v_i^2  & 
 z_i^2\varepsilon_i + 2\beta z_i \varepsilon_i v_i + \beta^2 z_i v_i^2 \\
   z_i^2\varepsilon_i + 2\beta z_i \varepsilon_i v_i + \beta^2 z_i v_i^2 &
   z_i^2\varepsilon_i^2 + 2\beta z_i^2 \varepsilon_i v_i + \beta^2 z_i^2 v_i^2  
  \end{array}
\right]
\end{align*}
The next step is to work out the joint distribution of $v$ and the other primitives of our model:
\begin{align*}
  T^*=0, z = 0 &\implies 0 = \pi_0 + v \implies v = -\pi_0 = -p_0^*\\
  T^*=1, z = 0 &\implies 1 = \pi_0 + v \implies v = 1-\pi_0 = 1-p_0^*\\
  T^*=0, z = 1 &\implies 0 = \pi_0 + \pi_1 + v \implies v = -(\pi_0 + \pi_1) = -p_1^*\\
  T^*=1, z = 1 &\implies 1 = \pi_0 + \pi_1 + v \implies v = 1-(\pi_0 + \pi_1) = 1-p_1^*
\end{align*}
Thus,
\begin{align*}
  \mathbb{P}(T^*=0, z = 0) &= \mathbb{P}(v = -p_0^*) = (1-p_0^*)(1-q)\\
  \mathbb{P}(T^*=1, z = 0) &= \mathbb{P}(v = 1-p_0^*) = p_0^*(1-q)\\
  \mathbb{P}(T^*=0, z = 1) &= \mathbb{P}(v = -p_1^*) = (1-p_1^*)q\\
  \mathbb{P}(T^*=1, z = 1) &= \mathbb{P}(v = 1-p_1^*) = p_1^* q 
\end{align*}
Notice that, as must be true \emph{by construction}, $v$ is mean zero:
\begin{align*}
  \mathbb{E}[v] &= -p_0^*(1-p_0^*)(1 - q) + (1 - p_0^*)p_0^*(1-q) - p_1^*(1-p_1^*)q + (1-p_1^*)p_1^*q  \\
  &= (1-q)(1 - p_0^*) (p_0^* - p_0^*) + q(1-p_1^*)(p_1^* - p_1^*) = 0
\end{align*}
and uncorrelated with $v$:
\begin{align*}
  \mathbb{E}[zv] &= q\mathbb{E}[v|z=1] = q\left\{ p_1^*  \mathbb{E}[v|T^*=1,z=1] + (1 - p_1^*)\mathbb{E}[v|T^*=0,z=1]\right\} \\
  &= q \left[ p_1^* (1 - p_1^*) - p_1^* (1 - p_1^*)  \right] = 0
\end{align*}
Now, to calculate $\Sigma$, we need $\mathbb{E}[v^2], \mathbb{E}[\varepsilon v], \mathbb{E}[z\varepsilon v], \mathbb{E}[z^2\varepsilon v], \mathbb{E}[zv^2]$, and $\mathbb{E}[z^2v^2]$:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[v^2] &=p_0^{*2}(1-p_0^*)(1 - q) + (1 - p_0^*)^2 p_0^*(1-q) + p_1^{*2}(1-p_1^*)q + (1-p_1^*)^2p_1^*q  \\ 
  &= (1-q)p_0^*(1 - p_0^*)\left[p_0^* + (1 - p_0^*)  \right] + qp_1^*(1 - p_1^*)\left[ p_1^* + (1 - p_1^*) \right]\\
  &= (1-q)p_0^*(1 - p_0^*) + qp_1^*(1 - p_1^*)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[\varepsilon v] &= \mathbb{E}\left[ v \mathbb{E}\left( \varepsilon|v \right) \right]\\
  &= -p_0^*\mathbb{P}(v = -p_0^*)\mathbb{E}(\varepsilon|v = -p_0^* ) - p_1^* \mathbb{P}(v = -p_1^* )\mathbb{E}(\varepsilon|v = -p_1^*) \\
  & \qquad {} + (1 - p_0^*)\mathbb{P}(v = 1-p_0^*)\mathbb{E}(\varepsilon|v = 1-p_0^*) + (1 -p_1^*)\mathbb{P}(v = 1- p_1^*)\mathbb{E}(\varepsilon|v = 1-p_1^*) \\
   &= -p_0^*(1-p_0^*)(1-q)\mathbb{E}[\varepsilon|T^*=0,z=0] - p_1^* (1-p_1^*)q\mathbb{E}[\varepsilon|T^*=0,z=1]\\
   &\qquad {} + (1 - p_0^*) p_0^*(1-q)\mathbb{E}[\varepsilon|T^*=1,z=0] + (1 - p_1^*)p_1^*q \mathbb{E}[\varepsilon|T^*=1,z=1]\\
   &= -p_0^*(1-p_0^*)(1-q)(m^*_{00}-c) -p_1^* (1-p_1^*)q(m^*_{01} - c)\\
   &\qquad {} + (1-p_0^*)p_0^*(1-q)(m^*_{10}-c) + (1 - p_1^*)p_1^*q (m^*_{11}-c)\\
   &= (1-q)p_0^*(1-p_0^*)(m_{10}^* - m_{00}^*) + qp_1^*(1-p_1^*)(m_{11}^* - m_{01}^*)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[z\varepsilon v] &= \mathbb{E}[z^2\varepsilon v] = \mathbb{E}\left[ z^2 \mathbb{E}\left[ \varepsilon v|z \right] \right] = q \mathbb{E}\left[ \varepsilon v|z=1 \right] = q \mathbb{E}_{T^*|z=1}\left[ \mathbb{E}\left[ \varepsilon v |T^*=1,z=1 \right] \right]\\
  &= q\left\{p_1^* \mathbb{E}\left[ \varepsilon v \right|T^*=1,z=1] + (1 - p_1^*)\mathbb{E}[\varepsilon v|T^*=0,z=1]\right\}\\
  &= q\left\{p_1^*(1 - p_1^*) \mathbb{E}\left[ \varepsilon  \right|T^*=1,z=1] - p_1^* (1 - p_1^*)\mathbb{E}[\varepsilon |T^*=0,z=1]\right\}\\
  &= q p_1^*(1-p_1^*)\left[m^*_{11} - m^*_{01} \right]
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \mathbb{E}[z v^2] &= \mathbb{E}[z^2v^2] =  \mathbb{E}\left[ z^2 \mathbb{E}\left[ v^2|z \right]  \right] = q\mathbb{E}[v^2|z=1] = q \mathbb{E}_{T^*|z=1}\left[ \mathbb{E}\left[ v^2|T^*,z=1 \right] \right]\\
  &= q\left\{ p_1^* \mathbb{E}[v^2|T^*=1,z=1] + (1 - p_1^*) \mathbb{E}\left[ v^2|T^*=0,z=1 \right] \right\}\\
  &= q\left\{ p_1^*(1-p_1^*)^2 + p_1^{*2} (1 - p_1^*) \right\}\\
  &= qp_1^*(1 - p_1^*)\left[ (1-p_1^*)+ p_1^* \right]\\
  &= qp_1^*(1 - p_1^*)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Using these calculations, we find the elements of $\Sigma$ as follows:
\begin{align*}
  \mathbb{E}[\eta^2] &= \mathbb{E}[\varepsilon^2 + 2\beta \varepsilon v + \beta^2 v^2]  \\
  &= \sigma_\varepsilon^2 + 2 \beta \left[ (1-q)p_0^*(1-p_0^*)(m_{10}^* - m_{00}^*) + qp_1^*(1-p_1^*)(m_{11}^* - m_{01}^*)\right]\\
  &\qquad {} + \beta^2\left[(1-q)p_0^*(1 - p_0^*) + qp_1^*(1 - p_1^*)\right]\\
  &= \sigma_\varepsilon^2 + (1-q)p_0^*(1-p_0^*)\beta\left[ \beta + 2 \left( m_{10}^* - m_{00}^* \right) \right] + q p_1^*(1 - p_1^*)\beta\left[ \beta + 2\left( m_{11}^* - m_{01}^* \right) \right]\\
  \mathbb{E}[z^2 \eta^2] =  \mathbb{E}[z \eta^2] &= \mathbb{E}[ z \varepsilon^2 + 2\beta z \varepsilon v + \beta^2 z v^2 ] = \sigma_\varepsilon^2 + 2\beta \mathbb{E}[z \varepsilon v] + \beta^2 \mathbb{E}[zv^2]\\
  &= q\sigma_\varepsilon^2 + 2\beta q p_1^*(1-p_1^*)\left( m_{11}^* - m_{01}^* \right) + \beta^2 q p_1^*(1-p_1^*) \\
  &= q\left\{\sigma_\varepsilon^2 + p_1^*(1-p_1^*)\beta\left[ \beta + 2 \left( m_{11}^* - m_{01}^* \right) \right]\right\}
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Now, using the fact that $\mathbb{E}[z^2\eta] = \mathbb{E}[z\eta]$, the asymptotic variance of the reduced form estimator can be written as:
\begin{align*}
  \mbox{AVAR}\left[ \sqrt{n}\left( \widehat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}\right) \right] &= \frac{1}{q^2(1-q)^2}
  \left[
  \begin{array}{cc}
    q & -q \\ -q & 1
  \end{array}
\right]
\left[
\begin{array}{cc}
  \mathbb{E}(\eta^2) & \mathbb{E}(z\eta^2)\\
  \mathbb{E}(z\eta^2) & \mathbb{E}(z\eta^2)
\end{array}
\right]
\left[
\begin{array}{cc}
  q & -q \\ -q & 1
\end{array}
\right]\\
&= 
\frac{1}{q^2(1-q)^2}
\left[
\begin{array}{cc}
  q & -q \\ -q & 1
\end{array}
\right]
\left[
\begin{array}{cc}
  q\left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} &
  \mathbb{E}(z\eta^2) - q \mathbb{E}(\eta^2)\\
  0 & (1-q)\mathbb{E}(z\eta^2)
\end{array}
\right] \\ 
&= 
\frac{1}{q^2(1-q)^2}
\left[
\begin{array}{cc}
  q^2 \left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} & 
  -q^2 \left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} \\
  -q^2 \left\{ \mathbb{E}(\eta^2) - \mathbb{E}(z\eta^2) \right\} &
  q^2 \mathbb{E}(\eta^2) + (1-2q) \mathbb{E}(z\eta^2)
\end{array}
\right]
\end{align*}
Now, we are only interested in the reduced form slope coefficient $\gamma_1$.
The asymptotic variance of its OLS estimator is:
\begin{align*}
  \mbox{AVAR}\left[ \sqrt{n}(\widehat{\gamma}_1 - \gamma_1) \right] &= 
  \frac{1}{q^2(1-q)^2}\left[q^2 \mathbb{E}(\eta^2) + (1-2q) \mathbb{E}(z\eta^2)\right]\\
  &= \frac{\mathbb{E}(\eta^2)}{(1-q)^2} + \left[ \frac{1-2q}{q^2(1-q)^2} \right]\mathbb{E}(z\eta^2)
\end{align*}
In general, this will lead to quite a complicated expression.
In our simulation design, however, 
\begin{align*}
  q &= 1/2\\ 
  p^*_0(1-p^*_0) &= \delta(1-\delta)\\
  p^*_1(1-p^*_1) &= (1 -\delta) \delta\\
  (m^*_{10} - m^*_{00}) &= (m^*_{11} - m_{01}^*) > 0 
\end{align*}
leading to the following simplifications:
\small
\begin{align*}
  \mbox{AVAR}\left[ \sqrt{n}(\widehat{\gamma}_1 - \gamma_1) \right] &= 
  4 \mathbb{E}(\eta^2)\\
  &= 4\left\{ \sigma_\varepsilon^2 + 1/2 \times \delta(1 - \delta)\beta\left[ \beta + 2 \left( m_{10}^* - m_{00}^* \right) \right] + 1/2 \times \delta(1 - \delta)\beta\left[ \beta + 2\left( m_{11}^* - m_{01}^* \right) \right]\right\}\\
  &= 4\left\{ \sigma_\varepsilon^2 + \delta(1 - \delta)\beta\left[ \beta + 2\left( m_{11}^* - m_{01}^* \right) \right]\right\}
\end{align*}
\normalsize
Notice that, since $(m_{11}^* - m_{01}^*)$ is positive, as in the simulation design, the asymptotic variance is \emph{smallest} when $\beta = 0$ so that there is no treatment effect.

\paragraph{Robust Standard Errors} 
When implementing the reduced form estimator in practice we base our inference on sample residuals:
\[
  \widehat{\eta}_i = y_i - \widehat{\gamma}_0 - \widehat{\gamma}_1 z_i
\]
As we saw above, however, the errors $\eta$ are heteroskedastic: $\mathbb{E}(z\eta^2) \neq \mathbb{E}(z)\mathbb{E}(\eta^2)$ etc.
For this reason, we must use robust standard errors:
\[
  \widehat{\Sigma} = \frac{1}{n} \sum_{i=1}^n \left[
  \begin{array}{cc}
    \widehat{\eta}_i^2 & z_i \widehat{\eta}_i^2\\
    z_i \widehat{\eta}_i^2 & z_i^2 \widehat{\eta}_i^2\\
  \end{array}
\right]
\]
leading to
\[
  \widehat{\mbox{AVAR}}\left[ \sqrt{n}(\widehat{\gamma}_1 - \gamma_1) \right] = \frac{\widehat{\mathbb{E}}(\eta^2)}{(1-\widehat{q})^2} + \left[ \frac{1-2\widehat{q}}{\widehat{q}^2(1-\widehat{q})^2} \right]\widehat{\mathbb{E}}(z\eta^2)
\]
Note that $\widehat{q}$ is fixed and equal to $1/2$ in our simulation design, so this becomes $4 \widehat{\sigma}^2_{\eta}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Values of $m^*_{tk}$ in the Simulation}
To calculate $\Sigma$ in our simulation design, we'll need to know the values of $m_{tk}^*$ in the threshold-crossing model with bivariate normal errors:
\begin{align*}
  \left[
  \begin{array}{c}
    \varepsilon \\ \xi
  \end{array}
  \right] &\sim N\left( \left[
  \begin{array}{c}
    0 \\ 0
  \end{array}
\right], \left[
\begin{array}{cc}
  1 & \rho \\
  \rho & 1
\end{array}
\right]\right)\\ \\
  T^* &= \mathbf{1}\left\{ \kappa_0 + \kappa_1 z + \xi > 0 \right\}\\
  \kappa_0 &= \Phi^{-1}(\delta)\\
  \kappa_1 &= \Phi^{-1}(1-\delta) - \Phi^{-1}(\delta)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
  m_{00}^* - c &= \mathbb{E}[\varepsilon|T^*=0,z=0] = \mathbb{E}[\varepsilon|\xi \leq -\kappa_0]\\
  m_{01}^* - c &= \mathbb{E}[\varepsilon|T^*=0,z=1] = \mathbb{E}[\varepsilon|\xi\leq -(\kappa_0 + \kappa_1)]\\
  m_{10}^* - c &= \mathbb{E}[\varepsilon|T^*=1,z=0] = \mathbb[\varepsilon|\xi > -\kappa_0]\\
  m_{11}^* - c &= \mathbb{E}[\varepsilon|T^*=1,z=1] = \mathbb{E}[\varepsilon|\xi > -(\kappa_0 + \kappa_1)]
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
  \xi|\varepsilon \sim N(\rho \varepsilon, 1 - \rho^2)
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
\mathbb{P}(\varepsilon \leq x|\xi \leq a) &= \frac{\mathbb{P}(\varepsilon \leq x,\xi \leq a)}{\mathbb{P}(\xi \leq a)} = \frac{\int_{-\infty}^x \int_{-\infty}^a f(\xi|\varepsilon)f(\varepsilon) d \xi \, d \varepsilon }{\Phi(a)}\\
&=\frac{\int_{-\infty}^x F_{\xi|\varepsilon}(a)f(\varepsilon) d\varepsilon}{\Phi(a)} = \frac{\displaystyle\int_{-\infty}^x \varphi(\varepsilon)\Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)d\varepsilon }{\Phi(a)} \\ \\
  f(\varepsilon|\xi\leq a) &= \frac{\varphi(\varepsilon)}{\Phi(a)}\Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right) \\ \\
  \mathbb{E}\left[ \varepsilon|\xi \leq a \right] & = \frac{1}{\Phi(a)} \int_{-\infty}^\infty \varepsilon \varphi(\varepsilon) \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right) d\varepsilon
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
\mathbb{P}(\varepsilon \leq x|\xi > a) &= \frac{\mathbb{P}(\varepsilon \leq x,\xi > a)}{\mathbb{P}(\xi > a)} = \frac{\int_{-\infty}^x \int_a^\infty f(\xi|\varepsilon)f(\varepsilon) d \xi \, d \varepsilon }{1 - \Phi(a)}\\
&=\frac{\int_{-\infty}^x [1 - F_{\xi|\varepsilon}(a)]f(\varepsilon) d\varepsilon}{1- \Phi(a)} = \frac{\displaystyle\int_{-\infty}^x \varphi(\varepsilon)\left[1 - \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)\right]d\varepsilon }{1 - \Phi(a)} \\ \\
f(\varepsilon|\xi > a) &= \frac{\varphi(\varepsilon)}{1-\Phi(a)}\left[1 - \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)\right] \\ \\
\mathbb{E}\left[ \varepsilon|\xi > a \right] & = \frac{1}{1-\Phi(a)} \int_{-\infty}^\infty \varepsilon \varphi(\varepsilon)\left[1 -  \Phi\left( \frac{a - \rho \varepsilon}{\sqrt{1 - \rho^2}} \right)\right] d\varepsilon
\end{align*}

\todo[inline]{I've checked all of these integrals in R and they're definitely correct.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{IV Estimator}
First we calculate the probability limits of the IV estimators of $\beta$ and $c$.
Notice that \emph{both} are inconsistent:
\begin{align*}
  \widehat{\beta}_{IV} &= \left( \widetilde{Z}\widetilde{T}/n \right)^{-1}\left(\widetilde{Z}'\mathbf{y}/n\right) =  \left( \widetilde{Z}\widetilde{T}/n \right)^{-1}\widetilde{Z}' \left( \widetilde{T}^* \widetilde{\beta} + \boldsymbol{\varepsilon} \right)/n\\
&= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \left[\widetilde{Z}'\widetilde{T}^*/n \right] \widetilde{\beta} + \widetilde{Z}'\boldsymbol{\varepsilon}/n \right) \\ \\
  \widetilde{Z}'\boldsymbol{\varepsilon}/n &\rightarrow_p \mathbb{E}\left[
  \begin{array}{c}
    \varepsilon \\ z \varepsilon
  \end{array}
\right] = \mathbf{0}\\ \\ 
\left(\widetilde{Z}'\widetilde{T}/n\right)^{-1} &= \left[
\begin{array}{cc}
  1 & \bar{T} \\ \bar{z} & \mathbf{z}'\mathbf{T}/n
\end{array}
\right] \rightarrow_p \left[
\begin{array}{cc}
  1 & p \\ q & p_1 q
\end{array}
\right] = \frac{1}{q(p_1 - p)}\left[
\begin{array}{cc}
  p_1 q & -p \\ -q & 1
\end{array}
\right] \\ \\ 
\widetilde{Z}'\widetilde{T}^*/n &= \left[
\begin{array}{cc}
  1 & \bar{T^*} \\ \bar{z} & \mathbf{z}'\mathbf{T}^*/n
\end{array}
\right] \rightarrow_p \left[
\begin{array}{cc}
  1 & p^* \\ q & p_1^* q
\end{array}
\right] \\ \\ 
\left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\widetilde{T}^*/n \right) &\rightarrow_p \frac{1}{q(p_1 -p)}\left[
\begin{array}{cc}
  qp_1 & - p\\
  -q & 1 
\end{array}
\right]
\left[
\begin{array}{cc}
  1 & p^*\\
  q & qp_1^*
\end{array}
\right]= 
\left[
\begin{array}{cc}
  1 & (p_1 p^* - p_1^* p)/(p_1 - p)\\
  0 & (p_1^* - p^*) / (p_1 - p)
\end{array}
\right] \\ \\ 
\frac{p_1^* - p^*}{p_1 - p} &= \frac{1}{p_1 - p}\left( \frac{p_1 - \alpha_0}{1 - \alpha_0 - \alpha_1} - \frac{p - \alpha_0}{1 - \alpha_0 - \alpha_1} \right) = \frac{1}{1 - \alpha_0 - \alpha_1}\\ \\
\frac{p_1 p^* - p_1^*p}{p_1 - p} &= \frac{1}{p_1 - p}\left[ \frac{p_1(p - \alpha_0)}{1 - \alpha_0 - \alpha_1} -  \frac{p(p_1 - \alpha_0)}{1 - \alpha_0 - \alpha_1} \right] = \frac{-\alpha_0}{1 - \alpha_0 - \alpha_1}\\ \\
\boldsymbol{\beta}_{IV} &\rightarrow_p 
\left[
\begin{array}{cc}
  1 & (p_1 p^* - p_1^* p)/(p_1 - p)\\
  0 & (p_1^* - p^*) / (p_1 - p)
\end{array}
\right]\left[
\begin{array}{c}
  c \\ \beta
\end{array}
\right] = \frac{1}{1 - \alpha_0 - \alpha_1}\left[
\begin{array}{c} 
  c - \alpha_0 \beta \\
  \beta
\end{array}
\right] \equiv
\left[
\begin{array}{c}
  c_{IV}\\ \beta_{IV}
\end{array}
\right]
\end{align*}
Now that we have the probability limit of the IV estimator, we can work out the error term $\omega$ that corresponds to it as a function of the ``primitives'' of our model.
We have:
\begin{align*}
  \zeta &= y - (c_{IV} + \beta_{IV} T) = (c + \beta T^* + \varepsilon) - \frac{1}{1 - \alpha_0 - \alpha_1}\left[ \left(c - \alpha_0 \beta\right) + \beta T \right] \\
  &= \varepsilon + c - \left(\frac{c - \alpha_0 \beta}{1 - \alpha_0 - \alpha_1} \right) + \beta\left( T^* - \frac{T}{1 - \alpha_0 - \alpha_1} \right)\\
  &= \varepsilon + \left[\frac{\alpha_0 (\beta - c) - \alpha_1 c}{1 - \alpha_0 - \alpha_1} \right] + \beta\left[ \frac{(T^* - T) - (\alpha_0 + \alpha_1)T^*}{1 - \alpha_0 - \alpha_1} \right]\\
  &= \varepsilon + \left[\frac{\alpha_0 (\beta - c) - \alpha_1 c}{1 - \alpha_0 - \alpha_1} \right] + \beta\left[ \frac{w - (\alpha_0 + \alpha_1)(\pi_0 + \pi_1 z + v)}{1 - \alpha_0 - \alpha_1} \right]
\end{align*}
\todo[inline]{This looks pretty complicated. I'm also not sure we need to work this out analytically. All we really need is to write out the joint limit distribution of the IV and RF\ldots}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Joint Distribution of IV and RF}
To carry out inference for the identified set, we need to work out the joint distribution of the reduced form and IV estimators:
\begin{align*}
  \widehat{\boldsymbol{\beta}}_{IV} &= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\mathbf{y}/n \right)\\
  \widehat{\boldsymbol{\gamma}} &= \left( \widetilde{Z}'\widetilde{Z} \right)^{-1}\left( \widetilde{Z}'\boldsymbol{y}/n \right)
\end{align*}
The probability limits of each estimator define an associated error term, each of which depends in a complicated way on our ``primitive'' model parameters:
\begin{align*}
  y &= c_{IV} + \beta_{IV} T + \zeta \\
  y &= \gamma_0 + \gamma_1 z + \eta 
\end{align*}
But because these two estimators depend only on observable quantities, we can use their residuals to work out the covariance matrix of $(\zeta, \eta)$. 
Accordingly, we proceed as follows:
\begin{align*}
  \widehat{\boldsymbol{\beta}}_{IV} &= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\left[ \widetilde{T}\boldsymbol{\beta}_{IV} + \boldsymbol{\zeta} \right]/n \right) = \boldsymbol{\beta}_{IV} +\left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\boldsymbol{\zeta}/n\right) \\
  \widehat{\boldsymbol{\gamma}} &= \left( \widetilde{Z}'\widetilde{Z} \right)^{-1}\left( \widetilde{Z}'\left[ \widetilde{Z}\boldsymbol{\gamma} + \boldsymbol{\eta} \right]/n \right) = \boldsymbol{\gamma} + \left( \widetilde{Z}'\widetilde{Z}/n \right)^{-1}\left( \widetilde{Z}\boldsymbol{\eta}/n \right)
\end{align*}
yielding
\begin{align*}
  \sqrt{n}\left(\widehat{\boldsymbol{\beta}}_{IV} - \boldsymbol{\beta}_{IV}\right) &= \left( \widetilde{Z}'\widetilde{T}/n \right)^{-1}\left( \widetilde{Z}'\boldsymbol{\zeta}/\sqrt{n}\right) \\
  \sqrt{n}\left(\widehat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}\right) &=  \left( \widetilde{Z}'\widetilde{Z}/n \right)^{-1}\left( \widetilde{Z}'\boldsymbol{\eta}/\sqrt{n} \right)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other Stuff\ldots}

\begin{itemize}
  \item Manski was interested in a heterogenous treatment effect model and whether we could bound \emph{ATE} rather than \emph{LATE}.
  \item Would it help in the performance of the GMM estimator if we enforced the bounds on $\alpha_0$ and $\alpha_1$?
\end{itemize}



\paragraph{Exogenous Covariates in a Linear Model:} These should be very easy to handle because we can just stack the GMM moment conditions to include an IV estimator for the parameter on the exogenous covariates in the main equation.
Recall that the usual IV estimator for these parameters is unaffected by measurement error.
We should write this out since it's the case that many people will use in practice given the extreme sample size demands of fully non-parametric estimation!

\paragraph{More About Weak Identification:}
Sophocles pointed out in an email exchange that I had been assuming (incorrectly) that $\mbox{Cov}(y,z)$ is always well-behaved.
This is not the case if $z$ is a weak instrument.
I don't think we can simply assume we have a strong instrument and consider the weak identification that arises from $\beta \approx 0$ in isolation.
I think the two problems of $\beta \approx 0$ and weak $z$ interact in an important way since, as we saw from above, the determinant $|G_\theta|$ that measures the strength of identification depends on the \emph{product} of $\beta$ and $Cov(z,T)$.
I think it the correct interpretation of this is that the magnitude of $\beta$ that gives strong identification should is always relative to the strength of $z$.
If $z$ is very strong, then $\beta$ can be smaller without causing problems.
But if $z$ is weak then I think $\beta$ needs to be really large to get strong identification.
If I recall correctly, we uncovered something in our simulations that appears to agree with this intuition but I need to go back and check.

To see why $\mbox{Cov}(y,z)$ is badly behaved when $z$ is weak, write out an explicit first-stage equation for our model as follows:
\[
T^* = \pi_0 + \pi_1 z + v
\]
where
\begin{align*}
\pi_0 &= \mathbb{E}[T^*|z=0] = p^*_0\\
\pi_1 &= \mathbb{E}[T^*=1|z=1] - \mathbb{E}[T^*|z=0] = p^*_1 - p^*_0
\end{align*}
and $\mathbb{E}[zv]=0$ by construction.
Now,
\begin{align*}
  \mbox{Cov}(z,y) &= \mathbb{E}(zy) - \mathbb{E}(z)\mathbb{E}(y)\\
  &=\mathbb{E}\left[z\left( c + \beta T^* + \varepsilon \right) \right] - q \mathbb{E}\left( c + \beta T^* + \varepsilon \right)\\
  &=\mathbb{E}\left[z\left\{ c + \beta \left( \pi_0 + \pi_1 z + v \right) + \varepsilon \right\} \right] - q \mathbb{E}\left[ c + \beta \left( \pi_0 + \pi_1 z + v \right) + \varepsilon \right]\\
  &= q (c + \beta \pi_0) + \beta \pi_1 \mathbb{E}(z^2) - q\left( c + \beta \pi_0 + \beta \pi_1 \mathbb{E}[z] \right) \\
  &= q (c + \beta \pi_0 + \beta \pi_1)  - q\left( c + \beta \pi_0 + \beta \pi_1 q \right) \\
  &= q(\beta \pi_1  - \beta \pi_1 q) = \beta \pi_1 q(1 - q)\\
  &= \beta (p^*_1 - p^*_0) q(1-q)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Auxiliary Moment Inequalities}
Notice that if $\beta=0$, then the preceding moment equalities do \emph{not} identify $\alpha_1$.
However, we do have auxiliary moment \emph{inequalities} that partially identify $\alpha_1$ regardless of the value of $\beta$.
The simplest of these comes from the relationship  
\[
  p_k^* = \frac{p_k - \alpha_0}{1 - \alpha_0 - \alpha_1}, \quad
  1 - p_k^* = \frac{1 - p_k - \alpha_1}{1 - \alpha_0 - \alpha_1}
\]
where $p_k = P(T=1|z_k)$ and $p_k^* = P(T^*=1|z_k)$.
(This follows from the Law of Total Probability and our assumption that the mis-classification probabilities rates depend only on $T^*$, not $z$.)
Under our assumption that $\alpha_0 + \alpha_1 < 1$, we obtain $\alpha_0 < \min_k p_k$ and $\alpha_1 < \min_k (1 - p_k)$.
If $\alpha_0 = 0$, as we assume in the present special case, then without any assumption on the true value of $\alpha_1$ we have 
\[0 \leq \alpha_1 < \min_k (1 - p_k) = 1 - \max_k p_k.\] 
Is there some way to use these moment inequalities in estimation?



\paragraph{Under Normality}
  In our simulation for the CDF bounds on $\alpha_0$ and $\alpha_1$, we found that the upper bounds were in fact equal to the true parameter values.
  This is very surprising and is very likely comes from the specific parametric model from which we simulated.
  This happens to have been a model with normally distributed errors.
  Can we say anything about such a model theoretically?
  Perhaps try to write down the likelihood function?
  This could also be a useful way to look at the weak identification problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{April 2017 -- New GMM Formulation}
  Consider the special case in which $\alpha_0 = 0$ so the model is identified from
\begin{align*}
  \mbox{Cov}(y,z) - \left( \frac{\beta}{1 -\alpha_1} \right) \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \left(\frac{\beta}{1 - \alpha_1}\right)\left[2\mbox{Cov}(yT,z)- \beta \mbox{Cov}(T,z) \right] &= 0
\end{align*}
Above we wrote this in a standard GMM form by adding auxiliary moment equations to identify $\mathbb{E}[y^2]$, $\mathbb{E}[yT]$, etc.\
But there's a simpler and more transparent way to do this.
Under our assumptions and $\alpha_0 = 0$, some algebra shows that 
\todo[inline]{Add the algebra from the whiteboard notes later}
\[
  \mathbb{E}\left[ y - \frac{\beta}{1 - \alpha_1}T \right] = c
\]
and that
\[
  \mathbb{E}\left[ y^2 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha} T \right] = \sigma_{\varepsilon\varepsilon} + c^2
\]
where $c$ is the intercept from the regression model and $\sigma_{\varepsilon\varepsilon} = \mbox{Var}(\varepsilon)$.
Now, re-writing the first covariance equation using the linearity of expectation,
\begin{align*}
  \mathbb{E}\left[ yz - \frac{\beta}{1 - \alpha_1} Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ y - \frac{\beta}{1 - \alpha} T\right] &=  0\\
  \mathbb{E}\left[ yz - \frac{\beta}{1 - \alpha_1} Tz \right] - \mathbb{E}[z] c&=  0\\
  \mathbb{E}\left[ \left\{y - c -  \frac{\beta}{1 - \alpha_1} T\right\}z \right] &=  0
\end{align*}
and proceeding similarly for the second covariance equation
\begin{align*}
  \mathbb{E}\left[ y^2z - \frac{\beta}{1 - \alpha_1} 2yTz + \frac{\beta^2}{1 - \alpha_1} Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ y - \frac{\beta}{1 - \alpha_1}yT + \frac{\beta^2}{1 - \alpha_1} T \right] &= 0\\
  \mathbb{E}\left[ y^2z - \frac{\beta}{1 - \alpha_1} 2yTz + \frac{\beta^2}{1 - \alpha_1} Tz \right] - \mathbb{E}[z]\left( \sigma_{\varepsilon\varepsilon} + c^2 \right) &= 0\\
  \mathbb{E}\left[ \left\{y^2 - \sigma_{\varepsilon\varepsilon} - c^2 -\frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right\}z \right] = 0 
\end{align*}
Thus, we can express our estimator in terms of the following four moment equations:
\begin{align*}
  \mathbb{E}\left[ y - c -  \frac{\beta}{1 - \alpha_1} T \right] &= 0\\
  \mathbb{E}\left[ y^2 - \sigma_{\varepsilon\varepsilon} - c^2 -\frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right] &= 0 \\
  \mathbb{E}\left[ \left\{y - c -  \frac{\beta}{1 - \alpha_1} T\right\}z \right] &= 0\\
  \mathbb{E}\left[ \left\{y^2 - \sigma_{\varepsilon\varepsilon} - c^2 -\frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right\}z \right] &= 0 
\end{align*}
To simplify the notation, let $\boldsymbol{\theta} = (\alpha_1, \beta, c, \sigma_{\varepsilon\varepsilon})'$ and define
\begin{align*}
  u(\boldsymbol{\theta}) &= y - c - \frac{\beta}{1 - \alpha_1} T\\
  v(\boldsymbol{\theta}) &= y^2 - \sigma_{\varepsilon\varepsilon} - c^2 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1}
\end{align*}
Then we can express the four moment equalities as 
\[
  \mathbb{E}\left[ g_1(\mathbf{x}, \boldsymbol{\theta}) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    u(\boldsymbol{\theta})\\ v(\boldsymbol{\theta})
  \end{array}
\right] = \mathbf{0}, \quad
\mathbb{E}\left[ g_2(\mathbf{x}, \boldsymbol{\theta}) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    u(\boldsymbol{\theta}) z\\ v(\boldsymbol{\theta}) z
  \end{array}
\right] = \mathbf{0}
\]
We also have two moment inequalities, namely $\alpha_1 \leq 1 - p_1$ and $\alpha_1 \leq 1 - p_0$.
After some algebra (see the whiteboard), we can show that these are equivalent to
\[
  \mathbb{E}\left[ h(\mathbf{x},\theta) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    (1 - \alpha_1) - T(1 - z)/(1 - q)\\
    (1 - \alpha_1) - Tz/q
  \end{array}
\right] \geq \mathbf{0}
\]
where $q = \mathbb{E}[z]$.
We will \emph{condition} on $z$, i.e.\ hold it fixed in repeated samples, so we will not add an extra moment condition for $q$.
Instead we will simply substitute the sample analogue.

To formulate the GMM estimator with moment inequalities as in Moon and Schorfheide (2009), we introduce some further notation.
Let $\boldsymbol{\lambda}$ denote the slack in $h(\mathbf{x}, \theta)$, so that
\[
  \mathbb{E}[h(\mathbf{x},\theta)] = \boldsymbol{\lambda} \geq \mathbf{0} \iff \mathbb{E}[h(\mathbf{x}, \theta) - \boldsymbol{\lambda}] = \mathbf{0}
\]
Further define
\[
  f(\mathbf{x}) = \left[
  \begin{array}{c}
    g(\mathbf{x},\theta) \\ h(\mathbf{x},\boldsymbol{\theta})
  \end{array}
\right], \quad
\psi(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda}) = 
\left[
\begin{array}{c}
  g(\mathbf{x}, \boldsymbol{\theta})\\
  h(\mathbf{x}, \boldsymbol{\theta}) - \boldsymbol{\lambda}
\end{array}
\right]
\]
and let $\Theta = \left\{ \boldsymbol{\theta}\colon \alpha_1 \geq 0, \sigma_{\varepsilon\varepsilon}\geq 0 \right\}$.
Then, the GMM estimator based on our moment equalities and inequalities is
\[
  (\widehat{\boldsymbol{\theta}}, \widehat{\boldsymbol{\lambda}}) = \underset{\boldsymbol{\theta} \in \Theta, \boldsymbol{\lambda} \geq \mathbf{0}}{\arg\min} \quad Q_n(\boldsymbol{\theta}, \boldsymbol{\lambda})
\]
where
\[ 
  Q_n(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \frac{1}{2}\bar{\psi}_n(\mathbf{x},\boldsymbol{\theta}, \boldsymbol{\lambda})' \mathbf{W}_n \bar{\psi}_n(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda})
\]
\[
  \bar{\psi}_n(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \frac{1}{n}\sum_{i=1}^n \psi(\mathbf{x}_i, \boldsymbol{\theta}, \boldsymbol{\lambda}) 
\]
and $\mathbf{W}_n$ is a weighting matrix that should be irrelevant in our case because we're just identified.
We scale the criterion by $1/2$ so that the derivative looks nice.
\todo[inline]{Is this still the case with the moment inequalities? Mechanically we have introduced two new parameters to match the two extra conditions.}

At various points we will need the derivatives of the moment equations.
Let
\[
  \mathbf{G}(\boldsymbol{\theta}) = \mathbb{E}[\nabla_{\boldsymbol{\theta}'} g(\mathbf{x},\boldsymbol{\theta})], \quad \mathbf{H}(\boldsymbol{\theta}) = \mathbb{E}[\nabla_{\boldsymbol{\theta}'} h(\mathbf{x},\boldsymbol{\theta})]
\]
and define 
\begin{align*}
  \boldsymbol{\Psi}(\boldsymbol{\theta}, \boldsymbol{\lambda}) &= \mathbb{E}
  \left[
  \begin{array}{cc}
    \nabla_{\boldsymbol{\theta}'} \psi(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda}) &
    \nabla_{\boldsymbol{\lambda}'} \psi(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\lambda}) 
  \end{array}
\right]\\
&= \mathbb{E}\left[
\begin{array}{cc}
 fill & this \\
 in & later
\end{array}
\right]
\end{align*}

\section{April 1--15, 2017 -- Andrews \& Soares (2010)}
Our moment equalities from above do not identify $\alpha$ when $\beta=0$.
More generally, the estimator based on them performs poorly when $\beta$ is relatively small compared to the error variance.
Continue to assume that $\alpha_0 = 0$ so the moment conditions simplify.

We now consider an inference procedure following Andrews \& Soares.
The basic idea is to ``isolate'' the problematic parameters, in our case $\alpha$ and $\beta$, and carry out joint inference for these using the Anderson-Rubin test statistic.
This is constructed by substituting a null hypothesis $H_0\colon \theta = \theta_0$ into the sample analogue of the GMM moment conditions and relying on the fact that this sample analogue remains ``well-behaved'' even in situations where inference for the GMM \emph{parameter estimator} breaks down.
Examples include parameters on the boundary, and parameters that may not be identified, e.g.\ $\alpha_1$ if $\beta = 0$

\subsection{Simple Example: $\alpha_0 = 0$, $c=0$, and $\sigma_{\varepsilon\varepsilon}=1$}
In our problem $c$ and $\sigma_{\varepsilon\varepsilon}$ are essentially nuisance parameters.
Fortunately, they are \emph{always identified} from our moment conditions regardless of the values of $\beta$ and $\alpha$, as we will discuss further below.
For the moment, we will suppose that $c$ is known to equal zero and $\varepsilon_{\varepsilon\varepsilon}$ is known to equal one as is the case in our baseline simulation.
Later we will estimate them which requires only a small modification of the procedure we know outline.
With the simplifications $\alpha_0, c=0, \sigma_{\varepsilon\varepsilon}=1$ the equality moment conditions become
\begin{align*}
  \mathbb{E}\left[ \left\{y - \frac{\beta}{1 - \alpha_1} T\right\}z \right] &= 0\\
  \mathbb{E}\left[ \left\{y^2 - 1 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1} T\right\}z \right] &= 0 
\end{align*}
since we no longer need the $g_1$ block of moment conditions to identify the ``intercepts'' $c$ and $\sigma_{\varepsilon\varepsilon}$.
For this simplified set of moment conditions, our parameter vector is $\boldsymbol{\theta} = (\alpha_1, \beta)'$ and the residuals are given by
\begin{align*}
  u(\boldsymbol{\theta}) &= y - \frac{\beta}{1 - \alpha_1} T\\
  v(\boldsymbol{\theta}) &= y^2 - 1 - \frac{\beta}{1 - \alpha_1} 2yT + \frac{\beta^2}{1 - \alpha_1}
\end{align*}
and we can write the equality moment conditions as
\[
\mathbb{E}\left[ g(\mathbf{x}, \boldsymbol{\theta}) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    u(\boldsymbol{\theta}) z\\ v(\boldsymbol{\theta}) z
  \end{array}
\right] = \mathbf{0}
\]
The inequality moment conditions are unchanged from above, namely
\[
  \mathbb{E}\left[ h(\mathbf{x},\theta) \right] = \mathbb{E}
  \left[
  \begin{array}{c}
    (1 - \alpha_1) - T(1 - z)/(1 - q)\\
    (1 - \alpha_1) - Tz/q
  \end{array}
\right] \geq \mathbf{0}
\]
where $q = \mathbb{E}[z]$.
As above we will \emph{condition} on $z$, i.e.\ hold it fixed in repeated samples, so we will not add an extra moment condition for $q$.
Instead we will simply substitute the sample analogue.
Note that this means we should hold $q$ \emph{fixed} when bootstrapping below.
We now introduce some notation from Andrews and Soares (2010).

\paragraph{Population Moment Conditions}
\[
  \mathbb{E} \left[ m_j(\mathbf{w}_i,\theta_0) \right]
  \left\{
  \begin{array}{cc}
    \geq 0 & \mbox{for } j = 1, \cdots, p\\
    = 0 & \mbox{for } j = p + 1, \cdots,k \mbox{ where } k = p + v
  \end{array}
  \right.
\]
where $p$ is the number of inequality moment conditions (in our case $p = 2$), $v$ is the number of equality moment conditions (in our case $v = 2$), $\theta_0$ is the true parameter vector, and $\mathbf{w}_i$ is the vector of observations for individual $i$ (in our case $\mathbf{w}_i = (T_i, z_i, y_i)$).

\paragraph{Sample Moment Functions, etc.}
\[
  \bar{m}_n(\theta) = \left[
  \begin{array}{c}
    \bar{m}_{n,1}(\theta)\\
    \vdots \\
    \bar{m}_{n,k}(\theta)\\
  \end{array}
\right], \quad
\bar{m}_{n,j} = \frac{1}{n} \sum_{i=1}^{n} m_j(\mathbf{w}_i, \theta) \mbox{ for } j = 1, \cdots, k
\]
Now, let $\Sigma(\theta_0)$ denote the asymptotic variance of $\sqrt{n}\; \bar{m}_n(\theta)$.
We estimate this quantity using $\widehat{\Sigma}_n(\theta)$.
For iid observations, as in our example, the estimator is
\[
  \widehat{\Sigma}(\theta) = \frac{1}{n} \sum_{i=1}^n \left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]\left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]', \quad 
  m(\mathbf{w}_i, \theta) = \left[
  \begin{array}{c}
    m_1(\mathbf{w}_i, \theta)\\
    \vdots \\
    m_k(\mathbf{w}_i, \theta)\\
  \end{array}
\right]
\]

\paragraph{Test Statistic}
The test statistic takes the form $T_n(\theta) = S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right)$ for some real-valued function $S$.
The example we will use is $S_1$, defined by
\[
  S_1(m, \Sigma) = \sum_{j=1}^p [m_j/\sigma_j]^2_- + \sum_{j=p+1}^{p+v} (m_j/\sigma_j)^2
\]
where $m = (m_1, \cdots, m_k)'$,
\[
  [x]_- = \left\{
  \begin{array}{cc}
    x, & \mbox{if } x <0\\
    0, & \mbox{if } x \geq 0
  \end{array}
\right.
\]
and $\sigma_j^2$ is the $j$th diagonal element of $\Sigma$.
Notice that $S_1$ only gives weight to inequality moment conditions that are \emph{violated}.

\paragraph{Basic Idea of the Test}
Let's return to our specific example for a moment.
The idea is essentially to plug a null hypothesis $\theta^* = (\alpha_1^*, \beta^*)'$ into the sample analogue:
\[
  \sqrt{n}\; \bar{m}_n(\alpha_1^*, \beta^*) = \frac{1}{\sqrt{n}}\sum_{i=1}^n
  \left[
  \begin{array}{r}
    (1 - \alpha_1^*) - T_i (1 - z_i)/(1-q)\\ \\
    (1 - \alpha_1^*) - T_i z_i/q\\ \\
    \left(y_i - \displaystyle\frac{\beta^*}{1 - \alpha_1^*}T_i\right) z_i \\ \\
    \left( y_i^2 - 1 - \displaystyle \frac{\beta^*}{1 - \alpha_1^*} 2y_i T_i + \displaystyle\frac{\beta^{*2}}{1 - \alpha_1^*}T_i\right)z_i 
  \end{array}
\right]
\]
and see if the result is ``large'' after standardizing and squaring the individual elements.
We only give weight to an inequality if it is violated.
The variance matrix of the sample analogue is calculated \emph{under the null}, i.e.\ assuming that $\theta = \theta^*$.
Note that we also use the \emph{centered variance matrix estimator}.

We reject the null if the test statistic is too large.
This gives us \emph{joint inference} for $\alpha$ and $\beta$ \emph{simultaneously}.
To construct a joint confidence region, we need to test pairs $(\alpha_1, \beta)$.
Of course, we restrict $\alpha_1$ to lie in $[0, 1)$.
The resulting confidence region \emph{need not be convex}.
In fact it could even be disconnected!
However, in our particular example, it might be possible to prove that one gets a connected or even convex region.
This is something we should think about since it would reduce the computational burden substantially.
To get \emph{marginal} inference, say for $\beta$ only, one projects the joint confidence set.
This is necessarily conservative, but may not be too bad in practice. 
We'll have to see\ldots

A particularly salient null hypothesis is $\beta = 0$.
Imposing this yields
\[
  \sqrt{n}\; \bar{m}_n(\alpha_1^*, 0) = \frac{1}{\sqrt{n}}\sum_{i=1}^n
  \left[
  \begin{array}{r}
    (1 - \alpha_1^*) - T_i (1 - z_i)/(1-q)\\ 
    (1 - \alpha_1^*) - T_i z_i/q\\ 
    y_i z_i \\ 
    \left( y_i^2 - 1 \right)z_i 
  \end{array}
\right]
\]
We see that this function depends on $\alpha_1^*$ \emph{only via the moment inequalities}.
What is more, the test statistic based on $S_1$ does \emph{not} depend on $\alpha_1^*$ unless the inequality constraints are violated.

\paragraph{Calculating the Critical Value}
The test statistic we will use to test $\theta = \theta^*$ is fairly simple to compute: we simply substitute into the GMM sample analogue.
The critical value for the test, however, is much more complicated.
Following Andrews \& Soares (2010), we use the following bootstrap procedure.
First we define some additional notation.
All of the test statistics considered in Andrews \& Soares (2010) satisfy
\[
  T_n = S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right) = S\left( \widehat{D}^{-1/2}(\theta) \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Omega}_n(\theta) \right)
\]
where
\[
  \widehat{D}_n(\theta) = \mbox{diag}\left( \widehat{\Sigma}\left( \theta \right) \right), \quad \widehat{\Omega}_n(\theta) = \widehat{D}_n^{-1/2}(\theta)\, \widehat{\Sigma}(\theta)\, \widehat{D}_n^{-1/2}(\theta)
\]
Now, let $\left\{ \mathbf{w}_i^* \right\}_{i=1}^n$ denote a bootstrap sample and define the associated bootstrap quantities
\begin{align*}
  M_n^*(\theta) &=  \sqrt{n} \left( \widehat{D}^*\left(\theta\right) \right)^{-1/2} \left( \bar{m}^*_n\left( \theta \right) - \bar{m}_n\left( \theta) \right) \right)\\
  \widehat{\Omega}^*(\theta) &= \left( \widehat{D}^*\left( \theta \right) \right)^{-1/2} \widehat{\Sigma}_n^*(\theta)\left( \widehat{D}^*\left( \theta \right) \right)^{-1/2} \\
  \widehat{D}^*\left( \theta \right) &= \mbox{diag}\left( \widehat{\Sigma}_n^*\left( \theta \right) \right)\\
  m_n^*(\theta) &= \frac{1}{n} \sum_{i=1}^n m(\mathbf{w}_i^*,\theta)\\
  \widehat{\Sigma}(\theta)^* &= \frac{1}{n} \sum_{i=1}^n \left[ m(\mathbf{w}_i^*, \theta) - \bar{m}_n^*(\theta) \right]\left[ m(\mathbf{w}_i^*, \theta) - \bar{m}_n^*(\theta) \right]'
\end{align*}
Note that $M_n^*(\theta)$ is centered around the \emph{non-bootstrap}  sample analogue $\bar{m}_n(\theta)$: this is \emph{very important!}
Now we describe the procedure for calculating the bootstrap critical value:
\begin{enumerate}
  \item Calculate $\sqrt{n}\; \bar{m}_n\left( \theta_0 \right)$ and $\widehat{\Sigma}(\theta_0)$ under the null hypothesis $H_0\colon \theta = \theta_0$.
  \item Determine which inequality moment conditions are ``far from binding'' as follows:
    \begin{itemize}
      \item Let $j \in J = \{ 1, \cdots, p\}$ index the inequality moment conditions.
      \item Let $\widehat{\sigma}_{n,j}(\theta_0)^2$ denote the $(j,j)$ element of $\widehat{\Sigma}(\theta_0)$
      \item For each $j\in J$ calculate the ``t-statistic'' $ t_{n,j} = \sqrt{n}\; \bar{m}_j(\theta_0)/\widehat{\sigma}_{n,j}(\theta_0)$
      \item Let $\mathcal{FB}$ denote the subset of $J$ for which $t_{n,j} >\sqrt{\log n}$.
        These are the inequality moment conditions that are ``far from binding'' under $H_0\colon\theta = \theta_0$. 
    \end{itemize}
  \item Calculate the test statistic $T_n = S_1\left(\sqrt{n}\; \bar{m}_n\left( \theta_0 \right), \widehat{\Sigma}\left( \theta_0 \right)  \right)$.
  \item Calculate the bootstrap critical value for the test as follows:
    \begin{itemize}
      \item Draw $R$ bootstrap samples -- each with sample size $n$. 
      \item For each bootstrap sample, $r$, calculate $M^{**}_{n,r}(\theta_0)$ and $\widehat{\Omega}^{**}_{n,r}(\theta_0)$ -- the bootstrap versions of $M_n(\theta_0)$ and $\widehat{\Omega}(\theta_0)$, defined above but with a slight change: \emph{drop} any moment inequality $j \in \mathcal{FB}$.
        That is, drop any inequality that we determined was far from binding \emph{on the basis of the real data} (i.e.\ \emph{not} this bootstrap sample!)
      \item For each bootstrap sample $r$ calculate $T_{n,r}^{**} = S_1\left(M^{**}_{n,r}\left( \theta_0 \right), \widehat{\Omega}^{**}_{n,r}\left( \theta_0 \right)  \right)$.
      \item Set $\widehat{c}_n(\theta_0, 1-\delta)$ equal to the $1-\delta$ sample quantile of the $\left\{ T_{n,r}^{**} \right\}_{r=1}^R$
    \end{itemize}
  \item Reject $H_0\colon \theta = \theta_0$ if $T_n > \widehat{c}_n(\theta_0, 1-\delta)$
  \item To construct a $(1 - \delta)\times 100\%$ confidence set, invert the test of $H_0\colon \theta = \theta_0$ for $\theta_0 \in \Theta$.
\end{enumerate}

\section{April 15--18, 2017}
 
\subsection*{Second Moment Inequalities}

\paragraph{Step 1: Second Moment Bounds}
Our model has $\varepsilon = y - c - bT^*$ where $\mathbb{E}[\varepsilon]=0$.
Conditional on $T^*=0$, $\varepsilon = y-c$ and conditional on $T^*=1$, $\varepsilon = y - c -\beta$.
Hence,
\begin{align*}
  \mathbb{E}\left[ \varepsilon^2| T^*=0, z=k\right] &= \mathbb{E}\left[ y^2 - 2cy|T^*=0, z=k \right] + c^2\\
  \mathbb{E}\left[ \varepsilon^2| T^*=1, z=k\right] &= \mathbb{E}\left[ y^2 - 2(\beta + c)y|T^*=1, z=k \right] + (\beta + c)^2
\end{align*}
The bounds will impose that $\mbox{Var}(\varepsilon|T^*,z)>0$ which is equivalent to $\mathbb{E}\left[ \varepsilon^2|T^*,z \right]>0$ since $\varepsilon$ is mean zero:
\begin{align*}
  \mathbb{E}\left[ y^2|T^*=0, z=k \right]&> 2c \mathbb{E}\left[ y|T^*=0, z=k \right] - c^2\\
  \mathbb{E}\left[ y^2|T^*=1, z=k \right]&> 2(\beta + c) \mathbb{E}\left[ y|T^*=1, z=k \right] - (\beta + c)^2
\end{align*}

\todo[inline]{These are not in fact the bounds we want! I messed up since $\varepsilon$ is \emph{not} mean zero \emph{conditional} on $T^*$ and $z$. These bounds are still correct, but they're not the best we can do and may not in fact be better than the simple first-moment bounds\ldots}

\paragraph{Step 2: Relate $\mathbb{E}[y^r|T^*,z]$ to $\mathbb{E}[y^r|T,z]$}
By iterated expectations and the assumption that $\mathbb{E}\left[ y^r|T,T^*,z \right]= \mathbb{E}\left[ y^r|T^*,z \right]$,
\begin{align*}
  \mathbb{E}\left[ y^r|T=0, z_k \right] &= \mathbb{E}\left[ y^r|T^*=0, z_k \right]\mathbb{P}(T^*=0|T=0, z_k) + \mathbb{E}\left[ y^r|T^*=1, z_k \right]\mathbb{P}(T^*=1|T=0, z_k)\\
  \mathbb{E}\left[ y^r|T=1, z_k \right] &= \mathbb{E}\left[ y^r|T^*=0, z_k \right]\mathbb{P}(T^*=0|T=1, z_k) + \mathbb{E}\left[ y^r|T^*=1, z_k \right]\mathbb{P}(T^*=1|T=1, z_k)
\end{align*}
The preceding is a linear system of the form
\begin{align*}
  a &= \pi x + (1 - \pi)y \\
  b &= (1 - \delta) x + \delta y
\end{align*}
and hence its solution is
\begin{align*}
  x &= \left[ \frac{\delta}{\pi\delta - (1 - \pi)(1 - \delta)} \right] a + \left[ \frac{-(1 - \pi)}{\pi \delta - (1 - \pi)(1 - \delta)} \right] b\\
  y &= \left[ \frac{-(1 - \delta)}{\pi\delta - (1 - \pi)(1 - \delta)} \right] a + \left[ \frac{\pi}{\pi \delta - (1 - \pi)(1 - \delta)} \right] b\\
\end{align*}
by Bayes' rule, as we show in the appendix to sick-instruments,
\begin{align*}
  \pi &= \mathbb{P}(T^*=0|T=0, z_k) =(1 - \alpha_0)(1 - p_k^*)/(1 - p_k)\\
  1 - \pi &= \mathbb{P}(T^*=1|T=0, z_k) =\alpha_1p_k^*/(1 - p_k)\\
  \delta &= \mathbb{P}(T^*=1|T=1, z_k) =(1 - \alpha_1)p_k^*/p_k\\
  1-\delta &= \mathbb{P}(T^*=0|T=1, z_k) = \alpha_0 (1 - p_k^*)/p_k
\end{align*}
Some algebra shows that 
\[
  \pi\delta - (1 - \pi)(1 - \delta) = \frac{(p_k - \alpha_0)(1 - p_k - \alpha_1)}{1 - \alpha_0 - \alpha_1}
\]
Hence, after simplifying and rearranging, it follows that 
\begin{align*}
  p_k(1 - p_k)(1 - p_k - \alpha_1)\mathbb{E}\left[ y^r|T^*=0, z_k \right] &= (1 - \alpha_1)(1 - p_k) \mathbb{E}[y^r|T=0, z_k] - \alpha_1 p_k \mathbb{E}[y^r|T=1,z_k]\\
  p_k(1 - p_k)(p_k - \alpha_0)\mathbb{E}\left[ y^r|T^*=1, z_k \right] &= (1 - \alpha_0)p_k\mathbb{E}[y^r|T=1,z_k] - \alpha_0(1 - p_k) \mathbb{E}[y^r|T=0,z_k]
\end{align*}


\paragraph{Step 3: Convert Conditional to Unconditional Moments}
By iterated expectations and the assumption that $\mathbb{E}\left[ y^r|T,T^*,z \right]= \mathbb{E}\left[ y^r|T^*,z \right]$
\begin{align*}
  p_k &= \mathbb{E}[T|z=k] = \mathbb{E}\left[ T \mathbf{1}(z=k) \right]/\mathbb{P}(z=k)\\
  \mathbb{E}\left[ y^r |T=0, z=k\right] &= \mathbb{E}\left[ y^r(1 - T) \mathbf{1}(z=k) \right]/\left[ (1 - p_k) \mathbb{P}(z=k)\right]\\
  \mathbb{E}\left[ y^r |T=1, z=k\right] &= \mathbb{E}\left[ y^r T \mathbf{1}(z=k) \right]/\left[p_k \mathbb{P}(z=k)\right]
\end{align*}

\paragraph{Step 4: Substitute Steps 2--3 into Step 1}
After some algebra, we find that
\begin{align*}
  \mathbb{E}\left[ \mathbf{1}(z=k)\left\{ T - (1 - \alpha_1) \right\}\left(y^2 - 2cy \right) \right] &< c^2 \mathbb{P}(z=k)p_k(1 - p_k)(1 - p_k - \alpha_1)\\
  \mathbb{E}\left[ \mathbf{1}(z=k)(\alpha_0 - T)\left\{ y^2 - 2(\beta + c)y \right\} \right] &< (\beta + c)^2 \mathbb{P}(z=k) p_k(1 - p_k)(p_k - \alpha_0)
\end{align*}

\paragraph{Step 5: Complete the Square}
\begin{align*}
  \mathbb{E}\left[ \mathbf{1}(z=k)\left\{ 1 - \alpha_1 - T \right\}\left(y - c\right)^2 \right] &> c^2 \mathbb{P}(z=k)(1 - p_k - \alpha_1)\left[ 1 - p_k(1 - p_k) \right]\\
  \mathbb{E}\left[ \mathbf{1}(z=k)(T - \alpha_0)\left\{ y - (\beta + c) \right\}^2 \right] &> (\beta + c)^2 \mathbb{P}(z=k) (p_k - \alpha_0)\left[ 1 - p_k(1 - p_k) \right]
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{April 21--23}
\subsection*{The Full Set of Moment Inequalities}
Above we considered the special case in which $\alpha_0= 0$ so that first and second moments were sufficient to identify the model.
We now derive the GMM-style moment conditions for the general case in which $\alpha_0$ may not equal zero and we need to use third moments for identification.

\todo[inline]{Note that we have checked all of these equalities numerically in our simulation study and they are indeed correct!}

\paragraph{Notation and Identification} Define the following re-parameterization
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right]
\end{align*}
Using this notation, the covariance equations from above become
\begin{align*}
  \mbox{Cov}(y,z) - \theta_1 \mbox{Cov}(T,z) &= 0\\
  \mbox{Cov}(y^2,z) - \theta_1 2\mbox{Cov}(yT,z) + \theta_2 \mbox{Cov}(T,z)&= 0\\
  \mbox{Cov}(y^3,z) - \theta_1 3 \mbox{Cov}(y^2T,z) + \theta_2 3\mbox{Cov}(yT,z) - \theta_3\mbox{Cov}(T,z) &= 0
\end{align*}
Note that it is trivial to prove that $\theta_1, \theta_2$ and $\theta_3$ are identified.
To show that $\beta, \alpha_0$ and $\alpha_1$ are identified, write
\begin{align*}
  \theta_2/\theta_1^2 &= 1 + \left( \alpha_0 - \alpha_1 \right)\\
  \theta_3/\theta_1^3 &= (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0 (1 - \alpha_1) 
\end{align*}
which we can always do provided that $\theta_1\neq 0$ which is equivalent to $\beta \neq 0$.
Re-arranging the first equation allows us to solve for $\alpha_0$ as a function of $\alpha_1$.
Substituting this into the second gives us a quadratic in $\alpha_1$ only.
This should be identical to the quadratic we obtained in our original identification proof.

\paragraph{First Moment Equalities}
Expanding,
\begin{align*}
  \mbox{Cov}(y,z) - \theta_1 \mbox{Cov}(T,z) &= 0\\
  \mathbb{E}[yz - \theta_1 Tz] - \mathbb{E}[z]\mathbb{E}[y - \theta_1 T]&= 0
\end{align*}
and
\begin{align*}
  \mathbb{E}[y - \theta_1 T] &= c + \beta \mathbb{E}\left[ T^* - T/(1 - \alpha_0 - \alpha_1) \right]\\
  &= c + \beta\left\{ \frac{\mathbb{E}[T] - \alpha_0}{1 - \alpha_0 - \alpha_1} - \frac{\mathbb{E}[T]}{1 - \alpha_0 - \alpha_1} \right\}\\
&= c - \alpha_0 \theta_1 
\end{align*}
Thus,
\[
  \mathbb{E}\left[ \left\{ y - \left( c - \alpha_0 \theta_1 \right) - \theta_1 T\right\}z \right] = 0
\]
Now define 
\begin{align*}
\kappa_1 &= c - \alpha_0 \theta_1\\
u_1 &= y - \kappa_1 - \theta_1 T
\end{align*}
Using this notation, we obtain the unconditional moment equalities
\[
  \mathbb{E}\left[
  \begin{array}{l}
    u_1(\kappa_1, \theta_1)\\ u_1(\kappa_1, \theta_1) z
  \end{array}
\right] = \mathbf{0}
\]

\paragraph{Second Moment Equalities}

\begin{align*}
  \mbox{Cov}(y^2,z) - \theta_1 2\mbox{Cov}(yT,z) + \theta_2 \mbox{Cov}(T,z)&= 0\\
  \mathbb{E}\left[ y^2z - \theta_1 2yTz + \theta_2 Tz \right] -\mathbb{E}\left[ z \right]\mathbb{E}\left[ y^2 - \theta_1 2yT + \theta_2 T \right] &= 0
\end{align*}

Now, using some lemmas from the notes in our sick-instruments paper,
\begin{align*}
  \mathbb{E}[T\varepsilon] &= \mbox{Cov}(T,\varepsilon) \\
  &= \mbox{Cov}(T^*,\varepsilon) + \mbox{Cov}(w,\varepsilon)\\
  &= \mbox{Cov}(T^*,\varepsilon) - \mbox{Cov}(T^*,\varepsilon)(\alpha_0 + \alpha_1)\\
  &= \mathbb{E}(T^*\varepsilon)(1 - \alpha_0 - \alpha_1)
\end{align*}
hence,
\begin{align*}
  \mathbb{E}[yT] &= c \mathbb{E}[T] + \beta \mathbb{E}[TT^*] + \mathbb{E}\left[T,\varepsilon \right] \\
  &= cp + \beta \mathbb{P}(T=1,T^*=1) + \mathbb{E}[T^*\varepsilon](1 - \alpha_0 - \alpha_1)\\
  &= cp + \beta (1 - \alpha_1)p^* + \mathbb{E}[T^*\varepsilon](1 - \alpha_0 - \alpha_1)
\end{align*}
Combining this with
\[
  \mathbb{E}[y^2] = c^2 + \beta^2 p^* + \sigma_{\varepsilon\varepsilon} + 2 c\beta p^* + 2 \beta \mathbb{E}[T^*\varepsilon]
\]
we find that 
\[
  \mathbb{E}\left[ y^2 - \theta_1 2yT + \theta_2 T \right] = \cdots = c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0(\theta_2 - 2c\theta_1)
\]
\todo[inline]{For the steps, see our whiteboard notes from 2017-04-21 17.06.32}

Now, define
\begin{align*}
  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
  u_2 &= y^2 - \kappa_2 - \theta_1 2 yT + \theta_2 T
\end{align*}
Using this notation, we obtain the unconditional moment equalities
\[
  \mathbb{E}\left[
  \begin{array}{l}
    u_2(\kappa_2, \theta_1, \theta_2)\\ 
    u_2(\kappa_2, \theta_1, \theta_2) z
  \end{array}
\right] = \mathbf{0}
\]


\paragraph{Third Moment Equalities} 
\begin{align*}
  \mbox{Cov}(y^3,z) - \theta_1 3 \mbox{Cov}(y^2T,z) + \theta_2 3\mbox{Cov}(yT,z) - \theta_3\mbox{Cov}(T,z) &= 0\\
  \mathbb{E}\left[ y^3 z - \theta_1 3 y^2 Tz + \theta_2 3 yTz - \theta_3 Tz \right] - \mathbb{E}[z]\mathbb{E}\left[ y^3 - \theta_1 3 y^2T + \theta_2 3 yT - \theta_3 T \right] &= 0
\end{align*}

\[
  \mathbb{E}[y^3] = \cdots = c^3 + \beta p^* \left( 3 c^2 + 3 c\beta + \beta^2 \right) + 3 \beta \mathbb{E}[\varepsilon T^*] (2c + \beta) + 3 c \sigma_{\varepsilon\varepsilon} + 3\beta \mathbb{E}[\varepsilon^2 T^*] + \mathbb{E}[\varepsilon^3]
\]

\todo[inline]{For derivations of the following, see our whiteboard notes from April 21st and 22nd, 2017}
\[
  \mathbb{E}\left[ y^2T \right] = \cdots = c^2 p + \beta(1 - \alpha_1)p^* + \mathbb{E}[T\varepsilon^2] + 2 c \beta(1 - \alpha_1) p^* + 2 c \mathbb{E}[T\varepsilon] + 2 \beta \mathbb{E}[TT^* \varepsilon] 
\]
and 
\begin{align*}
  \mathbb{E}[T\varepsilon^2] &= \cdots = \alpha_0 \sigma_{\varepsilon\varepsilon} + (1 - \alpha_0 - \alpha_1) \mathbb{E}[\varepsilon^2 T^*]\\
  \mathbb{E}[TT^*\varepsilon] &= \cdots = (1 - \alpha_1) \mathbb{E}[T^*\varepsilon] 
\end{align*}

We then need to calculate $\mathbb{E}[y^3 - \theta_1 3 y^2T + \theta_2 3yT - \theta_3 T]$.
Using the preceding expressions, we can show after some algebra that the $\mathbb{E}[T^*\varepsilon]$ and $\mathbb{E}[T^*\varepsilon^2]$ terms drop out, hence
\begin{align*}
\mathbb{E}[y^3 - \theta_1 3 y^2T + \theta_2 3yT - \theta_3 T]&=
\left\{ c^3 + \beta p^* \left( 3c^2 + 3 c\beta + \beta^2 \right) \right\} + 3c \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \theta_1 3 \alpha_0 \sigma_{\varepsilon\varepsilon} - \theta_3 p\\
&- \theta_1 3 \left\{ c^2p + \beta^2(1 - \alpha_1)p^* + 2 c \beta (1 - \alpha_1) p^* \right\} + \theta_2 3 \left\{ cp + \beta (1 - \alpha_1)p^* \right\}
\end{align*}
After \emph{even more algebra} we can show that this expression depends neither on $p$ nor on $p^*$:
\small
\begin{align*}
\mathbb{E}[y^3 - \theta_1 3 y^2T + \theta_2 3yT - \theta_3 T]&=
c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_2 \frac{1 - \alpha_1}{1 + \alpha_0 - \alpha_1} \right]\\
&= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]\\
\end{align*}
\normalsize
Now let 
\begin{align*}
  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]\\
  u_3 &= y^3 - \kappa_3 - \theta_1 3y^2 T + \theta_2 3yT - \theta_3 T
\end{align*}
we obtain the following moment equalities
\[
  \mathbb{E}\left[
  \begin{array}{l}
    u_3(\kappa_3, \theta_1, \theta_2, \theta_3)\\ 
    u_3(\kappa_3, \theta_1, \theta_2, \theta_3) z
  \end{array}
\right] = \mathbf{0}
\]

\paragraph{Putting Everything Together} Define
\begin{align*}
\boldsymbol{\kappa} &= (\kappa_1, \kappa_2, \kappa_3)' \\
\boldsymbol{\theta} &= (\theta_1, \theta_2, \theta_3)'\\
\mathbf{u}(\boldsymbol{\kappa}, \boldsymbol{\theta}) &= 
\left[
\begin{array}{ccc}
  u_1(\kappa_1, \theta_1) & 
  u_2(\kappa_2, \theta_2) &
  u_3(\kappa_3, \theta_3)
\end{array}
\right]'
\end{align*}
where
\begin{align*}
\kappa_1 &= c - \alpha_0 \theta_1\\
  \kappa_2 &= c^2 + \sigma_{\varepsilon\varepsilon} + \alpha_0 (\theta_2 - 2c \theta_1)\\
  \kappa_3 &= c^3 + 3\left( c - \theta_1 \alpha_0 \right) \sigma_{\varepsilon\varepsilon} + \mathbb{E}[\varepsilon^3] - \alpha_0 \theta_3 - 3 c \alpha_0 \left[ \theta_1 \left( c + \beta \right) - 2\theta_1^2 (1 - \alpha_1) \right]
\end{align*}
and
\begin{align*}
u_1(\kappa_1, \theta_1) &= y - \kappa_1 - \theta_1 T\\
  u_2(\kappa_2, \theta_1, \theta_2) &= y^2 - \kappa_2 - \theta_1 2 yT + \theta_2 T\\
  u_3(\kappa_3, \theta_1, \theta_2, \theta_3) &= y^3 - \kappa_3 - \theta_1 3y^2 T + \theta_2 3yT - \theta_3 T
\end{align*}
Then the full system of moment equalities is given by
\[
  \mathbb{E}\left[
  \begin{array}{l}
    \mathbf{u}\left( \boldsymbol{\kappa}, \boldsymbol{\theta} \right)\\
    \mathbf{u}\left( \boldsymbol{\kappa}, \boldsymbol{\theta} \right)z
  \end{array}
\right] = \mathbf{0}
\]
where
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right]
\end{align*}


\paragraph{What about exogenous covariates?}
At one extreme, we could simply carry out the above \emph{conditional} on $\mathbf{x}$.
This would entail treating $\boldsymbol{\kappa}$ and $\boldsymbol{\theta}$ as functions of $\mathbf{x}$ such that $\alpha_0, \alpha_1$ and $\beta$ would be allowed to depend on $\mathbf{x}$.
To implement this, one would need to estimate a number of conditional mean functions: $\mathbb{E}[y|\mathbf{x}]$, $\mathbb{E}[T|\mathbf{x}]$, $\mathbb{E}[yT|\mathbf{x}]$, $\mathbb{E}[y^3|\mathbf{x}]$, and $\mathbb{E}[y^2T|\mathbf{x}]$.
It would appear that this leads to a standard two-step estimation problem although clearly you'd need a lot of data to have any chance!
Another idea would be to impose some restrictions on the way in which $\mathbf{x}$ affects $\alpha_0$ etc, leading to a semi-parametric estimator.
At the other extreme, we could impose the assumption that $\mathbf{x}$ affects $y$ linearly and $\alpha_0, \alpha_1$ do not vary with $\mathbf{x}$:
\[
  y = c + \beta T^* + \mathbf{x}'\boldsymbol{\gamma} + \varepsilon
\]
Under this assumption the result of Frazis \& Loewenstein (2003) holds -- the IV estimator would recover $\kappa_1, \theta_1, \boldsymbol{\gamma}$.
Since $(y - \mathbf{x}'\boldsymbol{\gamma} ) = c + \beta T^* + \varepsilon$, all of the preceding moment conditions should still hold only with $(y - \mathbf{x}'\boldsymbol{\gamma})$ in place of $y$.
To identify $\boldsymbol{\gamma}$ we add an extra moment condition in which $\mathbf{x}$ multiplies the $u_1$ error term, re-defined to subtract $\mathbf{x}'\boldsymbol{\gamma}$.

%\subsection*{Correcting the Second Moment Inequalities}
%In the derivation from above I mistakenly claimed that since $\varepsilon$ is mean zero, it is sufficient to work with the conditional means of $\varepsilon^2$ given $T^*$ and $z$. 
%This is not correct since the \emph{conditional} means of $\varepsilon$ are not zero.
%The bounds from above are not wrong, but they can be improved by looking at the conditional variance of $\varepsilon$ rather than simply $\mathbb{E}[\varepsilon^2]$.
%Steps 2--3 from above are still usable, but we need to change step 1.
%
%Our inequalities come from the fact that
%\[
%  \mbox{Var}(\varepsilon|T^*,z) > 0 \iff \mathbb{E}[\varepsilon^2 |T^*,z] > \left\{\mathbb{E}[\varepsilon|T^*,z]\right\}^2
%\]
%We've already done the calculations for $\varepsilon^2|T^*,z$.
%Now we simply need to do the same for $\varepsilon|T^*,z$, namely
%\begin{align*}
%  \mathbb{E}[\varepsilon|T^*=0,z=k] &= \mathbb{E}\left[ y |T^*=0, z=k \right] - c\\
%  \mathbb{E}[\varepsilon|T^*=1,z=k] &= \mathbb{E}\left[ y |T^*=1, z=k\right] - (c + \beta)
%\end{align*}
%Now we relate $\mathbb{E}[y|T^*,z]$ to $\mathbb{E}[y|T,z]$ using the calculations from above:
%\begin{align*}
%  \mathbb{E}\left[ y|T^*=0, z_k \right] &= \frac{(1 - \alpha_1)}{p_k (1 - p_k - \alpha_1)} \mathbb{E}[y|T=0, z_k] - \frac{\alpha_1 }{(1 - p_k)(1 - p_k - \alpha_1)}\mathbb{E}[y|T=1,z_k]\\
%  \mathbb{E}\left[ y|T^*=1, z_k \right] &= \frac{(1 - \alpha_0)}{( 1 - p_k)(p_k - \alpha_0)}\mathbb{E}[y|T=1,z_k] - \frac{\alpha_0}{p_k (p_k - \alpha_0)} \mathbb{E}[y|T=0,z_k]
%\end{align*}
%After lots of algebra (see our handwritten notes), we obtain:
%\small
%\begin{align*}
%  p_k (1 - p_k)(1 - p_k - \alpha_1) \left\{ (1  - \alpha_1)(1 - p_k)\mathbb{E}\left[ y^2|T=0,z_k \right] - \alpha_1 p_k \mathbb{E}[y^2|T=1,z_k \right\}  \\
%    > \left\{ (1 - \alpha_1)(1 - p_k)\mathbb{E}[y|T=0,z_k] - \alpha_1 p_k \mathbb{E}[y|T=1,z_k] \right\}^2
%\end{align*}
%and
%\begin{align*}
%  p_k (1 - p_k)(p_k - \alpha_0) \left\{ (1  - \alpha_0)p_k\mathbb{E}\left[ y^2|T=1,z_k \right] - \alpha_0 (1-p_k) \mathbb{E}[y^2|T=0,z_k \right\}  \\
%    > \left\{ (1 - \alpha_0)p_k\mathbb{E}[y|T=1,z_k] - \alpha_0 (1-p_k) \mathbb{E}[y|T=0,z_k] \right\}^2
%\end{align*}
%Finally, we need to convert these into expressions involving unconditional moments via
%\begin{align*}
%  p_k &= \mathbb{E}[T|z=k] = \mathbb{E}\left[ T \mathbf{1}(z=k) \right]/\mathbb{P}(z=k)\\
%  \mathbb{E}\left[ y^r |T=0, z=k\right] &= \mathbb{E}\left[ y^r(1 - T) \mathbf{1}(z=k) \right]/\left[ (1 - p_k) \mathbb{P}(z=k)\right]\\
%  \mathbb{E}\left[ y^r |T=1, z=k\right] &= \mathbb{E}\left[ y^r T \mathbf{1}(z=k) \right]/\left[p_k \mathbb{P}(z=k)\right]
%\end{align*}
%These appear to match our expressions from the sick-instruments paper.
%
%
%\subsection*{Estimated Parameters in Andrews \& Soares}
%In our simple example from above applying the Andrews and Soares (2010) generalized moment selection procedure to the case where $\alpha_0 = 0$, we assumed for simplicity that $c$ and $\sigma_{\varepsilon\varepsilon}$ were known.
%Without $\alpha_0 = 0$, the analogous assumption would be that $\boldsymbol{\kappa}$ is known.
%In practice, however, this parameter is estimated from the data. 
%Accounting for this fact requires a change in the procedure for calculating the variance matrix $\Sigma$ of the moment conditions under the null hypothesis. 
%We know explain how this works.

\section{May 5th, 2017 -- Todo for Revised Paper}

\begin{enumerate}
  \item New notation to accomodate covariates etc.\ as in Mahajan and Lewbel
  \item Move Mahajan stuff into appendix; possibly convert back to his notation later -- I have handwritten notes on this.
  \item Identification results: identification from higher moments, lack of identification from conditional means alone
  \item Explain briefly how to do fully non-parametric estimation, specialize to some simple cases, e.g.\ linear model
  \item Explain about inference versus estimation.
  \item Possibly show that Mahajan, Lewbel, etc.\ also suffer from a weak identification problem. This might be helpful for selling the paper.
    Would need to think about what inequalities to use in their case.
    Presumably we could still use the ``weak'' bounds, but there might be others we could exploit.
  \item Testing $\beta = 0$ should be very easy: don't need Andrews \& Soares at all. I think we can either just use the Stock \& Wright GMM-AR or even a plain old-fashioned GMM inference for testing a linear restriction.
  \item Canay et al.\ projection inference for $\beta$? Need to figure out if this actually gives a speed-up in our case.
    If so, could be very useful for simulations.
    Would be nice to show that we have power to reject the probability limit of the IV and reduced form.
    Is it easier to handle the strongly-identified parameters using their method?
  \item Could also talk briefly about the continuum of moment conditions idea and how one obtains identification from quantiles. There's also a question of using more inequalities by imposing the independence assumption for the measurement error.
    Could also proceed without our higher moment assumptions and just get inference for the identified set.
  \item Is there any way to handle treatment effect heterogeneity? Should be ok if it's modeled heterogeneity but maybe there are some other special cases we can handle? What about that quantile treatment effect idea?
  \item Simulations: want to show that we can actually learn something useful. Things we want to show are that we have more power than just the RF for testing $\beta=0$, that we have power to reject the RF and IV plims in certain cases.
  \item Need to show at least in simulations that our confidence regions aren't insane. Would be good if we could formally argue that they must be convex and connected, for example.
    Maybe the key is to show that our inequalities are always well-behaved and that adding the equalities cann't make the problem suddenly become badly behaved.
  \item Andrews \& Soares with estimates of the strongly identified parameters to get a joint region for $\alpha_0, \alpha_1$, and $\beta$. But this is slow, so we probably can't do simulations although it would be interesting in the empirical examples.
  \item Try to get the projection inference for $\beta$ working 
  \item Empirical example or examples: Oreopolous? Maybe the Heckman JPTA dataset? Petra suggested emailing someone about this but I forget who it was\ldots

\end{enumerate}

\section{Lewbel (2007)}

\paragraph{Notation}
Observe $Y,Z$ and $T$ where $T$ is a proxy for $T^*$ and
\begin{align*}
  h^*(X,T^*) &= E(Y|X,T^*)\\
  T^* &= \mbox{unobserved binary regressor}\\
  X &= \mbox{vector of covariates}\\
  Y &= \mbox{outcome}
\end{align*}
Since $T^*$ is binary, without loss of generality,
\begin{align*}
  h^*(X,T^*) &= h_0^*(X) + \tau^*(x)T^*\\
  h^*_0(X) &= h^*(X,0)\\
  \tau^*(x) &= h^*(x,1) - h^*(x,0)
\end{align*}
Goal is to estimate $\tau^*(x)$.
Below we will partition $X$ according to $X = (V,Z)$ where $V$ is an ``instrument-like variable'' and $Z$ is the set of remaining covariates.

\paragraph{Assumption A1} 
There exists $E(Y|X,T^*,T) = E(Y|X,T^*)$.
Equivalently, $Y$ is mean-independent of $T-T^*$, conditional on $X,T^*$ -- mis-classification does not affect true expected outcome.
This rules out placebo effects, etc.

\paragraph{More Notation}
\begin{align*}
  r^*(x) &= E(T^*|X=x) = P(T^*=1|X=x)\\
  b_0(x) &= P(T=1|T^*=0,X=x)\\
  b_1(x) &= P(T=0|T^*=1,X=x)\\
  r(x) &= E(T|X=x)\\
  \tau(x) &= h(x,1) - h(x,0)\\
  h(x,t) &= E(Y|X=x,T=t)
\end{align*}

\paragraph{Assumption A2} There exist $b_0(x) + b_1(x) <1$ and $0 < r^*(x) <1$ for all $x$ in the support of $X$.
\paragraph{Theorem 1} Under Assumption A1 there exists a function $m(x)$ such that $|m(x)|\leq 1$ and $\tau(x) = \tau^*(x)m(x)$.
If we add Assumption A2 then $m(x)>0$ as well.

\begin{itemize}
  \item Analogous to attenuation bias under classical measurement error: A1 implies that $\tau(x)$ is a lower bound for the magnitude of $\tau^*(x)$.
    Adding A2, sign of $\tau(x)$ agrees with that of $\tau^*(x)$.
  \item Assumptions A1 and A2 imply that $\tau^*(x)=0$ iff $\tau(x)=0$. Thus, if we only want to test $\tau^*(x) = 0$ we can simply ignore mis-classification.
  \item As shown below:
    \begin{align*}
      m(x) &= P(T^*=1|T=1,X=x) - P(T^*=1|T=0,X=x) \implies |m(x)|< 1\\
      m(x) &= M[b_0(x),b_1(x),r(x)] = \frac{1}{1 - b_1(x) - b_0(x)}\left\{ 1 -  \frac{\left[ 1 - b_1(x) \right]b_0(x)}{r(x)} - \frac{\left[ 1 - b_0(x) \right]b_1(x)}{1 - r(x)} \right\} \\
      m(x) &= \frac{[1 - r^*(x)]r^*(x)}{\left[ 1 - r(x) \right]r(x)}
      \left[ 1 - b_0(x) - b_1(x) \right] \implies m(x) >0 \mbox{ if } 1 - b_0(x) + b_1(x) > 0 
    \end{align*}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proof}[Proof of Theorem 1]
Let $p_t(X) = P(T^*=1|X,T=t)$. 
By A1 we have 
\[
  E(Y|X,T^*,T) = E(Y|X,T^*) = h^*_0(X) + \tau^*(X) T^*
\]
combining this with iterated expectations, 
\begin{align*}
  E(Y|X,T=t) &= E_{T^*|X,T=t}\left[ E(Y|X,T=t,T^*) \right] = E_{T^*|X,T=t}\left[ h^*_0(X) + \tau^*(X) T^* \right]\\
  &= p_t(X) \left[ h^*_0(X) + \tau^*(X) \right] + \left[ 1 - p_t(X) \right] h^*_0(X)\\
  &= h^*_0(X) + p_t(X) \tau^*(X)
\end{align*}
Taking the difference of the preceding expression evaluated at $T=1$ and $T=0$
\[
  E(Y|X,T=1) - E(Y|X,T=0) = \left[p_1(X) - p_0(X)\right] \tau^*(X)
\]
whereas we defined 
\[
  \tau(X) = h(X,1) - h(X,0) = E(Y|X,T=1) - E(Y|X,T=0)
\]
Combining these two equations, we see that
\[
  \tau(X) = \left[ p_1(X) - p_0(X) \right] \tau^*(X) 
\]
so that the function $m(x)$ defined in Theorem 1 equals $p_1(x) - p_0(x)$.
Since $m$ is a difference of probabilities, it follows immediately that $-1 \leq m(x) \leq 1$.
Now, by Bayes' Rule, 
\begin{align*}
  p_0(x) &= P(T^*=1|X=x,T=0) = \frac{P(T=0|X=x,T^*=1)P(T^*=1|X=x)}{P(T=0|X=x)} = \frac{b_1(x) r^*(x)}{1-r(x)} \\
  p_1(x) &= P(T^*=1|X=x,T=1) = \frac{P(T=1|X=x,T^*=1)P(T^*=1|X=x)}{P(T=1|X=x)} = \frac{[1-b_1(x)] r^*(x)}{r(x)} 
\end{align*}
and by iterated expectations,
\begin{align*}
  r(x) &= E(T|X=x) = E(T|X=x,T^*=1)P(T^*=1|X=x) + E(T|X=x,T^*=0)P(T^*=0|X=x)\\ 
    &= [1 - b_1(x)]r^*(x) + b_0(x) [1 - r^*(x)] = b_0(x) + r^*(x)[1 - b_0(x) - b_1(x)] 
\end{align*}
Using this expression, if $b_0(x) + b_1(x) = 1$ then $r(x) = b_0$.
If instead $b_0(x) + b_1(x) \neq 1$, then we can divide through by $1 - b_0(x) - b_1(x)$ to solve for $r^*(x)$ and $1 - r^*(x)$ as follows:
\begin{align*}
  r^*(x) &= \frac{r(x) - b_0(x)}{1 - b_0(x) - b_1(x)}\\
  1 - r^*(x) &= \frac{[1 - b_0(x) - b_1(x)] - [r(x) - b_0(x)]}{1 - b_0(x) - b_1(x)} = \frac{1 - b_1(x) - r(x)}{1 - b_0(x) - b_1(x)}
\end{align*}
Using the expressions we have just derived for $p_0, p_1, r^*$ and $1 - r^*$ and suppressing the dependence on $x$ for simplicity, it follows that
\begin{align*}
  m &= p_1 - p_0 = \frac{(1 - b_1)r^*}{r} - \frac{b_1r^*}{1- r} = \frac{(1 - r)(1 - b_1) r^* - r b_1 r^*}{r(1 - r)}\\
  &= \frac{r^*( 1 - r - b_1 + r b_1 - r b_1 )}{r (1 - r)} = \frac{r^*(1 - r - b_1)}{r(1-r)}
\end{align*}
Rearranging,
\[
  (1-r)rm = r^*(1 - r - b_1)
\]
and combining this with $1 - r^* = (1 - b_1 - r) / (1 - b_0 - b_1)$, we have
\[
  (1-r)rm = (1 - r^*)r^*(1 - b_0 - b_1)
\]
Both $r$ and $r^*$ are strictly between zero and one.
Thus, if $b_0 + b_1 = 1$ we have $m = 0$.
If instead $b_0 + b_1 <1$ we have $m>0$ and if $b_0 + b_1 > 1$ then $m <0$.
Finally,
\[
  m = \frac{r^*(1 - r - b_1)}{r(1-r)} = \frac{r - b_0}{1 - b_0 - b_1}\left[ \frac{1 - r - b_1}{r(1 - r)} \right] 
\]
and since 
\begin{align*}
 (r - b_0)(1 - r - b_1) &= r(1-r) - b_0(1-r) - b_1r + b_1 b_0  \\
  &= r(1-r) - b_0(1-r) - b_1r + b_1 b_0  + (b_1 b_0 r - b_1 b_0 r)\\
  &= r(1-r) - b_0(1-r) - b_1r + b_1 b_0(1 - r)  + b_1 b_0 r \\
  &= r(1-r) - b_0(1-r) - b_1r(1 - b_0) + b_1 b_0(1 - r) \\
  &= r(1-r) + b_0(1-r)(b_1 - 1) - b_1r(1 - b_0) \\
  &= r(1-r) - b_0(1-r)(1 - b_1) - b_1r(1 - b_0) 
\end{align*}
we find that
\begin{align*}
  m &= M(b_0, b_1, r) = \frac{1}{1 - b_0 - b_1}\left[ \frac{r(1-r) - (1 - b_1)b_0(1-r) - (1-b_0)b_1r}{r(1-r)} \right]\\
  &= \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r} - \frac{(1-b_0)b_1}{1-r} \right].
\end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Assumption A3} Assume that $r(x)$ and $\tau(x)$ are identified.
Note that this only requires that we can consistently estimate conditional expectations of observable quantities.

\paragraph{Assumption A4} Suppose that we have partitioned $X$ into two subvectors: $X = (V,Z)$. 
We assume that for each $z$ in the support of $Z$ there exists a subset $\Omega_z$ of the support of $V$ such that:
\begin{enumerate}[(i)]
  \item for all $v, v'\in \Omega_z$, $b_0(v,z)=b_0(v',z)$, $b_1(v,z)=b_1(v',z)$, and $\tau^*(v,z) = \tau^*(v',z)$
  \item for all $v,v' \in \Omega_z$ such that $v\neq v'$, $r^*(v,z) \neq r^*(v',z)$.
\end{enumerate}
The basic idea here is that $V$ affects the probability of being treated $r^*$ but not the treatment effect $\tau^*$ after we have conditioned on $Z$.
As sufficient condition for the restriction on $\tau^*$ to hold is $E(Y|Z=z,V=v,T^*=t) = s_1(z,t) + s_2(z,v)$ for some functions $s_1$ and $s_2$.

\paragraph{Some More Notation} Let $b_0(z), b_1(z), \tau^*(z)$ denote $b_0(v,z), b_1(v,z), \tau^*(v_z)$ for $v \in \Omega_z$ since, under A4, these do not vary with $v$.

\paragraph{Assumption A5} Each set $\Omega_z$ from A4 contains three elements $v_k$, $k=0,1,2$ such that
\[
  \left[ \frac{\tau(v_0,z)}{r(v_1,z)} - \frac{\tau(v_1,z)}{r(v_0,z)} \right]\left[ \frac{\tau(v_0,z)}{1 - r(v_2,z)} - \frac{\tau(v_2,z)}{1 - r(v_0,z)} \right] \neq 
  \left[ \frac{\tau(v_0,z)}{r(v_2,z)} - \frac{\tau(v_2,z)}{r(v_0,z)} \right]\left[ \frac{\tau(v_0,z)}{1 - r(v_1,z)} - \frac{\tau(v_1,z)}{1 - r(v_0,z)} \right] 
\]
\begin{itemize}
  \item The key requirement is that $V$ takes on at least three values.
  \item Assumption A5 depends only on observables, so we can test it.
  \item An equivalent way to state A5 is: $\tau^*(z)\neq 0$, $b_0(z)+b_1(z) \neq 1$ and an inequality involving $r$ and $r^*$ which is explained below.
  \item The triplets $(v_0, v_1, v_2)$ are allowed to depend on $z$.
\end{itemize}

\paragraph{Theorem 2} Under A1--A5, the mis-classification probabilities $b_0(x),b_1(x)$ are identified as is the probability of treatment $r^*(x)$ and treatment effect $\tau^*(x)$.
If we replace $b_0(x)+b_1(x)<1$ from A2 with $b_0(x)+b_1(x) \neq 1$, then $\tau^*(x)$ is identified up to sign. 
\begin{itemize}
  \item For simplicity, suppress dependence on $z$. By Theorem 1 and A4,
    \[
      \tau(v_k) M\left[ b_0, b_1, r(v_0) \right] = \tau(v_0)M\left[ b_0, b_1, r(v_k) \right]
    \]
  \item Again suppressing dependence on $z$, recall from above that \[M[b_0, b_1, r(v_k)] = \frac{1}{1 - b_1 - b_0}\left\{ 1 - \frac{(1 - b_1)b_0}{r(v_k)} - \frac{(1 - b_0)b_1}{1 - r(v_k)} \right\}\]
  \item Evaluating the two preceding expressions at $k=1$ and $k=2$ gives two equations relating the identified functions $r$ and $\tau$ to the unknown mis-classification probabilities $b_0$ and $b_1$.
  \item Since each equation involves $v_0$, we need $V$ to take on at least three values to get as many equations as unknowns (two).
  \item The proof of Theorem 2 shows that the two equations admit a unique solution, so that $b_0$ and $b_1$ are identified.
\item Given knowledge of $b_0$ and $b_1$, Theorem 1 allows us to solve for $r^*$ and $\tau^*$.
\item If $V$ only took on two values, we could not identify $b_0$ and $b_1$ without further restrictions. However if either $b_0$ or $b_1$ were known, e.g.\ known to be zero as in a one-sided mis-classification setting, then we could identify the model using only a binary $V$.
\end{itemize}

\begin{proof}[Proof of Theorem 2]
For a fixed value of the covariates $z$, Assumption A4 ensures that there is a subset  $\Omega_z$ of the support of $V$ -- a subset that may depend on $z$ -- such that for all $v, v' \in \Omega_z$ we have $b_0(v,z) = b_0(v',z)$, $b_1(v,z) = b_1(v',z)$ and $\tau^*(v,z) = \tau^*(v',z)$.
Assumption A5 ensures that $\Omega_z$ contains at least three values: $v_0, v_1$ and $v_2$.
Suppress dependence on the covariates $z$: let $r_k = r(v_k)$ and $\tau_k = \tau(v_k)$ for $k=0, 1, 2$.
Since $b_0$ and $b_1$ do not depend on $v$ for $v \in \Omega_z$, there is no subscript on these quantities.
By Theorem 1, $\tau(v,z) = \tau^*(v,z)m(v,z)$.
Again suppressing dependence on $z$ and using the expression for $m$ derived in the proof of Theorem 1,
  $\tau_k = M(b_0, b_1, r_k) \tau^*_k$
for all $k = 0, 1, 2$.
But by Assumption A4, $\tau^*_0 = \tau^*_1 = \tau^*_2$, which yields
\[
  \frac{\tau_k}{M(b_0, b_1, r_k)} = \frac{\tau_\ell}{M(b_0, b_1, r_\ell)}
\]
for any $k,\ell$.
Rearranging, 
\[
  M(b_0, b_1, r_k)\tau_\ell = M(b_0, b_1, r_\ell)\tau_k
\]
For $k \neq \ell$ this yields a nontrivial equation relating $b_0$ and $b_1$ to observables: $\tau_k, r_k$ and $\tau_\ell, r_\ell$.
In particular, take $\ell = 0$ and $k = 1, 2$.
We obtain,
\begin{align*}
  0 &= M(b_0, b_1, r_k)\tau_0 - M(b_0, b_1, r_0)\tau_k \\
  0 &= \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r_k} - \frac{(1-b_0)b_1}{1-r_k} \right]\tau_0 - \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r_0} - \frac{(1-b_0)b_1}{1-r_0} \right]\tau_k\\
  0 &= \left[ 1 - \frac{(1 - b_1)b_0}{r_k} - \frac{(1 - b_0)b_1}{1-r_k} \right]\tau_0 - \left[ 1 - \frac{(1 - b_1)b_0}{r_0} - \frac{(1 - b_0)b_1}{1-r_0} \right]\tau_k\\
  0 &= (1 - b_1)b_0 \left( \frac{\tau_k}{r_0} - \frac{\tau_0}{r_k} \right) + (1 - b_0)b_1 \left( \frac{\tau_k}{1 - r_0} - \frac{\tau_0}{1 - r_k} \right) + (\tau_0 - \tau_k)\\
  0 &= (1 - b_1)b_0 \left( \frac{\tau_0}{r_k} - \frac{\tau_k}{r_0} \right) + (1 - b_0)b_1 \left( \frac{\tau_0}{1 - r_k} - \frac{\tau_k}{1 - r_0} \right) + (\tau_k - \tau_0)
\end{align*}
This is a \emph{linear} equation of the form
\[
  0 = B_0 w_{0k} + B_1 w_{1k} + w_{2k}
\]
where the unknowns $B_0, B_1$ are defined as
\begin{align*}
  B_0 &= b_0 (1- b_1)\\
  B_1 &= b_1(1- b_0)
\end{align*}
and the observable constants $w_{0k}, w_{1k}, w_{2k}$ are
\begin{align*}
  w_{0k} &= \frac{\tau_0}{r_k} - \frac{\tau_k}{r_0}\\
  w_{1k} &= \frac{\tau_0}{1 - r_k} - \frac{\tau_k}{1 - r_0}\\
  w_{2k} &= \tau_k - \tau_0
\end{align*}
Since we have an equation for $k=1$ and $k=2$, we have a linear system of $k$ equations in two unknowns.
In matrix form, 
\[
\left[\begin{array}{cc}
w_{01} & w_{11}\\
w_{02} & w_{12}
\end{array}\right]\left[\begin{array}{c}
B_{0}\\
B_{1}
\end{array}\right]=\left[\begin{array}{c}
-w_{21}\\
-w_{22}
\end{array}\right]
\]
In this notation, Assumption A5 is simply $w_{01}w_{12}-w_{02}w_{11}\neq0$ which ensures the system has a unique solution.

Now, let $s = 1 - \alpha_0 - \alpha_1$.
Since $B_{0}=(1-b_{1})b_{0}$ and $B_{1}=(1-b_{0})b_{1}$, we have
\[
  (s + b_0)b_0 = (1 - b_0 - b_1 + b_0) b_0 =  (1 - b_1) b_0 = B_0\\
\]
and 
\begin{align*}
  B_0 - B_1 + 1 - s &= (1 - b_1)b_0 - (1 - b_0)b_1 + 1 - (1 - b_0 - b_1)\\
  &= b_0 - b_1 b_0 - b_1 + b_1 b_0 + b_0 + b_1 = 2 b_0
\end{align*}
Substituting $2b_0 = B_0 - B_1 + 1 - s$ into $B_0 = (s + b_0)b_0$, we obtain 
\begin{align*}
  4 B_0 &= (2s + 2 b_0) 2 b_0 = (2s + B_0 - B_1 + 1 - s)(B_0 - B_1 + 1 - s)\\
  &= (s + B_0 - B_1 + 1)(B_0 - B_1 + 1 - s)\\
  &= [(B_0 - B_1 + 1) + s][(B_0 - B_1 + 1) - s]\\
  &= (B_0 - B_1 + 1)^2 - s^2
\end{align*}
and rearranging to solve for $s$,
\[
  s = \pm \sqrt{(B_0 - B_1 + 1)^2 - 4 B_0}
\]
This identifies $s$ up to sign so long as $s\neq0$, and since
\begin{align*}
  M(b_0, b_1, r_k) &= \frac{1}{1 - b_0 - b_1}\left[ 1 - \frac{(1 - b_1)b_0}{r_k} - \frac{(1-b_0)b_1}{1-r_k} \right]  
= \frac{1}{s}\left[ 1 - \frac{B_0}{r_k} - \frac{B_1}{1 - r_k} \right]
\end{align*}
and $\tau_k = M(b_0,b_1,r_k)\tau^*$, it follows that $\tau^*$ is identified up to sign.
If $s > 0$, then $s$ is identified as are $b_0$ and $b_1$.
To solve for the misclassification probabilities, first use the fact that $B_0 - B_1 + 1 - s = 2 b_0$ as shown above to yield,
\[
  b_0 = (B_0 - B_1 + 1 - s) / 2
\]
and similarly, use the fact that
\begin{align*}
  -(B_0 - B_1 - 1 + s) &= -\left[ (1 - b_1)b_0 - (1 - b_0)b_1  - b_0 - b_1 \right] \\
  &= -(b_0 - b_1 b_0 - b_1 + b_1 b_0 - b_0 - b_1)\\ 
  &= 2 b_1
\end{align*}
we obtain
\[
  b_1 = -(B_0 - B_1 - 1 + s) / 2.
\]

\todo[inline]{Still need to verify the following:}
Finally, let 
\[
R_k = \displaystyle \frac{(1 - r_k^*)r_k^*}{(1 - r_k)r_k}
\]
Using the fact that $\tau_k = M(b_0, b_1, r_k) \tau^*$ along with 
\[
  m = \frac{(1 - b_1)r^*}{r} - \frac{b_1 r^*}{1-r}
\] 
the determinant condition can be expressed as  
\[
  \left[ \left( \frac{R_0}{r_1} - \frac{R_1}{r_0} \right)\left( \frac{R_0}{1 - r_2} - \frac{R_2}{1 - r_0} \right) - \left( \frac{R_0}{r_2} - \frac{R_2}{r_0} \right)\left( \frac{R_0}{1 - r_1} - \frac{R_1}{1 - r_0} \right) \right] \times (1 - b_0 - b_1) \tau^* \neq 0
\]

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notes on Mahajan (2006)}


\subsection{The Case of Exogenous $x^*$}
\paragraph{Model and Basic Notation:}
Consider a model of the form
\begin{equation}
  \mathbb{E}\left[ y - g(\widetilde{z})\left. \right| \widetilde{z} \right] = 0
  \label{eq:mahajanI}
\end{equation}
where $\widetilde{z} = (x^*, z)$, $x^*$ is an unobserved binary random variable, $z$ is  a $d_z\times 1$ vector of observed random variables (covariates), and the conditional expectation function $g(\widetilde{z}) = \mathbb{E}(y|\widetilde{z})$ is unknown.\footnote{The fact that $g$ is the conditional expectation of $y$ given $\widetilde{z}$ follows from the linearity of expectation and the fact that $g$ is a function of $\widetilde{z}$ only, since $0 = \mathbb{E}[y - g(\widetilde{z})|\widetilde{z}] = \mathbb{E}(y|\widetilde{z}) - \mathbb{E}[g(\widetilde{z})|\widetilde{z}] = \mathbb{E}(y|\widetilde{z}) - g(\widetilde{z})$.}
In addition to $z$ we also observe $x$, a binary, mis-classified surrogate for $x^*$ along with a ``special'' random variable $v$ -- in essence an instrumental variable.

\paragraph{Basic Model Assumption}
The basic assumption of the model is
\begin{equation}
  \mathbb{E}[y - g(\widetilde{z}) | \widetilde{z}, x, v] = \mathbb{E}[y - g(x^*,z) |x^*, x, v,z] = 0.
  \label{eq:mahajan1}
\end{equation}
Now, since
\[
  \mathbb{E}[y - g(\widetilde{z}) | \widetilde{z}, x, v] =
  \mathbb{E}[y| \widetilde{z}, x, v] - g(\widetilde{z}) = \mathbb{E}[y|\widetilde{z},z,v] - E[y|\widetilde{z}] = 0
\]
we see that this assumption is equivalent to 
\[
  \mathbb{E}[y|x^*,x,v,z] = \mathbb{E}[y|x^*,z] = g(x^*,z)
\]
which entails both non-differential measurement error and an exclusion restriction for $v$.

\paragraph{Relationship to an Additively Separable Model}
Our object of interest here is the conditional mean function $g(\widetilde{z})$.
Defining $\varepsilon = y - g(\widetilde{z})$, by construction we have $y = g(\widetilde{z}) + \varepsilon$ and $\mathbb{E}(\varepsilon|x^*,z)= 0$ which gives us an additively separable model.
Note that $g$ \emph{need not} be a structural function: unless $\widetilde{z}$ is exogenous, we cannot give it a causal interpretation.
But if we are only interested in learning the conditional mean function $g$, we do not have to worry about endogeneity since $x^*$ is by construction exogenous for the error term $\varepsilon$.
Note that by iterated expectations and Equation \ref{eq:mahajan1},
\begin{align*}
  \mathbb{E}[y|x^*,x,z]  &= 
  \mathbb{E}_{v|\widetilde{z},x}\left[\mathbb{E}\left( y|\widetilde{z},x,v \right)  \right]
  = \mathbb{E}_{v|\widetilde{z},x}\left[ g(\widetilde{z}) \right] = g(x^*,z)\\
  \mathbb{E}[y|x^*,v,z]  &= 
  \mathbb{E}_{x|\widetilde{z},v}\left[\mathbb{E}\left( y|\widetilde{z},x,v \right)  \right]
  = \mathbb{E}_{x|\widetilde{z},v}\left[ g(\widetilde{z}) \right] = g(x^*,z)
\end{align*}
or expressed in terms of $\varepsilon$,
\[
  \mathbb{E}[\varepsilon|x^*,x,z,v] = \mathbb{E}[\varepsilon|x^*,x,z] = \mathbb{E}[\varepsilon|x^*,v,z] = 0.
\]

\paragraph{A Note Concerning Covariates}
Throughout the following, the covariates $z$ are held fixed at $z = z_a$ so we will sometimes suppress the dependence of various functions on $z$.

\paragraph{Notation: Misclassification Rates}
(Note that these correspond to $\alpha_0$ and $\alpha_1$ in our notation.)
\begin{align*}
  \eta_0(z_a) = \mathbb{P}(x=1|x^*=0,z_a)\\
  \eta_1(z_a) = \mathbb{P}(x=0|x^*=1,z_a)\\
\end{align*}

\paragraph{Notation: Observed and Unobserved First-stage} 
(Note that these correspond to $p_k$ and $p_k^*$ in our notation.)
\begin{align*}
  \eta_2(z_a, v) &= \mathbb{P}(x=1|z_a, v) \\
  \eta_2^*(z_a,v) &= \mathbb{P}(x^*=1|z_a,v)
\end{align*}

In addition to Equations \ref{eq:mahajanI} and \ref{eq:mahajan1}, we maintain the following:
\begin{assump}[Identification of $g$ if $x^*$ were observed]\mbox{}
     \\ Knowledge of the distribution of $(y,x^*,z)$ suffices to identify $g(x^*,z_a)$.
  \label{assump:A1}
\end{assump}
\begin{assump}[Positive correlation between $x^*$ and $x$]\mbox{}
      \\ The sum of the mis-classification error rates is less than one: $\eta_0(z_a) + \eta_1(z_a) < 1$.
  \label{assump:A2}
\end{assump}
Assumption \ref{assump:A2} requires that the mis-classification is not so severe that $x$ is uncorrelated with or negatively correlated with $x^*$.

\begin{assump}
      The surrogate $x$ is conditionally independent of $v$ given $x^*$ and $z_a$.
  \label{assump:A3}
\end{assump}

In essence, Assumption \ref{assump:A3} requires that $v$ is excluded for the mis-classification error process.
In particular, it implies that
\begin{align*}
  \mathbb{P}(x=1|x^*=0,v, z_a) &= \mathbb{P}(x=1|x^*=0,z_a) = \eta_0(z_a)\\
  \mathbb{P}(x=0|x^*=1,v, z_a) &= \mathbb{P}(x=0|x^*=1,z_a) = \eta_1(z_a)
\end{align*}

\begin{assump}[First-stage]\mbox{}
  For some $v_1 \neq v_2$, $\eta^*_2(z_a,v_1) \neq \eta_2(z_a,v_2)$.
  \label{assump:A4}
\end{assump}

Assumption \ref{assump:A4} is just the standard IV relevance condition, conditional on $z = z_a$.


\begin{assump}[Relevance of $x^*$] The function $g$ depends on $x^*$:  $g(1,z_a) \neq g(0,z_a)$. 
  \label{assump:A5}
\end{assump}
The mis-classification error rates $\eta_0$ and $\eta_1$ are not identified if Assumption \ref{assump:A5} fails. 

\begin{thm}
  Under Assumptions \ref{assump:A1}--\ref{assump:A5} $g(x^*,z_a)$,  $\eta_0(z_a)$, and $\eta_1(z_a)$ are identified.
  \label{thm:mahajan}
\end{thm}
  
\paragraph{Steps in the Proof}
\begin{enumerate}
  \item Show that if $\eta_0(z_a)$ and $\eta_1(z_a)$ are identified, so is $g(x^*,z_a)$.
  \item Show that $\eta_0(z_a)$ and $\eta_1(z_a)$ are identified.
\end{enumerate}
In the proof we will suppose that $v$ is binary and takes on values $v_1\neq v_2$.
We will also suppress dependence on $(z,v)$ unless needed and employ the convention that $w = (x, y, xy)$.

\paragraph{Some Additional Notation}
  \begin{align*}
    r(z_a,v) \equiv \mathbb{E}(xy|z_a,v)\\
    \eta_2(z_a,v) \equiv \mathbb{E}(x|z_a,v)\\
    t(z_a,v) \equiv \mathbb{E}(y|z_a,v)
  \end{align*}
Note that we have already defined $\eta_2$ above. 
The definition given here is equivalent since $x$ is binary and hence $\mathbb{P}(x=1|\cdots) = \mathbb{E}(x|\cdots)$.

\begin{lem}
  $[1 - \eta_0(z_a) - \eta_1(z_a)]\, \eta^*_2(z_a,v) = \eta_2(z_a,v) - \eta_0(z_a)$.
  \label{lem:star_nonstar}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:star_nonstar}]
  By the Law of Total Probability and the fact that $\mathbb{P}(x=1|x^*,z_a, v)$ does not depend on $v$ by Assumption \ref{assump:A3}, we have
  \begin{align*}
    \eta_2(z_a,v) &= \mathbb{P}(x=1|x^*=0,z_a,v)\left[ 1 - \eta_2^*(z_a,v)]\right] + \mathbb{P}(x=1|x^*=1,z_a,v)\eta^*_2(z_a,v)\\
    &= \mathbb{P}(x=1|x^*=0,z_a)\left[ 1 - \eta_2^*(z_a,v)]\right] + \mathbb{P}(x=1|x^*=1,z_a)\eta^*_2(z_a,v)\\
    &= \eta_0(z_a)\left[ 1 - \eta_2^*(z_a,v)]\right] + \left[1 - \eta_1(z_a)\right]\eta^*_2(z_a,v)\\
    &= \eta_0(z_a) + \left[ 1 - \eta_0(z_a) - \eta_1(z_a) \right] \eta_2^*(z_a,v)
  \end{align*}
  The result follows by re-arranging.
\end{proof}

\paragraph{Note:} In our notation, the preceding corresponds to $(1 - \alpha_0 - \alpha_1) p_k^* = p_k - \alpha_0$.

\begin{lem}
  $t(z_a,v) = g(0, z_a)\left[ 1 - \eta_2^*(z_a,v) \right] + g(1,z_a)\eta_2^*(z_a,v)$.
  \label{lem:t}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:t}]
\end{proof}

\begin{lem}
  $r(z_a,v) = g(0,z_a)\eta_0(z_a) \left[ 1 - \eta_2^*(z_a,v) \right] + g(1, z_a) \left[ 1 - \eta_1(z_a) \right]\eta_2^*(z_a,v)$
  \label{lem:r} 
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:r}]
\end{proof}

\begin{lem}
  \[r(z_a,v_1) - r(z_a,v_2) = \left[ \eta_2^*(z_a,v_1) - \eta_2^*(z_a,v_2)\left\{ \left[ 1 - \eta_1(z_a) \right]g(1,z_a) - \eta_0(z_a)g(0,z_a) \right\} \right]\]
  \label{lem:r_diff}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:r_diff}]
\end{proof}

\begin{lem}[IV Estimator]
  \[\bar{t}(z_a) = k(a)\left[ g(1,z_a) - g(0,z_a) \right]\]
  where 
\[
\bar{t}(z_a) = \frac{t(z_a,v_1) - t(z_a,v_2)}{\eta_2(z_a,v_1) - \eta_2(z_a,v_2)}, \quad k(z_a) = \frac{1}{1 - \eta_0(z_a) - \eta_1(z_a)}
\]
  \label{lem:t_bar}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:t_bar}]
\end{proof}

\begin{lem}[``Modified'' IV Estimator]
  \[\bar{r}(z_a) = k(a)\left\{\left[1 - \eta_1(z_a)\right] g(1,z_a) - \eta_0(z_a) g(0,z_a) \right]\]
    where 
\[
\bar{r}(z_a) = \frac{r(z_a,v_1) - r(z_a,v_2)}{\eta_2(z_a,v_1) - \eta_2(z_a,v_2)}, \quad k(z_a) = \frac{1}{1 - \eta_0(z_a) - \eta_1(z_a)}
\]
  \label{lem:r_bar}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:r_bar}]
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:mahajan} -- Step 1]
  In this step, we assume that $\eta_0(z_a)$ and $\eta_1(z_a)$ are both identified.
  We then show that Assumptions \ref{assump:A1}--\ref{assump:A5} suffice to identify $g(x^*,z)$ given knowledge of $r(z_a,v), \eta_2(z_a,v)$, and $t(z_a,v)$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:mahajan} -- Step 2]
  In this step, we show that $\eta_0(z_a)$ and $\eta_1(z_a)$ are both identified under Assumptions \ref{assump:A1}--\ref{assump:A5}.
\end{proof}

\subsection{The Case of Endogenous $x^*$}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notation and Results for New Draft -- May 8, 2017}


Additively separable model
\[
  y = m(T^*,\mathbf{x})+\varepsilon
\]
where $\varepsilon$ is a mean-zero error term, $T^*$ is an endogenous binary regressor of interest and $\mathbf{x}$ is a vector of exogenous controls.
Since $T^*$ is binary, we can re-write this as linear in $T^*$ conditional on $\mathbf{x}$
\begin{align*}
  y &= c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon\\
  \beta(\mathbf{x}) &= m(1,\mathbf{x}) - m(0,\mathbf{x})\\
  c(\mathbf{x}) &= m(0,\mathbf{x})
\end{align*}
Goal is to use an instrumental variable $z$ to identify $\beta(\mathbf{x})$ when we observe not $T^*$ but a mis-measured binary surrogate $T$. 
Define
\begin{align*}
  \alpha_0(\mathbf{x},z) &= \mathbb{P}\left(T=1|T^*=0,\mathbf{x},z  \right)\\
  \alpha_1(\mathbf{x},z) &= \mathbb{P}\left(T=0|T^*=1,\mathbf{x},z  \right)
\end{align*}
Identification will only rely on two values for $z$.

\begin{assump} \mbox{}
  \begin{enumerate}[(i)] 
    \item $y = c(\mathbf{x}) + \beta(\mathbf{x})T^* + \varepsilon$ where $\mathbb{E}[\varepsilon]=0$.
    \item $\alpha_0(\mathbf{x},z) = \alpha_0(\mathbf{x})$,   $\alpha_1(\mathbf{x},z) = \alpha_1(\mathbf{x})$
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon|\mathbf{x},z, T^*]$
    \item $\mathbb{E}\left[ T^*|\mathbf{x},z=k \right] \neq \mathbb{E}\left[ T^*|\mathbf{x},z=\ell \right]$ two distinct values $k, \ell$ in the support of $z$.
    \item $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) \neq 1$ ($T$ is relevant for $T^*$)
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z] = 0$
  \end{enumerate}
\end{assump}

\begin{assump} \mbox{}
  \label{assump:2ndMoment}
  \begin{enumerate}[(i)] 
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^2|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z] = \mathbb{E}[\varepsilon^2|\mathbf{x}]$
  \end{enumerate}
\end{assump}

\begin{assump} \mbox{}
  \begin{enumerate}[(i)] 
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^3|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z] = \mathbb{E}[\varepsilon^3|\mathbf{x}]$
  \end{enumerate}
\end{assump}

\begin{assump}
  $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) <1$ ($T$ is positively correlated with $T^*$)
\end{assump}


\paragraph{Notation}

\begin{align}
  \label{eq:theta1_def}
  \theta_1(\mathbf{x}) &= \beta(\mathbf{x})\left[ 1 - \left\{ \alpha_0(\mathbf{x}) + \mathbf{\alpha}_1(\mathbf{x}) \right\} \right]^{-1}\\
  \label{eq:theta2_def}
  \theta_2(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^2 \left[ 1 + \left\{\alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})\right\}\right] \\
  \label{eq:theta3_def}
  \theta_3(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^3\left[ \left( 1 - \left\{\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x})\right\} \right)^2 + 6\alpha_0(\mathbf{x})\left\{ 1 - \alpha_1(\mathbf{x}) \right\} \right]
\end{align}

\begin{align*}
  \mbox{Cov}(y,z|\mathbf{x}) - \mbox{Cov}(T,z|\mathbf{x}) \theta_1(\mathbf{x}) &= 0\\
  \mbox{Cov}(y^2,z|\mathbf{x}) - 2\mbox{Cov}(yT,z|\mathbf{x}) \theta_1(\mathbf{x}) + \mbox{Cov}(T,z|\mathbf{x})\theta_2(\mathbf{x}) &= 0\\
  \mbox{Cov}(y^3,z|\mathbf{x}) - 3 \mbox{Cov}(y^2T,z|\mathbf{x}) \theta_1(\mathbf{x}) + 3\mbox{Cov}(yT,z|\mathbf{x}) \theta_2(\mathbf{x}) - \mbox{Cov}(T,z|\mathbf{x}) \theta_3(\mathbf{x})&= 0
\end{align*}

\begin{align*}
  q(\mathbf{x}) &= \mathbb{P}(z=1|\mathbf{x})\\
  \pi(\mathbf{x}) &= \mbox{Cov}(T,z|\mathbf{x})\\
  \eta_j(\mathbf{x}) &= \mbox{Cov}(y^j,z|\mathbf{x})\\
  \tau_j(\mathbf{x}) &= \mbox{Cov}(Ty^j,z|\mathbf{x})
\end{align*}

\begin{align*}
 \eta_1(\mathbf{x}) &= \pi(\mathbf{x})\theta_1(\mathbf{x}) \\
  \eta_2(\mathbf{x}) &=  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x}) \\
  \eta_3(\mathbf{x}) &=  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})
\end{align*}



\begin{lem}[Various bits and pieces, e.g.\ $\eta_1$ equation]
  
\end{lem}


\begin{lem}[Lemma for Appendix only with Bayes' Rule]
For mis-classification probabilities
\begin{align*}
  P(T^*=1|T=1, Z=k) &= P(T=1 | T^*=1) \left(\frac{p_k^*}{p_k}\right) = (1 - \alpha_1)\left( \frac{p_k^*}{p_k} \right)\\
  P(T^*=1|T=0, Z=k) &= P(T=0 | T^*=1) \left(\frac{p_k^*}{1 - p_k}\right) = \alpha_1 \left( \frac{p_k^*}{1 - p_k} \right)\\
  P(T^*=0|T=1, Z=k) &= P(T=1 | T^*=0) \left(\frac{1 - p_k^*}{p_k}\right) = \alpha_0 \left( \frac{1 - p_k^*}{p_k} \right)\\
  P(T^*=0|T=0, Z=k) &= P(T=0 | T^*=0) \left(\frac{1 - p_k^*}{1 - p_k}\right) = (1 - \alpha_0)\left( \frac{1 - p_k^*}{1 - p_k} \right)
\end{align*}
\end{lem}

\begin{thm}[Non-identification Result]
  \label{thm:nonident}
\end{thm}
\begin{proof}[Proof of Theorem \ref{thm:nonident}]
\end{proof}

\begin{pro}
  Under Assumptions ???, $\eta_2(\mathbf{x}) =  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x})$.
  \label{pro:eta2}
\end{pro}
\begin{proof}[Proof of Proposition \ref{pro:eta2}]
  Since $z$ is binary, $\mbox{Cov}(w,z|\mathbf{x}) = q(\mathbf{x})\left[ 1 - q(\mathbf{x}) \right] \Delta_z\mathbb{E}(w|\mathbf{x},z)$ for any $w$, where we define $\Delta_z \mathbb{E}(w|\mathbf{x},z) \equiv \mathbb{E}(w|\mathbf{x},z=1) - \mathbb{E}(w|\mathbf{x},z=0)$.
Hence it suffices to show that 
\[
  \Delta_z\mathbb{E}(y^2|\mathbf{x},z) = 2 \Delta_z \mathbb{E}(Ty|\mathbf{x},z) \theta_1(\mathbf{x}) - \Delta_z\mathbb{E}(T|\mathbf{x},z)\theta_2(\mathbf{x}).
\]
By iterated expectations 
  \begin{align*} 
      \mathbb{E}\left( y^2|\mathbf{x},z \right) &= \mathbb{E}(y^2|\mathbf{x},T^*=0,z)\mathbb{P}(T^*=0|\mathbf{x},z) + \mathbb{E}(y^2|\mathbf{x},T^*=1,z)\mathbb{P}(T^*=1|\mathbf{x},z)\\
      \mathbb{E}\left( Ty|\mathbf{x},z \right) &= \mathbb{E}(Ty|\mathbf{x},T^*=0,z)\mathbb{P}(T^*=0|\mathbf{x},z) + \mathbb{E}(Ty|\mathbf{x},T^*=1,z)\mathbb{P}(T^*=1|\mathbf{x},z)
    \end{align*}
    and since $y = c(\mathbf{x}) + \beta(\mathbf{x})T^* + \varepsilon$,
  \begin{align*}
    \mathbb{E}\left( y^2|\mathbf{x},T^*=1,z \right) &= \left[c(\mathbf{x}) + \beta(\mathbf{x})\right]^2 + 2\left[ c(\mathbf{x}) + \beta(\mathbf{x}) \right]\mathbb{E}\left( \varepsilon|\mathbf{x},T^*=1,z \right) + \mathbb{E}\left( \varepsilon^2|\mathbf{x},T^*=1,z \right)\\
  \mathbb{E}\left( Ty|\mathbf{x},T^*=1,z \right)&= \left[c(\mathbf{x}) + \beta(\mathbf{x})\right]\mathbb{E}(T|\mathbf{x},T^*=1,z) + \mathbb{E}(T\varepsilon|\mathbf{x},T^*=1,z)\\
    \mathbb{E}\left( y^2|\mathbf{x},T^*=0,z \right) &= c(\mathbf{x})^2 + 2c(\mathbf{x})\mathbb{E}\left( \varepsilon|\mathbf{x},T^*=0,z \right) + \mathbb{E}\left( \varepsilon^2|\mathbf{x},T^*=0,z \right)\\
    \mathbb{E}\left( Ty|\mathbf{x},T^*=0,z \right)&= c(\mathbf{x})\,\mathbb{E}(T|\mathbf{x},T^*=0,z) + \mathbb{E}(T\varepsilon|\mathbf{x},T^*=0,z).
  \end{align*}


  using Assumptions ?? and \ref{assump:2ndMoment}.
\end{proof}
  

\begin{pro}
  Under Assumptions ???, $\eta_3(\mathbf{x}) =  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})$
  \label{pro:eta3}
\end{pro}
\begin{proof}[Proof of Proposition \ref{pro:eta3}]
\end{proof}

\begin{cor}
  \label{cor:theta_ident}
  Suppose that $\eta_1(\mathbf{x}), \eta_2(\mathbf{x}), \eta_3(\mathbf{x}), \tau_1(\mathbf{x}), \tau_2(\mathbf{x})$ and $\pi(\mathbf{x})$ are identified and that $\pi(\mathbf{x})\neq 0$.
  Then, under the conditions of Lemmas ???, $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$, and $\theta_3(\mathbf{x})$ are identified.
\end{cor}
\begin{proof}[Proof of Corollary \ref{cor:theta_ident}]
  For ease of notation we suppress dependence on $\mathbf{x}$ throughout this argument.
  Propositions ??? yield a triangular linear system of equations in $\theta_1, \theta_2, \theta_3$, namely
\begin{align*}
 \pi\theta_1 &= \eta_1 \\
 2\tau_1 \theta_1 - \pi \theta_2 &= \eta_2 \\
 3\tau_2 \theta_1 - 3\tau_1 \theta_2 + \pi\theta_3 &= \eta_3
\end{align*}
By inspection, the determinant of the system is $-\pi^3$ so a unique solution exists if $z$ is correlated with $T^*$.
In particular,
\begin{align*}
  \theta_1 &= \eta_1 / \pi\\
  \theta_2 &= 2\tau_1\eta_1/\pi^2 - \eta_2/\pi\\
  \theta_3 &= \eta_3/\pi - 3(\tau_2\eta_1 + \tau_1\eta_2)/\pi^2 + 6\tau_1\eta_1/\pi^3.
\end{align*}
\end{proof}


\begin{thm}[Identification of $\beta$, $\alpha_0$, $\alpha_1$]
  \label{thm:main_ident}
\end{thm}
\begin{proof}[Proof of Theorem \ref{thm:main_ident}]
  For ease of notation we suppress dependence on $\mathbf{x}$ throughout this argument.
  So long as $\beta \neq 0$, we can rearrange Equations \ref{eq:theta2_def} and \ref{eq:theta3_def} to obtain 
  \begin{align}
    \label{eq:quadraticA}
  A &= \theta_2/\theta_1^2 = 1 + (\alpha_0 - \alpha_1)  \\
  \label{eq:quadraticB}
  B &= \theta_3/\theta_1^3 = (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1)
  \end{align}
  Equation \ref{eq:quadraticA} gives $(1 - \alpha_1)= A - \alpha_0$.
  Hence $(1 - \alpha_0 - \alpha_1) = A - 2\alpha_0$ and $\alpha_0(1 - \alpha_1) = \alpha_0(A - \alpha_0)$.
  Substituting into Equation \ref{eq:quadraticB} and simplifying, $(A^2 - B) + 2A \alpha_0 - 2\alpha_0^2=0$.
  Substituting for $\alpha_0$ analogously yields a quadratic in $(1 - \alpha_1)$ with \emph{identical} coefficients.
It follows that one root of $(A^2-B) + 2Ar - 2r^2=0$ is $\alpha_0$ and the other is $1 - \alpha_1$.
Solving,
  \begin{equation}
    r = \frac{A}{2} \pm \sqrt{3 A^2 - 2B} = \frac{1}{\theta_1^2}\left(\frac{\theta_2}{2} \pm  \sqrt{3\theta_2^2  - 2\theta_1 \theta_3}\right).
  \end{equation}
By Equations \ref{eq:theta2_def} and \ref{eq:theta3_def}, 
  \begin{align*}
    3\theta^2_2 - 2\theta_1 \theta_3 &= 3 \left[ \theta_1^2 \left( 1 + \alpha_0 - \alpha_1 \right) \right]^2 - 2 \theta_1 \left\{ \theta_1^3 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0 (1 - \alpha_1) \right] \right\} \\
    &= \theta_1^4 \left\{ 3(1 + \alpha_0 - \alpha_1)^2 - 2 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1) \right] \right\}.
  \end{align*}
Expanding the first term we find that 
  \begin{align*}
    3(1 + \alpha_0 - \alpha_1)^2 
    %&= 3\left[ 1 + (\alpha_0 - \alpha_1) \right]^2 
    &= 3\left[ 1 + 2(\alpha_0 - \alpha_1) + (\alpha_0 - \alpha_1)^2 \right]\\
    &= 3 + 6\alpha_0 - 6\alpha_1 + 3 \alpha_0^2 + 3 \alpha_1^2 - 6\alpha_0\alpha_1 
  \end{align*}
and expanding the second 
  \begin{align*}
    2\left[ (1 - \alpha_0 - \alpha_1)^2 + 6\alpha_0(1 - \alpha_1) \right]
    %&= 2\left\{ \left[ 1 - (\alpha_0 + \alpha_1) \right]^2 + 6\alpha_0(1 - \alpha_1) \right\}\\
  &=2\left[ 1 - 2(\alpha_0 + \alpha_1) + (\alpha_0 + \alpha_1)^2 + 6\alpha_0 - 6 \alpha_0 \alpha_1 \right]\\
    %&= 2 - 4(\alpha_0 + \alpha_1) + 2(\alpha_0 + \alpha_1)^2 + 12\alpha_0 - 12 \alpha_0 \alpha_1 \\
    &= 2 + 8\alpha_0 - 4\alpha_1 + 2\alpha_0^2 +  2\alpha_1^2 - 8 \alpha_0 \alpha_1.
  \end{align*}
Therefore
  \begin{align*}
    3\theta^2_2 - 2\theta_1 \theta_3 
    %&= \theta_1^4 \left\{ 3(1 + \alpha_0 - \alpha_1)^2 - 2 \left[ (1 - \alpha_0 - \alpha_1)^2 + 6 \alpha_0 (1 - \alpha_1) \right] \right\}\\
    &= \theta_1^4 \left\{ 1 - 2 \alpha_0 - 2 \alpha_1 + \alpha_0^2 - \alpha_1^2 + 2\alpha_0 \alpha_1 \right\}\\
    &= \theta_1^4 \left[ (1 - \alpha_0 - \alpha_1)^2 \right]
  \end{align*}
which is strictly greater than zero since $\theta_1 \neq 0$ and $\alpha_0 + \alpha_1 \neq 0$.
It follows that both roots of the quadratic are real.
Moreover, $3\theta_2^2/\theta_1^4 - 2\theta_3/\theta_1^3$ identifies $(1 - \alpha_0 - \alpha_1)^2$.
Substituting into Equation \ref{eq:theta1_def}, it follows that $\beta$ is identified up to sign.
If $\alpha_0 + \alpha_1 < 1$ then $\mbox{sign}(\beta) = \mbox{sign}(\theta_1)$ so that both the sign and magnitude of $\beta$ are identified.
If $\alpha_0 + \alpha_1 < 1$ then $1 - \alpha_1 > \alpha_0$ so $(1 - \alpha_1)$ is the larger root of $(A^2 - B) + 2Ar - 2r^2=0$ and $\alpha_0$ is the smaller root.
\end{proof}

\section{Notes on Bugni, Canay \& Shi (2017)}

\todo[inline]{I've played around with this a bit for our example and I don't think it's going to work. The problematic step is the construction of the profiled test statistic. If we want, for example, to carry out inference for $\beta$, we need to estimate $\alpha_0$ and $\alpha_1$ under the null $\beta = \beta_0$. But if $\beta_0$ is small, the moment equalities contain almost no information about $\alpha_0$ and $\alpha_1$ so the optimization problem goes haywire.}

The notation follows the paper nearly verbatim, although I have made a few minor changes to the notation to avoid ambiguity.
Regarding the sequence $\kappa_n$, used to carry out moment selection, the paper only requires that $\kappa_n \rightarrow \infty$ and $\kappa_n/\sqrt{n} \rightarrow 0$ as in Andrews \& Soares.
The leading example, and indeed the choice recommended by Andrews \& Soares, is the BIC-type penalty $\kappa_n = \sqrt{\ln n}$ so in all the expressions below I replace $\kappa_n$ with this quantity.

\paragraph{Setup} Partially identified model defined by $p$ moment  inequalities and $k - p$ equalities:
\begin{align*}
  E_F\left[ m_j(W_i,\theta) \right] &\geq 0 \mbox{ for } j = 1, \cdots, p\\
  E_F\left[ m_j(W_i,\theta) \right] &= 0 \mbox{ for } j = p+1, \cdots, k
\end{align*}
where $W_1, \hdots, W_n \sim \mbox{ iid } F$.
Function $m = (m_1, \hdots, m_k)$ is known and $\theta \in \Theta$ is a finite-dimensional parameter.
The goal is to construct confidence sets for $\theta$.


\paragraph{The Problem} 
The existing literature tests the \emph{joint} hypothesis $\theta = \theta_0$ and inverts this test to construct a confidence set (CS) for $\theta$.
In applied work, we often want inference for a \emph{subset} of $\theta$ and the usual approach is to \emph{project} a joint confidence set for $\theta$.
For example if $\theta = (\alpha, \beta, \gamma, \delta)$, a projection confidence set for $\alpha$ is the set of all values for which we can find \emph{some} triple $(\beta_0, \gamma_0, \delta_0)$, allowed to depend on $\alpha$,  such that $\theta = (\alpha, \beta_0, \gamma_0, \delta_0)$ is not rejected.
This approach is computationally intensive: even if we are only interested in one element of $\theta$ we have to invert the test over the high-dimensional \emph{joint} parameter space.
Projection inference can also have poor power compared to the joint inference upon which it is based.

\paragraph{This Paper} 
The authors propose the \emph{minimum resampling test}: a direct test of
\[
  H_0\colon \lambda(\theta) = \lambda_0 \, \mbox{ vs. } \, H_1\colon \lambda(\theta) \neq \lambda_0
\]
where $\lambda$ is a \emph{function} of $\theta$.
The leading case is where $\lambda$ is simply a sub-vector or individual coordinate of $\theta$.
The test controls size uniformly, has high power, and is in general more computationally efficient than projection inference whenever $\lambda$ is of lower dimension that $\theta$. 


\paragraph{Decision Rule for Minimum Resampling (MR) Test}
The test rejects $H_0\colon \lambda(\theta) = \lambda_0$ when a \emph{profiled test statistic} $T_n(\lambda_0)$ is large.
The rejection rule is
\[
  \text{Reject if} \quad T_n(\lambda_0) > \widehat{c}_n^{MR}(\lambda_0, 1-\alpha) 
\]
where $\alpha$ is the significance level and $\widehat{c}_n^{MR}(\lambda_0, 1-\alpha)$ is the MR critical value, which is calculated by a bootstrap procedure described below.

\paragraph{Notation}
\begin{align*}
  \bar{m}_{n,j}(\theta) &\equiv \frac{1}{n}\sum_{i=1}^n m_j(W_i,\theta), \mbox{ for } j = 1, \hdots, k\\
  \widehat{\sigma}^2_{n,j} &\equiv \frac{1}{n}\left[ m_j(W_i,\theta) - \bar{m}_j(W_i,\theta) \right]^2, \mbox{ for } j = 1, \hdots, k\\
[x]_{-} &\equiv \min\left\{ x, 0 \right\}\\
  Q_n(\theta) &= \underbrace{\sum_{i=1}^p \left[ \frac{\sqrt{n}\bar{m}_{n,j}(\theta)}{\widehat{\sigma}_{n,j}(\theta)} \right]_{-}^{2}}_{\text{inequalities}} + \underbrace{\sum_{j=p+1}^{k} \left\{ \frac{\sqrt{n}\bar{m}_{n,j}(\theta)}{\widehat{\sigma}_{n,j}(\theta)} \right\}^2}_{\text{equalities}}\\
  \Theta(\lambda_0) &\equiv \left\{ \theta \in \Theta\colon \lambda(\theta) = \lambda_0 \right\}\\
  T_n(\lambda_0) &= \inf_{\theta \in \Theta(\lambda_0)} Q_n(\theta)
\end{align*}

\paragraph{Calculating the MR Critical Value}
The MR critical value $\widehat{c}_n(\lambda_0, 1 - \alpha)$ is calculated from \emph{two different approximations} to the sampling distribution of $T_n(\lambda_0)$: \emph{discarded resampling} (DR) and \emph{penalized resampling} (PR), specifically
\begin{align*}
  \widehat{c}_n^{MR}(\lambda_0, 1 - \alpha) &\equiv \mbox{(conditional) } 1 - \alpha \mbox{ quantile of } T_n^{MR}(\lambda_0) \\
  T_n^{MR}(\lambda_0) &\equiv \min\left\{ T_n^{DR}(\lambda_0), T_n^{PR}(\lambda_0) \right\}
\end{align*}
where we approximate the distribution of $T_n^{MR}$ by simulation, as described in more detail below.
Both $T_n^{DR}$ and $T_n^{PR}$ are constructed from $v^*_{n,j}(\theta)$, defined as: 
\begin{align*}
  v^*_{n,j}(\theta) &= \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \xi_i \left[ \frac{m_j(W_i,\theta) - \bar{m}_{n,j}(\theta)}{\widehat{\sigma}_{n,j}(\theta)} \right], \, \mbox{for } j = 1, \hdots, p \\
  \xi_1, \cdots, \xi_n &\sim \mbox{ iid } N(0,1) \mbox{ independent of the data } W_1, \cdots, W_n.
\end{align*}
In particular, both the $T_n^{DR}$ and $T_{n}^{PR}$ take the form
\[
  \inf_{\theta \in \widetilde{\Theta}}\left(\underbrace{\sum_{j=1}^p \left[ v_{n,j}^*(\theta) + s_j(\theta) \right]_{-}^2}_{\text{inequalities}} + \underbrace{\sum_{j=p+1}^{k}\left\{   v^*_{n,j}(\theta) + s_j(\theta)\right\}^2}_{\text{equalities}}  \right)
\]
where $v^*_{n,j}(\theta)$ is as defined in the preceding pair of displayed equations, $\widetilde{\Theta}$ is some set, and $s_j(\theta)$ is a \emph{slackness function}.
The difference between DR and PR is the choice of $\widetilde{\Theta}$ and $s_j(\theta)$ as explained in the following paragraphs.

\paragraph{Discarded Resampling Statistic: $T_n^{DR}(\lambda_0)$}
\begin{align*}
  T_n^{DR}(\lambda_0) &\equiv \inf_{\theta \in \widehat{\Theta}_I(\lambda_0)}\left(\sum_{j=1}^p \left[ v_{n,j}^*(\theta) + \varphi_j(\theta) \right]_{-}^2 + \sum_{j=p+1}^{k}\left\{   v^*_{n,j}(\theta) + \varphi_j(\theta)\right\}^2  \right)\\
  \widehat{\Theta}_I(\lambda_0) &\equiv \left\{ \theta\in \Theta(\lambda_0)\colon Q_n(\theta) \leq T_n(\lambda_0) \right\}\\
  \varphi_j(\theta) &= \left\{
  \begin{array}{ll}
    \infty & \mbox{if } \quad \sqrt{n}\, \bar{m}_{n,j}(\theta)/[ \widehat{\sigma}_{n,j}(\theta) \sqrt{\ln n}] > 1 \quad \mbox{ and } j \leq p\\
    0 & \mbox{if } \quad \sqrt{n}\, \bar{m}_{n,j}(\theta)/[ \widehat{\sigma}_{n,j}(\theta) \sqrt{\ln n}]  \leq 1 \quad \mbox{ or } j > p
  \end{array}
  \right.
\end{align*}
Note that $\widehat{\Theta}_I(\lambda_0)$ contains the values of $\theta$ that \emph{minimize} $T_n(\lambda_0) = \inf_{\theta \in \Theta(\lambda_0)} Q_n(\theta)$.
Note further that $\varphi_j$ only affects the moment \emph{inequalities}: it is zero for $j > p$.

\paragraph{Penalized Resampling Test Statistic: $T_n^{PR}(\lambda_0)$}
\begin{align*}
  T_n^{PR}(\lambda_0) &\equiv \inf_{\theta \in \Theta(\lambda_0)}\left(\sum_{j=1}^p \left[ v_{n,j}^*(\theta) + \ell_j(\theta) \right]_{-}^2 + \sum_{j=p+1}^{k}\left\{   v^*_{n,j}(\theta) + \ell_j(\theta)\right\}^2  \right)\\
  \ell_j(\theta) &= \sqrt{n}\, \bar{m}_{n,j}(\theta)/\left[ \widehat{\sigma}_{n,j}(\theta) \sqrt{\ln n} \right] 
\end{align*}
Note that to calculate $T_n^{PR}(\lambda_0)$ we minimize over $\Theta(\lambda_0)$ -- the set of all $\theta$ for which $\lambda(\theta) = \lambda_0$.
Note further that $\ell_j(\theta)$ affects \emph{both} the moment \emph{equalities} and inequalities.

\paragraph{Computational Cost}
In practice we generate draws for the test statistics using a large number $B$ of simulation replications.
These are essentially bootstrap draws, but we bootstrap the limit distribution rather than the data.
In particular, each of these bootstrap replications makes $n$ iid standard normal draws: $\xi_1, \hdots, \xi_n$ which are used in the calculation of \emph{both} $T_n^{DR}$ and $T_n^{PR}$.
(It is important to make sure that we use the same draws for both!)
The computational cost of the MR test comes from the $2B +1$ optimization problems it requires us to solve.
First, we need to calculate the test statistic $T_n(\lambda_0)$ based on the \emph{actual data} -- this requires us to solve one minimization problem.
Then for each of the $B$ bootstrap replications we need to calculate $T_n^{DR}$ and $T_n^{PR}$ -- this requires us to solve two minimization problems.
The authors claim that this is in general more efficient than projection inference which requires us to search over a high-dimensional space.
Presumably the gains depend sensitively on how efficiently one can carry out the required minimizations.


\paragraph{Sub-vector Inference}
The leading application of the MR test is to sub-vector inference.
In this case we only want to carry out a test for some subset of the coordinates of $\theta$.
Let $\theta_s$ denote this sub-vector.
Then $\Theta(\lambda_0) = \left\{ \theta\in \Theta \colon \theta_s = \lambda_0 \right\}$. 
A particularly simple example is inference for a single coordinate of $\theta$, say $\theta_1$.
In this case, the optimization problems are over all \emph{other} coordinates of $\theta$ besides $\theta_1$ since $\theta_1 = \lambda_0$ under the null that we are testing.

\paragraph{Steps to Carry out the MR Test}
For simplicity, in this section I assume that we wish to carry out inference on a sub-vector of $\theta$.
Let $\theta = (\beta, \gamma)$ where $\beta$ and $\gamma$ could be scalars or vectors.
The parameters of interest are $\beta$ and our null hypothesis is $\beta = \beta_0$.
Let $\Gamma$ denote the parameter space for $\gamma$.
I write any function of $\theta$ explicitly as a function of $(\beta, \gamma)$ so that any calculation carried out under the null is evaluated at $(\beta_0, \gamma)$
\begin{enumerate}
  \item Define the following functions of $\gamma$ under the null $\beta = \beta_0$
    \begin{align*}
      \bar{m}_n(\gamma) &= \frac{1}{n} \sum_{i=1}^n m(W_i,\beta_0,\gamma) \\
      \widehat{D}_n(\gamma) &= \mbox{diag}\left\{ \frac{1}{n} \sum_{i=1}^{n} \left[ m(W_i,\beta_0, \gamma) - \bar{m}_n(\gamma) \right]\left[ m(W_i,\beta_0, \gamma) - \bar{m}_n(\gamma) \right]'\right\} \\
      v(\gamma) &= \sqrt{n} \widehat{D}^{-1/2}_n(\gamma)\bar{m}_n(\gamma) \\
      Q(\gamma) &=  \sum_{j=1}^p \left[ v_j(\gamma) \right]^2_{-} + \sum_{j=p+1}^k v_j^2(\gamma) 
    \end{align*}
  \item Calculate the test statistic using the observed data $\{W_1, \hdots, W_n\}$
    \[
      T_n = \min_{\gamma \in \Gamma} Q(\gamma) 
    \]
  \item Calculate $\widehat{\Gamma}_I$ -- the ``estimated set of minimizers'' of $Q(\gamma)$ 
    \[
      \widehat{\Gamma}_I = \left\{\gamma \in \Gamma \colon Q(\gamma) \leq T_n \right\} 
    \]
  \item For each of $B$ independent bootstrap replications, do the following:
    \begin{enumerate}[(i)]
      \item Draw $\xi_1, \xi_2, \hdots, \xi_n \sim \mbox{ iid } N(0,1)$
      \item Construct $v^*(\gamma)$ -- the re-centered, bootstrap version  of $v(\gamma)$ -- as follows
        \[
          v^*(\gamma) = n^{-1/2} \widehat{D}_n^{-1/2}(\gamma) \sum_{i=1}^n \xi_i \left[ m(W_i, \beta_0, \gamma) - \bar{m}_n(\gamma) \right]
        \]
    \end{enumerate}
\end{enumerate}


\section{May 18th, 2017}
Today I figured out that the Bugni, Canay \& Shi idea isn't going to work for our example since the optimization problem involved in computing the profiled test statistic is very badly behaved under the null $\beta = \beta_0$ when $\beta_0$ is small.
This means that we'll have to go back to doing joint inference over $(\alpha_0, \alpha_1, \beta)$ using Andrews \& Soares.
Here are a few ideas:
\begin{enumerate}
  \item I think it could be faster to use the ``asymptotic version'' of Andrews and Soares.
    The asymptotic version involves making normal draws instead of bootstrapping the sample.
    Is there a tradeoff in terms of accuracy?
    Presumably it won't matter much for reasonably large samples.
  \item $\theta_1$ is strongly identified regardless of the values of $\beta$, $\alpha_0$ and $\alpha_1$.
    This means that if we wanted to do inference for $\alpha_0, \alpha_1$ we could use a plug-in estimator of $\theta_1$ provided that we correct the asymptotic variance matrix estimator as described in section 10.2 of Andrews and Soares.
  \item Might it be interesting to look at a test for the presence of mis-classification? This would entail testing the null that $\alpha_0 = \alpha_1 = 0$.
    This is a null for which the moment inequalities cannot provide any information so presumably we could just use Stock and Wright's GMM-AR test.
    It might also be interesting to try this with Mahajan or Lewbel and show the size distortions that arise if one does not use the AR test.
  \item Our implementation of Andrews and Soares should use preliminary estimators for $\boldsymbol{\kappa}$ and the parameters associated with exogenous covariates.
    We can use the simplest possible estimator for $\boldsymbol{\kappa}$ under the null: three sample means, e.g.\ $\kappa_1 = \bar{y} - \beta_0 \bar{T}/(1 - \alpha_0 - \alpha_1)$.
    Similarly, for the coefficient on the exogenous covariates you just have a regression under the null in which $y - \beta_0 T/(1 - \alpha_0 - \alpha_1)$ replaces $y$.
  \item Is there a clever way to use the fact that $\theta_1$ is strongly identified to help us carry out inference for $\beta$?
   Specifically, suppose we construct a confidence set for $\alpha_0, \alpha_0$ by profiling out $\theta_1$. 
   Can we use this to back out inference for $\beta$?
\end{enumerate}

\section{May 18--19, 2017: More on Andrews \& Soares}
\paragraph{Recall the notation from above:}
\[
  \mathbb{E} \left[ m_j(\mathbf{w}_i,\theta_0) \right]
  \left\{
  \begin{array}{cc}
    \geq 0 & \mbox{for } j = 1, \cdots, p\\
    = 0 & \mbox{for } j = p + 1, \cdots,k \mbox{ where } k = p + v
  \end{array}
  \right.
\]

\[
  \bar{m}_n(\theta) = \left[
  \begin{array}{c}
    \bar{m}_{n,1}(\theta)\\
    \vdots \\
    \bar{m}_{n,k}(\theta)\\
  \end{array}
\right], \quad
\bar{m}_{n,j} = \frac{1}{n} \sum_{i=1}^{n} m_j(\mathbf{w}_i, \theta) \mbox{ for } j = 1, \cdots, k
\]
Now, let $\Sigma(\theta_0)$ denote the asymptotic variance of $\sqrt{n}\; \bar{m}_n(\theta)$.
We estimate this quantity using $\widehat{\Sigma}_n(\theta)$.
For iid observations, as in our example, the estimator is
\[
  \widehat{\Sigma}_n(\theta) = \frac{1}{n} \sum_{i=1}^n \left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]\left[ m(\mathbf{w}_i, \theta) - \bar{m}_n(\theta) \right]', \quad 
  m(\mathbf{w}_i, \theta) = \left[
  \begin{array}{c}
    m_1(\mathbf{w}_i, \theta)\\
    \vdots \\
    m_k(\mathbf{w}_i, \theta)\\
  \end{array}
\right]
\]
The test statistic takes the form $T_n(\theta) = S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right)$ for some real-valued function $S$.
The example we will use is $S_1$, defined by
\[
  S_1(m, \Sigma) = \sum_{j=1}^p [m_j/\sigma_j]^2_- + \sum_{j=p+1}^{p+v} (m_j/\sigma_j)^2
\]
where $m = (m_1, \cdots, m_k)'$, $\sigma_j^2$ is the $j$th diagonal element of $\Sigma$, and
\[
  [x]_- = \left\{
  \begin{array}{cc}
    x, & \mbox{if } x <0\\
    0, & \mbox{if } x \geq 0
  \end{array}
\right.
\]


\paragraph{Calculating the Critical Value for the \emph{Asymptotic} Version of GMS}
Above we described the calculation of the \emph{bootstrap} critical value for a test of $\theta = \theta_0$. 
We now present an alternative method that uses iid standard normal draws to ``bootstrap the limit experiment.''

\begin{enumerate}
  \item Calculate $\bar{m}_n(\theta_0)$, $\widehat{\Sigma}_n(\theta_0)$, and $T_n(\theta_0) = S\left( \sqrt{n}\; \bar{m}_n(\theta_0), \widehat{\Sigma}_n(\theta_0) \right)$
  \item Calculate the following quantities:
    \begin{align*}
      \widehat{\Omega}_n(\theta_0) &= \mbox{Diag}^{-1/2}\left(\widehat{\Sigma}_n(\theta_0)  \right)\left(\widehat{\Sigma}_n(\theta_0)\right)\mbox{Diag}^{-1/2}\left( \widehat{\Sigma}_n(\theta_0) \right)\\
      \xi_n(\theta_0) &= \mbox{Diag}^{-1/2}\left( \widehat{\Sigma}_n(\theta_0) \right) \sqrt{n}\; \bar{m}_n(\theta_0) / \sqrt{\ln n}\\
      \varphi\left( \xi_n(\theta_0), \widehat{\Omega}_n(\theta_0) \right) &= \left\{
      \begin{array}{cc}
        0, & \mbox{if } \xi_j \leq 1 \mbox{ and } j \leq p \\
        \infty & \mbox{if } \xi_j > 1 \mbox{ or } j > p\\
      \end{array}
      \right.
    \end{align*}
  \item Draw $Z_1^*, \hdots, Z_R^* \sim \mbox{ iid } N(0_k, I_k)$ for $R$ large.
  \item The critical value is the $1 - \alpha$ quantile of $\left\{S\left( \widehat{\Omega}^{1/2}(\theta_0) Z_r^* + \varphi\left( \xi_n(\theta_0), \widehat{\Omega}_n(\theta_0) \right), \widehat{\Omega}_n(\theta_0) \right)\right\}_{r=1}^R$
\end{enumerate}

\paragraph{Clearer Explanation of the Asymptotic GMS Critical Value Calculation}
The preceding paragraph used the explanation from Andrews \& Soares but it's a little obscure. 
Here's a simpler and clearer explanation.
Recall that the statistics from Andrews \& Soares (2010) satisfy
\begin{align*}
  T_n &= S\left( \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Sigma}(\theta) \right) = S\left( \widehat{D}_n^{-1/2}(\theta) \sqrt{n}\; \bar{m}_n(\theta), \widehat{\Omega}_n(\theta) \right)\\
  \widehat{D}_n(\theta) &= \mbox{diag}\left( \widehat{\Sigma}\left( \theta \right) \right)\\
  \widehat{\Omega}_n(\theta) &= \widehat{D}_n^{-1/2}(\theta)\, \widehat{\Sigma}(\theta)\, \widehat{D}_n^{-1/2}(\theta)
\end{align*}
The procedure for the asymptotic version of the GMS test is as follows:
\begin{enumerate}
  \item Calculate $\sqrt{n}\; \bar{m}_n\left( \theta_0 \right)$ and $\widehat{\Sigma}(\theta_0)$ under the null hypothesis $H_0\colon \theta = \theta_0$.
  \item Calculate the test statistic $T_n(\theta_0) = S_1\left(\sqrt{n}\; \bar{m}_n\left( \theta_0 \right), \widehat{\Sigma}\left( \theta_0 \right)  \right)$.
  \item Determine which inequality moment conditions are ``far from binding'' under $H_0\colon \theta = \theta_0$
    \begin{itemize}
      \item Let $j \in J = \{ 1, \cdots, p\}$ index the inequality moment conditions.
      \item Let $\widehat{\sigma}_{n,j}(\theta_0)^2$ denote the $(j,j)$ element of $\widehat{\Sigma}(\theta_0)$
      \item For each $j\in J$ calculate the ``t-statistic'' $ t_{n,j} = \sqrt{n}\; \bar{m}_j(\theta_0)/\widehat{\sigma}_{n,j}(\theta_0)$
      \item Let $\mathcal{FB}$ denote the subset of $J$ for which $t_{n,j} >\sqrt{\log n}$.
        These are the inequality moment conditions that are ``far from binding'' under $H_0\colon\theta = \theta_0$. 
    \end{itemize}
  \item Calculate the asymptotic critical value by simulation as follows:
    \begin{itemize}
      \item Draw $R$ standard normal $k$-vectors: $Z^*_1, \hdots, Z^*_R \sim \mbox{ iid } N(0_k, I_k)$
      \item Construct the $k$-vector $\varphi(\theta_0)$ from its coordinates $\varphi_j$ as follows:
        \[
          \varphi_j(\theta_0) = \left\{
          \begin{array}{ll}
            \infty & j\in \mathcal{FB} \mbox{ and } j\leq p\\
            0 & j>p \mbox{ or } 
          \end{array}
          \right.
        \]
      \item Set $M^*_{n,r}(\theta_0) = \widehat{\Omega}_n^{1/2}(\theta_0) Z_r^*$, yielding $R$ iid $N(0_k, \widehat{\Omega}_n)$ draws.
      \item Calculate $T^{*(r)}_n(\theta_0) = S_1\left(M^*_{n,r}(\theta_0) + \varphi(\theta_0), \;\widehat{\Omega}_n(\theta_0)\right)$ for $r=1, 2, \hdots, R$. Notice that adding $\varphi(\theta_0)$ is equivalent to dropping any moment conditions that we have determined are far from binding before calculating the test statistic.
      \item Set $\widehat{c}_n(\theta_0, 1 - \delta)$ equal to the $1 - \delta$ quantile of $\left\{ T_n^{*(r)}(\theta_0) \right\}_{r=1}^R$.
    \end{itemize}
  \item Reject $H_0\colon \theta = \theta_0$ if $T_n(\theta_0) > \widehat{c}_n(\theta_0, 1-\delta)$
  \item To construct a $(1 - \delta)\times 100\%$ confidence set, invert the test of $H_0\colon \theta = \theta_0$ for $\theta_0 \in \Theta$.
\end{enumerate}


\paragraph{Preliminary Estimation of an Identified Parameter}
Suppose that the moment equations take the form $m_j(W_i, \theta, \tau)$ where $\tau$ is identified under the null that $\theta = \theta_0$.
Let $\widehat{\tau}_n(\theta_0)$ be a consistent, asymptotically normal estimator for $\tau$ under the null that $\theta = \theta_0$.
As long as $\sqrt{n} \; \bar{m}_n(\theta, \widehat{\tau}_n(\theta))$ is asymptotically normal we can plug in our estimator of $\tau$ and carry out the GMS test almost exactly as above.
Only one small change is required: the variance matrix estimator $\widehat{\Sigma}_n(\theta_0)$ needs to be adjusted to take account of the fact that $\tau$ has been estimated.
This is fairly straightforward using standard calculations for moment condition models.
The explanation we give here follows Hall (2005) although the notation is changed to match Andrews and Soares.

Let $m(W_i, \theta, \tau)$ be a vector of moment functions that we will use to test $\theta = \theta_0$.
Some of these moment functions will enter as equalities and others as inequalities but this is totally immaterial for the derivation to follow.
Let $h(W_i, \theta, \tau)$ be another collection of moment functions that will be used to construct the estimator $\widehat{\tau}_n(\theta)$ but not to test $\theta = \theta_0$.
Let $g(W_i, \theta, \tau) = (m', h')'$ denote the full set of moment functions.
This argument will use a mean-value expansion and hence will rely on $\tau$ being on the interior of the parameter space.
So the idea here is that $\theta$ contains the ``problematic'' parameters, those that are weakly identified or may lie on a boundary, while $\tau$ contains the ``well-behaved'' parameters.
Define: 
\begin{align*}
  \bar{g}_n(\theta, \tau) &= 
  \left[
  \begin{array}{c}
    \bar{m}_n(\theta, \tau)\\
    \bar{h}_n(\theta, \tau)\\
  \end{array}
\right] = 
  \frac{1}{n}\sum_{i=1}^n g(W_i, \theta, \tau) \\
G_n(\theta, \tau) &= \left[
\begin{array}{c}
  M_n(\theta, \tau)\\
  H_n(\theta, \tau)
\end{array}
\right] = n^{-1}\sum_{i=1}^n \partial g(W_i, \theta, \tau)/\partial \tau'
\end{align*}
Throughout this argument, we will hold $\theta$ \emph{fixed} at $\theta_0$.
Under $\theta = \theta_0$ we have the following GMM estimator for $\tau(\theta_0)$, the true value of $\tau$ assuming that $\theta = \theta_0$,
\[
  \widehat{\tau}_n(\theta_0) = \underset{\tau \in \mathscr{T}}{\arg \min} \; \bar{h}_n(\theta_0, \tau)'\, \Xi_n \, \bar{h}_n(\theta_0, \tau)
\]
where $\Xi_n \rightarrow_p \Xi$.
To simplify the notation, we suppress dependence on $\theta$.
Unless otherwise specified, \emph{every function} is evaluated at $\theta = \theta_0$, e.g.\ 
\[
  \widehat{\tau}_n = \widehat{\tau}_n(\theta_0), \quad \bar{g}_n(\tau) = \bar{g}_n(\theta_0, \tau), \quad G_n(\tau) = G_n(\theta_0, \tau)
\]
Now, mean-value expanding $\bar{g}_n$, viewed as a function of $\tau$ \emph{only}, around $\tau_0 \equiv \tau(\theta_0)$,
\begin{equation}
  \bar{g}_n\left(\widehat{\tau}_n\right) = \bar{g}_n(\tau_0) + G_n(\widehat{\tau}_n, \tau_0, \lambda_n) \left( \widehat{\tau}_n - \tau_0 \right)
  \label{eq:gn_MVE}
\end{equation}
where the $i$th row of $G_n(\widehat{\tau}_n, \tau_0, \lambda_n)$ equals $i$th row of $G_n(\widetilde{\tau}_n^{(i)})$ and 
\[
\widetilde{\tau}_n^{(i)} = \lambda_{n,i}\tau_0 + (1 - \lambda_{n,i}) \widehat{\tau}_n
\] 
for some $0 \leq \lambda_{n,i} \leq 1$.
The preceding mean-value expansion is for $\bar{g}_n = (\bar{m}_n', \bar{h}_n')'$.
Restricting our attention to the sub-vector $\bar{h}_n$, we have
\[
  \bar{h}_n(\widehat{\tau}_n) = \bar{h}_n(\tau_0) + H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)(\widehat{\tau}_n - \tau_0) 
\]
where $\lambda^h_n$ is the sub-vector of $\lambda_n$ that corresponds to $H_n$. 
Pre-multiplying by $H_n(\widehat{\tau}_n)'\Xi_n$, we have
\[
  H_n(\widehat{\tau}_n)'\Xi_n\bar{h}_n(\widehat{\tau}_n) = H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\tau_0) + H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)(\widehat{\tau}_n - \tau_0).
\]
But by the first-order conditions for $\widehat{\tau}_n$, we have  $H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\widehat{\tau}_n) = 0$ and thus
\[
  H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\tau_0) + H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)(\widehat{\tau}_n - \tau_0) = 0.
\]
Rearranging,
\begin{equation}
  (\widehat{\tau}_n - \tau_0) = -\left[ H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)\right]^{-1} H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\tau_0).
  \label{eq:tau_solution}
\end{equation}
Now, substituting Equation \ref{eq:tau_solution} into Equation \ref{eq:gn_MVE},
\begin{align*}
  \bar{g}_n\left(\widehat{\tau}_n\right) &= \bar{g}_n(\tau_0) + G_n(\widehat{\tau}_n, \tau_0, \lambda_n) \left( \widehat{\tau}_n - \tau_0 \right)\\
  &= \bar{g}_n(\tau_0) - G_n(\widehat{\tau}_n, \tau_0, \lambda_n) \left[ H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)\right]^{-1} H_n(\widehat{\tau}_n)'\Xi_n \bar{h}_n(\tau_0)
\end{align*}
And since $\bar{g}_n = (\bar{m}_n', \bar{h}_n')$ and $G_n = (M_n', H_n')'$, 
\begin{align*}
  \left[
  \begin{array}{cc}
    \sqrt{n}\;\bar{m}_n(\widehat{\tau}_n)\\
    \sqrt{n}\;\bar{h}_n(\widehat{\tau}_n)
  \end{array}
\right] &=
  \left[
  \begin{array}{cc}
    \sqrt{n}\;\bar{m}_n(\tau_0)\\
    \sqrt{n}\;\bar{h}_n(\tau_0)
  \end{array}
\right] - 
\left[
\begin{array}{c}
  M_n(\widehat{\tau}_n, \tau_0, \lambda_n^m)\\
  H_n(\widehat{\tau}_n, \tau_0, \lambda_n^h)
\end{array}
\right]
\left[ H_n(\widehat{\tau}_n)'\Xi_n H_n(\widehat{\tau}_n, \tau_0, \lambda^h_n)\right]^{-1} H_n(\widehat{\tau}_n)'\Xi_n \left[\sqrt{n}\;\bar{h}_n(\tau_0)\right]\\
  &= \left[
  \begin{array}{cc}
    \sqrt{n}\;\bar{m}_n(\tau_0)\\
    \sqrt{n}\;\bar{h}_n(\tau_0)
  \end{array}
\right] - 
\left[
\begin{array}{r}
M(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi \\
H(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi 
\end{array}
\right]
\sqrt{n}\;\bar{h}_n(\tau_0) + o_p(1)
\end{align*}
where $M(\tau) = E[\partial m(W_i, \theta_0, \tau)/\partial \tau']$ and $H(\tau) = E[\partial h(W_i, \theta_0, \tau)/\partial \tau']$.
We are only interested in the behavior of $\sqrt{n} \; m_n(\widehat{\tau}_n)$.
Restricting attention to this sub-vector,
\begin{align*}
  \sqrt{n}\; \bar{m}_n(\widehat{\tau}_n) &= \sqrt{n}\; \bar{m}_n(\tau_0) - M(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi \times \sqrt{n}\; \bar{h}_n(\tau_0) + o_p(1) \\
  &=  
  \left[
  \begin{array}{cc}
    \mathbf{I}_k & -M(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi 
  \end{array}
\right]
\left[
\begin{array}{c}
  \sqrt{n}\; \bar{m}_n(\tau_0)\\ 
  \sqrt{n}\; \bar{h}_n(\tau_0)
\end{array}
\right] + o_p(1) \\
&=  
\left[
\begin{array}{cc}
  \mathbf{I}_k & B(\tau_0)
\end{array}
\right] \sqrt{n}\; \bar{g}_n(\tau_0)
\end{align*}
where $\mathbf{I}_k$ is the $k\times k$ identity matrix and $B(\tau_0) = M(\tau_0)\left\{ H(\tau_0)'\Xi H(\tau_0)\right\}^{-1} H'(\tau_0)\Xi$.
We now have all the ingredients we need to calculate our variance matrix adjustment.
Restoring explicit dependence on $\theta_0$, we have 
\[
  \sqrt{n}\; \bar{m}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right) =  \left[
  \begin{array}{cc}
    \mathbf{I}_k & B\left( \theta_0, \tau(\theta_0) \right) 
\end{array}
\right] \; \sqrt{n}\; \bar{g}_n\left( \theta_0, \tau(\theta_0) \right)
\]
By an appropriate Central Limit Theorem, $\sqrt{n}\; \bar{g}_n\left( \theta_0, \tau(\theta_0) \right)$ is asymptotically normal with variance matrix $\mathcal{V}\left( \theta_0, \tau(\theta_0) \right)$, partitioned as follows:
\[
  \mathcal{V}\left( \theta_0, \tau(\theta_0) \right) = 
  \left[
  \begin{array}{cc}
    \mathcal{V}_{mm}\left( \theta_0, \tau(\theta_0) \right) & \mathcal{V}_{mh}\left( \theta_0, \tau(\theta_0) \right) \\
    \mathcal{V}_{hm}\left( \theta_0, \tau(\theta_0) \right) & \mathcal{V}_{hh}\left( \theta_0, \tau(\theta_0) \right) \\
  \end{array}
\right].
\]
Thus, we calculate the asymptotic variance matrix $\Sigma\left(\theta_0, \widehat{\tau}_n(\theta_0)\right)$ of $\sqrt{n}\; \bar{m}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right)$ as follows, 
suppressing dependence on $\theta$ and $\tau$ for simplicity:
\begin{align*}
\Sigma &= 
\left[
\begin{array}{cc}
  \mathbf{I}_k & B
\end{array}
\right] 
\left[
\begin{array}{cc}
  \mathcal{V}_{mm} & \mathcal{V}_{mh}\\
  \mathcal{V}_{hm} & \mathcal{V}_{hh}
\end{array}
\right]
\left[
\begin{array}{c}
\mathbf{I}_k \\ B'
\end{array}
\right]
\end{align*}
So to estimate $\Sigma\left( \theta_0, \tau(\theta_0) \right)$ we require estimators of $B\left( \theta_0, \tau(\theta_0) \right)$ and $\mathcal{V}\left( \theta_0, \tau(\theta_0) \right)$.
In our example, $\tau(\theta_0)$ is just-identified, leading to the following simplification:
\[
  B = -M\left( H'\Xi H\right)^{-1} H'\Xi 
  = -M\left( H' H\right)^{-1} H' 
  = -M H^{-1} (H')^{-1} H' = -M H^{-1}.
\]
Thus, we can construct the desired variance matrix estimator $\widehat{\Sigma}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right)$ as follows:
\begin{align*}
  \widehat{\mathcal{V}}_n\left( \theta_0\right) &= \frac{1}{n} \sum_{i=1}^n \left[ g\left(W_i, \theta_0, \widehat{\tau}_n(\theta_0)\right) - \bar{g}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right) \right] \left[ g\left(W_i, \theta_0, \widehat{\tau}_n(\theta_0)\right) - \bar{g}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right) \right]'\\
  \widehat{M}_n\left( \theta_0 \right) &= \frac{1}{n} \sum_{i=1}^n \frac{\partial m\left( W_i, \theta_0, \widehat{\tau}_n(\theta_0) \right)}{\partial \tau'}\\
  \widehat{H}_n\left( \theta_0 \right) &= \frac{1}{n} \sum_{i=1}^n \frac{\partial h\left( W_i, \theta_0, \widehat{\tau}_n(\theta_0) \right)}{\partial \tau'}\\
  \widehat{A}_n(\theta_0) &= \left[
  \begin{array}{cc}
    \mathbf{I}_k &  -\widehat{M}_n(\theta_0) \widehat{H}_n^{-1}(\theta_0)\\
  \end{array}
\right]\\
\widehat{\Sigma}_n\left( \theta_0, \widehat{\tau}_n(\theta_0) \right) &= \widehat{A}_n(\theta_0) \widehat{\mathcal{V}}_n(\theta_0) \widehat{A}_n(\theta_0)'
\end{align*}
Notice that if $M = 0$, so that the $m$ moment functions do not depend on $\tau$, then this reduces to the expression from Andrews \& Soares \emph{without} preliminary estimation of identified parameters.

\section{May 20th -- A\&S (2010) w/ Preliminary Estimates}
\todo[inline]{Notice that in this special case we have $\theta_2 = \beta \theta_1$. Since $\theta_1$ is strongly identified, this means that we could concentrate it out and conduct inference for $\beta$ alone.
But is this a good idea? It would mean that the inequalities don't give us any information. We should possibly try this out in simulations and see what happens. Note that the third moment condition simplifies in this example as well, so that $\theta_3 = \beta^2 \theta_1$. We could also try using this to see how valuable the over-identifying information can be. Could be interesting to try the same thing using quantiles to get over-identifying moment equalities. Can one carry out A\&S (2010) using a continuum of moment equalities and inequalities? Could at least try it out with deciles.}

\paragraph{Simple Example:}
We now return to the simple example of Andrews \& Soares from above, in which we assume that $\alpha_0 = 0$ and use only the ``weak'' bounds for $\alpha_1$.
The difference is that we will now estimate both intercepts: $\kappa_1$ and $\kappa_2$.
The moment equalities in this case are:
\[
  \mathbb{E}\left[ u_1(\kappa_1, \theta_1) \right] = 0, \quad
  \mathbb{E}\left[ u_1(\kappa_1, \theta_1) z \right] = 0, \quad
  \mathbb{E}\left[ u_2(\kappa_2, \theta_1, \theta_2) \right] = 0, \quad
  \mathbb{E}\left[ u_2(\kappa_2, \theta_1, \theta_2) z \right] = 0
\]
where
\begin{align*}
u_1(\kappa_1, \theta_1) &= y - \kappa_1 - \theta_1 T & \theta_1 &= \beta/(1 - \alpha_1)\\
  u_2(\kappa_2, \theta_1, \theta_2) &= y^2 - \kappa_2 - \theta_1 2 yT + \theta_2 T & \theta_2 &= \beta^2/(1 - \alpha_1) = \beta \theta_1
\end{align*}

Using the notation of the preceding section, the $h$ block of moment equalities used for preliminary estimation of $\tau = (\kappa_1, \kappa_2)$ is 
\[
  h(W_i, \theta, \kappa) = \left[
  \begin{array}{l}
    u_{1,i}(\kappa_1,\theta_1)\\
    u_{2,i}(\kappa_2, \theta_1, \theta_2)\\
  \end{array}
\right] = 
\left[
\begin{array}{l}
  y_i - \kappa_1 - \theta_1 T_i\\
  y^2_i - \kappa_2 - \theta_1 2 y_iT_i + \theta_2 T_i
\end{array}
\right]
\]
while the $m$ block of moment inequalities and equalities used for inference are
\[
  m(W_i, \theta, \kappa) = \left[
  \begin{array}{l}
    (1 - \alpha_1) - T_i(1 - z_i)/(1 - q)\\
    (1 - \alpha_1) - T_iz_i/q\\
    u_{1,i}(\kappa_1, \theta_1) z_i \\
    u_{2,i}(\kappa_2, \theta_1, \theta_2) z_i
  \end{array}
\right]
   = \left[
  \begin{array}{l}
    (1 - \alpha_1) - T_i(1 - z_i)/(1 - q)\\
    (1 - \alpha_1) - T_iz_i/q\\
    (y_i - \kappa_1 - \theta_1 T_i) z_i\\
    (y_i^2 - \kappa_2 - \theta_1 2 y_i T_i + \theta_2 T_i) z_i 
  \end{array}
\right]
\]
Under the null $(\beta, \alpha_1) = (\beta^0, \alpha_1^0)$, or equivalently $(\theta_1, \theta_2) = (\theta_1^0, \theta_2^0)$, we can estimate $\kappa_1$ and $\kappa_2$:
\begin{align*}
  \widehat{\kappa}_1(\theta_0) &= \frac{1}{n} \sum_{i=1}^n y_i - \theta_1^0 T_i\\
  \widehat{\kappa}_2(\theta_0) &= \frac{1}{n}\sum_{i=1}^n y^2_i - \theta_1^0 2y_i T_i + \theta_2^0 T_i 
\end{align*}
Note that this estimator is just identified, so the simplification $B = -MH^{-1}$ obtains.
To calculate this quantity, we first need the expected derivative matrices: 
\[
  H = \mathbb{E}\left[ \frac{\partial h}{\partial \kappa'} \right] = 
  \left[
  \begin{array}{rr}
    -1 & 0 \\
    0 & -1
  \end{array}
\right] = -\mathbf{I}_2
\]
and 
\[
  M = \mathbb{E}\left[ \frac{\partial m}{\partial \kappa'} \right] = 
  \left[
  \begin{array}{rr}
    0 & 0 \\ 
    0 & 0 \\ 
    -q & 0 \\
    0 & -q 
  \end{array}
\right] = 
-q \left[
\begin{array}{c}
  \mathbf{0}_{2} \\ \mathbf{I}_2
\end{array}
\right]
\]
which imply 
\[
  B = -MH^{-1} = -q \left[
  \begin{array}{cc}
    \mathbf{0}_2\\ \mathbf{I}_2
  \end{array}
\right] \mathbf{I}_2^{-1} = -q \left[
\begin{array}{c}
  \mathbf{0}_{2} \\ \mathbf{I}_2
\end{array}
\right]
\]
Notice that in our example $B$ does not depend on parameters so it does not have to be re-computed for different null hypotheses.
And since $A = \left[
\begin{array}{cc}
  I & B
\end{array}
\right]$,
the same is true of $A$.
In contrast, $\widehat{\mathcal{V}}_n(\theta_0)$ \emph{does} depend on the null:
\[
  \widehat{\mathcal{V}}_n(\theta_0) = \frac{1}{n}\sum_{i=1}^n 
  \left[
  \begin{array}{c}
    m(W_i, \theta_0, \widehat{\kappa}(\theta_0)) - \bar{m}_n(\theta_0, \widehat{\kappa}(\theta_0))\\
    h(W_i, \theta_0, \widehat{\kappa}(\theta_0)) - \bar{h}_n(\theta_0, \widehat{\kappa}(\theta_0))
  \end{array}
\right]
  \left[
  \begin{array}{c}
    m(W_i, \theta_0, \widehat{\kappa}) - \bar{m}_n(\theta_0, \widehat{\kappa})\\
    h(W_i, \theta_0, \widehat{\kappa}) - \bar{h}_n(\theta_0, \widehat{\kappa})
  \end{array}
\right]'
\]
Finally, we set $\widehat{\Sigma}_n\left( \theta_0, \widehat{\kappa}(\theta_0) \right) = A \widehat{\mathcal{V}}_n A'$ and use this in the Andrews \& Soares (2010) GMS test to correctly account for the fact that $\kappa_1$ and $\kappa_2$ have been estimated in a preliminary GMM step.

%\section{May 21st, 2017 -- Adding Second Moment Bounds}
%Continuing from the preceding section, we now add the second moment bounds for $\alpha_1$ from above:
%\begin{align*}
%  p_k (1 - p_k)(1 - p_k - \alpha_1) \left\{ (1  - \alpha_1)(1 - p_k)\mathbb{E}\left[ y^2|T=0,z_k \right] - \alpha_1 p_k \mathbb{E}[y^2|T=1,z_k \right\}  \\
%    > \left\{ (1 - \alpha_1)(1 - p_k)\mathbb{E}[y|T=0,z_k] - \alpha_1 p_k \mathbb{E}[y|T=1,z_k] \right\}^2
%\end{align*}
%where $k = 0, 1$.
%We continue, for the moment, to assume that $\alpha_0 = 0$.
%Recall that
%\begin{align*}
%  \mathbb{E}\left[ y^r |T=0, z=k\right] &= \mathbb{E}\left[ y^r(1 - T) \mathbf{1}(z=k) \right]/\left[ (1 - p_k) \mathbb{P}(z=k)\right]\\
%  \mathbb{E}\left[ y^r |T=1, z=k\right] &= \mathbb{E}\left[ y^r T \mathbf{1}(z=k) \right]/\left[p_k \mathbb{P}(z=k)\right]
%\end{align*}
%from which we obtain:
%\begin{align*}
%  (1 - p_k)\mathbb{E}[y|T=0, z=0] &= \mathbb{E}[y(1 - T)(1 - z)]/(1 -q)\\
%  p_k \mathbb{E}[y|T=1, z=0] &= \mathbb{E}[yT(1 - z)]/(1 -q)\\
%  (1 - p_k)\mathbb{E}[y^2|T=0, z=0] &= \mathbb{E}[y^2(1 - T)(1 - z)]/(1 -q)\\
%  p_k\mathbb{E}[y^2|T=1, z=0] &= \mathbb{E}[y^2T(1 - z)]/(1 - q)
%\end{align*}
%Thus, the bound for $k = 0$ is
%\begin{align*}
% (1-q) p_0 (1 - p_0)(1 - p_0 - \alpha_1) \left\{ (1  - \alpha_1)\mathbb{E}\left[ y^2(1-T)(1-z) \right] - \alpha_1 \mathbb{E}[y^2 T(1-z)] \right\}  \\
%    > \left\{ (1 - \alpha_1)\mathbb{E}[y(1-T)(1-z)] - \alpha_1 \mathbb{E}[yT(1-z)] \right\}^2 
%\end{align*}
%Proceeding similarly for $k = 1$ we obtain
%\begin{align*}
%  (1 - p_k)\mathbb{E}[y|T=0, z=1] &= \mathbb{E}[y(1 - T)z]/q\\
%  p_k \mathbb{E}[y|T=1, z=1] &= \mathbb{E}[yTz]/q\\
%  (1 - p_k)\mathbb{E}[y^2|T=0, z=1] &= \mathbb{E}[y^2(1 - T)z]/q\\
%  p_k\mathbb{E}[y^2|T=1, z=1] &= \mathbb{E}[y^2Tz]/q
%\end{align*}
%and hence
%\begin{align*}
% q p_1 (1 - p_1)(1 - p_1 - \alpha_1) \left\{ (1  - \alpha_1)\mathbb{E}\left[ y^2(1-T)z \right] - \alpha_1 \mathbb{E}[y^2 Tz] \right\}  \\
%    > \left\{ (1 - \alpha_1)\mathbb{E}[y(1-T)z] - \alpha_1 \mathbb{E}[yTz] \right\}^2 
%\end{align*}
%To convert these bounds into moment functions of the form required by Andrews \& Soares, we introduce additional parameters for we will carry out a preliminary moment-based estimation step: 
%\begin{align*}
%  \mathbb{E}[p_0 - T(1 - z)/(1 - q)] &= 0\\
%  \mathbb{E}[p_1 - Tz/q] &= 0\\
%  \mathbb{E}[\mu_{00} - y(1 - T)(1 - z)] &= 0\\
%  \mathbb{E}[\mu_{10} - yT(1 - z)] &= 0\\
%  \mathbb{E}[\mu_{01} - y(1 - T)z] &= 0\\
%  \mathbb{E}[\mu_{11} - yTz] &= 0
%\end{align*}
%We will continue, however, to treat $q=\mathbb{P}(z=1)$ as fixed in repeated sampling.
%In terms of the newly introduced parameters $\mu_{00}, \mu_{10}, \mu_{01}$, and $\mu_{11}$, the second moment bounds become
%\begin{align*}
%  \mathbb{E}\left[(1 - p_0 - \alpha_1)y^2(1 - z)(1 - T - \alpha_1)   - \frac{\left\{ (1 - \alpha_1)\mu_{00} - \alpha_1 \mu_{10} \right\}^2}{p_0(1-p_0)(1-q)} \right] &\geq 0 \\
%   \mathbb{E}\left[(1 - p_1 - \alpha_1)y^2 z(1 - T - \alpha_1)   - \frac{\left\{ (1 - \alpha_1)\mu_{01} - \alpha_1 \mu_{11} \right\}^2}{p_1(1-p_1)q} \right]&\geq 0 
%\end{align*}
%Notice that the parameters $p_0, p_1, \mu_{00}, \mu_{01}, \mu_{10}$, and $\mu_{11}$ enter these expressions in a fairly complicated way.
%This means that the adjustment to the asymptotic variance matrix will be more involved.
%Fortunately, each of these parameters should be fairly well estimated in practice.
%Notice further that we have taken care to write these inequalities so that the \emph{second term} within the expectation is always \emph{non-positive} and there is no danger of division by zero provided that the instrument not degenerate.
%\todo[inline]{Still need to check these inequalities against the version from the other paper. They should always provide additional information beyond that in the ``weak'' bounds, however. The reason for this is hinted at in the appendix to sick-instruments, but I'll clarify it here. Consider the equality version of the condition: it's a quadratic so there are two roots: $r_1 < r_2$. (How can we be sure the roots aren't equal?) From the weak bounds, we get the sign of the leading term of the quadratic, telling us that it opens upwards. This means that the inequality is satisfied for $\alpha_1 \leq r_2$ and for $\alpha_1 \geq r_1$.
%  But we also know that there's a point in between $r_1$ and $r_2$ that is exactly the boundary for one of the weak bounds, meaning that $r_2$ must \emph{violate} one of the weak bounds. Hence the second moment bound is $\alpha_1 \leq r_1$ and this must be an improvement over the weak bound. Note that we still need the weak bound to rule out one branch of the quadratic: in practice we have to include to make sure that we don't end up in the wrong branch of the quadratic. It would be helpful to understand when the second moment bound becomes degenerate and reduces to the first moment bound. Presumably this has something to do with $\beta$ being small, but I think we need to make sure we understand this fully before preceding. For example, it might be the case that when $\beta$ is zero the second moment bounds are equal to the weak bounds, but the second moment bounds are necessarily higher in variance, so we should still want to include the first moment bounds I think.}


\section{May 22nd, 2017 -- Unsolved Mystery}
A while back we noticed that the CDF bounds appeared to identify the true $\alpha_0$ and $\alpha_1$ in our normal simulation.
At one point I tried to figure out if this was really true and if so why.
I think I wrote out a few notes on paper but didn't make much progress.
It would be good to eventually figure this out.
There are a few calculations for our simulation design above on pages 44--45 of these notes.
Maybe these could be used to figure it out.
I think the CDF bounds may be worth coming back to since one can impose them without making our stronger assumptions about the IV.
This means they could be used with Mahajan or Lewbel, for example.
One needn't use all of them in practice: something like quintile bounds could be useful.

\section{June 1--3, 2017 -- Second Moment Bounds}
Camilo triple-checked the conditional variance inequalities from our sick-instrument paper: they're definitely correct.
Moreover, in spite of the fact that we defined $u = \varepsilon + c$ in that paper, they continue to hold exactly as written since $c$ is a constant.
To see why this is the case, let $u = c + \varepsilon$.
Then
\begin{align*}
  \mbox{Var}\left( u|T,z \right) &= \mathbb{E}(u^2|T,z) - \left[ E(u|T,z) \right]^2\\
&= \mathbb{E}(\varepsilon^2|T,z) + 2c\mathbb{E}(\varepsilon|T,z) + c^2 - \left[ c + \mathbb{E}(\varepsilon|T,z)  \right]^2\\
&= \mbox{Var}(\varepsilon|T,z)
\end{align*}
Camilo then re-wrote the inequalities as follows:
\begin{align*}
  \mathbb{E}\left[ (1- p_k - \alpha_1) y^2 \mathbf{1}\left\{ z=k \right\}(1 - T - \alpha_1) - \mathbb{P}(z=k)\left\{ \alpha_1 p_k \mu_{1k} - (1 - \alpha_1)(1 - p_k)\mu_{0k} \right\}^2 \right] &>0 \\
  \mathbb{E}\left[ (p_k - \alpha_0) y^2 \mathbf{1}\left\{ z=k \right\}(T - \alpha_0) - \mathbb{P}(z=k)\left\{ (1 - \alpha_0) p_k \mu_{1k} - \alpha_0(1 - p_k)\mu_{0k} \right\}^2 \right] &>0 
\end{align*}
where $p_k = \mathbb{P}(T=1|z=k)$ and $\mu_{tk} = \mathbb{E}(y|T=t,z=k)$.
I derived one of these expressions as well (the $\alpha_1$ inequality) and my derivation matches.
To incorporate these inequalities into our inference procedure, we will need auxiliary moment conditions to estimate $\mu_{tk}$ and $p_k$.
We will continue, however, to treat $\mathbb{P}(z=k)$ as fixed in repeated sampling.

We can re-write the preceding expressions in a slightly simpler form that also makes the derivatives we'll need to take to compute the covariance matrix adjustment for the GMS test much simpler. 
Recall that 
\begin{align*}
  \mu_{0k} &= \mathbb{E}\left( y|T=0, z_k \right) = \frac{\mathbb{E}\left[ y(1 - T) \mathbf{1}(z=k) \right]}{(1 - p_k) \mathbb{P}(z=k)}\\ \\
  \mu_{1k} &= \mathbb{E}\left( y|T=1, z_k \right) = \frac{\mathbb{E}\left[ yT \mathbf{1}(z=k) \right]}{p_k \mathbb{P}(z=k)}
\end{align*}
Rearranging,
\begin{align*}
  \mathbb{P}(z=k)(1-p_k)\mu_{0k} &=  \mathbb{E}\left[ y(1 - T) \mathbf{1}(z=k) \right]\\
  \mathbb{P}(z=k)p_k\mu_{1k} &= \mathbb{E}\left[ yT \mathbf{1}(z=k) \right]
\end{align*}
Thus, defining
\begin{align*}
  m_{0k} &= \mathbb{P}(z=k)(1-p_k)\mu_{0k} =  \mathbb{E}\left[ y(1 - T) \mathbf{1}(z=k) \right]\\
  m_{1k} &= \mathbb{P}(z=k)p_k\mu_{1k} = \mathbb{E}\left[ yT \mathbf{1}(z=k) \right]
\end{align*}
by multiplying both sides by $\mathbb{P}(z=k)$ we can write the second moment inequalities as
\begin{align*}
  \mathbb{E}\left[\mathbb{P}(z=k) (1- p_k - \alpha_1) y^2 \mathbf{1}\left\{ z=k \right\}(1 - T - \alpha_1) - \left\{ \alpha_1 m_{1k} - (1 - \alpha_1)m_{0k} \right\}^2 \right] &>0 \\
  \mathbb{E}\left[ \mathbb{P}(z=k)(p_k - \alpha_0) y^2 \mathbf{1}\left\{ z=k \right\}(T - \alpha_0) - \left\{ (1 - \alpha_0) m_{1k} - \alpha_0 m_{0k} \right\}^2 \right] &>0 
\end{align*}
Thus, the six quantities for which we require preliminary estimators are
\begin{align*}
  m_{0k} &= \mathbb{E}\left[ y(1 - T) \mathbf{1}(z=k) \right]\\
  m_{1k} &= \mathbb{E}\left[ yT \mathbf{1}(z=k) \right]\\
  p_k &= \mathbb{E}[T\mathbf{1}(z=k)] / \mathbb{P}(z=k)
\end{align*}
for $k = 0, 1$.
Again, we will treat $\mathbb{P}(z=k)$ as fixed in repeated sampling.
Since $m_{tk}$ and $p_k$ are just sample means, they should be very precisely estimated.

\subsection*{Are the 2nd Moment Bounds Tighter?} 
The second moment bounds given above are equivalent to those from the sick-instruments paper, namely
\begin{align}
  \label{ineq:a0}
  (p_k - \alpha_0) \left[ (1 - \alpha_0)p_k \sigma^2_{1k} - \alpha_0 (1 - p_k)\sigma_{0k}^2 \right] &\geq \alpha_0 (1 - \alpha_0)p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2\\
  \label{ineq:a1}
  (1 - p_k - \alpha_1) \left[ (1 - \alpha_1)(1 - p_k) \sigma^2_{0k} - \alpha_1 p_k\sigma_{1k}^2 \right] &\geq \alpha_1 (1 - \alpha_1)p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2
\end{align}
where I have re-written $\bar{y}_{tk}$ as $\mu_{tk}$ to match the notation we use in our derivations above and allowed for the possibility that the bounds are not strict.
(This is a degenerate situation in which an unobservable variance is zero, but I just want to consider all possibilities!)
We will now argue that these bounds must generically be tighter than the ``weak'' bounds that use only $p_k$.
This holds even if $\beta = 0$.
We will proceed by analyzing the quadratic equations along which the preceding expressions hold with equality.
Throughout the following argument we assume that $p_k \neq 0,1$ and that \emph{at least one} of $\sigma^2_{0k}, \sigma^2_{1k}$ is strictly positive. 

\paragraph{The Bounds for $\alpha_0$}
Rearranging, we can write Inequality \ref{ineq:a0} as $\varphi_k(\alpha_0) \geq 0$ where
\begin{align*}
  \varphi_k(\alpha_0) &= A_k \alpha_0^2 + B^0_k \alpha_0 + C^0_k \\
  A_k &= p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2 + (1 - p_k) \sigma_{0k}^2 + p_k \sigma_{1k}^2\\
  B_k^0 &= - \left[ \sigma_{1k}^2 p_k(1 + p_k) + p_k (1 - p_k)\sigma_{0k}^2 + p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2 \right]\\
  C_k^0 &= p_k^2 \sigma_{1k}^2. 
\end{align*}
Since we assume that $p_k \neq 0,1$ and that at least one of $\sigma_{0k}^2,\sigma^2_{1k}$ is positive $A_k > 0$.
Thus the quadratic function $\varphi_k(\alpha_0)$ opens upwards.
Now, if $\alpha_0 = 0$ then the RHS of Inequality \ref{ineq:a0} becomes zero while the LHS becomes $p_k^2 \sigma_{1k}^2$.
Thus, Inequality \ref{ineq:a0} is always satisfied when $\alpha_0 = 0$.
Similarly $\alpha_0 = 1$, the RHS is again zero while the LHS becomes $(1 - p_k)^2\sigma_{0k}^2$.
Thus, inequality \ref{ineq:a0} is always satisfied when $\alpha_0 = 1$.
Since $\alpha_0$ is a probability it follows that, so long as $\varphi_k$ has two distinct roots $r_1 < r_2$, Inequality \ref{ineq:a0} is satisfied if and only if $\alpha_0 \in [0, r_1]$ or $\alpha_0 \in [r_2, 1]$.
We now show that $\varphi$ generically has two distinct roots, that the bound involving $r_2$ is extraneous, that that $r_1$ is generically strictly smaller than $p_k$ so that the second moment bound is tighter than the weak bound $\alpha_0 \leq p_k$ unless $\mu_{1k} = \mu_{0k}$ in which case the weak and second moment bounds coincide.

If $\alpha_0 = p_k$, the LHS of \ref{ineq:a0} becomes zero.
There are two cases.
Suppose first that $\mu_{1k} \neq \mu_{0k}$.
In this case, the RHS of the inequality becomes becomes $p_k^2(1-p_k)^2(\mu_{1k} - \mu_{0k})^2 >0$ so the inequality is violated.
Thus, $\mu_{1k} \neq \mu_{0k}$ implies that $\varphi_k$ has two distinct roots and that $p_k$ is strictly \emph{between} them.
Since the weak bound gives us $\alpha_0 < p_k$, the bound arising from the larger of the two roots is extraneous.
Now suppose that $\mu_{1k}=\mu_{0k}$.
In this case the RHS and LHS of the inequality are both zero so $\alpha_0 = p_k$ is a root of $\varphi_k$.
When $\mu_{1k}=\mu_{0k}$, the coefficients of $\varphi_k$ become
\begin{align*}
  a &= (1 - p_k) \sigma_{0k}^2 + p_k \sigma_{1k}^2 = \sigma_{0k}^2 + p_k(\sigma_{1k}^2 - \sigma_{0k}^2)\\
  b &= - \left[ \sigma_{1k}^2 p_k(1 + p_k) + p_k (1 - p_k)\sigma_{0k}^2 \right] = -\left[ p_k\sigma_{1k}^2 + p_k \sigma_{0k}^2 + p_k^2(\sigma_{1k}^2 - \sigma_{0k}^2)  \right] = -p_k\left(\sigma_{1k}^2 + a  \right) \\
  c &= p_k^2 \sigma_{1k}^2. 
\end{align*}
Thus, we find that
\begin{align*}
  b^2 - 4ac &= p_k^2 (\sigma_{1k}^2 + a)^2 - 4a p_k^2 \sigma_{1k}^2 = p_k^2 \left[ \sigma_{1k}^4 + 2a \sigma_{1k}^2 + a^2 - 4a \sigma_{1k}^2 \right]\\
  &= p_k^2\left( \sigma_{1k}^2 - a \right)^2
\end{align*}
so the roots of the quadratic are
\[
  \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{p_k(\sigma_{1k}^2 + a) \pm p_k(\sigma_{1k}^2 - a)}{2a} = \left\{\frac{p_k \sigma_{1k}^2}{a}, \, p_k  \right\}
\]
Substituting the definition of $a$,
\[
  \frac{p_k\sigma_{1k}^2}{a} = \frac{p_k \sigma_{1k}^2}{p_k \sigma_{1k}^2 + (1 - p_k) \sigma_{0k}^2}
\]
Rearranging, $p_k$ is the \emph{smaller root} when $\sigma_{0k}^2<\sigma_{1k}^2$ and the \emph{larger root} when the inequality is reversed.
When $\sigma_{0k}^2 = \sigma_{1k}^2$, the two roots are equal.

\paragraph{Bounds for $\alpha_1$}
Proceeding analogously, we can write Inequality \ref{ineq:a1} as $\psi_k(\alpha_1)\geq 0$ where
\begin{align*}
  \psi_k(\alpha_1) &= A_k \alpha_1^2 + B_k^1 \alpha_1 + C_k^1\\ 
  A_k &= p_k (1 - p_k)(\mu_{1k} - \mu_{0k})^2 + (1 - p_k) \sigma_{0k}^2 + p_k \sigma_{1k}^2\\
  B^1_{k} &= - \left[p_k (1 - p_k) \sigma_{1k}^2 + (1 - p_k)(2 - p_k)\sigma_{0k}^2 + p_k(1 - p_k)(\mu_{1k} - \mu_{0k})^2 \right]\\
  C^1_{k} &= (1 - p_k)^2 \sigma_{0k}^2.
\end{align*}
Note that the coefficient on the quadratic term is \emph{the same} for $\psi_k$ and $\varphi_k$. 
When $\mu_{1k} = \mu_{0k}$, the coefficients of $\psi_k$ become
\begin{align*}
  a &= (1 - p_k)\sigma_{0k}^2 + p_k \sigma_{1k}^2 = \sigma_{0k}^2 + p_k(\sigma_{1k}^2 - \sigma_{0k}^2)\\
  b &= -\left[ p_k(1 - p_k)\sigma_{1k}^2 + (1 - p_k)(2 - p_k)\sigma_{0k}^2 \right] = -(1 - p_k)\left[ p_k(\sigma_{1k}^2 - \sigma_{0k}^2) + 2 \sigma_{0k}^2 \right] = -(1 - p_k)(\sigma_{0k}^2 + a)\\
  c &= (1 - p_k)^2 \sigma_{0k}^2
\end{align*}

\begin{align*}
  b^2 - 4ac &= (1 - p_k)^2(\sigma_{0k}^2 + a)^2 - 4 a (1 - p_k)^2 \sigma_{0k}^2 = (1 - p_k^2)\left[ (\sigma_{0k}^4 + 2a \sigma_{0k}^2 + a^2) - 4 a \sigma_{0k}^2 \right] \\
  &= (1 - p_k)^2 \left( \sigma_{0k}^4 - 2 a \sigma_{0k}^2 + a^2 \right) = (1 - p_k)^2 (\sigma_{0k}^2 - a)^2
\end{align*}

\[
  \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{(1 - p_k)(\sigma_{0k}^2 + a) \pm (1 - p_k)(\sigma_{0k}^2 - a)}{2a} = \left\{ \frac{(1 - p_k)\sigma_{0k}^2}{a},\; (1 - p_k) \right\}
\]

\[
  \frac{(1 - p_k)\sigma_{0k}^2}{a} = \frac{(1 - p_k)\sigma_{0k}^2}{(1 - p_k)\sigma_{0k}^2 + p_k \sigma_{1k}^2}
\]
Rearranging, $1 - p_k$ is the \emph{smaller root} when $\sigma_{1k}^2<\sigma_{0k}^2$ and the \emph{larger root} when the inequality is reversed.
When $\sigma_{0k}^2 = \sigma_{1k}^2$, the two roots are equal.

\paragraph{What can we say about the case where $\mu_{1k} = \mu_{0k}$?}
Even if $\mu_{1k} = \mu_{0k}$, the second moment bounds are generically strictly better than the weak bounds.
Suppose first that $\sigma_{0k}^2 \neq \sigma_{1k}^2$.
If $\sigma_{0k}^2 < \sigma_{1k}^2$ then $p_k$ is the smaller root of $\varphi_k$ but $(1 - p_k)$ is the \emph{larger} root of $\psi_k$ so the second moment bound for $\alpha_1$ is strictly better than the weak bound $\alpha_1 \leq 1 - p_k$.
If instead $\sigma_{1k}^2 < \sigma_{0k}^2$, then $(1 - p_k)$ is the smaller root of $\psi_k$ but $p_k$ is the \emph{larger} root of $\varphi_k$ so the second moment bound for $\alpha_0$ is strictly better than the weak bound $\alpha_0 \leq p_k$.
Only in the non-generic case where $\mu_{0k} = \mu_{1k}$ \emph{and} $\sigma^2_{0k} = \sigma_{1k}^2$ for all $k$ do the weak bounds and second moment bounds coincide.
This turns out to imply that the second moment bounds remain informative \emph{even if the treatment effect is zero}.
If $\beta = 0$ then $y = c + \varepsilon$ so $\mu_{tk} = c + \mathbb{E}(\varepsilon|T=t,z=k)$ and hence
\[
  0 = \mu_{1k} - \mu_{0k} = \mathbb{E}(\varepsilon|T=1,z_k) - \mathbb{E}(\varepsilon|T=0,z_k)
\]
Thus, if $\mu_{0k} = \mu_{1k}$ then we must have $\mathbb{E}(\varepsilon|T=1,z_k) = \mathbb{E}(\varepsilon|T=0,z_k)$ for all $k$.
But by iterated expectations and the assumption of non-differential measurement error we have
\begin{align*}
  \mathbb{E}(\varepsilon|T=1, z_k) &= \mathbb{E}(\varepsilon|T^*=1,z_k)\mathbb{P}(T^*=1|T=1,z_k) + \mathbb{E}(\varepsilon|T^*=0,z_k)\mathbb{P}(T^*=0|T=1,z_k)\\
  \mathbb{E}(\varepsilon|T=0, z_k) &= \mathbb{E}(\varepsilon|T^*=1,z_k)\mathbb{P}(T^*=1|T=0,z_k) + \mathbb{E}(\varepsilon|T^*=0,z_k)\mathbb{P}(T^*=0|T=0,z_k)
\end{align*}
Since $\mathbb{E}(\varepsilon|T=1,z_k) = \mathbb{E}(\varepsilon|T=0,z_k)$, these equations are a linear system of the form
\begin{align*}
  c &= p x + (1 - p)y\\
  c &= q x + (1 - q)y
\end{align*}
where 
\begin{align*}
  p &= \mathbb{P}(T^*=1|T=1,z_k) = (1 - \alpha_1) \frac{p_k^*}{p_k}\\
  q &= \mathbb{P}(T^*=1|T=0,z_k) = \alpha_1 \left(\frac{p_k^*}{1 - p_k}\right)\\
  x &= \mathbb{E}(\varepsilon|T^*=1,z_k)\\
  y &= \mathbb{E}(\varepsilon|T^*=0,z_k)
\end{align*}
Thus, unless $p = q$, we must have $\mathbb{E}(\varepsilon|T^*=1,z_k) = \mathbb{E}(\varepsilon|T^*=0,z_k)$ for all $k$.
But since $z$ is a valid instrument, $\mathbb{E}(\varepsilon|z_k) = 0$ for all $k$ and thus, by iterated expectations
\begin{align*}
  0 &= \mathbb{E}(\varepsilon|z_k) = \mathbb{E}_{T^*|z_k}\left[ \mathbb{E}(\varepsilon|T^*,z_k) \right]\\
  &= p^*_k \mathbb{E}(\varepsilon|T^*=1,z_k) + (1 - p_k^*) \mathbb{E}(\varepsilon|T^*=0,z_k)\\
  &= \left[ p_k^* + (1 - p_k^*) \right] \mathbb{E}(\varepsilon|T^*=1,z_k)\\
  &= \mathbb{E}(\varepsilon|T^*=1,z_k)\\
  &= \mathbb{E}(\varepsilon|T^*=0,z_k)
\end{align*}
for all $k$. 
Thus, even if $\beta = 0$ it will \emph{still} not in general be true that $\mu_{1k} = \mu_{0k}$.
For this to be the case we require the additional condition that $\mathbb{E}(\varepsilon|T^*_t,z = k) = 0$ for all $t,k$.
In other words, we require $z$ and $T^*$ to be \emph{jointly} first moment independent of $\varepsilon$.
This implies $\mathbb{E}(\varepsilon|T^*)=0$, i.e.\ that $T^*$ is \emph{exogenous}.

Now suppose that $\beta = 0$ \emph{and} $z,T^*$ are jointly mean independent of $\varepsilon$.
The second moment bounds are \emph{still} strictly better than the weak bounds provided that $\sigma_{0k}^2 \neq \sigma_{1k}^2$.
From a proof in the appendix of the sick instruments paper,
\begin{eqnarray*}
  \sigma^2_{1k}&=&  \frac{(1 - \alpha_1) p_k^* }{p_k} s^{*2}_{1k} + \frac{\alpha_0(1 - p_k^*)}{p_k} s^{*2}_{0k} + \frac{\alpha_0 ( 1 - \alpha_1) (1 - p_k)^2 \left( \mu_{1k} - \mu_{0k} \right)^2}{(p_k - \alpha_0)(1 - p_k - \alpha_1)}\\
  \sigma^2_{0k} &=&  \frac{\alpha_1 p_k^* }{1 - p_k} s^{*2}_{1k} + \frac{(1 - \alpha_0)(1 - p_k^*)}{1 - p_k} s^{*2}_{0k} + \frac{\alpha_1 ( 1 - \alpha_0) p_k^2 \left( \mu_{1k} - \mu_{0k} \right)^2}{(p_k - \alpha_0)(1 - p_k - \alpha_1)} 
\end{eqnarray*}
where $s^{*2}_{tk} = \mbox{Var}(\varepsilon|T^*=t, z=k)$.\footnote{See the explanation above for why $\mbox{Var}(u|T^*,z) = \mbox{Var}(\varepsilon|T^*,z)$.}
If $\mu_{1k} = \mu_{0k}$ this reduces to 
\begin{eqnarray*}
  \sigma^2_{1k}&=&  \frac{(1 - \alpha_1) p_k^* }{p_k} s^{*2}_{1k} + \frac{\alpha_0(1 - p_k^*)}{p_k} s^{*2}_{0k} \\
  \sigma^2_{0k} &=&  \frac{\alpha_1 p_k^* }{1 - p_k} s^{*2}_{1k} + \frac{(1 - \alpha_0)(1 - p_k^*)}{1 - p_k} s^{*2}_{0k}
\end{eqnarray*}
In other words,
\begin{eqnarray*}
  \sigma^2_{1k}&=&  \mathbb{P}(T^*=1|T=1,z_k) s^{*2}_{1k} + \mathbb{P}(T^*=0|T=1,z_k) s^{*2}_{0k} \\
  \sigma^2_{0k} &=&  \mathbb{P}(T^*=1|T=0,z_k) s^{*2}_{1k} + \mathbb{P}(T^*=0|T=0,z_k) s^{*2}_{0k}
\end{eqnarray*}
So if $\sigma_{0k}^2 = \sigma_{1k}^2$, we have a linear system of the form
\begin{align*}
  c &= p x + (1 - p)y\\
  c &= q x + (1 - q)y
\end{align*}
as above, where $p$ and $q$ are defined as before but $x = s^{*2}_{1k}$ and $y = s^{*2}_{0k}$.
Again, unless $p = q$, we must have $x = y$ which in this case means $s^{*2}_{0k} = s^{*2}_{1k}$ for all $k$.
Now, since 
\[
  s^{*2}_{tk} = \mathbb{E}(\varepsilon^2|T^*=t,z_k) - \left[ \mathbb{E}(\varepsilon|T^*=t,z_k) \right]^2
\]
under the assumption that $\mathbb{E}(\varepsilon|T^*=0,z_k) = \mathbb{E}(\varepsilon|T^*=1,z_k)$, we see that
\[
  s_{1k}^{*2} - s_{0k}^{*2} = \mathbb{E}(\varepsilon^2|T^*=1,z_k) - \mathbb{E}(\varepsilon^2|T^*=0,z_k)
\]
so that $s_{1k}^{*2} = s_{0k}^{*2}$ if and only if 
\[
   \mathbb{E}(\varepsilon^2|T^*=1,z_k) = \mathbb{E}(\varepsilon^2|T^*=0,z_k)
\]
Suppose this is the case.
Recall that we have assumed $\mathbb{E}\left( \varepsilon^2|z_k \right) = \mathbb{E}(\varepsilon^2)$ for all $k$.
Thus, by iterated expectations,
\begin{align*}
  \mathbb{E}(\varepsilon^2) &= \mathbb{E}\left( \varepsilon^2|z_k \right) = \mathbb{E}_{T^*|z_k}\left[ \mathbb{E}\left( \varepsilon|T^*,z_k \right) \right]\\
  &= p_k^* \mathbb{E}\left( \varepsilon^2|T^*=1, z_k \right) + (1 - p_k^*) \mathbb{E}\left( \varepsilon^2|T^*=0, z_k \right)\\
  &= \left[p_k^* + (1 - p_k^*)\right] \mathbb{E}\left( \varepsilon^2|T^*=1, z_k \right)\\
  &= \mathbb{E}\left( \varepsilon^2|T^*=1, z_k \right)\\
  &= \mathbb{E}\left( \varepsilon^2|T^*=0, z_k \right)
\end{align*}
and thus $\mathbb{E}(\varepsilon^2|T^*=t,z=k)=\mathbb{E}(\varepsilon^2)$ for all $t,k$.
This implies $\mathbb{E}(\varepsilon^2|T^*)=\mathbb{E}(\varepsilon^2)$.

To summarize, we have shown the following:
\begin{enumerate}
  \item Unless $\mu_{1k} = \mu_{0k}$ \emph{and} $\sigma_{1k}^2 = \sigma^2_{0k}$ for all $k$, then the second moment bounds are strictly tighter for at least one of $\alpha_0$ and $\alpha_1$.
  \item If $\beta = 0$, the second moment bounds are still generically tighter.
    In this case, the only way to obtain $\mu_{0k} = \mu_{1k}$ and $\sigma_{0k}^2 = \sigma_{1k}^2$ for all $k$ is if $\mathbb{E}(\varepsilon|T^*,z)=0$ and $\mathbb{E}(\varepsilon^2|T^*,z) = \mathbb{E}(\varepsilon^2)$.
    These conditions in turn imply $\mathbb{E}(\varepsilon|T^*)=0$ and $\mathbb{E}(\varepsilon^2|T^*) = \mathbb{E}(\varepsilon^2)$.
    So even if $\beta = 0$, the second moment bounds are tighter unless $T^*$ is exogenous \emph{and} $\varepsilon$ is homoskedastic with respect to $T^*$.
\end{enumerate}

\section{June 4, 2017: Inference Procedure (no covariates)}
We now describe the full inference procedure for our model, including preliminary estimation of strongly identified parameters and using both the weak and second moment bounds.
We place no restriction on $\alpha_0$ and $\alpha_1$, but assume for the moment that there are no covariates.


\paragraph{Overview and Notation}
We test the null hypothesis $\boldsymbol{\theta} = \boldsymbol{\theta}_0$ for a vector $\boldsymbol{\theta}$ of weakly identified parameters based on a set of equality moment conditions $m^E$ and inequality moment conditions $m^I$
\[
  \mathbb{E}\left[ m^E\left(W_i, \boldsymbol{\theta}_0, \widehat{\boldsymbol{\gamma}}_0\right) \right] = \mathbf{0}, \quad \mathbb{E}\left[ m^I(W_i, \boldsymbol{\theta}_0, \widehat{\boldsymbol{\gamma}}_0) \right] \geq \mathbf{0}
\]
where $\widehat{\boldsymbol{\gamma}}_0 = \widehat{\boldsymbol{\gamma}}(\theta_0)$ is preliminary estimator of the strongly identified parameters $\boldsymbol{\gamma}$ constructed from the moment equalities $\mathbb{E}\left[ h(W_i, \boldsymbol{\theta}_0) - \gamma \right]=0$.

\paragraph{Moment Equality Conditions}
The moment functions $m^E$ are defined as 
\begin{align*}
  m^E(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=  \mathbf{u}_i(\boldsymbol{\theta}, \boldsymbol{\kappa}) z_i\\
  \mathbf{u}_i(\boldsymbol{\theta}, \boldsymbol{\kappa}) &= 
  \left[
  \begin{array}{l}
 y_i - \kappa_1 - \theta_1 T_i\\
 y^2_i - \kappa_2 - \theta_1 2 y_i T_i + \theta_2 T_i\\
 y^3_i - \kappa_3 - \theta_1 3y_i^2 T + \theta_2 3y_iT_i - \theta_3 T_i
  \end{array}
\right]
\end{align*}
where $\boldsymbol{\kappa}' = (\kappa_1, \kappa_2, \kappa_3)$ and  $\boldsymbol{\theta}' = (\theta_1, \theta_2, \theta_3)$ with
\begin{align*}
  \theta_1 &= \beta/(1 - \alpha_0 - \alpha_1)\\
  \theta_2 &= \theta_1^2 \left[ 1 + (\alpha_0 - \alpha_1) \right]\\
  \theta_3 &= \theta_1^3\left[ \left( 1 - \alpha_0 - \alpha_1 \right)^2 + 6\alpha_0\left( 1 - \alpha_1 \right) \right].
\end{align*}

\paragraph{Moment Functions for Preliminary Estimator}
The moment functions $h$ for the preliminary estimator of $\boldsymbol{\gamma}' = (\mathbf{p}', \boldsymbol{\nu}', \boldsymbol{\kappa}')$ are
\begin{align*}
  h(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &= h(W_i,\boldsymbol{\theta}) - \boldsymbol{\gamma} =  \left[
  \begin{array}{l}
    h_p(W_i) - \mathbf{p}\\
    h_\nu(W_i) - \boldsymbol{\nu}\\
    h_\kappa(W_i,\boldsymbol{\theta}) - \boldsymbol{\kappa}
  \end{array}
\right]\\
h_p(W_i) - \mathbf{p} &= \left[
\begin{array}{l}
  T_i (1-z_i)/(1-q)\\ 
  T_i z_i / q 
\end{array}
\right] - \left[
\begin{array}{c}
  p_0 \\ p_1
\end{array}
\right]\\
h_\nu(W_i) - \boldsymbol{\nu} &= \left[
\begin{array}{l}
  y_i (1 - T_i) (1-z_i) / \sqrt{1-q}\\ 
  y_i T_i (1-z_i) / \sqrt{1-q} \\
  y_i (1 - T_i) z_i / \sqrt{q} \\
  y_i T_i z_i/\sqrt{q} 
\end{array}
\right] - \left[
\begin{array}{c}
  \nu_{00} \\
  \nu_{10} \\
  \nu_{01} \\
  \nu_{11} 
\end{array}
\right]\\
h_\kappa(W_i, \boldsymbol{\theta}) - \boldsymbol{\kappa} &= \mathbf{u}_i(\boldsymbol{\theta}, \boldsymbol{\kappa})
\end{align*}
where $\mathbf{p}'= (p_0, p_1)$, $\boldsymbol{\nu}' = (\nu_{00}, \nu_{10}, \nu_{01}, \nu_{11})$, and $\boldsymbol{\kappa'} = (\kappa_1, \kappa_2, \kappa_3)$.

\paragraph{The Preliminary Estimator}
We estimate $\boldsymbol{\gamma}$ under the null $\boldsymbol{\theta} = \boldsymbol{\theta}_0$ using the just-identified system of moment functions $h$.
Specifically, 
\begin{align*}
  \widehat{\boldsymbol{\gamma}}_0' &= \widehat{\boldsymbol{\gamma}}(\boldsymbol{\theta}_0)' = \left[
\begin{array}{ccc}
  \widehat{\mathbf{p}}' & \widehat{\boldsymbol{\nu}}' &  \widehat{\boldsymbol{\kappa}}(\boldsymbol{\theta}_0)'
\end{array}
\right]\\
\widehat{\mathbf{p}} &= 
\left[
\begin{array}{c}
  \widehat{p}_0 \\ \widehat{p}_1
\end{array}
\right] = \frac{1}{n} \sum_{i=1}^n h_p(W_i) =
\frac{1}{n}\sum_{i=1}^n \left[
\begin{array}{l}
  T_i (1-z_i)/(1-q)\\
  T_i z_i / q
\end{array}
\right] \\
\widehat{\boldsymbol{\nu}} &= \left[
\begin{array}{c}
  \widehat{\nu}_{00} \\ 
  \widehat{\nu}_{10} \\ 
  \widehat{\nu}_{01} \\ 
  \widehat{\nu}_{11}
\end{array}
\right] = \frac{1}{n}\sum_{i=1}^n h_\nu(W_i) = \frac{1}{n} \sum_{i=1}^n \left[
\begin{array}{l}
  y_i (1 - T_i) (1-z_i) / \sqrt{1-q} \\
  y_i T_i (1-z_i) / \sqrt{1-q} \\
  y_i (1 - T_i) z_i / \sqrt{q} \\
  y_i T_i z_i/\sqrt{q} 
\end{array}
\right]\\
\widehat{\boldsymbol{\kappa}}(\boldsymbol{\theta}_0) &= 
\left[
\begin{array}{c}
  \widehat{\kappa}_1(\boldsymbol{\theta}_0)\\
  \widehat{\kappa}_2(\boldsymbol{\theta}_0)\\
  \widehat{\kappa}_3(\boldsymbol{\theta}_0)
\end{array}
\right] = \frac{1}{n} \sum_{i=1}^n\left[
  \begin{array}{l}
 y_i - \theta_1^0 T_i\\
 y^2_i - \theta_1^0 2 y_i T_i + \theta_2^0 T_i\\
 y^3_i - \theta_1^0 3y_i^2 T + \theta_2^0 3y_iT_i - \theta_3^0 T_i
  \end{array}
\right]
\end{align*}
where $\boldsymbol{\theta}_0 = (\theta_1^0, \theta_2^0, \theta_3^0)$.
The preliminary estimators of $\boldsymbol{p}$ and $\boldsymbol{\nu}$ do not depend on $\boldsymbol{\theta}_0$.

\paragraph{Inequality Moment Conditions}
The inequality moment functions $m^I$ are
\begin{align*}
  m^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
  \left[
  \begin{array}{l}
    m_1^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) \\ 
    m_2^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) 
  \end{array}
\right] \\
  m_1^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
  \left[
  \begin{array}{r}
  p_0 - \alpha_0 \\
  (1 - p_0) - \alpha_1  \\
  p_1 - \alpha_0 \\
  (1 - p_1) - \alpha_1  
  \end{array}
\right]\\
m_2^I(W_i, \boldsymbol{\theta}, \boldsymbol{\gamma}) &=
\left[
\begin{array}{r}
  (p_0 - \alpha_0) \widetilde{y}_i^2(\alpha_0,z=0) - d^2(\alpha_0,z=0)\\
  (1- p_0 - \alpha_1) \widetilde{y}^2_i(\alpha_1,z=0) - d^2(\alpha_1,z=0)\\
  (p_1 - \alpha_0) \widetilde{y}^2_i(\alpha_0,z=1) - d^2(\alpha_0,z=1)\\
  (1- p_1 - \alpha_1) \widetilde{y}_i^2(\alpha_1,z=1) -  d^2(\alpha_1,z=1)
\end{array}
\right]
\end{align*}
where we define the shorthand
\begin{align*}
  \widetilde{y}_i^2(\alpha_0, z=0) &= y_i^2 (1 - z_i)(T_i - \alpha_0)\\ 
  \widetilde{y}_i^2(\alpha_1, z=0) &= y^2 (1-z_i)(1 - T_i - \alpha_1) \\
  \widetilde{y}_i^2(\alpha_0, z=1) &= y^2_i z_i(T_i - \alpha_0) \\
  \widetilde{y}_i^2(\alpha_1, z=1) &= y_i^2 z_i(1 - T_i - \alpha_1)
\end{align*}
and analogously
\begin{align*}
  d(\alpha_0,z=0) &=(1 - \alpha_0) \nu_{10} - \alpha_0 \nu_{00} \\ 
  d(\alpha_1, z=0) &=\alpha_1 \nu_{10} - (1 - \alpha_1)\nu_{00} \\
  d(\alpha_0, z=1) &=(1 - \alpha_0) \nu_{11} - \alpha_0 \nu_{01} \\
  d(\alpha_1, z=1) &=\alpha_1 \nu_{11} - (1 - \alpha_1)\nu_{01}
\end{align*}




\paragraph{Correcting the Asymptotic Variance Matrix}
To account for the preliminary estimation of $\boldsymbol{\gamma}$ we need to make an adjustment to the estimator of the asymptotic covariance matrix of the moment conditions $m^I$ and $m^E$, as explained above.
Let $m' = (m^{'I}, m^{'E})$ denote the full collection of moment conditions.
Because our estimator of $\boldsymbol{\gamma}$ is just identified, the quantity required to adjust the covariance matrix estimator is $B = -MH^{-1}$ where 
\[
  H = \mathbb{E}\left[ \frac{\partial h(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \boldsymbol{\gamma}'} \right], \,
  M = \mathbb{E}\left[ \frac{\partial m(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \boldsymbol{\gamma}'} \right]  
\]
But in our example $H = -\mathbf{I}$ so that 
\[
  B = M = \mathbb{E}\left[
\begin{array}{ccc}
  \displaystyle \frac{\partial m_1^I(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'} & \mathbf{0}_{4\times 4} & \mathbf{0}_{4 \times 3}\\ \\
  \displaystyle \frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'} &
\displaystyle \frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \boldsymbol{\nu}'} &
\mathbf{0}_{4\times 3} \\ \\
\mathbf{0}_{3 \times 2} & \mathbf{0}_{3\times 4} & 
\displaystyle \frac{\partial m^E\left( W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0 \right)}{\partial \boldsymbol{\kappa}'}
\end{array}
\right]
\]
where 
\begin{align*}
\mathbb{E}\left[\frac{\partial m_1^I(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'}\right] &= 
\left[
\begin{array}{rrr}
  1 & 0 \\
  -1 & 0 \\
  0 & 1 \\
  0 & -1 \\
\end{array}
\right] \\
\mathbb{E}\left[\frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \mathbf{p}'} \right]
&= \mathbb{E}\left[
\begin{array}{rr}
  \widetilde{y}_i^2(\alpha_0, z=0) & 0 \\
  -\widetilde{y}_i^2(\alpha_1, z=0) & 0 \\
  0 & \widetilde{y}_i^2(\alpha_0, z=1)\\ 
  0 & -\widetilde{y}_i^2(\alpha_1, z=1)
\end{array}
\right]\\
\mathbb{E}\left[\frac{\partial m^I_2(W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0)}{\partial \boldsymbol{\nu}'} \right]
 &= 
 \left[
 \begin{array}{rr}
   Q_1 & \mathbf{0}_{2\times 2}\\
   \mathbf{0}_{2\times 2} & Q_2
 \end{array}
 \right]\\
 Q_1 &=\left[
\begin{array}{rr}
   2\alpha_0\, d(\alpha_0, z=0)& -2(1 - \alpha_0)\, d(\alpha_0, z=0)\\
   2(1 - \alpha_1)\, d(\alpha_1, z=0)& -2\alpha_1\, d(\alpha_1, z=0) \\
\end{array}
\right]\\
Q_2 &= \left[
\begin{array}{cc}
  2 \alpha_0 \, d(\alpha_0, z=1) &
  -2 (1 - \alpha_0) \, d(\alpha_0, z=1)\\
  2 (1 - \alpha_1)\, d(\alpha_1, z=1) &
  -2\alpha_1 \, d(\alpha_1, z=1)
\end{array}
\right] \\
\mathbb{E}\left[\frac{\partial m^E\left( W_i, \boldsymbol{\theta}_0, \boldsymbol{\gamma}_0 \right)}{\partial \boldsymbol{\kappa}'}\right] &= -q\, \mathbf{I}_3
\end{align*}
and $\alpha_0,\alpha_1$ are understood to be hypothesized values under the null $\boldsymbol{\theta} = \boldsymbol{\theta}_0$.


\end{document}
