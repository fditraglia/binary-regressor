%!TEX root = ./main.tex
\section{Identification Results}

Additively separable model
\[
  y = h(T^*,\mathbf{x})+\varepsilon
\]
where $\varepsilon$ is a mean-zero error term, $T^*$ is an endogenous binary regressor of interest and $\mathbf{x}$ is a vector of exogenous controls.
Since $T^*$ is binary, we can re-write this as linear in $T^*$ conditional on $\mathbf{x}$
\begin{align*}
  y &= c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon\\
  \beta(\mathbf{x}) &= h(1,\mathbf{x}) - h(0,\mathbf{x})\\
  c(\mathbf{x}) &= h(0,\mathbf{x})
\end{align*}
Goal is to use a discrete instrumental variable $z$ to identify $\beta(\mathbf{x})$ when we observe not $T^*$ but a mis-measured binary surrogate $T$. 
Define
\begin{align*}
  \alpha_0(\mathbf{x},z) &= \mathbb{P}\left(T=1|T^*=0,\mathbf{x},z  \right)\\
  \alpha_1(\mathbf{x},z) &= \mathbb{P}\left(T=0|T^*=1,\mathbf{x},z  \right)
\end{align*}
Identification will only rely on two values for $z$.
\todo[inline]{Be clear about the fact that, with the exception of the non-identification results, we assume throughout that $z$ is binary.}

\noindent Assumptions that would suffice to identify the model if $T^*$ were observed
\begin{assump} \mbox{}
  \label{assump:model}
  \begin{enumerate}[(i)] 
    \item $y = c(\mathbf{x}) + \beta(\mathbf{x})T^* + \varepsilon$ where $T^* \in \left\{ 0,1 \right\}$ and $\mathbb{E}[\varepsilon]=0$.
    \item  $z \in \left\{ 0,1 \right\}$, where $0 < \mathbb{P}(z=1|\mathbf{x}) < 1$, and $\mathbb{P}(T^*=1|\mathbf{x},z=1) \neq \mathbb{P}(T^*=1|\mathbf{x},z=0)$.
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z] = 0$
  \end{enumerate}
\end{assump}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Point out that even though Assumption \ref{assump:model} (ii) refers to the unobserved $T^*$, under Assumptions \ref{assump:misclassification} (i) and (ii) we have $(p_k^* - p_\ell^*)(1 - \alpha_0 - \alpha_1) = p_k - p_\ell$ so it suffices for an \emph{observed} first-stage to exist.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Stronger version of Assumption \ref{assump:model} (iii) used by Frazis \& Loewenstein, Mahajan, etc.
This assumption gives point identification without higher moment restrictions.
There should eventually be a result about this case in the paper.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{assump} \mbox{}
  \label{assump:jointExog}
    $\mathbb{E}[\varepsilon|\mathbf{x},z, T^*] = 0$
\end{assump}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent Assumptions on the mis-classification
\begin{assump} \mbox{}
  \label{assump:misclassification}
  \begin{enumerate}[(i)] 
    \item $\alpha_0(\mathbf{x},z) = \alpha_0(\mathbf{x})$,   $\alpha_1(\mathbf{x},z) = \alpha_1(\mathbf{x})$
    \item $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) <1$ ($T$ is positively correlated with $T^*$)
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon|\mathbf{x},z, T^*]$
  \end{enumerate}
\end{assump}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Explain what would happen if Assumption \ref{assump:misclassification} (ii) were weakened to $\alpha_0 + \alpha_1 \neq 1$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent Second moment assumption identifies the difference of mis-classification error rates.
\begin{assump} \mbox{}
  \label{assump:2ndMoment}
    $\mathbb{E}[\varepsilon^2|\mathbf{x},z] = \mathbb{E}[\varepsilon^2|\mathbf{x}]$ 
\end{assump}

\noindent Additional moment assumptions to get identification
\begin{assump} \mbox{}
  \label{assump:3rdMoment}
  \begin{enumerate}[(i)] 
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^2|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z] = \mathbb{E}[\varepsilon^3|\mathbf{x}]$
% The following assumption isn't actually needed! Not sure why we thought it was.
    %\item $\mathbb{E}[\varepsilon^3|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^3|\mathbf{x},z, T^*]$ 
  \end{enumerate}
\end{assump}



\paragraph{Notation}

\begin{align}
  \label{eq:theta1_def}
  \theta_1(\mathbf{x}) &= \beta(\mathbf{x})\left[ 1 - \left\{ \alpha_0(\mathbf{x}) + \mathbf{\alpha}_1(\mathbf{x}) \right\} \right]^{-1}\\
  \label{eq:theta2_def}
  \theta_2(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^2 \left[ 1 + \left\{\alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})\right\}\right] \\
  \label{eq:theta3_def}
  \theta_3(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^3\left[ \left( 1 - \left\{\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x})\right\} \right)^2 + 6\alpha_0(\mathbf{x})\left\{ 1 - \alpha_1(\mathbf{x}) \right\} \right]
\end{align}

\begin{align*}
  \mbox{Cov}(y,z|\mathbf{x}) - \mbox{Cov}(T,z|\mathbf{x}) \theta_1(\mathbf{x}) &= 0\\
  \mbox{Cov}(y^2,z|\mathbf{x}) - 2\mbox{Cov}(yT,z|\mathbf{x}) \theta_1(\mathbf{x}) + \mbox{Cov}(T,z|\mathbf{x})\theta_2(\mathbf{x}) &= 0\\
  \mbox{Cov}(y^3,z|\mathbf{x}) - 3 \mbox{Cov}(y^2T,z|\mathbf{x}) \theta_1(\mathbf{x}) + 3\mbox{Cov}(yT,z|\mathbf{x}) \theta_2(\mathbf{x}) - \mbox{Cov}(T,z|\mathbf{x}) \theta_3(\mathbf{x})&= 0
\end{align*}

\begin{align*}
  q(\mathbf{x}) &= \mathbb{P}(z=1|\mathbf{x})\\
  \pi(\mathbf{x}) &= \mbox{Cov}(T,z|\mathbf{x})\\
  \eta_j(\mathbf{x}) &= \mbox{Cov}(y^j,z|\mathbf{x})\\
  \tau_j(\mathbf{x}) &= \mbox{Cov}(Ty^j,z|\mathbf{x})
\end{align*}

\begin{align*}
 \eta_1(\mathbf{x}) &= \pi(\mathbf{x})\theta_1(\mathbf{x}) \\
  \eta_2(\mathbf{x}) &=  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x}) \\
  \eta_3(\mathbf{x}) &=  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})
\end{align*}



\noindent Both of the following two lemmas are known results from the literature.
The first gives the relationship between the observed first stage probabilities and their unobserved counterparts,
\begin{lem}
  Under Assumption \ref{assump:misclassification}, 
\begin{align*}
  \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right]\mathbb{P}(T^*=1|\mathbf{x}, z=k) &= \mathbb{P}(T=1|\mathbf{x},z=k) - \alpha_0(\mathbf{x})\\
  \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right]\mathbb{P}(T^*=0|\mathbf{x}, z=k) &= \mathbb{P}(T=0|\mathbf{x},z=k) - \alpha_1(\mathbf{x})
\end{align*}
\label{lem:p_pstar}
\end{lem} 

\begin{proof}
  Simple calculation using law of total probability.
\end{proof}

\noindent The second shows that the IV estimand does not equal $\beta$ in the presence of mis-classification
\begin{lem}
  Let $\eta_1(\mathbf{x}) = \mbox{Cov}(y,z|\mathbf{x})$ and $\pi(\mathbf{x}) = \mbox{Cov}(T,z|\mathbf{x})$.  
  Then, under Assumptions \ref{assump:model} and \ref{assump:misclassification}, $\eta_1(\mathbf{x}) = \pi(\mathbf{x}) \theta_1(\mathbf{x})$ where $\theta_1(\mathbf{x})$ is as defined in Equation \ref{eq:theta1_def}.
  \label{lem:eta1}
\end{lem}

\begin{proof}
  Immediate since $\mbox{Cov}(T,y|\mathbf{x}) = \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right] \mbox{Cov}(T^*,z|\mathbf{x})$ by Lemma \ref{lem:p_pstar}.
\end{proof}

\noindent If we strengthen Assumption \ref{assump:model} (iii) to Assumption \ref{assump:jointExog}, then a binary instrument suffices to identify $\beta$ under Assumptions ???.
(See KRS, BBS, FL, and Mahajan. Also possibly put a proof in the online-only appendix).
However, without this additional condition $\beta$ is unidentified regardless of the number of values that the instrument takes on, as we now show.\footnote{Footnote mentioning how this relates to Mahajan's mistake. He imposed an extra assumption that contradicts the existence of a first-stage. Direct the reader an appendix explaining the error.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thm}[Sharp Identified Set I]
  \label{thm:sharpI}
  Under Assumptions \ref{assump:model} and \ref{assump:misclassification} (i)--(ii), the sharp identified set is characterized by 
\[
    \mathbb{E}[y|\mathbf{x},z=k] = c(\mathbf{x}) + \beta(\mathbf{x}) \left[\frac{\mathbb{P}(T=1|\mathbf{x},z_k) - \alpha_0(\mathbf{x})}{1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})}\right]
  \] 
and
\[\alpha_0(\mathbf{x}) \leq \mathbb{P}(T=1|\mathbf{x}, z=k) \leq 1 -  \alpha_1(\mathbf{x})\]
for all $k = 0, 1$. 
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In our discussion of the results for the sharp identified set, we can discuss what would happen if there were more than 2 values of $k$.
Our method of proof never actually relies on there being only a binary instrument. 
You're not identified in any case, regardless of how many values $z$ takes on.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Say how we get the following by plugging in the largest and smallest possible values for $\alpha_0 + \alpha_1$ and taking the difference of the expressions for $\mathbb{E}[y|z=k]$

\begin{cor}
  Under the conditions of Theorem \ref{thm:sharpI} the sharp identified set for $\beta(\mathbf{x})$ is the closed interval between $\Delta^y(\mathbf{x})$ and $\Delta^y(\mathbf{x})/\Delta^T(\mathbf{x})$ 
where $\Delta^y(\mathbf{x}) \equiv \mathbb{E}(y|\mathbf{x},z=1) - \mathbb{E}(y|\mathbf{x},z=0)$ and $\Delta^T(\mathbf{x})= \mathbb{E}(T|\mathbf{x},z=1) - \mathbb{E}(T|\mathbf{x},z=0)$.
\label{cor:sharpBeta1}
\end{cor}



Talk about applied settings in which there can be one-sided mis-classification.
This restricts the identified set.
Following corollary collects some common cases.

\begin{cor}
  Under the conditions of Theorem \ref{thm:sharpI}, restrictions on the misclassification probabilities shrink the sharp identified set for $\beta(\mathbf{x})$ to the closed interval between $B\Delta^y(\mathbf{x})/\Delta^T(\mathbf{x})$ and $\Delta^y(\mathbf{x})/\Delta^T(\mathbf{x})$ where
  \begin{enumerate}[(i)]
    \item $\alpha_0(\mathbf{x})=0$ implies $B = \max_k \mathbb{E}(T|\mathbf{x},z=k)$ 
    \item $\alpha_1(\mathbf{x})=0$ implies $B = 1 - \min_k \mathbb{E}(T|\mathbf{x},z=k)$ 
    \item $\alpha_0(\mathbf{x})=\alpha_1(\mathbf{x})$ implies $B =  1 - 2 \min\left\{ \min_k \mathbb{E}(T|\mathbf{x},z=k), 1 - \max_k \mathbb{E}(T|\mathbf{x},z=k) \right\}$ 
  \end{enumerate}
  for $k = 0,1$, where $\Delta^T(\mathbf{x})$ and $\Delta^y(\mathbf{x})$ are as defined in Corollary \ref{cor:sharpBeta1}.
\end{cor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The preceding results don't use the non-differential measurement error assumption 
Further imposing non-differential measurement error -- Assumption \ref{assump:misclassification} (iii) -- shrinks the identified set.
We impose some additional conditions that simplify the proof. 
These are discussed further below.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thm}[Sharp Identified Set II]
  \label{thm:sharpII}
  Suppose that the conditional distribution of $y$ given $(\mathbf{x}, T,z)$ is absolutely continuous with respect to the Lebesgue measure for any values of the conditioning variables and moreover that there is no $k$ such that $\mathbb{E}\left[ y|\mathbf{x},T=0,z=k \right] = \mathbb{E}\left[ y|\mathbf{x},T=1,z=k \right]$.
  Then, under Assumptions \ref{assump:model} and \ref{assump:misclassification}, the sharp identified set is characterized by
  \[
    BLAH BLAH BLAH
  \]
\end{thm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The preceding theorem assumes that $y$ is continuously distributed, which is natural in an additively separable model and holds in our simulations and empirical examples below.
If this is not the case then non-differential measurement error may impose further restrictions, although the basic idea of the argument is similar.
We also imposed $\mathbb{E}[y|\mathbf{x},T = 0, z=k] \neq \mathbb{E}[y|\mathbf{x},T=1, z=k]$ for any $k$, which holds generically.
If the two means are exactly equal for some $k$, then the intersection of $\left\{ (\alpha_0, \alpha_1)\colon \alpha_0 = p_k \mbox{ or } \alpha_1 = 1-p_k \right\}$ with the bounds for $\alpha_0, \alpha_1$ from Theorem \ref{thm:sharpI} must be added to the identified set.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lem}
  Let $\eta_2(\mathbf{x}) = \mbox{Cov}(y^2,z|\mathbf{x})$ and $\tau_1(\mathbf{x}) = \mbox{Cov}(yT,z|\mathbf{x})$.
Then, Under Assumptions \ref{assump:model}, \ref{assump:misclassification} and \ref{assump:2ndMoment}, 
\[
    \eta_2(\mathbf{x}) =  2\tau_1(\mathbf{x}) \theta_1(\mathbf{x}) - \pi(\mathbf{x})\theta_2(\mathbf{x})
\]
where $\pi(\mathbf{x})$ is as defined in Lemma \ref{lem:eta1} and $\theta_1(\mathbf{x})$ and $\theta_2(\mathbf{x})$ are given by Equations \ref{eq:theta1_def}--\ref{eq:theta2_def}.
  \label{lem:eta2}
\end{lem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\todo[inline]{We probably want to have a corollary that establishes identification in the one-sided version of the problem. Related to this, we probably want to show that we are not in general identified from second moments even if the instrument takes on many values. This could be a real pain but on the plus side we would end up deriving the sharp identified set. One slightly confusing point is that we are now making the 2nd moment IV assumption separately from the 2nd moment non-differential measurement error assumption but we need both to get the second moment bounds for $\alpha_0$ and $\alpha_1$.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lem}
  Let $\eta_3(\mathbf{x}) = \mbox{Cov}(y^3,z|\mathbf{x})$ and $\tau_2(\mathbf{x}) = \mbox{Cov}(Ty^2,z|\mathbf{x})$.
  Then, under Assumptions \ref{assump:model}, \ref{assump:misclassification}, \ref{assump:2ndMoment} and \ref{assump:3rdMoment},
\[
  \eta_3(\mathbf{x}) =  3\tau_2(\mathbf{x}) \theta_1(\mathbf{x}) - 3\tau_1(\mathbf{x}) \theta_2(\mathbf{x}) + \pi(\mathbf{x})\theta_3(\mathbf{x})
\]
where $\tau_1(\mathbf{x})$ and $\pi(\mathbf{x})$ are as defined in Lemma \ref{lem:eta2}, and $\theta_1(\mathbf{x}), \theta_2(\mathbf{x}), \theta_3(\mathbf{x})$ are defined in Equations \ref{eq:theta1_def}--\ref{eq:theta3_def}.
  \label{lem:eta3}
\end{lem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{cor}
  \label{cor:theta_ident}
  Suppose that $\pi(\mathbf{x}), \tau_1(\mathbf{x}), \tau_2(\mathbf{x}),  \eta_1(\mathbf{x}), \eta_2(\mathbf{x})$, and $\eta_3(\mathbf{x})$ -- defined in Lemmas ??? -- are identified.
  Then, under the conditions of Lemmas ???, $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$, and $\theta_3(\mathbf{x})$ -- defined in Equations \ref{eq:theta1_def}--\ref{eq:theta3_def} -- are identified if and only if $\pi(\mathbf{x}) \neq 0$.
\end{cor}


\begin{thm}[Identification of $\beta$, $\alpha_0$, $\alpha_1$]
  \label{thm:main_ident}
\end{thm}
