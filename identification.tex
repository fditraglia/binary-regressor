%!TEX root = ./main.tex
\section{Identification Results}
\label{sec:identification}

\subsection{Baseline Assumptions}
\label{sec:baseline}
As defined in the preceding section, our model is $y = c(\mathbf{x}) + \beta(\mathbf{x}) T^* + \varepsilon$, where $\varepsilon$ is a mean-zero error term, and the parameter of interest is $\beta(\mathbf{x})$ -- the effect of an unobserved, binary, endogenous regressor $T^*$.
Suppose we observe a valid and relevant binary instrument $z$.
In the discussion following Corollary \ref{cor:nonident} below, we explain how these results generalize to the case of an arbitrary discrete-valued instrument.
We assume that the model and instrument satisfy the following conditions:
\begin{assump} \mbox{}
  \label{assump:model}
  \begin{enumerate}[(i)] 
    \item $y = c(\mathbf{x}) + \beta(\mathbf{x})T^* + \varepsilon$ where $T^* \in \left\{ 0,1 \right\}$ and $\mathbb{E}[\varepsilon]=0$;
    \item  $z \in \left\{ 0,1 \right\}$, where $0 < \mathbb{P}(z=1|\mathbf{x}) < 1$, and $\mathbb{P}(T^*=1|\mathbf{x},z=1) \neq \mathbb{P}(T^*=1|\mathbf{x},z=0)$;
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z] = 0$.
  \end{enumerate}
\end{assump}

Assumptions \ref{assump:model}(ii) and (iii) are the standard instrument relevance and validity assumptions. 
Note that Assumption \ref{assump:model} (ii) states that $z$ is relevant for the unobserved regressor $T^*$. 
Under Assumption \ref{assump:misclassification}, however, this is equivalent to assuming that $z$ is relevant for the \emph{observed} regressor $T$ (see Lemma \ref{lem:p_pstar} below).
Hence, Assumption \ref{assump:model} (ii) is testable.
If $T^*$ were observed, Assumption \ref{assump:model} would suffice to identify  $\beta(\mathbf{x})$.  
Unfortunately we observe not $T^*$ but a mis-classified binary surrogate $T$.  
Define the following mis-classification probabilities:
\begin{align*}
  \alpha_0(\mathbf{x},z) &= \mathbb{P}\left(T=1|T^*=0,\mathbf{x},z  \right) &
  \alpha_0(\mathbf{x}) &= \mathbb{P}\left(T=1|T^*=0,\mathbf{x}  \right)\\ 
  \alpha_1(\mathbf{x},z) &= \mathbb{P}\left(T=0|T^*=1,\mathbf{x},z  \right) &
  \alpha_1(\mathbf{x}) &= \mathbb{P}\left(T=0|T^*=1,\mathbf{x}  \right).
\end{align*}
Following the existing literature for the case of an exogenous regressor \citep{Mahajan,BBS,FL,KRS,Lewbel}, we impose the following conditions on the mis-classification process.

\begin{assump} \mbox{}
  \label{assump:misclassification}
  \begin{enumerate}[(i)] 
    \item $\alpha_0(\mathbf{x},z) = \alpha_0(\mathbf{x})$,   $\alpha_1(\mathbf{x},z) = \alpha_1(\mathbf{x})$
    \item $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) <1$ 
    \item $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon|\mathbf{x},z, T^*]$
  \end{enumerate}
\end{assump}

Assumption \ref{assump:misclassification} (i) states that the mis-classification probabilities do not depend on $z$.
As we maintain this assumption throughout, we drop the dependence of $\alpha_0$ and $\alpha_1$ on $z$ and write $\alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$.
Assumption \ref{assump:misclassification} (ii) restricts the extent of mis-classification and is equivalent to requiring that $T$ and $T^*$ be positively correlated.
Assumption \ref{assump:misclassification} (iii) is often referred to as ``non-differential measurement error.''
Intuitively, it maintains that $T$ provides no additional information about $\varepsilon$, and hence $y$, given knowledge of $(T^*,z,\mathbf{x})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Point Identification Results from the Literature}
\label{sec:ident_literature}

Existing results from the literature -- see for example \cite{FL} and \cite{Mahajan} -- establish that $\beta(\mathbf{x})$ is point identified if Assumptions \ref{assump:model}--\ref{assump:misclassification} are augmented to include the following condition:
\begin{assump}[Joint Exogeneity] \mbox{}
  \label{assump:jointExog}
    $\mathbb{E}[\varepsilon|\mathbf{x},z, T^*] = 0$.
\end{assump}

Assumption \ref{assump:jointExog} strengthens the mean independence condition from Assumption \ref{assump:model} (iii) to hold \emph{jointly} for $T^*$ and $z$.
By iterated expectations, this implies that $T^*$ is exogenous, i.e.\ $\mathbb{E}[\varepsilon|\mathbf{x},T^*] = 0$.
If $T^*$ is endogenous, Assumption \ref{assump:jointExog} clearly fails.
\cite{Mahajan} argues, however, that the following restriction, along with our Assumptions \ref{assump:model}--\ref{assump:misclassification}, suffices to identify $\beta(\mathbf{x})$ when $T^*$ may be endogenous:
\begin{assump}[\cite{Mahajan} Equation 11] \mbox{}
  \label{assump:mahajan}
  $\mathbb{E}[\varepsilon|\mathbf{x}, z, T^*, T] = \mathbb{E}[\varepsilon|\mathbf{x},T^*]$.
\end{assump}
Assumption \ref{assump:mahajan} does not require $\mathbb{E}[\varepsilon|\mathbf{x},T^*]$ to be zero, but maintains that it does not vary with $z$.
We show in Appendix \ref{sec:mahajan}, however, that under 
Assumptions \ref{assump:model}--\ref{assump:misclassification}, Assumption \ref{assump:mahajan} can only hold if $T^*$ is exogenous.
If $z$ is a valid instrument and $T^*$ is endogenous, then Assumption \ref{assump:mahajan} implies that there is no first-stage relationship between $z$ and $T^*$.
As such, identification in the case where $T^*$ is endogenous is an open question. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Partial Identification}
\label{sec:partial}

In this section we derive the sharp identified set under Assumptions \ref{assump:model}--\ref{assump:misclassification} and show that  $\beta(\mathbf{x})$ is not point identified.
To simplify the notation, define the following shorthand for the unobserved and observed first stage probabilities
\begin{equation}
  p^*_k(\mathbf{x}) = \mathbb{P}(T^*=1|\mathbf{x},z=k), \quad
  p_k(\mathbf{x}) = \mathbb{P}(T=1|\mathbf{x},z=k).
  \label{eq:pk_def}
\end{equation}
We first state two lemmas that that will be used repeatedly below.
\begin{lem}
\label{lem:p_pstar}
  Under Assumption \ref{assump:misclassification} (i),
\begin{align*}
  \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right]p^*_k(\mathbf{x}) &= p_k(\mathbf{x}) - \alpha_0(\mathbf{x})\\
  \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right]\left[1 - p^*_k(\mathbf{x}) \right]&= 1 - p_k(\mathbf{x}) - \alpha_1(\mathbf{x})
\end{align*}
where the first-stage probabilities $p_k^*(\mathbf{x})$ and $p_k(\mathbf{x})$ are as defined in Equation \ref{eq:pk_def}.
\end{lem} 

\begin{lem}
  \label{lem:wald}
  Under Assumptions \ref{assump:model} and \ref{assump:misclassification} (i)--(ii), $$\beta(\mathbf{x}) \mbox{Cov}(z,T|\mathbf{x}) = \left[ 1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right]\mbox{Cov}(y,z|\mathbf{x})$$
\end{lem}

Lemma \ref{lem:p_pstar} relates the observed first-stage probabilities $p_k(\mathbf{x})$ to their unobserved counterparts $p^*_k(\mathbf{x})$ in terms of the mis-classification probabilities $\alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$.
By Assumption \ref{assump:misclassification} (ii), $1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) > 0$ so that Lemma \ref{lem:p_pstar} bounds $\alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$ in terms of the observed first-stage probabilities.
Lemma \ref{lem:wald} relates the instrumental variables (IV) estimand, $\mbox{Cov}(y,z|\mathbf{x})/\mbox{Cov}(z,T|\mathbf{x})$, to the mis-classification probabilities.
Since $1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) > 0$, IV is biased \emph{upwards} in the presence of mis-classification.
Together these lemmas bound the causal effect of interest: $\beta(\mathbf{x})$ lies between the reduced form and IV estimators.
Without Assumption \ref{assump:misclassification} (non-differential measurement error) these bounds are sharp.

\begin{thm}
  Under Assumptions \ref{assump:model} and \ref{assump:misclassification} (i)--(ii), $\alpha_0(\mathbf{x}) \leq p_k(\mathbf{x}) \leq 1 -  \alpha_1(\mathbf{x})$ for  $k = 0, 1$ and
  \begin{equation}
    \mathbb{E}[y|\mathbf{x},z=k] = c(\mathbf{x}) + \beta(\mathbf{x}) \left[\frac{p_k(\mathbf{x}) - \alpha_0(\mathbf{x})}{1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})}\right].
    \label{eq:identsetI}
  \end{equation}
Provided that $p_0(\mathbf{x}) \neq p_1(\mathbf{x})$, these expressions characterize the sharp identified set for $c(\mathbf{x})$, $\beta(\mathbf{x})$, $\alpha_0(\mathbf{x})$, and $\alpha_1(\mathbf{x})$.
%where $p_k(\mathbf{x})$ is defined in Equation \ref{eq:pk_def}.
  \label{thm:sharpI}
\end{thm}


\begin{cor}
  Under the conditions of Theorem \ref{thm:sharpI}, the sharp identified set for $\beta(\mathbf{x})$ is the closed interval between the reduced form estimand $\mbox{Cov}(y,z|\mathbf{x})/\mbox{Var}(z|\mathbf{x})$ and the IV estimand $\mbox{Cov}(y,z|\mathbf{x})/\mbox{Cov}(z,T|\mathbf{x})$.
\label{cor:sharpBeta1}
\end{cor}

Corollary \ref{cor:sharpBeta1} follows by taking differences of the expression for $\mathbb{E}[y|\mathbf{x},z=k]$ across $k=1$ and $k=0$, and substituting the maximum and minimum value for $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x})$ consistent with the observed first-stage probabilities.\footnote{If \emph{a priori} restrictions on $\alpha_0$ and $\alpha_1$ are available, e.g.\ $\alpha_0 = 0$, $\alpha_1 = 0$, or $\alpha_0 = \alpha_1$, these bounds can be improved. For more discussion, see Corollary 2.2 of \cite{DiTragliaGarciaWP2017}.}
While \cite{FL} point out that the IV estimand provides an upper bound for $\beta(\mathbf{x})$ and Lemmas \ref{lem:p_pstar}--\ref{lem:wald} are well-known in the literature \citep[see e.g.][]{FL,Mahajan}, we are unaware of any published result that explicitly states both bounds from Corollary \ref{cor:sharpBeta1} or proves that they are sharp.
%When the mis-classification probabilities are known \emph{a priori} to satisfy additional restrictions, these bounds can be tightened.\footnote{\cite{FL} consider a model in which $\alpha_0$ and $\alpha_1$ do not depend on the exogenous covariates $\mathbf{x}$. In this case $\alpha_0 \leq \mathbb{P}(T=1|\mathbf{x},z)\leq 1 - \alpha_1$ and they suggest minimizing the bounds over $\mathbf{x}$.}
%The following corollary collects results for two common cases: one-sided misclassification (either $\alpha_0(\mathbf{x})$ or $\alpha_1(\mathbf{x})$ equals zero), and symmetric mis-classification ($\alpha_0(\mathbf{x}) = \alpha_1(\mathbf{x})$).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{cor}
%  \label{cor:onsided}
%  Under the conditions of Theorem \ref{thm:sharpI}, the following restrictions on the misclassification probabilities $\alpha_0(\mathbf{x}), \alpha_1(\mathbf{x})$ shrink the sharp identified set for $\beta(\mathbf{x})$ to the closed interval between $\Delta \times \left[\mbox{Cov}(y,z|\mathbf{x})/\mbox{Cov}(z,T|\mathbf{x})\right]$ and $\mbox{Cov}(y,z|\mathbf{x})/\mbox{Cov}(z,T|\mathbf{x})$.
%  \begin{enumerate}[(i)]
%    \item If $\alpha_0(\mathbf{x})=0$ then $\Delta = \max_k \,p_k(\mathbf{x})$. 
%    \item If $\alpha_1(\mathbf{x})=0$ then $\Delta = 1 - \min_k\, p_k(\mathbf{x})$. 
%    \item If $\alpha_0(\mathbf{x})=\alpha_1(\mathbf{x})$ then $\Delta =  1 - 2 \min\left\{ \min_k \, p_k(\mathbf{x}), 1 - \max_k \, p_k(\mathbf{x}) \right\}$.
%  \end{enumerate}
%\end{cor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Theorem \ref{thm:sharpI} and Corollaries \ref{cor:sharpBeta1}--\ref{cor:onsided} do not impose Assumption \ref{assump:misclassification} (iii) -- non-differential measurement error.
Neither Theorem \ref{thm:sharpI} nor Corollary \ref{cor:sharpBeta1}, however, imposes Assumption \ref{assump:misclassification} (iii) -- non-differential measurement error.
We now show that this assumption yields further restrictions on probabilities $\alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$, but fails to point identify $\beta(\mathbf{x})$.
%While these restrictions are more complicated to describe than those from Theorem \ref{thm:sharpI}, they are straightforward to implement in practice and can be extremely informative, as we will show in our simulation exercises below.
The sharp bounds that we derive under Assumption \ref{assump:misclassification} (iii) are new to the literature.

Our result uses two additional conditions to simplify the proof of sharpness.
First, we assume that $y$ is continuously distributed.
This is natural in an additively separable model and holds in our simulation examples below. 
Without this assumption, the bounds that we derive are still valid, but may not be sharp. 
Nevertheless, the reasoning from our proof can be generalized to cases in which $y$ does not have a continuous support set.
We also impose $\mathbb{E}[y|\mathbf{x},T = 0, z=k] \neq \mathbb{E}[y|\mathbf{x},T=1, z=k]$ for any $k$.
This holds generically and is not essential to the proof: it merely simplifies the description of the identified set.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thm}
  \label{thm:sharpII}
  Suppose that the conditional distribution of $y$ given $(\mathbf{x}, T,z)$ is continuous for any values of the conditioning variables and $\mathbb{E}\left[ y|\mathbf{x},T=0,z=k \right] \neq \mathbb{E}\left[ y|\mathbf{x},T=1,z=k \right]$ for all $k$.
  Then, under Assumptions \ref{assump:model} and \ref{assump:misclassification}, the sharp identified set is characterized by Equation \ref{eq:identsetI} from Theorem \ref{thm:sharpI} along with $\alpha_0(\mathbf{x}) < p_k(\mathbf{x}) < 1 -  \alpha_1(\mathbf{x})$ for $k = 0,1$ and
\[
  \underline{\mu}_{tk}\bigg( \underline{q}_{tk}\big( \alpha_0(\mathbf{x}), \alpha_1(\mathbf{x}), \mathbf{x}\big) , \,\mathbf{x} \bigg)\leq 
  \mu_{k}\big( \alpha_0(\mathbf{x}),\mathbf{x} \big)\leq 
  \overline{\mu}_{tk}\bigg(\overline{q}_{tk}\big( \alpha_0(\mathbf{x}), \alpha_1(\mathbf{x}), \mathbf{x}\big), \,\mathbf{x} \bigg)
\]
for all pairs $(t,k)$ where
\begin{align*}
  \underline{\mu}_{tk}\big( q,\mathbf{x} \big) = \mathbb{E}\left[ y\left|\right.y\leq q, \mathbf{x},T=t, z=k\right], \quad \quad
  \overline{\mu}_{tk}\big(q,\mathbf{x} \big) = \mathbb{E}\left[ y\left|\right. y > q, \mathbf{x}, T=t, z=k\right]
\end{align*}
  \begin{align*}
  \mu_k\big(\alpha_0(\mathbf{x}),\mathbf{x}\big) &= 
  \frac{p_k(\mathbf{x}) \mathbb{E}[y|\mathbf{x},z=k,T=1] - \alpha_0(\mathbf{x}) \mathbb{E}[y|\mathbf{x},z=k]}{p_k(\mathbf{x}) - \alpha_0(\mathbf{x})}
\end{align*}
and we define 
\begin{align*}
  \underline{q}_{tk}\big(\alpha_0(\mathbf{x}),\alpha_1(\mathbf{x}),\mathbf{x}\big) &= F^{-1}_{tk}\bigg(r_{tk}\big(\alpha_0(\mathbf{x}),\alpha_1(\mathbf{x}), \mathbf{x}\big)\, \bigg|\,\mathbf{x}\bigg)\\
  \overline{q}_{tk}\big(\alpha_0(\mathbf{x}),\alpha_1(\mathbf{x}),\mathbf{x}\big) &= F^{-1}_{tk}\bigg(1 - r_{tk}\big(\alpha_0(\mathbf{x}), \alpha_1(\mathbf{x}),\mathbf{x}\big) \,\bigg|\,\mathbf{x}\bigg)
\end{align*}
where $F_{tk}^{-1}(\cdot|\mathbf{x})$ is the conditional quantile function of $y$ given $(\mathbf{x},T=t,z=k)$,  
\begin{align*}
  r_{0k}\big(\alpha_0(\mathbf{x}),\alpha_1(\mathbf{x}),\mathbf{x}\big) &= \frac{\alpha_1(\mathbf{x})}{1 - p_k(\mathbf{x})} \left[ \frac{p_k(\mathbf{x}) - \alpha_0(\mathbf{x})}{1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})} \right]\\
  r_{1k}\big(\alpha_0(\mathbf{x}),\alpha_1(\mathbf{x}),\mathbf{x}\big) &= \frac{1 - \alpha_1(\mathbf{x})}{p_k(\mathbf{x})} \left[ \frac{p_k(\mathbf{x}) - \alpha_0(\mathbf{x})}{1 - \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})} \right]
\end{align*}
and $p_k(\mathbf{x})$ is defined in Equation \ref{eq:pk_def}.
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The intuition for Theorem \ref{thm:sharpII} is as follows.
For simplicity, suppress dependence on $\mathbf{x}$.
Now, fix $(T=t, z=k)$ and $(\alpha_0, \alpha_1)$.
The observed distribution of $y$ given $(T=t,z=k)$, call it $F_{tk}$, is a mixture of two unobserved distributions: the distribution of $y$ given $(T=k,z=k,T^*=1)$, call it $F^1_{tk}$, and the distribution of $y$ given $(T=t,z=k,T^*=0)$, call it $F^{0}_{tk}$.
The mixing probabilities are $r_{tk}$ and $1-r_{tk}$ from the statement of Theorem \ref{thm:sharpII} and are fully determined by $(\alpha_0, \alpha_1)$ and $p_k$.
Assumptions \ref{assump:model} (i) and \ref{assump:misclassification} (ii) imply that the unobserved means $\mathbb{E}[y|T^*,T,z]$  are fully determined by $(\alpha_0, \alpha_1)$ given the observed means $\mathbb{E}[y|T,z]$.
The question is whether it is possible, given the observed distribution $F_{tk}$, to construct $F^1_{tk}$ and $F^{0}_{tk}$ with the required values for $\mathbb{E}[y|T^*,T,z]$ such that $F_{tk} = r_{tk} F^{1}_{tk} + (1 - r_{tk}) F^{0}_{tk}$ for all combinations $(t,k)$. 
If not, then $(\alpha_0, \alpha_1)$ does not belong to the identified set.
Our proof provides necessary and sufficient conditions for such a mixture to exist at a given point $(\alpha_0, \alpha_1)$.
We can then appeal to the reasoning from Theorem \ref{thm:sharpI} to complete the argument.
By ruling out values for $\alpha_0$ and $\alpha_1$, Theorem \ref{thm:sharpII} restricts $\beta$ via Lemma \ref{lem:wald}. 
While these restrictions can be very informative in practice, they do not yield point identification.

\begin{cor}
  Under Assumptions \ref{assump:model} and \ref{assump:misclassification} the identified set for $\beta(\mathbf{x})$ contains both the IV estimand $\mbox{Cov}(y,z|\mathbf{x})/\mbox{Cov}(z,T|\mathbf{x})$ and the true coefficient $\beta(\mathbf{x})$.
  \label{cor:nonident}
\end{cor}

Corollary \ref{cor:nonident} follows by Lemma \ref{lem:wald} because $\alpha_0(\mathbf{x})=\alpha_1(\mathbf{x})=0$ always belongs to the sharp identified set from Theorem \ref{thm:sharpII}.
Non-differential measurement error cannot exclude the possibility that there is no mis-classification because in this case it is trivial to construct the required mixtures.
Although we focus throughout this paper on the case of a binary instrument, one might wonder whether point identification can be achieved by increasing the support of $z$, perhaps along the lines of \cite{Lewbel}.
The answer turns out to be no.
Suppose that we were to modify Assumptions \ref{assump:model} and \ref{assump:misclassification} to hold for all values of $z$ in some discrete support set.
By Lemma \ref{lem:wald}, a binary instrument identifies $\beta(\mathbf{x})$ up to knowledge of the mis-classification probabilities $\alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$.
It follows that \emph{any} pair of values $(k,\ell)$ in the support set of $z$ identifies the same object.
Accordingly, to identify $\beta(\mathbf{x})$ it is necessary and sufficient to identify the mis-classification probabilities.
A binary instrument fails to identify these probabilities because we can never exclude the possibility of zero mis-classification.
The same is true of a discrete $K$-valued instrument. 
Increasing the support of $z$ does, however, shrink the identified set by increasing the number of restrictions available: in this case Theorems \ref{thm:sharpI}--\ref{thm:sharpII} continue to apply replacing ``$k=0,1$'' with ``for all $k$.''



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Point Identification}
\label{sec:point}
The results of the preceding section establish that $\beta(\mathbf{x})$ is not point identified under Assumptions \ref{assump:model} and \ref{assump:misclassification}.
In light of this, there are two possible ways to proceed: either one can report partial identification bounds based on our characterization of the sharp identified set from Theorem \ref{thm:sharpII}, or one can attempt to impose stronger assumptions to obtain point identification.
In this section we consider the second possibility.
We begin by defining the following functions of the model parameters: 
\begin{align}
  \label{eq:theta1_def}
  \theta_1(\mathbf{x}) &= \beta(\mathbf{x})\left[ 1 -  \alpha_0(\mathbf{x}) - \mathbf{\alpha}_1(\mathbf{x}) \right]^{-1}\\
  \label{eq:theta2_def}
  \theta_2(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^2 \left[ 1 + \alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x})\right] \\
  \label{eq:theta3_def}
  \theta_3(\mathbf{x}) &= \left[\theta_1(\mathbf{x})\right]^3\left[ \left\{ 1 -\alpha_0(\mathbf{x}) - \alpha_1(\mathbf{x}) \right\}^2 + 6\alpha_0(\mathbf{x})\left\{ 1 - \alpha_1(\mathbf{x}) \right\} \right]
\end{align}
Now consider the following additional assumption:
\begin{assump} \mbox{}
  \label{assump:2ndMoment}
    $\mathbb{E}[\varepsilon^2|\mathbf{x},z] = \mathbb{E}[\varepsilon^2|\mathbf{x}]$ 
\end{assump}
Assumption \ref{assump:2ndMoment} is a \emph{second moment} version of the standard mean exclusion restriction for the instrument $z$ -- Assumption \ref{assump:model} (iii).
It requires that the conditional variance of the error term given the covariates $\mathbf{x}$ does not depend on $z$, but does \emph{not} require homoskedasticity with respect to $\mathbf{x}, T^*$ or $T$.
Assumption \ref{assump:2ndMoment} allows us to derive the following lemma:

\begin{lem} Under Assumptions \ref{assump:model}, \ref{assump:misclassification} and \ref{assump:2ndMoment}, 
\[
  \mbox{Cov}(y^2,z|\mathbf{x}) = 2\mbox{Cov}(yT,z|\mathbf{x}) \theta_1(\mathbf{x}) -\mbox{Cov}(T,z|\mathbf{x})\theta_2(\mathbf{x}) 
\]
  where $\theta_1(\mathbf{x})$ and $\theta_2(\mathbf{x})$ are defined in Equations \ref{eq:theta1_def}--\ref{eq:theta2_def}.
  \label{lem:eta2}
\end{lem}

Lemma \ref{lem:wald} identifies $\theta_1(\mathbf{x})$.
Since $\mbox{Cov}(z,T|\mathbf{x}) \neq 0$ by Assumption \ref{assump:model} (ii), we can solve for $\theta_2(\mathbf{x})$ in terms of observables only, using Lemma \ref{lem:eta2}.
Given knowledge of $\theta_1(\mathbf{x})$, we can solve Equation \ref{eq:theta2_def} for the difference of mis-classification rates so long as $\beta(\mathbf{x}) \neq 0$.

\begin{cor}
  Under Assumptions \ref{assump:model}--\ref{assump:misclassification} and \ref{assump:2ndMoment}, $\alpha_1(\mathbf{x}) - \alpha_0(\mathbf{x})$ is identified so long as $\beta(\mathbf{x}) \neq 0$.
  \label{cor:alpha_diff}
\end{cor}
Corollary \ref{cor:alpha_diff} identifies the difference of mis-classification error rates.
Hence, under one-sided mis-classification, $\alpha_0(\mathbf{x}) = 0$ or $\alpha_1(\mathbf{x}) = 0$, augmenting our baseline Assumptions \ref{assump:model}--\ref{assump:misclassification} with Assumption \ref{assump:2ndMoment} suffices to identify $\beta(\mathbf{x})$.
Notice that $\beta(\mathbf{x})=0$ if and only if $\theta_1(\mathbf{x}) = 0$.
Thus, $\beta(\mathbf{x})$ is still identified in the case where Corollary \ref{cor:alpha_diff} fails to apply.

Assumption \ref{assump:2ndMoment} does not suffice to identify $\beta(\mathbf{x})$ without \emph{a priori} restrictions on the mis-classification error rates.
To achieve identification in the general case, we impose the following additional conditions:
\begin{assump} \mbox{}
  \label{assump:3rdMoment}
  \begin{enumerate}[(i)] 
    \item $\mathbb{E}[\varepsilon^2|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon^2|\mathbf{x},z, T^*]$
    \item $\mathbb{E}[\varepsilon^3|\mathbf{x},z] = \mathbb{E}[\varepsilon^3|\mathbf{x}]$
  \end{enumerate}
\end{assump}

Assumption \ref{assump:3rdMoment} (i) is a second moment version of the non-differential measurement error assumption, Assumption \ref{assump:misclassification} (iii).
It requires that, given knowledge of $(\mathbf{x}, T^*,z)$, $T$ provides no additional information about the variance of the error term.
Note that Assumption \ref{assump:3rdMoment} (i) does not require homoskedasticity of $\varepsilon$ with respect to $\mathbf{x}$ or $T^*$.
Assumption \ref{assump:3rdMoment} (ii) is a third moment version of Assumption \ref{assump:2ndMoment}.
It requires that the conditional third moment of the error term given $\mathbf{x}$ does not depend on $z$.
This condition neither requires nor excludes skewness in the error term conditional on covariates: it merely states that the skewness is unaffected by the instrument.

While Assumptions \ref{assump:2ndMoment} and \ref{assump:3rdMoment} may appear unfamiliar, we consider them to be fairly natural in the context of an additively separable model in which one has already assumed that $\mathbb{E}[\varepsilon|\mathbf{x},z]=0$ and $\mathbb{E}[\varepsilon|\mathbf{x},z,T^*,T] = \mathbb{E}[\varepsilon|\mathbf{x},z,T^*]$ -- Assumptions \ref{assump:model} (iii) and \ref{assump:misclassification} (iii) from above.\footnote{If one wishes to weaken our Assumption \ref{assump:model} (i) to allow for some form of unobserved heterogeneity, our higher moment assumptions may impose additional restrictions.}
For example, if an applied researcher reports results both for an outcome in logs and levels, she has implicitly assumed \emph{independence} rather than first moment exclusion. 
Assumptions \ref{assump:model} (iii), \ref{assump:2ndMoment} and \ref{assump:3rdMoment} (ii) are of course implied by $\varepsilon \perp z |\mathbf{x}$ while Assumptions \ref{assump:misclassification} (iii) and \ref{assump:3rdMoment} (i) are implied by $\varepsilon\perp T | (\mathbf{x}, T^*, z)$.
Achieving identification via Assumptions \ref{assump:2ndMoment}--\ref{assump:3rdMoment} involves using information beyond first moments and as such does places higher demands on the data.
Assumption \ref{assump:3rdMoment} allows us to derive the following Lemma which, combined with Lemma \ref{lem:eta2}, leads to point identification: 

\begin{lem}
  Under Assumptions \ref{assump:model}--\ref{assump:misclassification} and \ref{assump:2ndMoment}--\ref{assump:3rdMoment}, 
\[
    \mbox{Cov}(y^3,z|\mathbf{x}) = 3 \mbox{Cov}(y^2T,z|\mathbf{x}) \theta_1(\mathbf{x}) -3\mbox{Cov}(yT,z|\mathbf{x}) \theta_2(\mathbf{x}) + \mbox{Cov}(T,z|\mathbf{x}) \theta_3(\mathbf{x})
\]
where $\theta_1(\mathbf{x}),\theta_2(\mathbf{x})$ and $\theta_3(\mathbf{x})$ are defined in Equations \ref{eq:theta1_def}--\ref{eq:theta2_def}.
\label{lem:eta3}
\end{lem}

\begin{thm}
  Under Assumptions \ref{assump:model}--\ref{assump:misclassification} and \ref{assump:2ndMoment}--\ref{assump:3rdMoment}, $\beta(\mathbf{x})$ is identified.
  If $\mathbf{\beta}(\mathbf{x}) \neq 0$, then $\alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$ are likewise identified.
  \label{thm:main_ident}
\end{thm}

Lemmas \ref{lem:wald}--\ref{lem:eta3} yield a linear system of three equations in $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$ and $\theta_3(\mathbf{x})$.
Under Assumption \ref{assump:model} (ii), the system has a unique solution so $\theta_1(\mathbf{x}), \theta_2(\mathbf{x})$ and $\theta_3(\mathbf{x})$ are identified.
The proof of Theorem \ref{thm:main_ident} shows that, so long as $\beta(\mathbf{x})\neq 0$, Equations \ref{eq:theta1_def}--\ref{eq:theta3_def} can be solved for $\beta(\mathbf{x})$, $\alpha_0(\mathbf{x})$ and $\alpha_1(\mathbf{x})$.
In particular, using steps from the proof of Theorem \ref{thm:main_ident}
\[
  \beta(\mathbf{x}) = \mbox{sign}\big[ \theta_1(\mathbf{x}) \big] \sqrt{3 \big[ \theta_2(\mathbf{x})/\theta_1(\mathbf{x}) \big]^2 - 2\big[\theta_3(\mathbf{x})/\theta_1(\mathbf{x})\big]}.
\]
If we relax Assumption \ref{assump:misclassification} (ii) and assume $\alpha_0(\mathbf{x}) + \alpha_1(\mathbf{x}) \neq 1$ only, $\beta(\mathbf{x})$ is only identified up to sign: in this case the sign of $\theta_1(\mathbf{x})$ need not equal that of $\beta(\mathbf{x})$.

